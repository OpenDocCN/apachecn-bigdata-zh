# 转换和动作

转换和动作是 Apache Spark 程序的主要构件。在本章中，我们将查看 Spark 转换以推迟计算，然后查看应该避免哪些转换。然后我们将使用`reduce`和`reduceByKey`方法从数据集进行计算。然后，我们将执行触发图形实际计算的操作。到本章结束时，我们也将学会如何对不同的动作重用相同的`rdd`。

在本章中，我们将涵盖以下主题:

*   使用火花变换将计算推迟到以后
*   避免转变
*   使用`reduce`和`reduceByKey`方法计算结果
*   执行触发我们的**有向无环图** ( **DAG** )的实际计算的动作
*   对不同的动作重复使用相同的`rdd`

# 使用火花变换将计算推迟到以后

我们先来了解一下 Spark DAG 创作。我们将通过发布操作来执行 DAG，并将启动作业的决定推迟到最后一刻，以检查这种可能性给我们带来了什么。

让我们看一下我们将在本节中使用的代码。

首先，我们需要初始化 Spark。我们进行的每一次测试都是一样的。我们需要在开始使用它之前对它进行初始化，如下例所示:

```py
class DeferComputations extends FunSuite {
val spark: SparkContext = SparkSession.builder().master("local[2]").getOrCreate().sparkContext
```

然后，我们将进行实际测试。这里`test`叫做`should defer computation`。它很简单，但是展示了一个非常强大的 Spark 抽象。我们从创建`InputRecord`的`rdd`开始，如下例所示:

```py
test("should defer computations") {
 //given
    val input = spark.makeRDD(
        List(InputRecord(userId = "A"),
            InputRecord(userId = "B")))
```

`InputRecord`是一个有唯一标识符的 case 类，是一个可选的参数。

如果我们不提供它和所需的参数，它可以是随机的`uuid`，即`userId`。`InputRecord`将在本书中用于测试目的。我们已经创建了`InputRecord`的两个记录，我们将对其应用转换，如下例所示:

```py
//when apply transformation
val rdd = input
    .filter(_.userId.contains("A"))
    .keyBy(_.userId)
.map(_._2.userId.toLowerCase)
//.... built processing graph lazy
```

我们将只过滤`userId`字段中有`A`的记录。然后我们将其转换为`keyBy(_.userId)`，然后从值中提取`userId`，并将其映射到`toLowerCase`。这是我们的`rdd`。所以，在这里，我们只创建了 DAG，我们还没有执行它。让我们假设我们有一个复杂的程序，在实际逻辑之前，我们正在创建许多这样的非循环图。

Spark 的优点是在动作发出之前不会执行，但是我们可以有一些条件逻辑。例如，我们可以获得快速路径执行。假设我们有`shouldExecutePartOfCode()`，可以检查一个配置开关，或者去休息服务计算`rdd`计算是否仍然相关，如下例所示:

```py
if (shouldExecutePartOfCode()) {
     //rdd.saveAsTextFile("") ||
     rdd.collect().toList
  } else {
    //condition changed - don't need to evaluate DAG
 }
}
```

我们使用简单的方法进行测试，我们只是返回`true`进行测试，但是，在现实生活中，这可能是复杂的逻辑:

```py
private def shouldExecutePartOfCode(): Boolean = {
    //domain logic that decide if we still need to calculate
    true
    }
}
```

在它返回`true`之后，我们可以决定是否要执行 DAG。如果我们愿意，我们可以调用`rdd.collect().toList`或`saveAsTextFile`来执行`rdd`。否则，我们可以走捷径，决定不再对输入`rdd`感兴趣。通过这样做，将只创建图形。

当我们开始测试时，需要一些时间来完成并返回以下输出:

```py
"C:\Program Files\Java\jdk-12\bin\java.exe" "-javaagent:C:\Program Files\JetBrains\IntelliJ IDEA 2018.3.5\lib\idea_rt.jar=50627:C:\Program Files\JetBrains\IntelliJ IDEA 2018.3.5\bin" -Dfile.encoding=UTF-8 -classpath C:\Users\Sneha\IdeaProjects\Chapter07\out\production\Chapter07 com.company.Main

Process finished with exit code 0
```

我们可以看到我们的测试通过了，我们可以得出结论，它如预期的那样工作。现在，让我们看看应该避免的一些转换。

# 避免转变

在本节中，我们将研究应该避免的转换。在这里，我们将关注一个特殊的转变。

我们将从了解`groupBy` API 开始。然后，我们将研究使用`groupBy`时的数据分区，然后我们将看看什么是不对称分区，以及为什么要避免不对称分区。

这里，我们正在创建一个事务列表。`UserTransaction`是另一个包含`userId`和`amount`的模型类。下面的代码块显示了一个典型的事务，其中我们创建了一个包含五个事务的列表:

```py
test("should trigger computations using actions") {
 //given
 val input = spark.makeRDD(
     List(
         UserTransaction(userId = "A", amount = 1001),
         UserTransaction(userId = "A", amount = 100),
         UserTransaction(userId = "A", amount = 102),
         UserTransaction(userId = "A", amount = 1),
         UserTransaction(userId = "B", amount = 13)))
```

我们为`userId = "A"`创建了四个交易，为`userId = "B"`创建了一个交易。

现在，让我们考虑我们想要为一个特定的`userId`合并事务以得到事务列表。我们有一个`userId`分组的`input`，如下例所示:

```py
//when apply transformation
val rdd = input
    .groupBy(_.userId)
    .map(x => (x._1,x._2.toList))
    .collect()
    .toList
```

对于每个`x`元素，我们将创建一个元组。元组的第一个元素是一个标识，而第二个元素是该特定标识的每个事务的迭代器。我们将使用`toList`将其转换为列表。然后，我们将收集所有信息并将其分配给`toList`以获得我们的结果。让我们断言结果。`rdd`应该包含与`B`相同的元素，即密钥和一个事务，以及`A`，它有四个事务，如下代码所示:

```py
//then
rdd should contain theSameElementsAs List(
    ("B", List(UserTransaction("B", 13))),
    ("A", List(
        UserTransaction("A", 1001),
        UserTransaction("A", 100),
        UserTransaction("A", 102),
        UserTransaction("A", 1))
    )
  )
 }
}
```

让我们开始这个测试，并检查它是否如预期的那样运行。我们得到以下输出:

```py
"C:\Program Files\Java\jdk-12\bin\java.exe" "-javaagent:C:\Program Files\JetBrains\IntelliJ IDEA 2018.3.5\lib\idea_rt.jar=50822:C:\Program Files\JetBrains\IntelliJ IDEA 2018.3.5\bin" -Dfile.encoding=UTF-8 -classpath C:\Users\Sneha\IdeaProjects\Chapter07\out\production\Chapter07 com.company.Main

Process finished with exit code 0
```

乍一看，它已经过去了，工作正常。但是问题来了，为什么我们要把它分组。我们希望对其进行分组，以将其保存到文件系统中，或者执行一些进一步的操作，例如连接所有的数量。

我们可以看到我们的输入不是正态分布，因为几乎所有的交易都是为了`userId = "A"`。正因为如此，我们有一把倾斜的钥匙。这意味着一个键拥有其中的大部分数据，而其他键的数据量较少。当我们在 Spark 中使用`groupBy`时，它会获取具有相同分组的所有元素，在本例中为`userId`，并将这些值发送给完全相同的执行器。

例如，如果我们的执行器有 5 GB 的内存，并且我们有一个非常大的数据集，有数百 GB 的内存，一个键有 90%的数据，这意味着所有的数据都将流向一个执行器，其余的执行器将获取一小部分数据。因此，数据不会呈正态分布，而且由于分布不均匀，处理效率也不会尽可能高。

所以，我们在使用`groupBy`键的时候，首先要回答为什么要分组的问题。也许我们可以在`groupBy`之前过滤或者在更低的层次上聚合，然后我们只对结果进行分组，也可能我们根本不分组。在接下来的章节中，我们将研究如何使用 Spark API 解决这个问题。

# 使用 reduce 和 reduceByKey 方法计算结果

在本节中，我们将使用`reduce`和`reduceBykey`函数来计算我们的结果并理解`reduce`的行为。然后，我们将比较`reduce`和`reduceBykey`功能，以检查哪些功能应该在特定用例中使用。

我们将首先关注`reduce` API。首先，我们需要创建`UserTransaction`的输入。我们有金额为`10`的用户交易`A`，金额为`1`的用户交易`B`，金额为`101`的用户交易`A`。假设我们想找出全局最大值。我们对特定键的数据不感兴趣，而是对全局数据感兴趣。我们想要扫描它，取最大值，然后返回，如下例所示:

```py
test("should use reduce API") {
    //given
    val input = spark.makeRDD(List(
    UserTransaction("A", 10),
    UserTransaction("B", 1),
    UserTransaction("A", 101)
    ))
```

所以，这是简化的用例。现在，让我们看看如何实现它，如下例所示:

```py
//when
val result = input
    .map(_.amount)
    .reduce((a, b) => if (a > b) a else b)

//then
assert(result == 101)
}
```

对于`input`，我们需要首先绘制我们感兴趣的领域。在这种情况下，我们对`amount`感兴趣。我们取`amount`然后取最大值。

在前面的代码示例中，`reduce`有两个参数，`a`和`b`。一个参数将是我们正在传递的特定 Lambda 中的当前最大值，第二个参数将是我们的实际值，我们现在正在研究它。如果该值一直高于最大状态，我们将返回`a`；如果没有，它将返回`b`。我们将检查所有的元素，最后，结果将只是一个长数字。

所以，让我们测试一下这个，看看结果是否确实是`101`，如下图代码输出所示。这意味着我们的测试通过了:

```py
"C:\Program Files\Java\jdk-12\bin\java.exe" "-javaagent:C:\Program Files\JetBrains\IntelliJ IDEA 2018.3.5\lib\idea_rt.jar=50894:C:\Program Files\JetBrains\IntelliJ IDEA 2018.3.5\bin" -Dfile.encoding=UTF-8 -classpath C:\Users\Sneha\IdeaProjects\Chapter07\out\production\Chapter07 com.company.Main

Process finished with exit code 0
```

现在，让我们考虑一个不同的情况。我们想找到最大交易金额，但这次我们想根据用户来做。我们不仅要找出用户`A`的最大交易量，还要找出用户`B`的最大交易量，但是我们希望那些东西是独立的。因此，对于每个相同的键，我们只想从数据中获取最大值，如下例所示:

```py
test("should use reduceByKey API") {
    //given
    val input = spark.makeRDD(
    List(
        UserTransaction("A", 10),
        UserTransaction("B", 1),
        UserTransaction("A", 101)
    )
)
```

要做到这一点，`reduce`不是一个好的选择，因为它会遍历所有的值，给我们全局最大值。我们在 Spark 中有一些关键的操作，但是首先，我们希望针对一组特定的元素进行操作。我们需要使用`keyBy`来告诉 Spark 哪个 ID 应该作为唯一的 ID，它将只在特定的键内执行`reduce`功能。所以，我们使用`keyBy(_.userId)`然后得到`reducedByKey`函数。`reduceByKey`函数与`reduce`相似，但它是按键工作的，因此在 Lambda 内部，我们将只获取特定键的值，如下例所示:

```py
    //when
    val result = input
      .keyBy(_.userId)
      .reduceByKey((firstTransaction, secondTransaction) =>
        TransactionChecker.higherTransactionAmount(firstTransaction, secondTransaction))
      .collect()
      .toList
```

通过这样做，我们得到了第一笔交易，然后是第二笔交易。第一个是当前最大值，第二个是我们正在调查的交易。我们将创建一个获取这些事务的助手函数，并将其称为`higherTransactionAmount`。

`higherTransactionAmount`功能用于取`firstTransaction`和`secondTransaction`。请注意，对于`UserTransaction`类型，我们需要通过该类型。它还需要返回`UserTransaction`我们不能返回不同的类型。

如果您使用的是 Spark 中的`reduceByKey`方法，我们需要返回与`input`参数相同的类型。如果`firstTransaction.amount`高于`secondTransaction.amount`，我们只退`firstTransaction`，因为我们是退`secondTransaction`，所以交易对象不是总金额。这在以下示例中显示:

```py
object TransactionChecker {
    def higherTransactionAmount(firstTransaction: UserTransaction, secondTransaction: UserTransaction): UserTransaction = {
        if (firstTransaction.amount > secondTransaction.amount) firstTransaction else     secondTransaction
    }
}
```

现在，我们将收集、添加和测试事务。在我们的测试之后，我们有了输出，其中对于键`B`，我们应该得到事务`("B", 1)`，对于键`A`，得到事务`("A", 101)`。不会有交易`("A", 10)`因为我们过滤掉了，但是我们可以看到对于每一个键，我们都能够找到最大值。这在以下示例中显示:

```py
    //then
    result should contain theSameElementsAs
      List(("B", UserTransaction("B", 1)), ("A", UserTransaction("A", 101)))
  }

}
```

我们可以看到测试通过了，一切都如预期的那样，如以下输出所示:

```py
"C:\Program Files\Java\jdk-12\bin\java.exe" "-javaagent:C:\Program Files\JetBrains\IntelliJ IDEA 2018.3.5\lib\idea_rt.jar=50909:C:\Program Files\JetBrains\IntelliJ IDEA 2018.3.5\bin" -Dfile.encoding=UTF-8 -classpath C:\Users\Sneha\IdeaProjects\Chapter07\out\production\Chapter07 com.company.Main

Process finished with exit code 0
```

在下一节中，我们将执行触发数据计算的操作。

# 执行触发计算的操作

Spark 有很多发布 DAG 的动作，我们应该意识到所有这些动作，因为它们非常重要。在本节中，我们将了解什么是 Spark 中的操作，对操作进行演练，并测试这些操作是否按预期运行。

我们介绍的第一个动作是`collect`。除此之外，我们还介绍了两个动作——我们在上一节中介绍了`reduce`和`reduceByKey`。这两种方法都是操作，因为它们只返回一个结果。

首先，我们将创建事务的`input`，然后应用一些转换来进行测试。我们只取包含`A`的用户，使用`keyBy_.userId`，然后只取需要交易的金额，如下例所示:

```py
test("should trigger computations using actions") {
     //given
     val input = spark.makeRDD(
     List(
         UserTransaction(userId = "A", amount = 1001),
         UserTransaction(userId = "A", amount = 100),
         UserTransaction(userId = "A", amount = 102),
         UserTransaction(userId = "A", amount = 1),
         UserTransaction(userId = "B", amount = 13)))

//when apply transformation
 val rdd = input
     .filter(_.userId.contains("A"))
     .keyBy(_.userId)
     .map(_._2.amount)
```

我们已经意识到的第一个动作是`rdd.collect().toList`。下一个是`count()`，需要取所有的值，计算`rdd`里面有多少值。没有触发转换就没有办法执行`count()`。另外，Spark 中有不同的方法，如`countApprox`、`countApproxDistinct`、`countByValue`、`countByValueApprox`。下面的例子向我们展示了`rdd.collect().toList`的代码:

```py
//then
 println(rdd.collect().toList)
 println(rdd.count()) //and all count*
```

如果我们有一个巨大的数据集，近似计数器就足够了，你可以使用`countApprox`，因为它会快得多。然后我们使用`rdd.first()`，但是这个选项有点不同，因为它只需要取第一个元素。有时，如果您想要获取第一个元素并执行我们的 DAG 中的所有内容，我们需要专注于此，并通过以下方式进行检查:

```py
println(rdd.first())
```

此外，在`rdd`上，我们有`foreach()`，这是一个 for 循环，我们可以将任何函数传递给它。Scala 函数或 Java 函数被假定为 Lambda，但是为了执行我们的结果`rdd`的元素，需要计算 DAG，因为从这里开始，它是一个动作。`foreach()`方法的另一个变体是`foreachPartition()`，它获取每个分区并返回该分区的迭代器。在里面，我们有一个迭代器来再次进行迭代，然后打印我们的元素。我们还有`max()`和`min()`方法，不出所料，`max()`取最大值，`min()`取最小值。但是这些方法采用的是隐式排序。

如果我们有一个简单原始类型的`rdd`，比如`Long`，我们不需要在这里传递它。但是如果我们不使用`map()`，我们需要为火花定义`UserTransaction`的顺序，以找出哪个元素是`max`，哪个元素是`min`。这两件事需要执行 DAG，因此它们被归类为操作，如下例所示:

```py
 rdd.foreach(println(_))
 rdd.foreachPartition(t => t.foreach(println(_)))
 println(rdd.max())
 println(rdd.min())
```

然后我们有`takeOrdered()`，这是一个比`first()`更耗时的操作，因为`first()`取一个随机元素。`takeOrdered()`需要执行 DAG 并对所有内容进行排序。当所有的东西都被排序后，它才会取最上面的元素。

在我们的例子中，我们采用`num = 1`。但有时，出于测试或监控的目的，我们只需要采集数据样本。为了获取样本，我们使用`takeSample()`方法并传递一些元素，如下面的代码所示:

```py
 println(rdd.takeOrdered(1).toList)
 println(rdd.takeSample(false, 2).toList)
 }
}
```

现在，让我们开始测试，看看实现前面操作的输出，如下图所示:

```py
List(1001, 100, 102 ,1)
4
1001
1001
100
102
1
```

第一个操作返回所有值。第二个动作返回`4`作为计数。我们将考虑第一个元素`1001`，但这是一个随机值，它不是有序的。然后，我们将打印循环中的所有元素，如以下输出所示:

```py
102
1
1001
1
List(1)
List(100, 1)
```

然后我们得到类似于`first()`的`1001`和`1`的`max`和`min`值。之后我们得到一个有序列表，`List(1)`，样本`List(100, 1` `)`，随机。因此，在示例中，我们从输入数据和应用的转换中获得随机值。

在下一节中，我们将学习如何为不同的动作重用`rdd`。

# 为不同的操作重用相同的 rdd

在本节中，我们将对不同的动作重用相同的`rdd`。首先，我们将通过重用`rdd`来最小化执行时间。然后，我们将查看缓存和代码的性能测试。

下面的例子是前一部分的测试，但是有一点修改，因为这里我们用`currentTimeMillis()`和`result`来表示`start`。所以，我们只是在测量所有执行的动作的`result`:

```py
//then every call to action means that we are going up to the RDD chain
//if we are loading data from external file-system (I.E.: HDFS), every action means
//that we need to load it from FS.
    val start = System.currentTimeMillis()
    println(rdd.collect().toList)
    println(rdd.count())
    println(rdd.first())
    rdd.foreach(println(_))
    rdd.foreachPartition(t => t.foreach(println(_)))
    println(rdd.max())
    println(rdd.min())
    println(rdd.takeOrdered(1).toList)
    println(rdd.takeSample(false, 2).toList)
    val result = System.currentTimeMillis() - start

    println(s"time taken (no-cache): $result")

}
```

如果有人不太了解 Spark，他们会假设所有的动作都被巧妙地执行了。我们知道，每一个动作计数都意味着我们将上升到链中的`rdd`，这意味着我们将进行所有的转换来加载数据。在生产系统中，加载数据将来自外部 PI 系统，如 HDFS。这意味着每个操作都会导致对文件系统的调用，文件系统检索所有数据，然后应用转换，如下例所示:

```py
//when apply transformation
val rdd = input
    .filter(_.userId.contains("A"))
    .keyBy(_.userId)
    .map(_._2.amount)
```

这是一个非常昂贵的操作，因为每个动作都非常昂贵。当我们开始这个测试时，我们可以看到没有缓存所花费的时间将花费`632`毫秒，如下输出所示:

```py
List(1)
List(100, 1)
time taken (no-cache): 632
Process finished with exit code 0
```

让我们将这与缓存的使用进行比较。乍一看，我们的测试看起来非常相似，但是这不一样，因为您发出`cache()`而我们返回`rdd`。因此，`rdd`将已经被缓存，并且对`rdd`的每次后续调用都将经过`cache`，如下例所示:

```py
//when apply transformation
val rdd = input
    .filter(_.userId.contains("A"))
    .keyBy(_.userId)
    .map(_._2.amount)
    .cache()
```

第一个操作将执行 DAG，将数据保存到我们的缓存中，然后后续的操作将根据从内存中调用的方法检索特定的东西。不会有 HDFS 查找，所以让我们开始这个测试，按照下面的例子，看看需要多长时间:

```py
//then every call to action means that we are going up to the RDD chain
//if we are loading data from external file-system (I.E.: HDFS), every action means
//that we need to load it from FS.
    val start = System.currentTimeMillis()
    println(rdd.collect().toList)
    println(rdd.count())
    println(rdd.first())
    rdd.foreach(println(_))
    rdd.foreachPartition(t => t.foreach(println(_)))
    println(rdd.max())
    println(rdd.min())
    println(rdd.takeOrdered(1).toList)
    println(rdd.takeSample(false, 2).toList)
    val result = System.currentTimeMillis() - start

    println(s"time taken(cache): $result")

    }
}
```

第一个输出如下:

```py
List(1)
List(100, 102)
time taken (no-cache): 585
List(1001, 100, 102, 1)
4
```

第二个输出如下:

```py
1
List(1)
List(102, 1)
time taken(cache): 336
Process finished with exit code 0
```

没有缓存时，值为`585`毫秒，有缓存时，值为`336`。区别不大，因为我们只是在测试中创建数据。然而，在实际的生产系统中，这将是一个很大的区别，因为我们需要从外部文件系统中查找数据。

# 摘要

所以，让我们总结这一章。首先，我们使用 Spark 变换将计算推迟到以后，然后我们了解应该避免哪些变换。接下来，我们看一下如何使用`reduceByKey`和`reduce`来计算全局和每个特定键的结果。之后，我们执行触发计算的动作，然后了解到每个动作都意味着对加载数据的调用。为了缓解这个问题，我们学习了如何针对不同的动作减少相同的`rdd`。

在下一章中，我们将看到火花引擎不可改变的设计。