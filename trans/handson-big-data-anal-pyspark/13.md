# 利用火花图形接口

在本章中，我们将学习如何从数据源创建图表。然后我们将使用边缘应用编程接口和顶点应用编程接口进行实验。到本章结束时，您将知道如何计算顶点和页面排名的度数。

在本章中，我们将涵盖以下主题:

*   从数据源创建图形
*   使用顶点应用编程接口
*   使用边缘应用编程接口
*   计算顶点的度数
*   计算页面排名

# 从数据源创建图形

我们将创建一个加载器组件，用于加载数据，重新访问图形格式，并从文件中加载一个 Spark 图形。

# 创建加载器组件

`graph.g`文件由顶点到顶点的结构组成。在下面的`graph.g`文件中，如果我们将`1`与`2`对齐，这意味着在顶点标识`1`和顶点标识`2`之间有一条边。第二条线表示从顶点 ID `1`到`3`，然后从`2`到`3`，最后从`3`到`5`有一条边:

```py
1  2
1  3
2  3
3  5
```

我们将获取`graph.g`文件，加载它，看看它将如何在 Spark 中提供结果。首先，我们需要为我们的`graph.g`文件获取一个资源。我们将使用`getClass.getResource()`方法来获得它的路径，如下所示:

```py
package com.tomekl007.chapter_7

import org.apache.spark.SparkContext
import org.apache.spark.sql.SparkSession
import org.scalatest.FunSuite

class CreatingGraph extends FunSuite {
  val spark: SparkContext = SparkSession.builder().master("local[2]").getOrCreate().sparkContext

  test("should load graph from a file") {
    //given
    val path = getClass.getResource("/graph.g").getPath
```

# 重新审视图形格式

接下来，我们有`GraphBuilder`方法，这是我们自己的组件:

```py
    //when
    val graph = GraphBuilder.loadFromFile(spark, path)
```

以下是我们的`GraphBuilder.scala`文件为我们的`GraphBuilder`方法:

```py
package com.tomekl007.chapter_7

import org.apache.spark.SparkContext
import org.apache.spark.graphx.{Graph, GraphLoader}

object GraphBuilder {

  def loadFromFile(sc: SparkContext, path: String): Graph[Int, Int] = {
    GraphLoader.edgeListFile(sc, path)
  }
}
```

它使用来自`org.apache.spark.graphx.{Graph, GraphLoader}`包的`GraphLoader`类，我们正在指定格式。

这里指定的格式是`edgeListFile`。我们正在传递`sc`参数，它是`SparkContext`和`path`参数，它包含文件放置的路径。生成的图形将是`Graph [Int, Int]`，我们将使用它作为顶点的标识符。

# 从文件加载火花

一旦我们有了结果图，我们可以将`spark`和`path`参数传递给我们的`GraphBuilder.loadFromFile()`方法，此时，我们将有一个`graph`，这是一个`Graph [Int, Int]`的构造图，如下所示:

```py
  val graph = GraphBuilder.loadFromFile(spark, path)
```

为了迭代和验证我们的图是否被正确加载，我们将使用从`graph`开始的`triplets`，这是一对顶点到顶点以及这些顶点之间的边。我们将看到图的结构被正确加载:

```py
    //then
    graph.triplets.foreach(println(_))
```

最后，我们断言我们得到了`4`三元组(如前面*创建加载器组件*部分所示，我们从`graph.g`文件中得到四个定义):

```py
    assert(graph.triplets.count() == 4)
  }

}
```

我们将开始测试，看看我们是否能够正确加载我们的图表。

我们得到以下输出。这里有`(2, 1)`、`(3, 1)`、`(3,1)`、`(5,1)`、`(1,1)`、`(2,1)`、`(1,1)`、`(3,1)`:

![](assets/5623373e-3747-48f4-bcc0-935cffd2fed2.png)

因此，根据输出图，我们能够使用 Spark 重新加载我们的图。

# 使用顶点应用编程接口

在本节中，我们将使用边来构造图。我们将学习使用顶点应用编程接口，并利用边缘转换。

# 用顶点构造图

构建一个图不是一件微不足道的任务；我们需要提供它们之间的顶点和边。让我们专注于第一部分。第一部分由我们的`users`，`users`是`VertexId`和`String`的 RDD 组成，如下:

```py
package com.tomekl007.chapter_7

import org.apache.spark.SparkContext
import org.apache.spark.graphx.{Edge, Graph, VertexId}
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession
import org.scalatest.FunSuite

class VertexAPI extends FunSuite {
  val spark: SparkContext = SparkSession.builder().master("local[2]").getOrCreate().sparkContext

  test("Should use Vertex API") {
    //given
    val users: RDD[(VertexId, (String))] =
      spark.parallelize(Array(
        (1L, "a"),
        (2L, "b"),
        (3L, "c"),
        (4L, "d")
      ))
```

`VertexId`为`long`型；这只是`type`对`Long`的一个别名:

```py
type VertexID = Long
```

但是由于我们的图形有时有很多内容，因此`VertexId`应该是唯一的并且是一个非常长的数字。我们顶点的 RDD 中的每个顶点都应该有一个唯一的`VertexId`。与顶点相关联的自定义数据可以是任何类，但为了简单起见，我们将使用`String`类。首先，我们创建一个带有 ID `1`和字符串数据`a`的顶点，下一个带有 ID `2`和字符串数据`b`，下一个带有 ID `3`和字符串数据`c`，同样对于带有 ID `4`和字符串`d`的数据，如下所示:

```py
    val users: RDD[(VertexId, (String))] =
      spark.parallelize(Array(
        (1L, "a"),
        (2L, "b"),
        (3L, "c"),
        (4L, "d")
      ))
```

Creating a graph from only vertices will be correct but not very useful. A graph is the best way to find relationships between the data, which is why a graph is the main building block for social networks.

# 建立夫妻关系

在本节中，我们将创建顶点之间的耦合关系和边。在这里，我们会有一段`Edge`关系。一个`Edge`是`org.apache.spark.graphx`包中的一个案例类。这有点复杂，因为我们需要指定源顶点标识和目标顶点标识。我们要指定顶点 ID `1`和`2`有关系，所以我们给这个关系做个标签。在下面的代码中，我们将指定顶点标识`1`和标识`2`作为`friend`，然后我们将指定顶点标识`1`和标识`3`作为`friend`。最后，顶点标识`2`和标识`4`将是一个`wife`:

```py
    val relationships =
      spark.parallelize(Array(
        Edge(1L, 2L, "friend"),
        Edge(1L, 3L, "friend"),
        Edge(2L, 4L, "wife")
      ))
```

另外，标签可以是任何类型——它不需要是`String`类型；我们可以键入我们想要的内容并传递它。一旦我们有了顶点、用户和边的关系，我们就可以创建一个图。我们正在使用`Graph`类的`apply`方法来构建我们的火花图。我们需要通过`users`、`VertexId`和`relationships`，如下:

![](assets/f41c7ab7-c2ee-417c-a501-8baa146d05ed.png)

回归是一个 RDD，但它是一个特别的 RDD:

```py
    val graph = Graph(users, relationships)
```

当我们转到`Graph`类时，会看到`Graph`类有一个`vertices`的 RDD 和一个`edges`的 RDD，所以`Graph`类是两个 rdd 的伴随对象，如下图截图所示:

![](assets/3ed921b5-e3c9-454d-9c28-34296efda1c4.png)

我们可以通过发布一些方法得到`vertices`和`edges`的底层 RDD。例如，如果你想得到所有的顶点，我们可以映射所有的顶点，我们将得到属性和`VertexId`。在这里，我们只对该属性感兴趣，并将它转换为大写，如下所示:

```py
    val res = graph.mapVertices((_, att) => att.toUpperCase())
```

以下是属性:

```py
    val users: RDD[(VertexId, (String))] =
      spark.parallelize(Array(
        (1L, "a"),
        (2L, "b"),
        (3L, "c"),
        (4L, "d")
      ))
```

一旦我们将其转换为大写，我们就可以收集所有顶点并执行`toList()`，如下所示:

```py
    println(res.vertices.collect().toList)
  }

}
```

我们可以看到，在对值应用转换后，我们的图有以下顶点:

![](assets/c5639e74-f504-4de7-9c38-dad03641c394.png)

# 使用边缘应用编程接口

在本节中，我们将使用边缘应用编程接口来构建图形。我们也将使用顶点，但是这次我们将关注边缘变换。

# 用边构造图

正如我们在前面几节中看到的，我们有边和顶点，这是一个 RDD。由于这是一个 RDD，我们可以获得优势。我们有很多方法可以在普通的 RDD 上使用。我们可以使用`max`方法、`min`方法、`sum`方法以及所有其他动作。我们将应用`reduce`方法，所以`reduce`方法将采取两条边，我们将采取`e1`、`e2`，我们可以对其执行一些逻辑。

`e1`边是具有属性、目的地和来源的边，如下图所示:

![](assets/70fff1b2-97e3-4840-845f-7689be653d0a.png)

由于边将两个顶点链接在一起，我们可以在这里执行一些逻辑。例如，如果`e1`边属性等于`friend`，我们想要使用`filter`操作来提升一条边。所以`filter`法就是只取一个边，然后如果边`e1`是一个`friend`，就会自动感知。我们可以看到，最后我们可以`collect`它并执行`toList`，这样 Spark 上的应用编程接口就可以供我们使用了。以下代码将帮助我们实现我们的逻辑:

```py
import org.apache.spark.SparkContext
import org.apache.spark.graphx.{Edge, Graph, VertexId}
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession
import org.scalatest.FunSuite

class EdgeAPI extends FunSuite {
  val spark: SparkContext = SparkSession.builder().master("local[2]").getOrCreate().sparkContext

  test("Should use Edge API") {
    //given
    val users: RDD[(VertexId, (String))] =
      spark.parallelize(Array(
        (1L, "a"),
        (2L, "b"),
        (3L, "c"),
        (4L, "d")
      ))

    val relationships =
      spark.parallelize(Array(
        Edge(1L, 2L, "friend"),
        Edge(1L, 3L, "friend"),
        Edge(2L, 4L, "wife")
      ))

    val graph = Graph(users, relationships)

    //when
 val resFromFilter = graph.edges.filter((e1) => e1.attr == "friend").collect().toList
 println(resFromFilter)
```

它在标准 RDD 之上还有几个方法。例如，我们可以做一个地图边，它将采取一个边，我们可以采取一个属性，并将每个标签映射为大写，如下所示:

```py
    val res = graph.mapEdges(e => e.attr.toUpperCase)
```

On the graph, we can also perform group edges. Grouping edges is similar to `GROUP BY`, but only for edges.

键入以下命令打印线贴图边缘:

```py
    println(res.edges.collect().toList)
```

让我们开始我们的代码。在输出中我们可以看到我们的代码已经过滤了`wife`边——我们只感知到了从顶点 ID `1`到 ID `2`的`friend`边，也感知到了顶点 ID `1`到 ID `3`的边，并且映射了边，如下图截图所示:

![](assets/ad57af2a-f940-43aa-8b89-ed7248c18624.png)

# 计算顶点的度数

在这一节中，我们将介绍全部学位，然后我们将把它分成两部分——一个内学位和一个外学位——我们将理解这在代码中是如何工作的。

对于我们的第一个测试，让我们构建我们已经知道的图表:

```py
package com.tomekl007.chapter_7

import org.apache.spark.SparkContext
import org.apache.spark.graphx.{Edge, Graph, VertexId}
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession
import org.scalatest.FunSuite
import org.scalatest.Matchers._

class CalculateDegreeTest extends FunSuite {
  val spark: SparkContext = SparkSession.builder().master("local[2]").getOrCreate().sparkContext

  test("should calculate degree of vertices") {
    //given
    val users: RDD[(VertexId, (String))] =
      spark.parallelize(Array(
        (1L, "a"),
        (2L, "b"),
        (3L, "c"),
        (4L, "d")
      ))

    val relationships =
      spark.parallelize(Array(
        Edge(1L, 2L, "friend"),
        Edge(1L, 3L, "friend"),
        Edge(2L, 4L, "wife")
      ))
```

我们可以用`degrees`方法得到度数。`degrees`方法是返回`VertexRDD`，因为`degrees`是一个顶点:

```py
    val graph = Graph(users, relationships)

    //when
    val degrees = graph.degrees.collect().toList
```

结果如下:

```py
    //then
    degrees should contain theSameElementsAs List(
      (4L, 1L),
      (2L, 2L),
      (1L, 2L),
      (3L, 1L)
    )
  }
```

前面的代码解释了对于`VertexId`的`4L`实例，只有一个关系，因为`2L`和`4L`之间有关系。

那么`VertexId`的`2L`实例有两个，所以在`1L, 2L`和`2L, 4L`之间。对于`VertexId`的`1L`实例，有两个，分别是`1L, 2L`和`1L, 3L`，对于`VertexId 3L`，在`1L`和`3L`之间只有一个关系。这样，我们可以检查我们的图是如何耦合的，以及有多少关系。我们可以通过对它们进行排序来找出哪个顶点最出名，所以我们可以在下面的截图中看到我们的测试通过了:

![](assets/14383131-b9a0-495e-b272-bf7b723f32bd.png)

# 学位

度数告诉我们有多少顶点进入第二个顶点，但不是相反。这次，我们可以看到对于`VertexId`的`2L`实例，只有一个入站顶点。我们可以看到`2L`和`1L`有关系，`3L`和`1L`也有关系，`4L`和`1L`也有关系。在下面的结果数据集中，`VertexId 1L`没有数据，因为`1L`是输入。因此，`1L`将只是一个来源，而不是目的地:

```py
  test("should calculate in-degree of vertices") {
    //given
    val users: RDD[(VertexId, (String))] =
      spark.parallelize(Array(
        (1L, "a"),
        (2L, "b"),
        (3L, "c"),
        (4L, "d")
      ))

    val relationships =
      spark.parallelize(Array(
        Edge(1L, 2L, "friend"),
        Edge(1L, 3L, "friend"),
        Edge(2L, 4L, "wife")
      ))

    val graph = Graph(users, relationships)

    //when
    val degrees = graph.inDegrees.collect().toList

    //then
    degrees should contain theSameElementsAs List(
      (2L, 1L),
      (3L, 1L),
      (4L, 1L)
    )
  }
```

in-degree 的上述特性是一个非常有用的属性。当我们无法找出哪些页面非常重要时，我们会使用 in-degree，因为它们是通过页面链接的，而不是从页面链接的。

通过运行这个测试，我们可以看到它如预期的那样工作:

![](assets/b833ae5d-5154-4b61-9758-359815e92eaa.png)

# 外学位

出度解释了有多少顶点要出。这一次，我们将计算我们的边缘、关系的来源，而不是目的地，就像我们在 in-degree 方法中做的那样。

为了获得学位，我们将使用以下代码:

```py
val degrees = graph.outDegrees.collect().toList
```

`outDegrees`方法包含`RDD`和`VertexRDD`，我们使用`collect`和`toList`方法将其收集到一个列表中。

这里，`VertexId 1L`应该有两个出站顶点，因为`1L, 2L`和`1L, 3L`之间有关系:

```py
  test("should calculate out-degree of vertices") {
    //given
    val users: RDD[(VertexId, (String))] =
      spark.parallelize(Array(
        (1L, "a"),
        (2L, "b"),
        (3L, "c"),
        (4L, "d")
      ))

    val relationships =
      spark.parallelize(Array(
        Edge(1L, 2L, "friend"),
        Edge(1L, 3L, "friend"),
        Edge(2L, 4L, "wife")
      ))

    val graph = Graph(users, relationships)

    //when
    val degrees = graph.outDegrees.collect().toList

    //then
    degrees should contain theSameElementsAs List(
      (1L, 2L),
      (2L, 1L)
    )
  }

}
```

此外，`VertexId 2L`应该有一个出站顶点，因为`2L`和`4L`之间存在关系，而不是相反，如前面的代码所示。

我们将运行该测试并获得以下输出:

![](assets/572ee639-66b1-4f94-8249-ba5853949981.png)

# 计算页面排名

在本节中，我们将加载关于用户的数据，并重新加载关于他们的追随者的数据。我们将使用图形应用编程接口和我们的数据结构，我们将计算 PageRank 来计算用户的排名。

首先，我们需要加载`edgeListFile`，如下:

```py
package com.tomekl007.chapter_7

import org.apache.spark.graphx.GraphLoader
import org.apache.spark.sql.SparkSession
import org.scalatest.FunSuite
import org.scalatest.Matchers._

class PageRankTest extends FunSuite {
  private val sc = SparkSession.builder().master("local[2]").getOrCreate().sparkContext

  test("should calculate page rank using GraphX API") {
    //given
    val graph = GraphLoader.edgeListFile(sc, getClass.getResource("/pagerank/followers.txt").getPath)
```

我们有一个`followers.txt`文件；下面的截图显示了文件的格式，类似于我们在*创建加载器组件*部分看到的文件:

![](assets/ce7f1f3a-0eff-4b12-9171-bb5b3fe8a45c.png)

我们可以看到每个顶点标识之间都有关系。因此，我们从`followers.txt`文件加载`graph`，然后发布 PageRank。我们正在采取`vertices`这将是需要的，如下所示:

```py
    val ranks = graph.pageRank(0.0001).vertices
```

PageRank 将计算我们顶点之间的影响和关系。

# 加载和重新加载关于用户和关注者的数据

为了找出哪个用户有哪个名字，我们需要加载`users.txt`文件。`users.txt`文件给`VertexId`分配一个用户名和它自己的名字。我们使用以下代码:

```py
    val users = sc.textFile(getClass.getResource("/pagerank/users.txt").getPath).map { line =>
```

以下是`users.txt`文件:

![](assets/b5c42ab0-8639-4371-8915-5905dbba68bc.png)

我们在逗号上拆分，第一组是我们的整数，会是顶点 ID，然后`fields(1)`是顶点的名字，如下:

```py
      val fields = line.split(",")
      (fields(0).toLong, fields(1))
    }
```

接下来，我们将`join``users`换成`ranks`。我们将通过使用用户的`username`和`rank`使用`VertexId`来使用`join``users`。一旦我们有了这个，我们就可以用`rank`来排序所有的东西，所以我们将取元组的第二个元素，它应该被排序为`sortBy ((t) =>t.2`。在文件的开头，我们会有最有影响力的用户:

```py
    //when
 val rankByUsername = users.join(ranks).map {
      case (_, (username, rank)) => (username, rank)
    }.sortBy((t) => t._2, ascending = false)
      .collect()
      .toList
```

我们将打印以下内容并订购`rankByUsername`，如下所示:

```py
    println(rankByUsername)
    //then
    rankByUsername.map(_._1) should contain theSameElementsInOrderAs List(
      "BarackObama",
      "ladygaga",
      "odersky",
      "jeresig",
      "matei_zaharia",
      "justinbieber"
    )
  }

}
```

If we skip the `sortBy` method, Spark does not guarantee any ordering of elements; to keep the ordering, we need to issue the `sortBy` method.

运行代码后，我们得到以下输出:

![](assets/3d25a619-5388-4912-97c8-016a62cb11a9.png)

当我们开始运行这个测试时，我们可以看到 GraphX PageRank 是否能够计算出我们用户的影响力。我们得到了上一个截图中显示的输出，其中`BarackObama`首先是`1.45`的影响，然后是`ladygaga`的影响`1.39`、`odersky`的影响`1.29`、`jeresig`的影响`0.99`、`matai_zaharia`的影响`0.70`，最后是`justinbieber`的影响`0.15`。

根据前面的信息，我们能够用最少的代码计算复杂的算法。

# 摘要

在这一章中，我们深入研究了转换和动作，然后我们了解了 Spark 的不可变设计。我们研究了如何避免洗牌以及如何降低运营费用。然后，我们研究了如何以正确的格式保存数据。我们还学习了如何使用 Spark 键/值 API，以及如何测试 Apache Spark 作业。之后，我们学习了如何从数据源创建一个图，然后我们研究并实验了边和顶点 API。我们学会了如何计算顶点的度数。最后，我们看了 PageRank，以及如何使用 Spark GraphicX API 计算它。