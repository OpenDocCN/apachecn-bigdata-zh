# 强大的 MLlib 探索性数据分析

在本章中，我们将探讨 Spark 使用线性回归和**支持向量机** ( **支持向量机**)等模型执行回归任务的能力。我们将学习如何使用 MLlib 计算汇总统计数据，并使用皮尔逊和斯皮尔曼相关性发现数据集中的相关性。我们还将在大型数据集上测试我们的假设。

我们将涵盖以下主题:

*   用 MLlib 计算汇总统计
*   使用皮尔逊和斯皮尔曼方法发现相关性
*   在大数据集上测试我们的假设

# 用 MLlib 计算汇总统计

在本节中，我们将回答以下问题:

*   什么是汇总统计？
*   我们如何使用 MLlib 创建汇总统计？

MLlib 是 Spark 附带的机器学习库。最近有了一个新的发展，允许我们使用 Spark 的数据处理能力来导入 Spark 固有的机器学习能力。这意味着我们不仅可以使用 Spark 来摄取、收集和转换数据，还可以在 PySpark 平台上分析和使用它来构建机器学习模型，这使我们能够拥有更无缝的可部署解决方案。

汇总统计是一个非常简单的概念。我们熟悉平均值，或标准差，或特定变量的方差。这些是数据集的汇总统计信息。之所以称之为汇总统计，是因为它通过某种统计给你提供了某件事的汇总。例如，当我们谈论数据集的平均值时，我们总结了该数据集的一个特征，这个特征就是平均值。

让我们来看看如何在 Spark 中计算汇总统计。这里的关键因素是`colStats`功能。`colStats`函数计算`rdd`输入的列汇总统计。`colStats`函数接受一个参数，即`rdd`，它允许我们使用 Spark 计算不同的汇总统计。

让我们来看看`Chapter5.ipynb`中本章的 Jupyter 笔记本(可在[https://github . com/PacktPublishing/动手-大数据-分析-with-PySpark/tree/master/chapter 05](https://github.com/PacktPublishing/Hands-On-Big-Data-Analytics-with-PySpark/tree/master/Chapter05)上找到)中的代码。我们将首先从`kddcup.data.gz`文本文件中收集数据，并将其导入`raw_data`变量，如下所示:

```py
raw_data = sc.textFile("./kddcup.data.gz")
```

`kddcup.data`文件是一个**逗号分隔值** ( **CSV** )文件。我们必须按照`,`字符分割这些数据，并将其放入`csv`变量中，如下所示:

```py
csv = raw_data.map(lambda x: x.split(","))
```

先来看数据文件的第一个特征`x[0]`；这个特性代表了`duration`，也就是数据的各个方面。我们将在这里把它转换成一个整数，并将它包装在一个列表中，如下所示:

```py
duration = csv.map(lambda x: [int(x[0])])
```

这有助于我们对多个变量进行汇总统计，而不仅仅是其中一个。要激活`colStats`功能，我们需要导入`Statistics`包，如下图所示:

```py
from pyspark.mllib.stat import Statistics
```

这个`Statistics`包是`pyspark.mllib.stat`的子包。现在，我们需要调用`Statistics`包中的`colStats`函数，并向它提供一些数据。这里，我们讨论的是来自数据集的`duration`数据，我们将汇总统计信息输入到`summary`变量中:

```py
summary = Statistics.colStats(duration)
```

要访问不同的汇总统计，如均值、标准差等，可以调用`summary`对象的函数，访问不同的汇总统计。例如，我们可以访问`mean`，由于我们的`duration`数据集中只有一个要素，我们可以通过`00`索引对其进行索引，我们将得到数据集的平均值如下:

```py
summary.mean()[0]
```

这将为我们提供以下输出:

```py
47.97930249928637
```

类似地，如果我们从 Python 标准库中导入`sqrt`函数，我们可以创建在数据集中看到的持续时间的标准偏差，如下面的代码片段所示:

```py
from math import sqrt
sqrt(summary.variance()[0])
```

这将为我们提供以下输出:

```py
707.746472305374
```

如果不使用`[0]`对汇总统计进行索引，可以看到`summary.max()`和`summary.min()`还给出了一个数组，其中第一个元素就是我们想要的汇总统计，如下面的代码片段所示:

```py
summary.max()
array ([58329.]) #output
summary.min()
array([0.])  #output
```

# 使用皮尔逊和斯皮尔曼相关性发现相关性

在本节中，我们将研究两种不同的计算数据集相关性的方法，这两种方法被称为皮尔逊和斯皮尔曼相关性。

# 皮尔逊相关

皮尔逊相关系数向我们展示了两个不同的变量是如何同时变化的，然后根据它们的变化程度进行调整。如果您有数据集，这可能是计算相关性最流行的方法之一。

# 斯皮尔曼相关

Spearman 的等级相关性并不是 PySpark 内置的默认相关性计算，但它非常有用。斯皮尔曼相关系数是排名变量之间的皮尔逊相关系数。用不同的方法来看待相关性，可以让我们对相关性的工作原理有更多的理解。让我们看看如何在 PySpark 中计算这个。

# 计算皮尔逊和斯皮尔曼相关性

为了理解这一点，让我们假设我们从数据集中获取前三个数字变量。为此，我们想要访问我们之前定义的`csv`变量，这里我们简单地使用逗号(`,`)分割`raw_data`。我们将只考虑前三个数字列。我们不会拿任何有文字的东西；我们只对纯粹基于数字的功能感兴趣。在我们的例子中，在`kddcup.data`中，第一个特征在`0`被索引；特征 5 和特征 6 分别在`4`和`5`被索引，这是我们拥有的数字变量。我们使用`lambda`函数将这三个都放入一个列表中，并将其放入`metrics`变量中:

```py
metrics = csv.map(lambda x: [x[0], x[4], x[5]])
Statistics.corr(metrics, method="spearman")
```

这将为我们提供以下输出:

```py
array([[1\.       ,  0.01419628,  0.29918926],
 [0.01419628,  1\.        , -0.16793059],
 [0.29918926, -0.16793059,  1\.        ]])
```

在*用 MLlib* 计算汇总统计数据部分，我们简单地将第一个特征放入一个列表中，并创建一个长度为 1 的列表。这里，我们把三个变量的三个量放入同一个列表中。现在，每个列表都有三个长度。

为了计算相关性，我们在`metrics`变量上调用`corr`方法，并将`method`指定为`"spearman"`。PySpark 会给我们一个非常简单的矩阵，告诉我们变量之间的相关性。在我们的例子中，我们的`metrics`变量中的第三个变量比第二个变量更相关。

如果我们再次在`metrics`上运行`corr`，但是指定方法是`pearson`，那么它会给我们皮尔逊相关。所以，让我们来研究一下为什么我们需要有数据科学家或机器学习研究者的资格来调用这两个简单的函数，并简单地改变第二个参数的值。许多机器学习和数据科学都围绕着我们对统计学的理解，对数据行为的理解，对机器学习模型是如何建立的理解，以及是什么赋予了它们预测能力。

所以，作为一个机器学习实践者或者数据科学家，我们简单地把 PySpark 作为一个大计算器来使用。当我们使用计算器时，我们从不抱怨计算器使用简单——事实上，它帮助我们以更直接的方式完成目标。PySpark 也是如此；一旦我们从数据工程端转移到 MLlib 端，我们会注意到代码变得越来越容易。它试图隐藏其下数学的复杂性，但我们需要理解不同相关性之间的区别，我们还需要知道如何以及何时使用它们。

# 在大数据集上测试我们的假设

在本节中，我们将研究假设检验，并学习如何使用 PySpark 检验假设。让我们看看在 PySpark 中实现的一种特定类型的假设检验。这种形式的假设检验被称为皮尔逊卡方检验。卡方检验这两个数据集中的差异有多大可能是偶然出现的。

例如，如果我们有一家没有任何客流量的零售店，突然你得到客流量，这有多大可能是随机的，或者与以前相比，我们现在得到的游客数量是否有任何统计上的显著差异？之所以称之为卡方检验，是因为检验本身参考了卡方分布。您可以参考在线文档来了解更多关于卡方分布的信息。

皮尔逊的卡方检验有三种变化。我们将检查观察到的数据集是否与理论数据集中的分布不同。

让我们看看如何实现这一点。先从`pyspark.mllib.linalg`导入`Vectors`包开始。使用这个向量，我们将创建一个密集的向量，显示我们商店每天的访客频率。

让我们想象一下，频率从`0.13`一个小时到`0.61`、`0.8`和`0.5`，最后在周五在`0.3`结束。因此，我们将这些访客频率放入`visitors_freq`变量中。因为我们使用的是 PySpark，所以从`Statistics`包运行卡方测试非常简单，我们已经导入如下:

```py
from pyspark.mllib.linalg import Vectors
visitors_freq = Vectors.dense(0.13, 0.61, 0.8, 0.5, 0.3)
print(Statistics.chiSqTest(visitors_freq))
```

通过运行卡方检验，`visitors_freq`变量给了我们一堆有用的信息，如下图所示:

![](assets/ec0a248d-d599-476c-bbd7-665a504a76bc.png)

前面的输出显示了卡方检验总结。我们使用了`pearson`方法，在我们的皮尔逊卡方检验中有`4`个自由度，统计数据是`0.585`，这意味着`pValue`是`0.964`。这导致没有反对无效假设的假设。这样，观察到的数据遵循与预期相同的分布，这意味着我们的访问者实际上没有什么不同。这让我们对假设检验有了很好的理解。

# 摘要

在这一章中，我们学习了汇总统计和用 MLlib 计算汇总统计。我们还学习了皮尔逊和斯皮尔曼相关性，以及如何使用 PySpark 在我们的数据集中发现这些相关性。最后，我们学习了一种进行假设检验的特殊方法，叫做皮尔逊卡方检验。然后，我们使用 PySpark 的假设检验函数在大数据集上检验我们的假设。

在下一章中，我们将研究如何使用 Spark SQL 将结构放在大数据上。