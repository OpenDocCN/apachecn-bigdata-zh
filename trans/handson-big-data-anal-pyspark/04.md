# 将数据汇总成有用的报告

在本章中，我们将学习如何将数据聚合和汇总成有用的报告。我们将学习如何使用`map`和`reduce`函数计算平均值，执行更快的平均值计算，以及使用带有键值对数据点的透视表。

在本章中，我们将涵盖以下主题:

*   用`map`和`reduce`计算平均值
*   聚合的平均计算速度更快
*   具有键值对数据点的透视表列表

# 使用 map 和 reduce 计算平均值

我们将在这一部分回答以下三个主要问题:

*   我们如何计算平均值？
*   什么是地图？
*   什么是减少？

可以在[https://spark . Apache . org/docs/latest/API/python/pyspark . html 查看文档？高亮=地图#pyspark。RDD.map](https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=map#pyspark.RDD.map) 。

`map`函数接受两个参数，其中一个是可选的。`map`的第一个参数是`f`，这是一个通过`map`函数应用于整个 RDD 的函数。第二个参数是`preservesPartitioning`参数，默认为`False`。

如果我们看一下文档，它说`map`只是通过对这个 RDD 的每个元素应用一个函数来返回一个新的 RDD，显然，这个函数指的是我们输入到`map`函数本身的`f`。文档中有一个非常简单的例子，它说如果我们并行化一个包含三个字符列表的`rdd`方法，`b`、`a`和`c`，并且我们映射一个创建每个元素元组的函数，那么我们将创建一个三元组列表，其中原始字符放在元组的第一个元素中，`1`整数放在第二个元素中，如下所示:

```py
rdd =  sc.paralleize(["b", "a", "c"])
sorted(rdd.map(lambda x: (x, 1)).collect())
```

这将为我们提供以下输出:

```py
[('a', 1), ('b', 1), ('c', 1)]
```

`reduce`函数只取一个参数，就是`f`。`f`是将一个列表缩减为一个数字的功能。从技术角度来看，指定的交换和关联二元运算符减少了这个 RDD 的元素。

让我们用我们一直使用的 KDD 数据举个例子。我们启动了我们的 Jupyter Notebook 实例，它链接到一个 Spark 实例，就像我们之前做的那样。然后我们通过从本地磁盘加载一个`kddcup.data.gz`文本文件来创建一个`raw_data`变量，如下所示:

```py
raw_data = sc.textFile("./kddcup.data.gz")
```

接下来要做的是将这个文件分割成`csv`，然后我们将过滤特征 41 包含单词`normal`的行:

```py
csv = raw_data.map(lambda x: x.split(","))
normal_data = csv.filter(lambda x: x[41]=="normal.")
```

然后我们用`map`函数把这个数据转换成一个整数，最后我们用`reduce`函数计算`total_duration`，然后我们可以打印`total_duration`如下:

```py
duration = normal_data.map(lambda x: int(x[0]))
total_duration = duration.reduce(lambda x, y: x+y)
total_duration
```

然后，我们将获得以下输出:

```py
211895753
```

接下来要做的是将`total_duration`除以数据的计数，如下所示:

```py
total_duration/(normal_data.count())
```

这将为我们提供以下输出:

```py
217.82472416710442
```

经过一点计算，我们会使用`map`和`reduce`创建两个计数。我们刚刚学习了如何使用 PySpark 计算平均值，以及 PySpark 中的`map`和`reduce`函数。

# 聚合的平均计算速度更快

在上一节中，我们看到了如何使用`map`和`reduce`计算平均值。现在让我们看看用`aggregate`函数进行更快的平均计算。您可以参考上一节提到的文档。

`aggregate`是一个接受三个参数的函数，没有一个是可选的。

第一个是`zeroValue`参数，我们在这里放入聚合结果的基本情况。

第二个参数是顺序运算符(`seqOp`)，它允许您在`zeroValue`之上堆叠和聚合值。您可以从`zeroValue`开始，您输入到`aggregate`的`seqOp`功能从您的 RDD 获取值，并将其叠加或聚合到`zeroValue`之上。

最后一个参数是`combOp`，代表组合运算，这里我们简单的取现在通过`seqOp`参数聚合的`zeroValue`参数，组合成一个值，这样我们就可以用这个来总结聚合。

因此，在这里，我们使用一个组合函数和一个中性零值来聚合每个分区的元素，然后聚合所有分区的结果。在这里，我们要注意两件事:

1.  `op`功能允许修改`t1`，但不应该修改`t2`
2.  第一个函数`seqOp`可以返回不同的结果类型`U`

在这种情况下，我们都需要一个操作将一个`T`合并为`U`，一个操作将两个 Us 合并。

让我们去 Jupyter 笔记本看看这是怎么做到的。`aggregate`允许我们同时计算总持续时间和计数。我们称之为`duration_count`函数。然后我们拿走`normal_data`并聚集它。请记住，有三个参数需要汇总。第一个是初始值；即零值，`(0,0)`。第二种是顺序操作，如下所示:

```py
duration_count = duration.aggregate(
 (0,0),
 (lambda db, new_value: (db[0] + new_value, db[1] + 1))
)
```

我们需要用两个参数指定一个`lambda`函数。第一个参数是当前累加器，或者聚合器，或者也可以称为数据库(`db`)。然后，我们的`lambda`函数中的第二个参数为`new_value`，即我们在 RDD 处理的当前值。我们只想对数据库做正确的事情，也就是说，我们知道我们的数据库看起来像一个元组，第一个元素的持续时间和第二个元素的计数之和。在这里，我们知道我们的数据库看起来像一个元组，其中持续时间的总和是第一个元素，计数是第二个元素。每当我们看到一个新值时，我们需要将新值添加到当前运行总数中，并将`1`添加到当前运行计数中。

运行总数是第一个元素，`db[0]`。然后我们只需要将`1`添加到第二个元素`db[1]`中，这就是计数。这是顺序操作。

每次我们得到一个`new_value`，如前面的代码块所示，我们只需将它添加到运行总数中。而且，因为我们已经将`new_value`添加到运行总数中，我们需要将计数增加`1`。其次，我们需要加入组合运算。现在，我们只需将两个独立数据库`db1`和`db2`的各自元素组合起来，如下所示:

```py
duration_count = duration.aggregate(
 (0,0),
 (lambda db, new_value: (db[0] + new_value, db[1] + 1)),
 (lambda db1, db2: (db1[0] + db2[0], db1[1] + db2[1]))
)
```

由于持续时间计数是一个元组，它收集我们在第一个元素上的总持续时间，并计算我们在第二个元素中查看了多少持续时间，因此计算平均值非常简单。我们需要将第一个元素除以第二个元素，如下所示:

```py
duration_count[0]/duration_count[1]
```

这将为我们提供以下输出:

```py
217.82472416710442
```

您可以看到它返回了与我们在上一节中看到的相同的结果，这很好。在下一节中，我们将研究带有键值对数据点的透视表列表。

# 具有键值对数据点的透视表列表

透视表非常简单，易于使用。我们要做的是使用大数据集，比如 KDD 杯数据集，并通过特定的关键字对特定的值进行分组。

例如，我们有一个人和他们最喜欢的水果的数据集。我们想知道有多少人把苹果作为他们最喜欢的水果，所以我们将人数分组，这是价值，而不是一个关键，这是水果。这是透视表的简单概念。

我们可以使用`map`函数将 KDD 数据集移动到键值对范例中。我们使用`kv`键值中的`lambda`函数映射数据集的特征`41`，并将该值追加如下:

```py
kv = csv.map(lambda x: (x[41], x))
kv.take(1)
```

我们用特征`41`作为关键，数值就是数据点，也就是`x`。我们可以使用`take`函数从这些转换后的行中选择一行，看看它的外观。

现在让我们尝试类似于前面例子的东西。为了计算特征`41`中存在的每种类型值的总持续时间，我们可以再次使用`map`功能，并简单地将`41`特征作为我们的关键。我们可以将数据点中第一个数字的浮点数作为我们的值。我们将使用`reduceByKey`功能通过按键减少每个持续时间。

因此，`reduceByKey`不只是减少所有数据点，而不管它们属于哪个键，而是根据与哪个键相关联来减少持续时间数。可以在[https://spark . Apache . org/docs/latest/API/python/pyspark . html 查看文档？高亮=地图#pyspark。](https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=map#pyspark.RDD.reduceByKey)。`reduceByKey`使用关联和交换`reduce`函数合并每个键的值。它在将结果发送到缩减器之前对每个映射器执行本地合并，这类似于 MapReduce 中的合并器。

`reduceByKey`函数只需要一个参数。我们将使用`lambda`功能。我们取两个不同的持续时间并把它们加在一起，PySpark 足够聪明，可以根据一个键应用这个缩减函数，如下所示:

```py
kv_duration = csv.map(lambda x: (x[41], float(x[0]))).reduceByKey(lambda x, y: x+y)
kv_duration.collect()
```

结果输出如下图所示:

![](img/dc4b7020-d1a2-48ee-8e9f-6f152fefc69f.png)

如果我们收集键值持续时间数据，我们可以看到持续时间是由出现在特征`41`中的值收集的。如果我们在 Excel 中使用透视表，有一个方便的函数就是`countByKey`函数，它做的是完全一样的事情，演示如下:

```py
kv.countByKey()
```

这将为我们提供以下输出:

![](img/0e2d8e34-d50e-447b-818e-d2139cdaa519.png)

可以看到调用`kv.countByKey()`函数和调用`reduceByKey`函数是一样的，前面是从按键到时长的映射。

# 摘要

在本章中，我们学习了如何用`map`和`reduce`计算平均值。我们还通过`aggregate`学习了更快的平均计算。最后，我们了解到数据透视表允许我们基于不同的特性值来聚合数据，并且通过 PySpark 中的数据透视表，我们可以利用方便的函数，如`reducedByKey`或`countByKey`。

在下一章中，我们将学习 MLlib，它涉及到机器学习，这是一个非常热门的话题。