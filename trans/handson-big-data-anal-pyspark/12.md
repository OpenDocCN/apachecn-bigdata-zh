# 测试阿帕奇火花作业

在本章中，我们将测试 Apache Spark 作业，并学习如何从 Spark 引擎中分离逻辑。

我们将首先介绍代码的单元测试，然后由 SparkSession 中的集成测试使用。稍后，我们将使用部分函数模拟数据源，然后学习如何利用 ScalaCheck 进行基于属性的测试，以及 Scala 中的类型。到本章结束时，我们将在不同版本的 Spark 中进行测试。

在本章中，我们将涵盖以下主题:

*   从火花发动机单元测试中分离逻辑
*   使用 SparkSession 进行集成测试
*   使用部分函数模拟数据源
*   使用 ScalaCheck 进行基于属性的测试
*   在不同版本的 Spark 中进行测试

# 从火花发动机单元测试中分离逻辑

让我们从分离火花引擎的逻辑开始。

在本节中，我们将涵盖以下主题:

*   创建具有逻辑的组件
*   该组件的单元测试
*   将模型类中的案例类用于我们的领域逻辑

让我们先看看逻辑，然后是简单的测试。

所以，我们有一个只有一种方法的`BonusVerifier`对象，`quaifyForBonus`，它采用了我们的`userTransaction`模型类。根据我们在下面代码中的登录，我们加载用户事务并过滤所有有资格获得奖金的用户。首先，我们需要测试它来创建一个 RDD 并过滤它。我们需要创建一个 SparkSession，还需要创建模仿 RDD 或数据帧的数据，然后测试整个 Spark API。由于这涉及到逻辑，我们将单独测试它。逻辑如下:

```
package com.tomekl007.chapter_6
import com.tomekl007.UserTransaction
object BonusVerifier {
 private val superUsers = List("A", "X", "100-million")
def qualifyForBonus(userTransaction: UserTransaction): Boolean = {
 superUsers.contains(userTransaction.userId) && userTransaction.amount > 100
 }
}
```

我们有一份超级用户的名单，上面有`A`、`X`和`100-million`的用户标识。如果我们的`userTransaction.userId`在`superUsers`列表内，并且`userTransaction.amount`高于`100`，则用户有资格获得奖金；否则，他们不会。在现实世界中，奖金逻辑的限定符将更加复杂，因此孤立地测试逻辑非常重要。

下面的代码显示了我们使用`userTransaction`模型的测试。我们知道我们的用户交易包括`userId`和`amount`。以下示例显示了我们的域模型对象，它在 Spark 执行集成测试和我们的单元测试之间共享，与 Spark 分离:

```
package com.tomekl007

import java.util.UUID

case class UserData(userId: String , data: String)

case class UserTransaction(userId: String, amount: Int)

case class InputRecord(uuid: String = UUID.*randomUUID()*.toString(), userId: String)
```

我们需要为用户标识`X`和金额`101`创建我们的`UserTransaction`，如下例所示:

```
package com.tomekl007.chapter_6
import com.tomekl007.UserTransaction
import org.scalatest.FunSuite
class SeparatingLogic extends FunSuite {
test("test complex logic separately from spark engine") {
 //given
 val userTransaction = UserTransaction("X", 101)
//when
 val res = BonusVerifier.qualifyForBonus(userTransaction)
//then
 assert(res)
 }
}
```

然后我们将把`userTransaction`传给`qualifyForBonus`，结果应该是`true`。该用户应该有资格获得奖金，如以下输出所示:

![](assets/49908d22-b2a3-4b0b-9150-e533a6cda546.png)

现在，让我们为负用例编写一个测试，如下所示:

```
test(testName = "test complex logic separately from spark engine - non qualify") {
 //given
 val userTransaction = UserTransaction("X", 99)
//when
 val res = BonusVerifier.*qualifyForBonus*(userTransaction)
//then
 assert(!res)
 }
```

在这里，我们有一个用户，`X`，花费`99`，我们的结果应该是假的。当我们验证代码时，我们可以从下面的输出中看到，我们的测试已经通过:

![](assets/fee898ff-e179-4e5b-941a-a434a8c812c9.png)

我们已经介绍了两种情况，但是在现实场景中，还有更多。例如，如果我们想要测试我们指定的`userId`的情况，它不在这个超级用户列表中，并且我们有花费很多钱的`some_new_user`，在我们的情况下，`100000`，我们得到以下结果:

```
test(testName = "test complex logic separately from spark engine - non qualify2") {
 //given
 val userTransaction = UserTransaction("some_new_user", 100000)
//when
 val res = BonusVerifier.*qualifyForBonus*(userTransaction)
//then
 assert(!res)
 }
```

让我们假设它不应该合格，所以这样的逻辑有点复杂。因此，我们以单元测试的方式对其进行测试:

![](assets/028fcf7e-9829-424e-a479-5497fc29c89d.png)

我们的测试速度非常快，因此我们能够在完全不引入 Spark 的情况下检查一切是否按预期运行。在下一节中，我们将使用 SparkSession 通过集成测试来改变逻辑。

# 使用 SparkSession 进行集成测试

现在让我们学习使用 SparkSession 进行集成测试。

在本节中，我们将涵盖以下主题:

*   利用 SparkSession 进行集成测试
*   使用经过单元测试的组件

在这里，我们正在创建火花引擎。以下几行对集成测试至关重要:

```
 val spark: SparkContext = SparkSession.builder().master("local[2]").getOrCreate().sparkContext
```

仅仅创建一个轻量级对象并不是一条简单的线。SparkSession 是一个非常重的对象，从资源和时间的角度来看，从头构建它是一个昂贵的操作。与上一节中的单元测试相比，创建 SparkSession 等测试将花费更多的时间。

出于同样的原因，我们应该经常使用单元测试来转换所有的边缘用例，并且只对逻辑的较小部分使用集成测试，比如大写的边缘用例。

以下示例显示了我们正在创建的阵列:

```
 val keysWithValuesList =
 Array(
 UserTransaction("A", 100),
 UserTransaction("B", 4),
 UserTransaction("A", 100001),
 UserTransaction("B", 10),
 UserTransaction("C", 10)
 )
```

以下示例显示了我们正在创建的 RDD:

```
 val data = spark.parallelize(keysWithValuesList)
```

这是 Spark 第一次参与我们的集成测试。创建 RDD 也是一项耗时的工作。与仅仅创建一个数组相比，创建一个 RDD 真的很慢，因为这也是一个很重的对象。

我们现在将使用`data.filter`传递一个`qualifyForBonus`函数，如下例所示:

```
 val aggregatedTransactionsForUserId = data.filter(BonusVerifier.qualifyForBonus)
```

这个函数已经过单元测试，所以我们不需要考虑所有的边缘情况、不同的 id、不同的数量等等。我们只是创建了几个具有一定数量的标识来测试我们的整个逻辑链是否如预期的那样工作。

应用此逻辑后，我们的输出应该类似于以下内容:

```
 UserTransaction("A", 100001)
```

让我们开始这个测试，并检查执行单个集成测试需要多长时间，如以下输出所示:

![](assets/e492184d-4a2b-4069-bb7f-86937d6c0393.png)

执行这个简单的测试大约需要`646 ms`时间。

如果我们想要覆盖每一个边缘情况，与上一节的单元测试相比，这个值将会乘以 100 倍。让我们从三个边缘案例开始这个单元测试，如以下输出所示:

![](assets/4cb08050-118a-499c-8694-469a568f3a1e.png)

我们可以看到，我们的测试只花了`18 ms`，这意味着它比只有一个案例的集成测试快了 20 倍，即使我们覆盖了三个边缘案例。

在这里，我们已经用数百个边缘案例覆盖了许多逻辑，我们可以得出结论，在尽可能低的级别进行单元测试确实是明智的。

在下一节中，我们将使用部分函数来模拟数据源。

# 使用部分函数模拟数据源

在本节中，我们将涵盖以下主题:

*   创建一个从 Hive 读取数据的 Spark 组件
*   嘲笑组件
*   测试模拟组件

让我们假设以下代码是我们的生产线:

```
 ignore("loading data on prod from hive") {
 UserDataLogic.loadAndGetAmount(spark, HiveDataLoader.loadUserTransactions)
 }
```

这里我们使用的是`UserDataLogic.loadAndGetAmount`函数，它需要加载我们的用户数据交易，并得到交易的金额。这个方法需要两个参数。第一个参数是`sparkSession`第二个参数是`sparkSession`的`provider`，取`SparkSession`返回`DataFrame`，如下例所示:

```
object UserDataLogic {
  def loadAndGetAmount(sparkSession: SparkSession, provider: SparkSession => DataFrame): DataFrame = {
    val df = provider(sparkSession)
    df.select(df("amount"))
  }
}
```

对于生产，我们将加载用户事务，看到`HiveDataLoader`组件只有一个方法、`sparkSession.sql`和`("select * from transactions")`，如下面的代码块所示:

```
object HiveDataLoader {
 def loadUserTransactions(sparkSession: SparkSession): DataFrame = {
 sparkSession.sql("select * from transactions")
 }
}
```

这意味着该函数进入 Hive 检索我们的数据并返回一个数据帧。根据我们的逻辑，它执行返回数据帧的`provider`，而从数据帧中，它只选择`amount`。

这个逻辑并不简单，我们可以测试，因为我们的 SparkSession `provider`正在生产中与外部系统交互。因此，我们可以创建如下函数:

```
UserDataLogic.loadAndGetAmount(spark, HiveDataLoader.loadUserTransactions)
```

让我们看看如何测试这样的组件。首先，我们将创建一个用户事务的数据框架，这是我们的模拟数据，如下例所示:

```
 val df = spark.sparkContext
 .makeRDD(List(UserTransaction("a", 100), UserTransaction("b", 200)))
 .toDF()
```

但是，我们需要将数据保存到 Hive 中，嵌入它，然后启动 Hive。

因为我们使用的是分部函数，所以我们可以传递分部函数作为第二个参数，如下例所示:

```
val res = UserDataLogic.loadAndGetAmount(spark, _ => df)
```

第一个参数是`spark`，但是这次在我们的方法中没有用到。第二个参数是一个接受 SparkSession 并返回数据帧的方法。

然而，我们的执行引擎、架构和代码并没有考虑是否使用这个 SparkSession 或者是否进行了外部调用；它只想返回数据帧。我们可以`_`我们的第一个参数，因为它被忽略了，只返回 DataFrame 作为返回类型。

所以我们的`loadAndGetAmount`会得到一个模拟的数据帧，也就是我们创建的数据帧。

但是，对于所示的逻辑，它是透明的，不考虑数据帧是否来自 Hive、SQL、Cassandra 或任何其他来源，如下例所示:

```
 val df = provider(sparkSession)
 df.select(df("amount"))
```

在我们的例子中，`df`来自于我们为了测试而创建的内存。我们的逻辑继续，它选择金额。

然后，我们显示我们的列，`res.show()`，这个逻辑应该以一个列的数量结束。让我们开始这个测试，如下例所示:

![](assets/8dca11c2-8423-432f-9233-897f3fba401b.png)

从前面的例子中我们可以看到，我们得到的数据帧在`100`和`200`值中有一个列量。这意味着它像预期的那样工作，不需要开始嵌入 Hive。这里的关键是使用一个提供者，而不是在我们的逻辑中嵌入我们的选择开始。

在下一节中，我们将使用 ScalaCheck 进行基于属性的测试。

# 使用 ScalaCheck 进行基于属性的测试

在本节中，我们将涵盖以下主题:

*   基于属性的测试
*   创建基于属性的测试

让我们看一个简单的基于属性的测试。我们需要在定义属性之前导入依赖关系。我们还需要 ScalaCheck 库的依赖项，这是一个基于属性的测试库。

在前一节中，每个测试都扩展了`FunSuite`。我们使用了功能测试，但是我们必须显式地提供参数。在这个例子中，我们从 ScalaCheck 库中扩展`Properties`，并测试一个`StringType`，如下所示:

```
object PropertyBasedTesting extends Properties("StringType")
```

我们的 ScalaCheck 将为我们生成一个随机字符串。如果我们为一个自定义类型创建一个基于属性的测试，那么 ScalaCheck 不知道这一点。我们需要提供一个生成器来生成特定类型的实例。

首先，让我们用以下方式定义字符串类型的第一个属性:

```
property("length of strings") = forAll { (a: String, b: String) =>
 a.length + b.length >= a.length
 }
```

`forAll`是来自 ScalaCheck 属性的方法。我们将在这里传递任意数量的参数，但是它们必须是我们正在测试的类型。

让我们假设我们想要得到两个随机字符串，在这些字符串中，不变量应该被感知。

如果我们将弦的长度`a`加到弦的长度`b`上，两者之和应该大于或等于`a.length,`，因为如果`b`是`0`，那么它将是相等的，如下例所示:

```
a.length + b.length >= a.length
```

然而，这是`string`的一个不变量，对于每个输入字符串，它应该是`true`。

我们正在定义的第二个属性稍微复杂一点，如下面的代码所示:

```
property("creating list of strings") = forAll { (a: String, b: String, c: String) =>
 List(a,b,c).map(_.length).sum == a.length + b.length + c.length
 }
```

在前面的代码中，我们要求 ScalaCheck 运行时引擎这次共享三个字符串，即`a`、`b`和`c`。我们将在创建字符串列表时对此进行测试。

这里，我们正在创建一个字符串列表，即，`a`、`b`、`c`，如下面的代码所示:

```
List(a,b,c)
```

当我们将每个元素映射到`length`时，这些元素的总和应该等于将所有元素按长度相加。在这里，我们有`a.length + b.length + c.length`，我们将测试集合 API，以检查地图和其他功能是否按预期工作。

让我们开始这个基于属性的测试，检查我们的属性是否正确，如下例所示:

![](assets/9319cfda-c10b-40bf-8d3b-9e41fa26fe7f.png)

我们可以看到`string`的`StringType.length`属性通过并执行了`100`测试。执行`100`测试可能会令人惊讶，但是让我们使用下面的代码来尝试看看作为参数传递了什么:

```
println(s"a: $a, b: $b")
```

我们将打印`a`参数和`b`参数，并通过测试以下输出来重试我们的属性:

![](assets/a7df281d-ce0d-408d-a174-96b691f2a45b.png)

我们可以看到生成了很多奇怪的字符串，所以这是一个我们无法预先创建的边缘情况。基于属性的测试将创建一个非常奇怪的独特的代码，这不是一个合适的字符串。因此，这是一个很好的工具，可以测试我们的逻辑对于特定类型是否如预期的那样工作。

在下一部分，我们将在不同版本的 Spark 中进行测试。

# 在不同版本的 Spark 中进行测试

在本节中，我们将涵盖以下主题:

*   更改组件以使用 Spark 2 . x 之前的版本
*   2.x 之前的模拟测试
*   RDD 模拟测试

让我们从本章第三节的嘲讽数据源开始——*使用部分函数*嘲讽数据源。

由于我们正在测试`UserDataLogic.loadAndGetAmount`，请注意，所有的东西都在数据帧上运行，因此我们有一个火花会话和数据帧。

现在，让我们将其与 Spark 2 . x 之前的版本进行比较。我们可以看到，这一次，我们无法使用数据帧。让我们假设下面的例子显示了我们之前的 Sparks 的逻辑:

```
test("mock loading data from hive"){
 //given
 import spark.sqlContext.implicits._
 val df = spark.sparkContext
 .makeRDD(List(UserTransaction("a", 100), UserTransaction("b", 200)))
 .toDF()
 .rdd
//when
 val res = UserDataLogicPre2.loadAndGetAmount(spark, _ => df)
//then
 println(res.collect().toList)
 }
}
```

我们可以看到，这次我们无法使用数据帧。

在上一节中，`loadAndGetAmount`取的是`spark`和数据帧，但是下面例子中的数据帧是 RDD，不再是数据帧了，所以我们通过了一个`rdd`:

```
 val res = UserDataLogicPre2.loadAndGetAmount(spark, _ => rdd)
```

然而，我们需要为 Spark 创建一个不同的`UserDataLogicPre2`，它接受 SparkSession 并在映射一个整数的 RDD 之后返回一个 RDD，如下例所示:

```
object UserDataLogicPre2 {
 def loadAndGetAmount(sparkSession: SparkSession, provider: SparkSession => RDD[Row]): RDD[Int] = {
 provider(sparkSession).map(_.getAs[Int]("amount"))
 }
}
object HiveDataLoaderPre2 {
 def loadUserTransactions(sparkSession: SparkSession): RDD[Row] = {
 sparkSession.sql("select * from transactions").rdd
 }
}
```

在前面的代码中，我们可以看到`provider`正在执行我们的提供者逻辑，映射每个元素，将其作为`int`获取。然后，我们得到金额。`Row`是一个泛型类型，可以有可变数量的参数。

In Spark pre-2.x, we do not have `SparkSession` and therefore we need to use `SparkContext` and change our login accordingly.

# 摘要

在本章中，我们首先学习了如何从 Spark 引擎中分离逻辑。然后，我们查看了一个在没有 Spark 引擎的情况下在分离中经过良好测试的组件，并使用 SparkSession 进行了集成测试。为此，我们通过重用已经过良好测试的组件来创建一个 SparkSession 测试。通过这样做，我们不必覆盖集成测试中的所有边缘情况，并且我们的测试要快得多。然后，我们学习了如何利用部分函数来提供测试阶段提供的模拟数据。我们还介绍了基于属性测试的 ScalaCheck。到本章结束时，我们已经在不同版本的 Spark 中测试了我们的代码，并学习了如何将我们的 DataFrame 模拟测试更改为 RDD。

在下一章中，我们将学习如何利用 Spark GraphX API。