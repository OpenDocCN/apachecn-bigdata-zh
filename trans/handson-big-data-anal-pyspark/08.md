# 不变设计

在这一章中，我们将看看 Apache Spark 不可改变的设计。我们将深入研究 Spark RDD 的父/子链，并以不可改变的方式使用 RDD。然后，我们将使用数据框操作进行转换，以讨论高度并发环境中的不变性。在本章结束时，我们将以不可变的方式使用数据集应用编程接口。

在本章中，我们将涵盖以下主题:

*   探究火花 RDD 的亲子链
*   以不可改变的方式使用 RDD
*   使用数据框操作进行转换
*   高度并发环境中的不变性
*   以不可变的方式使用数据集应用编程接口

# 探究火花 RDD 的亲子链

在本节中，我们将尝试实现我们自己的 RDD，它继承了 RDD 的父属性。

我们将讨论以下主题:

*   扩展 RDD
*   把一个新的 RDD 和父母联系起来
*   测试我们的定制 RDD

# 扩展 RDD

这是一个简单的测试，有很多隐藏的复杂性。让我们从创建记录列表开始，如下面的代码块所示:

```py
class InheritanceRdd extends FunSuite {
  val spark: SparkContext = SparkSession
    .builder().master("local[2]").getOrCreate().sparkContext

  test("use extended RDD") {
    //given
    val rdd = spark.makeRDD(List(Record(1, "d1")))
```

`Record`只是一个有`amount``description`的案例类，所以`amount`是`1``d1`是描述。

然后我们创建`MultipledRDD`并将`rdd`传递给它，然后将乘数设置为等于`10`，如下代码所示:

```py
val extendedRdd = new MultipliedRDD(rdd, 10)
```

我们正经过母公司 RDD，因为它有在另一个 RDD 加载的数据。这样，我们构建了两个 RDD 的传承链。

# 把一个新的 RDD 和父母联系起来

我们首先创建了一个多重 RDD 类。在`MultipliedRDD`类中，我们有两个传递参数的东西:

*   简要的 RDD 的记录，也就是`RDD[Record]`
*   一个乘数，即`Double`

在我们的例子中，可能有多个 RDD 的连锁，这意味着在我们的 RDD 可能有多个 RDD。所以，这并不总是所有有向无环图的父图。我们只是延长了 RDD 的类型记录，所以我们需要通过延长的 RDD。

RDD 有很多方法，我们可以推翻任何我们想要的方法。然而，这一次，我们将使用`compute`方法，我们将覆盖计算方法来计算乘数。在这里，我们得到一个`Partition`分裂和`TaskContext`。这些由这个部分执行引擎传递给我们的方法，所以我们不需要担心这个。然而，我们需要返回与我们在继承链中通过 RDD 类传递的类型完全相同的迭代器。这将是记录的迭代器。

然后，我们执行第一个父逻辑，其中第一个父逻辑只是获取我们链中的第一个 RDD。这里的类型是`Record`，我们取的是`split`和`context`的`iterator`，这里的`split`只是一个将要执行的分区。我们知道 Spark RDD 是由分区器分区的，但是，在这里，我们只是得到了我们需要分割的特定分区。因此，迭代器获取分区和任务上下文，因此它知道应该从迭代方法中返回哪些值。对于迭代器中的每一条记录，也就是一个`salesRecord`，像`amount`和`description`，我们将`amount`乘以传递给构造函数的`multiplier`，得到我们的`Double`。

通过这样做，我们将我们的金额乘以乘数，然后我们可以返回具有新金额的新记录。所以，我们现在有了旧记录乘以我们的`multiplier`和`salesRecord`的描述。对于第二个过滤器，我们需要`override`是`getPartitions`，因为我们想要保持父 RDD 的分区。例如，如果以前的 RDD 有 100 个分区，我们也希望我们的`MultipledRDD`有 100 个分区。因此，我们希望保留分区信息，而不是丢失它。出于同样的原因，我们只是将其代理到`firstParent`。RDD 的`firstParent`将会从那个特定的 RDD 带走之前的分区。

这样，我们创建了一个新的`multipliedRDD`，它传递了父级和乘数。对于我们的`extendedRDD`，我们需要`collect`将其调用`toList`，我们的列表应该有`10`和`d1`，如下例所示:

```py
extendedRdd.collect().toList should contain theSameElementsAs List(
 Record(10, "d1")
 )
 }
}
```

Compute was executed automatically when we created the new RDD, and so it is always executed without the explicit method call.

# 测试我们的定制 RDD

让我们开始这个测试，看看这是否创造了我们的 RDD。通过这样做，我们可以扩展我们的父 RDD，并将行为添加到我们的 RDD。这显示在下面的截图中:

```py
"C:\Program Files\Java\jdk-12\bin\java.exe" "-javaagent:C:\Program Files\JetBrains\IntelliJ IDEA 2018.3.5\lib\idea_rt.jar=51687:C:\Program Files\JetBrains\IntelliJ IDEA 2018.3.5\bin" -Dfile.encoding=UTF-8 -classpath C:\Users\Sneha\IdeaProjects\Chapter07\out\production\Chapter07 com.company.Main

Process finished with exit code 0
```

在下一节中，我们将以不变的方式使用 RDD。

# 以不可改变的方式使用 RDD

既然我们知道了如何使用 RDD 继承创建一个执行链，那么让我们学习如何以一种不可改变的方式使用 RDD。

在本节中，我们将讨论以下主题:

*   理解 DAG 不变性
*   从一根 RDD 上长出两片叶子
*   检查两片叶子的结果

我们先来了解有向无环图的不变性，以及它给我们带来了什么。然后，我们将从一个节点 RDD 创建两个叶子，并检查如果我们在一个叶子 RDD 上创建转换，这两个叶子是否完全独立。然后，我们将检查当前 RDD 的两个叶子的结果，并检查任何叶子上的任何转化是否不改变或影响根 RDD。必须这样工作，因为我们已经发现，我们将无法从根 RDD 创造另一片叶子，因为根 RDD 将被改变，这意味着它将是可变的。为了克服这一点，星火设计师为我们创造了一个不可改变的 RDD。

有一个简单的测试表明，RDD 应该是不可改变的。首先，我们将从`0 to 5`创建一个 RDD，它被添加到来自 Scala 分支的序列中。`to`取的是`Int`，第一个参数是隐式的，来自 Scala 包，如下例所示:

```py
class ImmutableRDD extends FunSuite {
    val spark: SparkContext = SparkSession
        .builder().master("local[2]").getOrCreate().sparkContext

test("RDD should be immutable") {
    //given
    val data = spark.makeRDD(0 to 5)
```

一旦我们有了 RDD 数据，我们就可以创建第一片叶子。第一片叶子是一个结果(`res`)，我们只是映射每一个乘以`2`的元素。让我们创建第二个叶子，但是这次它将被`4`标记，如下例所示:

```py
//when
val res = data.map(_ * 2)

val leaf2 = data.map(_ * 4)
```

我们有根 RDD 和两片叶子。首先，我们将收集第一片叶子，看到其中的元素是`0, 2, 4, 6, 8, 10`，所以这里的一切都乘以`2`，如下例所示:

```py
//then
res.collect().toList should contain theSameElementsAs List(
    0, 2, 4, 6, 8, 10
)
```

然而，即使我们在`res`上有那个通知，数据还是和开始时完全一样，就是`0, 1, 2, 3, 4, 5`，如下例所示:

```py
data.collect().toList should contain theSameElementsAs List(
    0, 1, 2, 3, 4, 5
    )
  }
}
```

所以，一切都是不可变的，执行`* 2`的转换并没有改变我们的数据。如果我们为`leaf2`创建一个测试，我们将`collect`称之为`toList`。我们会看到它应该像`0, 4, 8, 12, 16, 20`一样`contain`元素，如下例所示:

```py
leaf2.collect().toList should contain theSameElementsAs List(
 0, 4, 8, 12, 16, 20
)
```

当我们运行测试时，我们将看到执行中的每个路径，根，即数据，或者第一个叶和第二个叶，彼此独立地运行，如下面的代码输出所示:

```py
"C:\Program Files\Java\jdk-12\bin\java.exe" "-javaagent:C:\Program Files\JetBrains\IntelliJ IDEA 2018.3.5\lib\idea_rt.jar=51704:C:\Program Files\JetBrains\IntelliJ IDEA 2018.3.5\bin" -Dfile.encoding=UTF-8 -classpath C:\Users\Sneha\IdeaProjects\Chapter07\out\production\Chapter07 com.company.Main

Process finished with exit code 0
```

每个突变都不一样；我们可以看到，考验通过了，这向我们表明，我们的 RDD 是不可改变的。

# 使用数据框操作进行转换

来自应用编程接口的数据下面有一个 RDD，所以数据帧不可能是可变的。在 DataFrame 中，不变性甚至更好，因为我们可以在不改变源数据集的情况下，动态地在其中添加和减去列。

在本节中，我们将涵盖以下主题:

*   理解数据帧的不变性
*   从一个根数据帧创建两片叶子
*   通过发出转换来添加新列

我们将首先使用来自操作的数据来转换我们的数据框架。首先，我们需要理解数据帧的不变性，然后我们将创建两个叶子，但这次是从一个根数据帧开始。然后我们将发布一个与 RDD 有点不同的转变。这将为我们生成的数据框添加一个新列，因为我们在数据框中是这样操作的。如果我们想要映射数据，那么我们需要从第一列获取数据，转换它，并将其保存到另一列，然后我们将有两列。如果我们不再感兴趣，我们可以删除第一列，但结果将是另一个数据框。

因此，我们将有第一个包含一列的数据帧，第二个包含结果和源，第三个只有一个结果。让我们看看这一部分的代码。

我们将创建一个数据框架，所以我们需要调用`toDF()`方法。我们正在创建`UserData`，其中`"a"`为`"1"`、`"b"`为`"2"`、`"d"`为`"200"`。`UserData`有`userID`和`data`，两个字段都是`String`，如下例所示:

```py
test("Should use immutable DF API") {
 import spark.sqlContext.implicits._
 //given
 val userData =
 spark.sparkContext.makeRDD(List(
 UserData("a", "1"),
 UserData("b", "2"),
 UserData("d", "200")
 )).toDF()
```

在测试中使用 case 类创建 RDD 很重要，因为当我们被调用到 DataFrame 时，这一部分将相应地推断模式和名称列。下面的代码以此为例，我们只从位于`"a"`的`userData`中过滤一个`userID`列:

```py
//when
    val res = userData.filter(userData("userId").isin("a"))
```

我们的结果应该只有一条记录，所以我们删除了两列，但是我们创建的`userData`源仍然会有`3`行。因此，通过过滤对其进行修改会创建另一个数据帧，我们称之为`res`，而不修改输入`userData`，如下例所示:

```py
    assert(res.count() == 1)
    assert(userData.count() == 3)

    }
}
```

因此，让我们开始这个测试，看看来自 API 的不可变数据是如何表现的，如下面的截图所示:

```py
"C:\Program Files\Java\jdk-12\bin\java.exe" "-javaagent:C:\Program Files\JetBrains\IntelliJ IDEA 2018.3.5\lib\idea_rt.jar=51713:C:\Program Files\JetBrains\IntelliJ IDEA 2018.3.5\bin" -Dfile.encoding=UTF-8 -classpath C:\Users\Sneha\IdeaProjects\Chapter07\out\production\Chapter07 com.company.Main

Process finished with exit code 0
```

如我们所见，我们的测试通过了，从结果(`res`)中，我们知道我们的父母没有被修改。例如，如果我们想在`res.map()`上映射一些东西，我们可以映射`userData`列，如下例所示:

```py
res.map(a => a.getString("userId") + "can")
```

另一个叶将有一个额外的列，而不改变`userId`源代码，所以这是数据帧的不变性。

# 高度并发环境中的不变性

我们看到了不变性如何影响程序的创建和设计，所以现在我们将了解它是如何有用的。

在本节中，我们将涵盖以下主题:

*   可变集合的缺点
*   创建同时修改可变集合的两个线程
*   关于并发程序的推理

让我们首先了解可变集合的原因。为此，我们将创建两个同时修改可变集合的线程。我们将在测试中使用这些代码。首先，我们将创建一个`ListBuffer`，这是一个可变列表。然后，我们可以添加和删除链接，而无需创建另一个列表进行任何修改。然后，我们可以创建一个具有两个线程的`Executors`服务。我们需要两个线程同时启动来修改状态。稍后，我们将使用来自`Java.util.concurrent:`的`CountDownLatch`构造。这在以下示例中显示:

```py
import java.util.concurrent.{CountDownLatch, Executors}
import org.scalatest.FunSuite
import scala.collection.mutable.ListBuffer
class MultithreadedImmutabilityTest extends FunSuite {

test("warning: race condition with mutability") {
//given
var listMutable = new ListBuffer[String]()
val executors = Executors.newFixedThreadPool(2)
val latch = new CountDownLatch(2)
```

`CountDownLatch`是一个帮助我们停止线程处理直到我们请求它们的构造。我们需要用我们的逻辑等待，直到两个线程都开始执行。然后，我们向`executors`提交一个`Runnable`，我们的`run()`方法通过在准备好行动时发出信号来执行`countDown()`，并将`"A"`附加到`listMutable`，如下例所示:

```py
 //when
 executors.submit(new Runnable {
     override def run(): Unit = {
         latch.countDown()
         listMutable += "A"
     }
 })
```

然后，另一个线程启动，并且也通过发信号通知准备启动来使用`countDown`。但是首先，它会检查列表是否包含`"A"`，如果不包含，它会追加那个`"A"`，如下例所示:

```py
 executors.submit(new Runnable {
     override def run(): Unit = {
         latch.countDown()
         if(!listMutable.contains("A")) {
             listMutable += "A"
         }
     }
 })
```

然后我们使用`await()`等待`countDown`发出，当它发出时，我们能够继续验证我们的程序，如下例所示:

```py
    latch.await()
```

`listMutable`包含`"A"`或者可以有`"A","A"`。`listMutable`检查列表是否包含`"A"`，如果不包含，则不会添加该元素，如下例所示:

```py
    //then
    //listMutable can have ("A") or ("A","A")
    }
}
```

但是这里有一个比赛条件。在检查`if(!listMutable.contains("A"))`之后，`run()`线程可能会将`"A"`元素添加到列表中。但是我们在`if`里面，所以我们将通过/使用`listMutable += "A"`添加另一个`"A"`。由于状态的可变性以及它是通过另一个线程修改的事实，我们可以有`"A"`或`"A","A"`。

我们在使用可变状态时需要小心，因为我们不能有这样一个损坏的状态。为了缓解这个问题，我们可以在上面使用`java.util`集合和同步列表。

但是如果我们有同步块，那么我们的程序会非常慢，因为我们需要专门协调对它的访问。我们也可以从`java.util.concurrent.locks`套餐中雇佣`lock`。我们可以使用一个实现，比如`ReadLock`或者`WriteLock`。在下面的例子中，我们将使用`WriteLock`:

```py
val lock = new WriteLock()
```

我们还需要`lock`我们的`lock()`然后才继续，如下例所示:

```py
lock.lock()
```

以后我们可以用`unlock()`。但是，我们也应该在第二个线程中这样做，这样我们的列表只有一个元素，如下例所示:

```py
lock.unlock()
```

输出如下:

![](img/9b19108c-26d0-4115-92a9-8191a1900bdb.png)

锁定是一项非常困难且昂贵的操作，因此不变性是性能程序的关键。

# 以不可变的方式使用数据集应用编程接口

在本节中，我们将以不可变的方式使用数据集应用编程接口。我们将涵盖以下主题:

*   数据集不变性
*   从一个根数据集创建两片叶子
*   通过发出转换来添加新列

数据集的测试用例非常相似，但是我们需要做一个`toDS()`来保证数据的类型安全。数据集的类型为`userData`，如下例所示:

```py
import com.tomekl007.UserData
import org.apache.spark.sql.SparkSession
import org.scalatest.FunSuite

class ImmutableDataSet extends FunSuite {
 val spark: SparkSession = SparkSession
 .builder().master("local[2]").getOrCreate()

test("Should use immutable DF API") {
 import spark.sqlContext.implicits._
 //given
 val userData =
 spark.sparkContext.makeRDD(List(
 UserData("a", "1"),
 UserData("b", "2"),
 UserData("d", "200")
 )).toDF()
```

现在，我们将发出`userData`的过滤器，并指定`isin`，如下例所示:

```py
   //when
    val res = userData.filter(userData("userId").isin("a"))
```

它将返回结果(`res`)，这是一片带有我们的`1`元素的叶子。`userData`仍然会有`3`元素，因为这个明显的根。让我们执行这个程序，如下例所示:

```py
    assert(res.count() == 1)
    assert(userData.count() == 3)

 }
}
```

我们可以看到我们的测试通过了，这意味着数据集也是 DataFrame 之上不可改变的抽象，并且采用了相同的特性。`userData`有一个非常有用的东西叫做排版，如果你使用`show()`方法，它会推断出模式并知道`"a"`字段是一个字符串或另一种类型，如下例所示:

```py
userData.show()
```

输出如下:

```py
+------+----+
|userId|data|
|----- |----|
|     a|   1|
|     b|   2|
|     d| 200|
+------|----+ 
```

在前面的输出中，我们有`userID`和`data`字段。

# 摘要

在这一章中，我们深入研究了 Spark RDD 父子链，并创建了一个乘数 RDD，该乘数能够基于父级 RDD 以及父级上的分区方案来计算一切。我们以不可改变的方式使用 RDD。我们看到从父级创建的叶子的修改没有修改零件。我们还学习了一个更好的抽象，也就是一个数据框架，所以我们知道我们可以在那里使用转换。然而，每一个转换都只是添加到另一个列中——它没有在适当的位置修改任何东西。接下来，我们只需在高度并发的环境中设置不变性。我们看到了当访问多个线程时可变状态是多么糟糕。最后，我们看到数据集应用编程接口也是以一种不可变的方式创建的，我们可以在这里利用这些东西。

在下一章，我们将看看如何避免洗牌和减少个人开支。