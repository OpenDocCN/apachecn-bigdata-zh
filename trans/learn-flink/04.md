# 第四章。使用表格应用编程接口的数据处理

在前面的章节中，我们讨论了 Apache Flink 提供的批处理和流数据处理 API。在这一章中，我们将讨论表应用编程接口，这是一个在 Flink 中用于数据处理的 SQL 接口。表应用编程接口在一个可以从数据集和数据流创建的表接口上运行。一旦数据集/数据流注册为表，我们就可以自由地应用关系操作，如聚合、连接和选择。

也可以像常规的 SQL 查询一样查询表。一旦执行了操作，我们需要将表转换回数据集或数据流。Apache Flink 内部使用另一个名为 Apache calculate[https://calcite.apache.org/](https://calcite.apache.org/)的开源项目来优化这些查询转换。

在本章中，我们将涵盖以下主题:

*   注册表格
*   访问已注册的表
*   经营者
*   数据类型
*   结构化查询语言

现在让我们开始吧。

为了使用 Table API，我们首先需要做的是创建一个 Java Maven 项目，并在其中添加以下依赖项:

```scala
  <dependency> 
      <groupId>org.apache.flink</groupId> 
      <artifactId>flink-table_2.11</artifactId> 
      <version>1.1.4</version> 
    </dependency> 

```

这个依赖项将下载类路径中所有需要的 JARs。一旦下载完成，我们都可以使用 Table API 了。

# 登记表格

为了对数据集/数据流进行操作，首先我们需要在`TableEnvironment`中注册一个表。一旦该表注册了一个唯一的名称，就可以很容易地从`TableEnvironment`访问它。

`TableEnvironment`维护表登记的内部表目录。下图显示了详细信息:

![Registering tables](img/B05653_image1.jpg)

拥有唯一的表名是非常重要的，否则你会得到一个异常。

## 注册数据集

为了对数据集执行 SQL 操作，我们需要在`BatchTableEnvironment`中将其注册为表。我们需要在注册表时定义一个 Java POJO 类。

例如，假设我们需要注册一个名为单词计数的数据集。该表中的每条记录都有单词和频率属性。相同的 Java POJO 如下所示:

```scala
public static class WC { 
    public String word; 
    public long frequency; 
    public WC(){ 
    } 

    public WC(String word, long frequency) { 
      this.word = word; 
      this.frequency = frequency; 
    } 

    @Override 
    public String toString() { 
      return "WC " + word + " " + frequency; 
    } 
  } 

```

Scala 中的同一个类可以定义如下:

```scala
case class WordCount(word: String, frequency: Long) 

```

现在我们可以登记这张桌子了。

在 Java 中:

```scala
ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); 

BatchTableEnvironment tEnv = TableEnvironment.getTableEnvironment(env); 

DataSet<WC> input = env.fromElements(new WC("Hello", 1), new WC("World", 1), new WC("Hello", 1)); 

// register the DataSet as table "WordCount" 
tEnv.registerDataSet("WordCount", input, "word, frequency"); 

```

在 Scala 中:

```scala
val env = ExecutionEnvironment.getExecutionEnvironment 

val tEnv = TableEnvironment.getTableEnvironment(env) 

val input = env.fromElements(WordCount("hello", 1), WordCount("hello", 1), WordCount("world", 1), WordCount("hello", 1)) 

//register the dataset  
tEnv.registerDataSet("WordCount", input, 'word, 'frequency) 

```

### 注

请注意，数据集表的名称不能与`^_DataSetTable_[0-9]+`模式匹配，因为它是为内部内存使用而保留的。

## 注册数据流

类似于数据集，我们也可以在`StreamTableEnvironment`注册一个数据流。我们需要在注册表时定义一个 Java POJO 类。

例如，假设我们需要注册一个名为字数统计的数据流。该表中的每条记录都有一个单词和频率属性。相同的 Java POJO 如下所示:

```scala
public static class WC { 
    public String word; 
    public long frequency; 

    public WC() { 
    }s 
    public WC(String word, long frequency) { 
      this.word = word; 
      this.frequency = frequency; 
    } 

    @Override 
    public String toString() { 
      return "WC " + word + " " + frequency; 
    } 
  } 

```

Scala 中的同一个类可以定义如下:

```scala
case class WordCount(word: String, frequency: Long) 

```

现在我们可以登记这张桌子了。

在 Java 中:

```scala
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); 
    StreamTableEnvironment tEnv = TableEnvironment.getTableEnvironment(env); 

    DataStream<WC> input = env.fromElements(new WC("Hello", 1), new WC("World", 1), new WC("Hello", 1)); 

    // register the DataStream as table "WordCount" 
    tEnv.registerDataStream("WordCount", input, "word, frequency"); 

```

在 Scala 中:

```scala
val env = StreamExecutionEnvironment.getExecutionEnvironment 

val tEnv = TableEnvironment.getTableEnvironment(env) 

val input = env.fromElements(WordCount("hello", 1), WordCount("hello", 1), WordCount("world", 1), WordCount("hello", 1)) 

//register the dataset  
tEnv.registerDataStream("WordCount", input, 'word, 'frequency) 

```

### 注

请注意，数据流表的名称不能与`^_DataStreamTable_[0-9]+`模式匹配，因为它是为内部内存使用而保留的。

## 注册表格

类似于数据集和数据流，我们也可以注册一个源自表应用编程接口的表。

在 Java 中:

```scala
ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); 
BatchTableEnvironment tEnv = TableEnvironment.getTableEnvironment(env); 

DataSet<WC> input = env.fromElements(new WC("Hello", 1), new WC("World", 1), new WC("Hello", 1)); 

tEnv.registerDataSet("WordCount", input, "word, frequency"); 

Table selectedTable = tEnv 
        .sql("SELECT word, SUM(frequency) as frequency FROM WordCount GROUP BY word having word = 'Hello'"); 

tEnv.registerTable("selected", selectedTable); 

```

在 Scala 中:

```scala
val env = ExecutionEnvironment.getExecutionEnvironment 

val tEnv = TableEnvironment.getTableEnvironment(env) 

val input = env.fromElements(WordCount("hello", 1), WordCount("hello", 1), WordCount("world", 1), WordCount("hello", 1)) 

tEnv.registerDataSet("WordCount", input, 'word, 'frequency) 

val table = tEnv.sql("SELECT word, SUM(frequency) FROM WordCount GROUP BY word") 

val selected = tEnv.sql("SELECT word, SUM(frequency) FROM WordCount GROUP BY word where word = 'hello'") 
    tEnv.registerTable("selected", selected) 

```

## 注册外部表源

Flink 允许我们使用`TableSource`注册外部表。表源可以允许我们访问存储在数据库中的数据，如 MySQL 和 Hbase。文件系统，如 CSV、Parquet 和 ORC，或者您也可以阅读消息系统，如 RabbitMQ 和 Kafka。

目前，Flink 允许使用 CSV 源从 CSV 文件中读取数据，并使用卡夫卡源从卡夫卡主题中读取 JSON 数据。

### CSV 表格来源

现在让我们看看如何使用 CSV 源直接读取数据，然后在表环境中注册该源。

默认情况下，`flink-table` API JAR 中提供了一个 CSV 源，因此不需要添加任何其他额外的 Maven 依赖项。下面的依赖关系已经足够好了:

```scala
    <dependency> 
      <groupId>org.apache.flink</groupId> 
      <artifactId>flink-table_2.11</artifactId> 
      <version>1.1.4</version> 
    </dependency> 

```

下面的代码片段显示了如何读取 CSV 文件和注册表源。

在 Java 中:

```scala
ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); 
BatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env); 

TableSource orders = new CsvTableSource("/path/to/file", ...) 

// register a TableSource as external table "orders" 
tableEnv.registerTableSource("orders", orders) 

```

在 Scala 中:

```scala
val env = ExecutionEnvironment.getExecutionEnvironment 
val tableEnv = TableEnvironment.getTableEnvironment(env) 

val orders: TableSource = new CsvTableSource("/path/to/file", ...) 

// register a TableSource as external table "orders" 
tableEnv.registerTableSource("orders", orders) 

```

### 卡夫卡 JSON 表来源

我们也可以在表环境中注册 Kafka JSON 表源。为了使用这个应用编程接口，我们需要添加以下两个依赖项:

第一个是表应用编程接口:

```scala
<dependency> 
      <groupId>org.apache.flink</groupId> 
      <artifactId>flink-table_2.11</artifactId> 
      <version>1.1.4</version> 
    </dependency> 

```

第二个依赖是卡夫卡弗林克连接器:

*   如果您使用的是卡夫卡 0.8，请应用:

```scala
        <dependency> 
            <groupId>org.apache.flink</groupId> 
            <artifactId>flink-connector-kafka-0.8_2.11</artifactId> 
            <version>1.1.4</version> 
        </dependency> 

```

*   如果您使用的是卡夫卡 0.9，请应用:

```scala
        <dependency> 
            <groupId>org.apache.flink</groupId> 
            <artifactId>flink-connector-kafka-0.9_2.11</artifactId> 
            <version>1.1.4</version> 
        </dependency> 

```

现在我们需要编写如下代码片段所示的代码:

```scala
String[] fields =  new String[] { "id", "name", "price"}; 
Class<?>[] types = new Class<?>[] { Integer.class, String.class, Double.class }; 

KafkaJsonTableSource kafkaTableSource = new Kafka08JsonTableSource( 
    kafkaTopic, 
    kafkaProperties, 
    fields, 
    types); 

tableEnvironment.registerTableSource("kafka-source", kafkaTableSource); 
Table result = tableEnvironment.ingest("kafka-source"); 

```

在前面的代码中，我们为 Kafka 0.8 定义了 Kafka 源，然后在表环境中注册该源。

# 访问已注册的表

一旦表被注册，我们就可以很容易地从`TableEnvironment`访问它，如下所示:

```scala
tableEnvironment.scan("tableName") 

```

前面的语句扫描了在`BatchTableEnvironment`中注册了名称`"tableName"`的表:

```scala
tableEnvironment.ingest("tableName") 

```

前面的陈述引用了在`StreamTableEnvironment`中注册了名称`"tableName"`的表格:

# 操作员

Flink 的 Table API 提供了各种运算符，作为其特定领域语言的一部分。大多数操作符都可以在 Java 和 Scala APIs 中找到。让我们一个接一个地看那些操作符。

## 选择运算符

`select`运算符就像一个 SQL select 运算符，它允许您选择表中的各种属性/列。

在 Java 中:

```scala
Table result = in.select("id, name"); 
Table result = in.select("*"); 

```

在 Scala 中:

```scala
val result = in.select('id, 'name); 
val result = in.select('*); 

```

## 操作符在哪里

`where`运算符用于过滤出结果。

在 Java 中:

```scala
Table result = in.where("id = '101'"); 

```

在 Scala 中:

```scala
val result = in.where('id == "101"); 

```

## 过滤器操作符

`filter`操作符可以作为`where`操作符的替代。

在 Java 中:

```scala
Table result = in.filter("id = '101'"); 

```

在 Scala 中:

```scala
val result = in.filter('id == "101"); 

```

## as 运算符

`as`运算符用于重命名字段:

在 Java 中:

```scala
Table in = tableEnv.fromDataSet(ds, "id, name"); 
Table result = in.as("order_id, order_name"); 

```

在 Scala 中:

```scala
val in = ds.toTable(tableEnv).as('order_id, 'order_name ) 

```

## 分组运算符

这类似于根据给定属性聚合结果的 SQL `groupBy`操作。

在 Java 中:

```scala
Table result = in.groupBy("company"); 

```

在 Scala 中:

```scala
val in = in.groupBy('company) 

```

## 连接运算符

`join`运算符用于连接表。我们必须指定至少一个等式连接条件。

在 Java 中:

```scala
Table employee = tableEnv.fromDataSet(emp, "e_id, e_name, deptId"); 

Table dept = tableEnv.fromDataSet(dept, "d_id, d_name"); 

Table result = employee.join(dept).where("deptId = d_id").select("e_id, e_name, d_name"); 

```

在 Scala 中:

```scala
val employee = empDS.toTable(tableEnv, 'e_id, 'e_name, 'deptId); 

val dept = deptDS.toTable(tableEnv, 'd_id, 'd_name); 

Table result = employee.join(dept).where('deptId == 'd_id).select('e_id, 'e_name, 'd_name); 

```

## 左外部连接运算符

`leftOuterJoin`运算符通过从左侧指定的表中获取所有值来连接两个表，并仅从右侧表中选择匹配的值。我们必须指定至少一个等式连接条件。

在 Java 中:

```scala
Table employee = tableEnv.fromDataSet(emp, "e_id, e_name, deptId"); 

Table dept = tableEnv.fromDataSet(dept, "d_id, d_name"); 

Table result = employee.leftOuterJoin(dept).where("deptId = d_id").select("e_id, e_name, d_name"); 

```

在 Scala 中:

```scala
val employee = empDS.toTable(tableEnv, 'e_id, 'e_name, 'deptId); 

val dept = deptDS.toTable(tableEnv, 'd_id, 'd_name); 

Table result = employee.leftOuterJoin(dept).where('deptId == 'd_id).select('e_id, 'e_name, 'd_name); 

```

## 右外部连接运算符

`rightOuterJoin`运算符通过从右侧指定的表中获取所有值来连接两个表，并从左侧表中仅选择匹配的值。我们必须指定至少一个等式连接条件。

在 Java 中:

```scala
Table employee = tableEnv.fromDataSet(emp, "e_id, e_name, deptId"); 

Table dept = tableEnv.fromDataSet(dept, "d_id, d_name"); 

Table result = employee.rightOuterJoin(dept).where("deptId = d_id").select("e_id, e_name, d_name"); 

```

在 Scala 中:

```scala
val employee = empDS.toTable(tableEnv, 'e_id, 'e_name, 'deptId); 

val dept = deptDS.toTable(tableEnv, 'd_id, 'd_name); 

Table result = employee.rightOuterJoin(dept).where('deptId == 'd_id).select('e_id, 'e_name, 'd_name); 

```

## 完全外部连接运算符

`fullOuterJoin`运算符通过获取两个表中的所有值来连接两个表。我们必须指定至少一个等式连接条件。

在 Java 中:

```scala
Table employee = tableEnv.fromDataSet(emp, "e_id, e_name, deptId"); 

Table dept = tableEnv.fromDataSet(dept, "d_id, d_name"); 

Table result = employee.fullOuterJoin(dept).where("deptId = d_id").select("e_id, e_name, d_name"); 

```

在 Scala 中:

```scala
val employee = empDS.toTable(tableEnv, 'e_id, 'e_name, 'deptId); 

val dept = deptDS.toTable(tableEnv, 'd_id, 'd_name); 

Table result = employee.fullOuterJoin(dept).where('deptId == 'd_id).select('e_id, 'e_name, 'd_name); 

```

## 联合运营商

`union`运算符合并两个相似的表。它会删除结果表中的重复值。

在 Java 中:

```scala
Table employee1 = tableEnv.fromDataSet(emp, "e_id, e_name, deptId"); 

Table employee2 = tableEnv.fromDataSet(emp, "e_id, e_name, deptId"); 

Table result = employee1.union(employee2); 

```

在 Scala 中:

```scala
val employee1 = empDS.toTable(tableEnv, 'e_id, 'e_name, 'deptId) 

val employee2 = empDS.toTable(tableEnv, 'e_id, 'e_name, 'deptId) 

Table result = employee1.union(employee2) 

```

## union all 运算符

`unionAll`运算符合并两个相似的表。

在 Java 中:

```scala
Table employee1 = tableEnv.fromDataSet(emp, "e_id, e_name, deptId"); 

Table employee2 = tableEnv.fromDataSet(emp, "e_id, e_name, deptId"); 

Table result = employee1.unionAll(employee2); 

```

在 Scala 中:

```scala
val employee1 = empDS.toTable(tableEnv, 'e_id, 'e_name, 'deptId) 

val employee2 = empDS.toTable(tableEnv, 'e_id, 'e_name, 'deptId) 

Table result = employee1.unionAll(employee2) 

```

## 相交运算符

`intersect`运算符从两个表中返回匹配的值。它确保结果表没有任何重复。

在 Java 中:

```scala
Table employee1 = tableEnv.fromDataSet(emp, "e_id, e_name, deptId"); 

Table employee2 = tableEnv.fromDataSet(emp, "e_id, e_name, deptId"); 

Table result = employee1.intersect(employee2); 

```

在 Scala 中:

```scala
val employee1 = empDS.toTable(tableEnv, 'e_id, 'e_name, 'deptId) 

val employee2 = empDS.toTable(tableEnv, 'e_id, 'e_name, 'deptId) 

Table result = employee1.intersect(employee2) 

```

## 交叉运算符

`intersectAll`运算符从两个表中返回匹配的值。结果表可能有重复的记录。

在 Java 中:

```scala
Table employee1 = tableEnv.fromDataSet(emp, "e_id, e_name, deptId"); 

Table employee2 = tableEnv.fromDataSet(emp, "e_id, e_name, deptId"); 

Table result = employee1.intersectAll(employee2); 

```

在 Scala 中:

```scala
val employee1 = empDS.toTable(tableEnv, 'e_id, 'e_name, 'deptId) 

val employee2 = empDS.toTable(tableEnv, 'e_id, 'e_name, 'deptId) 

Table result = employee1.intersectAll(employee2) 

```

## 减运算符

`minus`运算符从左表返回右表中不存在的记录。它确保结果表没有任何重复。

在 Java 中:

```scala
Table employee1 = tableEnv.fromDataSet(emp, "e_id, e_name, deptId"); 

Table employee2 = tableEnv.fromDataSet(emp, "e_id, e_name, deptId"); 

Table result = employee1.minus(employee2); 

```

在 Scala 中:

```scala
val employee1 = empDS.toTable(tableEnv, 'e_id, 'e_name, 'deptId) 

val employee2 = empDS.toTable(tableEnv, 'e_id, 'e_name, 'deptId) 

Table result = employee1.minus(employee2) 

```

## 负一点运算符

`minusAll`运算符从左表返回右表中不存在的记录。结果表可能有重复的记录。

在 Java 中:

```scala
Table employee1 = tableEnv.fromDataSet(emp, "e_id, e_name, deptId"); 

Table employee2 = tableEnv.fromDataSet(emp, "e_id, e_name, deptId"); 

Table result = employee1.minusAll(employee2); 

```

在 Scala 中:

```scala
val employee1 = empDS.toTable(tableEnv, 'e_id, 'e_name, 'deptId) 

val employee2 = empDS.toTable(tableEnv, 'e_id, 'e_name, 'deptId) 

Table result = employee1.minusAll(employee2) 

```

## 独特的操作符

`distinct`运算符仅从表中返回唯一值记录。

在 Java 中:

```scala
Table employee1 = tableEnv.fromDataSet(emp, "e_id, e_name, deptId"); 

Table result = employee1.distinct(); 

```

在 Scala 中:

```scala
val employee1 = empDS.toTable(tableEnv, 'e_id, 'e_name, 'deptId) 

Table result = employee1.distinct() 

```

## order by 运算符

`orderBy`运算符返回跨全局并行分区排序的记录。您可以选择升序或降序。

在 Java 中:

```scala
Table employee1 = tableEnv.fromDataSet(emp, "e_id, e_name, deptId"); 

Table result = employee1.orderBy("e_id.asc"); 

```

在 Scala 中:

```scala
val employee1 = empDS.toTable(tableEnv, 'e_id, 'e_name, 'deptId) 

Table result = employee1.orderBy('e_id.asc) 

```

## 极限操作符

`limit`运算符限制从给定偏移量开始跨全局并行分区排序的记录。

在 Java 中:

```scala
Table employee1 = tableEnv.fromDataSet(emp, "e_id, e_name, deptId"); 

//returns records from 6th record 
Table result = employee1.orderBy("e_id.asc").limit(5); 

//returns 5 records from 4th record 
Table result1 = employee1.orderBy("e_id.asc").limit(3,5); 

```

在 Scala 中:

```scala
val employee1 = empDS.toTable(tableEnv, 'e_id, 'e_name, 'deptId) 
//returns records from 6th record 
Table result = employee1.orderBy('e_id.asc).limit(5) 
//returns 5 records from 4th record 
Table result = employee1.orderBy('e_id.asc).limit(3,5) 

```

## 数据类型

表 API 支持常用的 SQL 数据类型，使用方便。在内部，它使用`TypeInformation`来识别各种数据类型。它目前不支持所有 Flink 数据类型:

<colgroup><col> <col> <col></colgroup> 
| 

表格应用编程接口

 | 

结构化查询语言

 | 

Java 类型

 |
| --- | --- | --- |
| `Types.STRING` | `VARCHAR` | `java.lang.String` |
| `Types.BOOLEAN` | `BOOLEAN` | `java.lang.Boolean` |
| `Types.BYTE` | `TINYINT` | `java.lang.Byte` |
| `Types.SHORT` | `SMALLINT` | `java.lang.Short` |
| `Types.INT` | `INTEGER`、`INT` | `java.lang.Integer` |
| `Types.LONG` | `BIGINT` | `java.lang.Long` |
| `Types.FLOAT` | `REAL`、`FLOAT` | `java.lang.Float` |
| `Types.DOUBLE` | `DOUBLE` | `java.lang.Double` |
| `Types.DECIMAL` | `DECIMAL` | `java.math.BigDecimal` |
| `Types.DATE` | `DATE` | `java.sql.Date` |
| `Types.TIME` | `TIME` | `java.sql.Time` |
| `Types.TIMESTAMP` | `TIMESTAMP(3)` | `java.sql.Timestamp` |
| `Types.INTERVAL_MONTHS` | 年至月的间隔 | `java.lang.Integer` |
| `Types.INTERVAL_MILLIS` | 间隔一天到第二天(3) | `java.lang.Long` |

随着社区的不断发展和支持，很快会支持更多的数据类型。

# SQL

表 API 还允许我们使用`sql()`方法编写自由形式的 SQL 查询。该方法还在内部使用 Apache 方解石进行 SQL 语法验证和优化。它执行查询并以表格格式返回结果。稍后，该表可以再次转换为数据集或数据流或`TableSink`以供进一步处理。

这里需要注意的一点是，对于访问表的 SQL 方法，它们必须注册到`TableEnvironment`。

对 SQL 方法的支持不断增加，因此如果有任何语法不被支持，就会出现`TableException`错误。

现在让我们看看如何在数据集和数据流上使用 SQL 方法。

## 数据流上的 SQL

可以使用`SELECT STREAM`关键字对注册了`TableEnvironment`的数据流执行 SQL 查询。大多数 SQL 语法在数据集和数据流之间是通用的。要了解更多关于流语法的知识，阿帕奇方解石的流文档会很有帮助。可在:[https://calcite.apache.org/docs/stream.html](https://calcite.apache.org/docs/stream.html)找到。

假设我们要分析定义为(`id`、`name`、`stock`)的产品模式。以下代码需要使用`sql()`方法编写。

在 Java 中:

```scala
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); 
StreamTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env); 

DataStream<Tuple3<Long, String, Integer>> ds = env.addSource(...); 
// register the DataStream as table "Products" 
tableEnv.registerDataStream("Products", ds, "id, name, stock"); 
// run a SQL query on the Table and retrieve the result as a new Table 
Table result = tableEnv.sql( 
  "SELECT STREAM * FROM Products WHERE name LIKE '%Apple%'"); 

```

在 Scala 中:

```scala
val env = StreamExecutionEnvironment.getExecutionEnvironment 
val tEnv = TableEnvironment.getTableEnvironment(env) 

val ds: DataStream[(Long, String, Integer)] = env.addSource(...) 
// register the DataStream under the name "Products" 
tableEnv.registerDataStream("Products", ds, 'id, 'name, 'stock) 
// run a SQL query on the Table and retrieve the result as a new Table 
val result = tableEnv.sql( 
  "SELECT STREAM * FROM Products WHERE name LIKE '%Apple%'") 

```

表应用编程接口使用类似于 Java 的词法策略来正确定义查询。这意味着标识符的大小写被保留，并且它们区分大小写地匹配。如果您的任何标识符包含非字母数字字符，那么您可以使用反勾号来引用它们。

例如，如果您想要定义一个名为`'my col'`的列，那么您需要使用反向记号，如下所示:

```scala
"SELECT col as `my col` FROM table " 

```

## 支持的 SQL 语法

如前所述，Flink 使用 Apache 方解石来验证和优化 SQL 查询。当前版本支持以下**巴克斯瑙形态** ( **BNF** ):

```scala
query: 
  values 
  | { 
      select 
      | selectWithoutFrom 
      | query UNION [ ALL ] query 
      | query EXCEPT query 
      | query INTERSECT query 
    } 
    [ ORDER BY orderItem [, orderItem ]* ] 
    [ LIMIT { count | ALL } ] 
    [ OFFSET start { ROW | ROWS } ] 
    [ FETCH { FIRST | NEXT } [ count ] { ROW | ROWS } ONLY] 

orderItem: 
  expression [ ASC | DESC ] 

select: 
  SELECT [ STREAM ] [ ALL | DISTINCT ] 
  { * | projectItem [, projectItem ]* } 
  FROM tableExpression 
  [ WHERE booleanExpression ] 
  [ GROUP BY { groupItem [, groupItem ]* } ] 
  [ HAVING booleanExpression ] 

selectWithoutFrom: 
  SELECT [ ALL | DISTINCT ] 
  { * | projectItem [, projectItem ]* } 

projectItem: 
  expression [ [ AS ] columnAlias ] 
  | tableAlias . * 

tableExpression: 
  tableReference [, tableReference ]* 
  | tableExpression [ NATURAL ] [ LEFT | RIGHT | FULL ] JOIN tableExpression [ joinCondition ] 

joinCondition: 
  ON booleanExpression 
  | USING '(' column [, column ]* ')' 

tableReference: 
  tablePrimary 
  [ [ AS ] alias [ '(' columnAlias [, columnAlias ]* ')' ] ] 

tablePrimary: 
  [ TABLE ] [ [ catalogName . ] schemaName . ] tableName 

values: 
  VALUES expression [, expression ]* 

groupItem: 
  expression 
  | '(' ')' 
  | '(' expression [, expression ]* ')' 

```

## 标量函数

表 API 和 SQL 支持各种内置标量函数。让我们试着一个一个地理解那些。

### 表 API 中的标量函数

以下是表应用编程接口中支持的标量函数列表:

<colgroup><col> <col> <col></colgroup> 
| **Java 功能** | **Scala 函数** | **描述** |
| `ANY.isNull` | `ANY.isNull` | 如果给定表达式为空，则返回`true`。 |
| `ANY.isNotNull` | `ANY.isNotNull` | 如果给定表达式不为空，则返回`true`。 |
| `BOOLEAN.isTrue` | `BOOLEAN.isTrue` | 如果给定的布尔表达式是`true`，则返回`true`。`False`不然。 |
| `BOOLEAN.isFalse` | `BOOLEAN.isFalse` | 如果给定的布尔表达式为假，则返回`true`。`False`否则。 |
| `NUMERIC.log10()` | `NUMERIC.log10()` | 计算给定值以 10 为底的对数。 |
| `NUMERIC.ln()` | `NUMERIC.ln()` | 计算给定值的自然对数。 |
| `NUMERIC.power(NUMERIC)` | `NUMERIC.power(NUMERIC)` | 计算给定数字的另一个值的幂。 |
| `NUMERIC.abs()` | `NUMERIC.abs()` | 计算给定值的绝对值。 |
| `NUMERIC.floor()` | `NUMERIC.floor()` | 计算小于或等于给定数字的最大整数。 |
| `NUMERIC.ceil()` | `NUMERIC.ceil()` | 计算大于或等于给定数字的最小整数。 |
| `STRING.substring(INT, INT)` | `STRING.substring(INT, INT)` | 在给定长度的给定索引处创建给定字符串的子字符串 |
| `STRING.substring(INT)` | `STRING.substring(INT)` | 创建从给定索引开始到结束的给定字符串的子字符串。起始索引从 1 开始，并且包含 1。 |
| `STRING.trim(LEADING, STRING)` `STRING.trim(TRAILING, STRING)` `STRING.trim(BOTH, STRING)` `STRING.trim(BOTH)` `STRING.trim()` | `STRING.trim(leading = true, trailing = true, character = " ")` | 从给定字符串中移除前导和/或尾随字符。默认情况下，两侧的空白会被删除。 |
| `STRING.charLength()` | `STRING.charLength()` | 返回字符串的长度。 |
| `STRING.upperCase()` | `STRING.upperCase()` | 使用默认区域设置的规则，以大写形式返回字符串中的所有字符。 |
| `STRING.lowerCase()` | `STRING.lowerCase()` | 使用默认区域设置的规则，以小写形式返回字符串中的所有字符。 |
| `STRING.initCap()` | `STRING.initCap()` | 将字符串中每个单词的首字母转换为大写。假设一个字符串只包含`[A-Za-z0-9]`，其他的都被视为空白。 |
| `STRING.like(STRING)` | `STRING.like(STRING)` | 如果字符串与指定的 LIKE 模式匹配，则返回 true。例如，`"Jo_n%"`匹配所有以`"Jo(arbitrary letter)n"`开头的字符串。 |
| `STRING.similar(STRING)` | `STRING.similar(STRING)` | 如果字符串与指定的 SQL 正则表达式模式匹配，则返回`true`。例如，`"A+"`匹配至少包含一个`"A"`的所有字符串。 |
| `STRING.toDate()` | `STRING.toDate` | 将形式为`"yy-mm-dd"`的日期字符串解析为一个 SQL 日期。 |
| `STRING.toTime()` | `STRING.toTime` | 将形式为`"hh:mm:ss"`的时间字符串解析为一个 SQL 时间。 |
| `STRING.toTimestamp()` | `STRING.toTimestamp` | 将`"yy-mm-dd hh:mm:ss.fff"`形式的时间戳字符串解析为一个 SQL 时间戳。 |
| `TEMPORAL.extract(TIMEINTERVALUNIT)` | 钠 | 提取时间点或时间间隔的一部分。将该部分作为长值返回。比如`2006-06-05 .toDate.extract(DAY)`通向`5`。 |
| `TIMEPOINT.floor(TIMEINTERVALUNIT)` | `TIMEPOINT.floor(TimeIntervalUnit)` | 将时间点向下舍入到给定单位。比如`"12:44:31".toDate.floor(MINUTE)`通向`12:44:00`。 |
| `TIMEPOINT.ceil(TIMEINTERVALUNIT)` | `TIMEPOINT.ceil(TimeIntervalUnit)` | 将时间点向上舍入到给定单位。比如`"12:44:31".toTime.floor(MINUTE)`通向`12:45:00`。 |
| `currentDate()` | `currentDate()` | 返回以世界协调时时区表示的当前 SQL 日期。 |
| `currentTime()` | `currentTime()` | 返回以世界协调时时区表示的当前 SQL 时间。 |
| `currentTimestamp()` | `currentTimestamp()` | 返回以世界协调时时区表示的当前 SQL 时间戳。 |
| `localTime()` | `localTime()` | 返回本地时区的当前 SQL 时间。 |
| `localTimestamp()` | `localTimestamp()` | 返回本地时区的当前 SQL 时间戳。 |

## SQL 中的 Scala 函数

以下是`sql()`方法中支持的标量函数列表:

<colgroup><col> <col></colgroup> 
| 功能 | 描述 |
| `EXP(NUMERIC)` | 计算给定幂的欧拉数。 |
| `LOG10(NUMERIC)` | 计算给定值以 10 为底的对数。 |
| `LN(NUMERIC)` | 计算给定值的自然对数。 |
| `POWER(NUMERIC, NUMERIC)` | 计算给定数字的另一个值的幂。 |
| `ABS(NUMERIC)` | 计算给定值的绝对值。 |
| `FLOOR(NUMERIC)` | 计算小于或等于给定数字的最大整数。 |
| `CEIL(NUMERIC)` | 计算大于或等于给定数字的最小整数。 |
| `SUBSTRING(VARCHAR, INT, INT) SUBSTRING(VARCHAR FROM INT FOR INT)` | 在给定长度的给定索引处创建给定字符串的子字符串。索引从 1 开始，并且包含 1，也就是说，索引处的字符包含在子字符串中。子字符串的长度等于或小于指定长度。 |
| `SUBSTRING(VARCHAR, INT)``SUBSTRING(VARCHAR FROM INT)` | 创建从给定索引开始到结束的给定字符串的子字符串。起始索引从 1 开始，并且包含 1。 |
| `TRIM(LEADING VARCHAR FROM VARCHAR) TRIM(TRAILING VARCHAR FROM VARCHAR) TRIM(BOTH VARCHAR FROM VARCHAR) TRIM(VARCHAR)` | 从给定字符串中移除前导和/或尾随字符。默认情况下，两侧的空白会被删除。 |
| `CHAR_LENGTH(VARCHAR)` | 返回字符串的长度。 |
| `UPPER(VARCHAR)` | 使用默认区域设置的规则，以大写形式返回字符串中的所有字符。 |
| `LOWER(VARCHAR)` | 使用默认区域设置的规则，以小写形式返回字符串中的所有字符。 |
| `INITCAP(VARCHAR)` | 将字符串中每个单词的首字母转换为大写。假设一个字符串只包含`[A-Za-z0-9]`，其他的都被视为空白。 |
| `VARCHAR LIKE VARCHAR` | 如果字符串与指定的 LIKE 模式匹配，则返回 true。例如，`"Jo_n%"`匹配所有以`"Jo(arbitrary letter)n"`开头的字符串。 |
| `VARCHAR SIMILAR TO VARCHAR` | 如果字符串与指定的 SQL 正则表达式模式匹配，则返回 true。例如，`"A+"`匹配至少包含一个`"A"`的所有字符串。 |
| `DATE VARCHAR` | 将形式为`"yy-mm-dd"`的日期字符串解析为一个 SQL 日期。 |
| `TIME VARCHAR` | 将形式为`"hh:mm:ss"`的时间字符串解析为一个 SQL 时间。 |
| `TIMESTAMP VARCHAR` | 将`"yy-mm-dd hh:mm:ss.fff"`形式的时间戳字符串解析为一个 SQL 时间戳。 |
| `EXTRACT(TIMEINTERVALUNIT FROM TEMPORAL)` | 提取时间点或时间间隔的一部分。将该部分作为长值返回。比如`EXTRACT(DAY FROM DATE '2006-06-05')`通向`5`。 |
| `FLOOR(TIMEPOINT TO TIMEINTERVALUNIT)` | 将时间点向下舍入到给定单位。比如`FLOOR(TIME '12:44:31' TO MINUTE)`通向`12:44:00`。 |
| `CEIL(TIMEPOINT TO TIMEINTERVALUNIT)` | 将时间点向上舍入到给定单位。比如`CEIL(TIME '12:44:31' TO MINUTE)`通向`12:45:00`。 |
| `CURRENT_DATE` | 返回以世界协调时时区表示的当前 SQL 日期。 |
| `CURRENT_TIME` | 返回以世界协调时时区表示的当前 SQL 时间。 |
| `CURRENT_TIMESTAMP` | 返回以世界协调时时区表示的当前 SQL 时间戳。 |
| `LOCALTIME` | 返回本地时区的当前 SQL 时间。 |
| `LOCALTIMESTAMP` | 以本地时区返回当前的 SQL 时间戳。 |

# 使用案例-使用弗林克表格 API 的运动员数据洞察

既然我们已经了解了表应用编程接口的细节，让我们尝试将这些知识应用到实际的用例中。假设我们有一个数据集，其中有关于奥运会运动员及其在各种比赛中表现的信息。

示例数据如下表所示:

<colgroup><col> <col> <col> <col> <col> <col> <col> <col></colgroup> 
| **玩家** | **国家** | **年** | **游戏** | **黄金** | **银色** | 青铜 | **总计** |
| 杨一琳 | 中国 | Two thousand and eight | 体操 | one | Zero | Two | three |
| 雷塞尔·琼斯 | 澳大利亚 | Two thousand | 游泳 | Zero | Two | Zero | Two |
| 去 Gi-Hyeon | 韩国 | Two thousand and two | 短道速滑 | one | one | Zero | Two |
| 陈若琳 | 中国 | Two thousand and eight | 潜水 | Two | Zero | Zero | Two |
| 凯蒂·莱德基 | 美国 | Two thousand and twelve | 游泳 | one | Zero | Zero | one |
| 半字节路径 | 立陶宛 | Two thousand and twelve | 游泳 | one | Zero | Zero | one |
| 尼尔·朱尔塔 | 匈牙利 | Two thousand and four | 游泳 | Zero | one | Zero | one |
| 阿丽亚娜方丹 | 意大利 | Two thousand and six | 短道速滑 | Zero | Zero | one | one |
| 奥尔加·格拉茨基赫 | 俄罗斯 | Two thousand and four | 艺术体操 | one | Zero | Zero | one |
| 喀里多尼亚 | 希腊 | Two thousand | 艺术体操 | Zero | Zero | one | one |
| 金·马丁 | 瑞典 | Two thousand and two | 冰曲棍球 | Zero | Zero | one | one |
| 凯拉·罗斯 | 美国 | Two thousand and twelve | 体操 | one | Zero | Zero | one |
| 加芙列拉·德拉吉 | 罗马尼亚 | Two thousand and eight | 体操 | Zero | Zero | one | one |
| 塔莎·施韦克-沃伦 | 美国 | Two thousand | 体操 | Zero | Zero | one | one |

现在我们想得到一些问题的答案，比如，有多少奖牌是由国家或游戏赢得的。作为结构化数据中的数据，我们可以使用 Table API 以 SQL 方式查询数据。让我们开始吧。

可用数据为 CSV 格式。因此，我们将使用 Flink API 提供的 CSV 阅读器，如以下代码片段所示:

```scala
ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); 
BatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env); 
DataSet<Record> csvInput = env 
          .readCsvFile("olympic-athletes.csv") 
          .pojoType(Record.class, "playerName", "country", "year",   
                    "game", "gold", "silver", "bronze", "total"); 

```

接下来，我们需要用这个数据集创建一个表，并在表环境中注册它，以便进一步处理:

```scala
Table atheltes = tableEnv.fromDataSet(csvInput); 
tableEnv.registerTable("athletes", atheltes); 

```

接下来，我们可以编写一个常规的 SQL 查询来从数据中获得更多的见解。或者我们可以使用表应用编程接口操作符来操作数据，如下面的代码片段所示:

```scala
Table groupedByCountry = tableEnv.sql("SELECT country, SUM(total) as frequency FROM athletes group by country"); 
DataSet<Result> result = tableEnv.toDataSet(groupedByCountry, Result.class); 
result.print(); 
Table groupedByGame = atheltes.groupBy("game").select("game, total.sum as frequency"); 
DataSet<GameResult> gameResult = tableEnv.toDataSet(groupedByGame, GameResult.class); 
gameResult.print(); 

```

这样，我们可以使用表应用编程接口以更简单的方式分析这些数据。这个用例的完整代码可以在 GitHub 上的[https://GitHub . com/deshandeltanmay/mastering-flink/tree/master/chapter 04/flink-table](https://github.com/deshpandetanmay/mastering-flink/tree/master/chapter04/flink-table)上找到。

# 总结

在本章中，我们了解了一个由 Flink 支持的基于 SQL 的应用编程接口，称为表应用编程接口。我们还学习了如何将数据集/流转换为表，用`TableEnvironment`注册表、数据集和数据流，然后使用注册的表执行各种操作。对于来自 SQL 数据库背景的人来说，这个应用编程接口是一个福音。

在下一章中，我们将讨论一个非常有趣的名为**复杂事件处理**的库，以及如何使用它来解决各种业务用例。