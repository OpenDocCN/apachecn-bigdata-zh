# 第十章。最佳实践

到目前为止，在这本书里，我们已经了解了关于弗林克的各种事情。我们从 Flink 的架构及其支持的各种 API 开始。我们还学习了如何使用 Flink 提供的图形和机器学习 API。在这一章的结尾，我们将讨论一些你应该遵循的最佳实践，以创建生产质量可维护的 Flink 应用程序。

我们将讨论以下主题:

*   记录最佳实践
*   使用自定义序列化程序
*   使用和监控 REST 应用编程接口
*   背压监控

让我们开始吧。

# 记录最佳实践

在任何软件应用程序中配置日志是非常重要的。日志有助于调试问题。我们不遵循这些日志记录实践，这将很难理解工作的进度或它是否有任何问题。我们可以使用几个库来获得更好的日志记录体验。

## 配置 Log4j

众所周知，Log4j 是使用最广泛的日志库之一。我们可以用很少的努力在任何 Flink 应用程序中配置它。我们只需要包含一个`log4j.properties`文件。我们可以通过传递`Dlog4j.configuration=/path/to/log4j.properties`参数来传递`log4j.properties`文件。

Flink 支持以下默认属性文件:

*   `log4j-cli.properties`:该文件由 Flink 命令行工具使用。这里是[的确切文件。](https://github.com/apache/flink/blob/master/flink-dist/src/main/flink-bin/conf/log4j-cli.properties)
*   `log4j-yarn-session.properties`:该文件由 Flink 纱会话使用。这是位于的确切文件。
*   `log4j.properties`:此文件由 Flink 作业管理器和任务管理器使用。这里是[https://github . com/Apache/flink/blob/master/flink-dist/src/main/flink-bin/conf/log4j . properties](https://github.com/apache/flink/blob/master/flink-dist/src/main/flink-bin/conf/log4j.properties)的确切文件。

## 配置日志记录

由于 Log4j 的特性，现在很多人更喜欢使用 Logback 而不是 Log4j。日志备份提供了更快的输入/输出、经过全面测试的库、大量文档等。Flink 还支持为应用程序配置 Logback。

我们需要使用相同的属性来配置`logback.xml`。`Dlogback.configurationFile=<file>`，或者我们也可以把`logback.xml`文件放在类路径中。示例`logback.xml`如下所示:

```
<configuration> 
    <appender name="file" class="ch.qos.logback.core.FileAppender"> 
        <file>${log.file}</file> 
        <append>false</append> 
        <encoder> 
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level   
            %logger{60} %X{sourceThread} - %msg%n</pattern> 
        </encoder> 
    </appender> 

    <!-- This affects logging for both user code and Flink --> 
    <root level="INFO"> 
        <appender-ref ref="file"/> 
    </root> 

    <!-- Uncomment this if you want to only change Flink's logging --> 
    <!--<logger name="org.apache.flink" level="INFO">--> 
        <!--<appender-ref ref="file"/>--> 
    <!--</logger>--> 

    <!-- The following lines keep the log level of common  
    libraries/connectors on 
         log level INFO. The root logger does not override this. You 
         have to manually 
         change the log levels here. --> 
    <logger name="akka" level="INFO"> 
        <appender-ref ref="file"/> 
    </logger> 
    <logger name="org.apache.kafka" level="INFO"> 
        <appender-ref ref="file"/> 
    </logger> 
    <logger name="org.apache.hadoop" level="INFO"> 
        <appender-ref ref="file"/> 
    </logger> 
    <logger name="org.apache.zookeeper" level="INFO"> 
        <appender-ref ref="file"/> 
    </logger> 

    <!-- Suppress the irrelevant (wrong) warnings from the Netty 
     channel handler --> 
    <logger name="org.jboss.netty.channel.DefaultChannelPipeline" 
    level="ERROR"> 
        <appender-ref ref="file"/> 
    </logger> 
</configuration> 

```

我们可以随时更改`logback.xml`文件，根据自己的喜好设置日志级别。

## 登录应用程序

在任何 Flink 应用程序中使用 SLF4J 时，我们需要导入以下包和类，并用类名启动记录器:

```
import org.slf4j.LoggerFactory 
import org.slf4j.Logger 

Logger LOG = LoggerFactory.getLogger(MyClass.class) 

```

使用占位符机制进行日志记录而不是使用字符串格式化程序也是一种最佳做法。占位符机制有助于避免不必要的字符串形式，相反，它只进行字符串连接。下面的代码片段显示了如何使用占位符:

```
LOG.info("Value of a = {}, value of b= {}", myobject.a, myobject.b); 

```

我们还可以在异常处理中使用占位符日志:

```
catch(Exception e){ 
  LOG.error("Error occurred {}",  e); 
} 

```

# 使用参数工具

从 Flink 0.9 开始，我们在 Flink 中有一个内置的`ParameterTool`，它有助于从参数、系统属性等外部来源，或者从属性文件中获取参数。在内部，它是一个字符串映射，将键作为参数名，将值作为参数值。

例如，我们可以考虑在我们的 DataStream API 示例中使用 ParameterTool，在这里我们需要设置 Kafka 属性:

```
String kafkaproperties = "/path/to/kafka.properties";
ParameterTool parameter = ParameterTool.fromPropertiesFile(propertiesFile);
```

## 来自系统属性

我们可以读取系统变量中定义的属性。我们需要通过设置`Dinput=hdfs://myfile`在初始化之前传递系统属性文件。

现在我们可以在`ParameterTool`中读取所有这些属性，如下所示:

```
ParameterTool parameters = ParameterTool.fromSystemProperties(); 

```

## 来自命令行参数

我们还可以从命令行参数中读取参数。我们必须在调用应用程序之前设置`--elements`。

下面的代码显示了如何从命令行参数中读取参数:

```
ParameterTool parameters = ParameterTool.fromArgs(args); 

```

## 从。属性文件

我们也可以从`.properties`文件中读取参数。以下是这方面的代码:

```
String propertiesFile = /my.properties"; 
ParameterTool parameters = ParameterTool.fromPropertiesFile(propertiesFile); 

```

我们可以读取 Flink 程序中的参数。下面显示了我们如何获取参数:

```
parameter.getRequired("key"); 
parameter.get("paramterName", "myDefaultValue"); 
parameter.getLong("expectedCount", -1L); 
parameter.getNumberOfParameters() 

```

# 命名大型图谱类型

我们知道，元组是一种复杂的数据类型，用于表示复杂的数据结构。它是各种原始数据类型的组合。一般建议不要使用大元组；相反，建议使用 Java POJOs。如果您想使用元组，建议使用一些自定义的 POJO 类型来命名它。

为大型元组创建自定义类型非常容易。例如，如果我们想使用`Tuple8`，那么我们可以定义如下:

```
//Initiate Record Tuple
RecordTuple rc = new RecordTuple(value0, value1, value2, value3, value4, value5, value6, value7);

// Define RecordTuple instead of using Tuple8
public static class RecordTuple extends Tuple8<String, String, Integer, String, Integer, Integer, Integer, Integer> {

         public RecordTuple() {
               super();
         }

         public RecordTuple(String value0, String value1, Integer value2, String value3, Integer value4, Integer value5,
                     Integer value6, Integer value7) {
               super(value0, value1, value2, value3, value4, value5, value6, value7);
         }
      } 
```

# 注册自定义序列化程序

在分布式计算世界中，处理好每一件小事是非常重要的。序列化就是其中之一。默认情况下，Flink 使用 Kryo 序列化程序。Flink 还允许我们编写自定义序列化程序，以防你认为默认的不够好。我们需要注册自定义序列化程序，以便 Flink 理解它。注册自定义序列化程序非常简单；我们只需要在 Flink 执行环境中注册它的类类型。下面的代码片段展示了我们是如何做到这一点的:

```
final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); 

// register the class of the serializer as serializer for a type 
env.getConfig().registerTypeWithKryoSerializer(MyCustomType.class, MyCustomSerializer.class); 

// register an instance as serializer for a type 
MySerializer mySerializer = new MySerializer(); 
env.getConfig().registerTypeWithKryoSerializer(MyCustomType.class, mySerializer); 

```

这里有一个完整的自定义序列化程序示例类，位于[https://github . com/deshandename/mastering-flink/blob/master/chapter 10/flink-batch-adv/src/main/Java/com/demo/flink/batch/recordserializer . Java](https://github.com/deshpandetanmay/mastering-flink/blob/master/chapter10/flink-batch-adv/src/main/java/com/demo/flink/batch/RecordSerializer.java)。

以及[处的自定义类型 https://github . com/desh pandestanmay/mastering-flink/blob/master/chapter 10/flink-batch-adv/src/main/Java/com/demo/flink/batch/record . Java](https://github.com/deshpandetanmay/mastering-flink/blob/master/chapter10/flink-batch-adv/src/main/java/com/demo/flink/batch/Record.java)。

我们需要确保自定义序列化程序必须扩展 Kryo 的序列化程序类。有了谷歌 Protobuf 和 Apache 节俭，这已经完成了。

### 注

你可以在[https://github.com/google/protobuf](https://github.com/google/protobuf)阅读更多关于谷歌原蟾的信息。关于阿帕奇节俭的详细信息可以在[https://thrift.apache.org/](https://thrift.apache.org/)阅读。

为了使用 Google Protobuf，您可以添加以下 Maven 依赖项:

```
<dependency> 
  <groupId>com.twitter</groupId> 
  <artifactId>chill-protobuf</artifactId> 
  <version>0.5.2</version> 
</dependency> 
<dependency> 
  <groupId>com.google.protobuf</groupId> 
  <artifactId>protobuf-java</artifactId> 
  <version>2.5.0</version> 
</dependency> 

```

# 指标

Flink 支持一个度量系统，该系统允许用户了解更多关于 Flink 设置和其上运行的应用程序的信息。如果您在一个非常大的生产系统中使用 Flink，这将非常有用，在这个系统中有大量的作业正在运行，我们需要获得每个作业的详细信息。我们也可以用这些来为外部监控系统提供信息。因此，让我们试着了解什么是可用的，以及如何使用它们。

## 注册指标

任何通过调用`getRuntimeContext().getMetricGroup()`扩展`RichFunction`的用户函数都可以使用公制函数。这些方法返回一个`MetricGroup`对象，该对象可用于创建和注册一个新的度量。

Flink 支持各种度量类型，例如:

*   计数器
*   测量
*   直方图
*   米

### 计数器

计数器可以用来在处理过程中对某些东西进行计数。计数器的一个简单用途是统计数据中的无效记录。您可以根据条件选择递增或递减计数器。下面的代码片段显示了这一点:

```
public class TestMapper extends RichMapFunction<String, Integer> { 
  private Counter errorCounter; 

  @Override 
  public void open(Configuration config) { 
    this.errorCounter = getRuntimeContext() 
      .getMetricGroup() 
      .counter("errorCounter"); 
  } 

  @public Integer map(String value) throws Exception { 
    this.errorCounter.inc(); 
  } 
} 

```

### 仪表

只要需要，仪表可以提供任何值。为了使用量规，首先我们需要创建一个实现`org.apache.flink.metrics.Gauge`的类。稍后，您可以在`MetricGroup`注册。

以下代码片段显示了 Flink 应用程序中仪表的使用:

```
public class TestMapper extends RichMapFunction<String, Integer> { 
  private int valueToExpose; 

  @Override 
  public void open(Configuration config) { 
    getRuntimeContext() 
      .getMetricGroup() 
      .gauge("MyGauge", new Gauge<Integer>() { 
        @Override 
        public Integer getValue() { 
          return valueToReturn; 
        } 
      }); 
  } 
} 

```

### 直方图

直方图提供了度量上长值的分布。这可以用来监控一段时间内的某些指标。下面的代码片段显示了如何使用它:

```
public class TestMapper extends RichMapFunction<Long, Integer> { 
  private Histogram histogram; 

  @Override 
  public void open(Configuration config) { 
    this.histogram = getRuntimeContext() 
      .getMetricGroup() 
      .histogram("myHistogram", new MyHistogram()); 
  } 

  @public Integer map(Long value) throws Exception { 
    this.histogram.update(value); 
  } 
} 

```

### 米

仪表用于监控特定参数的平均吞吐量。使用`markEvent()`方法记录事件的发生。我们可以使用`MeterGroup`上的`meter(String name, Meter meter)`方法注册电表:

```
public class MyMapper extends RichMapFunction<Long, Integer> { 
  private Meter meter; 

  @Override 
  public void open(Configuration config) { 
    this.meter = getRuntimeContext() 
      .getMetricGroup() 
      .meter("myMeter", new MyMeter()); 
  } 

  @public Integer map(Long value) throws Exception { 
    this.meter.markEvent(); 
  } 
} 

```

## 记者

通过在`conf/flink-conf.yaml`文件中配置一个或多个报告程序，可以向外部系统显示指标。你们大多数人可能都知道 JMX 这样的系统，它有助于监控许多系统。我们可以考虑在 Flink 中配置 JMX 报告。报告程序应具有某些属性，如下表所示:

<colgroup><col> <col></colgroup> 
| **配置** | **描述** |
| `metrics.reporters` | 具名记者名单 |
| `metrics.reporter.<name>.<config>` | 带`<name>`的记者配置 |
| `metrics.reporter.<name>.class` | 记者类用于记者命名`<name>` |
| `metrics.reporter.<name>.interval` | 名为`<name>`的记者的间隔时间 |
| `metrics.reporter.<name>.scope.delimiter` | 姓名为`<name>`的记者范围 |

以下是 JMX 报告程序的报告配置示例:

```
metrics.reporters: my_jmx_reporter 

metrics.reporter.my_jmx_reporter.class: org.apache.flink.metrics.jmx.JMXReporter 
metrics.reporter.my_jmx_reporter.port: 9020-9040 

```

一旦我们在`config/flink-conf.yaml`中添加了前面给定的配置，我们就需要启动 Flink 作业管理器流程。现在 Flink 将开始向 JMX 港`8789.`公开这些变量我们可以使用 JConsole 来监控 Flink 发布的报告。默认情况下，JConsole 随 JDK 安装一起提供。我们只需要去 JDK 安装目录开始`JConsole.exe`。一旦 JConsole 运行，我们需要选择要监控的 Flink Job Manager 进程，我们可以看到可以监控的各种值。下面是一个监控 Flink 的 JConsole 屏幕截图示例。

![Reporters](graphics/B05653_10_01-1024x318.jpg)

### 注

除了 JMX，弗林克还支持 Ganglia、石墨和 StasD 等记者。更多相关信息可在[https://ci . Apache . org/project/flink/flink-docs-release-1.2/monitoring/metrics . html # reporter](https://ci.apache.org/projects/flink/flink-docs-release-1.2/monitoring/metrics.html#reporter)上找到。

# 监测 REST 原料药

Flink 支持监控正在运行和已完成的应用程序的状态。Flink 自己的工作仪表板也使用这些 API。状态 API 支持`get`方法，该方法返回给出作业信息的 JSON 对象。目前，默认情况下，监控应用编程接口是在 Flink 作业管理器仪表板中启动的。也可以使用作业管理器控制面板访问此信息。

Flink 中有许多可用的 API。让我们开始了解其中的一些。

## 配置 API

这给出了 API 的配置细节:`http://localhost:8081/config`

以下是回应:

```
{ 
    "refresh-interval": 3000, 
    "timezone-offset": 19800000, 
    "timezone-name": "India Standard Time", 
    "flink-version": "1.0.3", 
    "flink-revision": "f3a6b5f @ 06.05.2016 @ 12:58:02 UTC" 
} 

```

## 概述原料药

这给出了弗林克星团的概述:`http://localhost:8081/overview`

以下是回应:

```
{ 
    "taskmanagers": 1, 
    "slots-total": 1, 
    "slots-available": 1, 
    "jobs-running": 0, 
    "jobs-finished": 1, 
    "jobs-cancelled": 0, 
    "jobs-failed": 0, 
    "flink-version": "1.0.3", 
    "flink-commit": "f3a6b5f" 
} 

```

## 作业概述

这给出了最近运行和当前正在运行的作业的概述:` http://localhost:8081/jobs`

以下是回应:

```
{ 
    "jobs-running": [], 
    "jobs-finished": [ 
        "cd978489f5e76e5988fa0e5a7c76c09b" 
    ], 
    "jobs-cancelled": [], 
    "jobs-failed": [] 
} 

```

`http://localhost:8081/joboverview` API 给出了 Flink 作业的完整概述。它包含作业标识、开始和结束时间、运行持续时间、任务数量及其状态。一个状态可以被启动、运行、终止或结束。

以下是回应:

```
{ 
    "running": [], 
    "finished": [ 
        { 
            "jid": "cd978489f5e76e5988fa0e5a7c76c09b", 
            "name": "Flink Java Job at Sun Dec 04 16:13:16 IST 2016", 
            "state": "FINISHED", 
            "start-time": 1480848197679, 
            "end-time": 1480848198310, 
            "duration": 631, 
            "last-modification": 1480848198310, 
            "tasks": { 
                "total": 3, 
                "pending": 0, 
                "running": 0, 
                "finished": 3, 
                "canceling": 0, 
                "canceled": 0, 
                "failed": 0 
            } 
        } 
    ] 
} 

```

## 特定工作的详细信息

这给出了具体工作的细节。我们需要提供之前的 API 返回的作业 ID。提交作业后，Flink 会为该作业创建一个有向无环作业(DAG)。该图包含作为作业和执行计划的任务的顶点。以下输出显示了相同的细节。`http://localhost:8081/jobs/<jobid>`

以下是回应:

```
{ 
    "jid": "cd978489f5e76e5988fa0e5a7c76c09b", 
    "name": "Flink Java Job at Sun Dec 04 16:13:16 IST 2016", 
    "isStoppable": false, 
    "state": "FINISHED", 
    "start-time": 1480848197679, 
    "end-time": 1480848198310, 
    "duration": 631, 
    "now": 1480849319207, 
    "timestamps": { 
        "CREATED": 1480848197679, 
        "RUNNING": 1480848197733, 
        "FAILING": 0, 
        "FAILED": 0, 
        "CANCELLING": 0, 
        "CANCELED": 0, 
        "FINISHED": 1480848198310, 
        "RESTARTING": 0 
    }, 
    "vertices": [ 
        { 
            "id": "f590afd023018e19e30ce3cd7a16f4b1", 
            "name": "CHAIN DataSource (at  
             getDefaultTextLineDataSet(WordCountData.java:70) 
             (org.apache.flink.api.java.io.CollectionInputFormat)) -> 
             FlatMap (FlatMap at main(WordCount.java:81)) ->   
             Combine(SUM(1), at main(WordCount.java:84)", 
            "parallelism": 1, 
            "status": "FINISHED", 
            "start-time": 1480848197744, 
            "end-time": 1480848198061, 
            "duration": 317, 
            "tasks": { 
                "CREATED": 0, 
                "SCHEDULED": 0, 
                "DEPLOYING": 0, 
                "RUNNING": 0, 
                "FINISHED": 1, 
                "CANCELING": 0, 
                "CANCELED": 0, 
                "FAILED": 0 
            }, 
            "metrics": { 
                "read-bytes": 0, 
                "write-bytes": 1696, 
                "read-records": 0, 
                "write-records": 170 
            } 
        }, 
        { 
            "id": "c48c21be9c7bf6b5701cfa4534346f2f", 
            "name": "Reduce (SUM(1), at main(WordCount.java:84)", 
            "parallelism": 1, 
            "status": "FINISHED", 
            "start-time": 1480848198034, 
            "end-time": 1480848198190, 
            "duration": 156, 
            "tasks": { 
                "CREATED": 0, 
                "SCHEDULED": 0, 
                "DEPLOYING": 0, 
                "RUNNING": 0, 
                "FINISHED": 1, 
                "CANCELING": 0, 
                "CANCELED": 0, 
                "FAILED": 0 
            }, 
            "metrics": { 
                "read-bytes": 1696, 
                "write-bytes": 1696, 
                "read-records": 170, 
                "write-records": 170 
            } 
        }, 
        { 
            "id": "ff4625cfad1f2540bd08b99fb447e6c2", 
            "name": "DataSink (collect())", 
            "parallelism": 1, 
            "status": "FINISHED", 
            "start-time": 1480848198184, 
            "end-time": 1480848198269, 
            "duration": 85, 
            "tasks": { 
                "CREATED": 0, 
                "SCHEDULED": 0, 
                "DEPLOYING": 0, 
                "RUNNING": 0, 
                "FINISHED": 1, 
                "CANCELING": 0, 
                "CANCELED": 0, 
                "FAILED": 0 
            }, 
            "metrics": { 
                "read-bytes": 1696, 
                "write-bytes": 0, 
                "read-records": 170, 
                "write-records": 0 
            } 
        } 
    ], 
    "status-counts": { 
        "CREATED": 0, 
        "SCHEDULED": 0, 
        "DEPLOYING": 0, 
        "RUNNING": 0, 
        "FINISHED": 3, 
        "CANCELING": 0, 
        "CANCELED": 0, 
        "FAILED": 0 
    }, 
    "plan": { 
//plan details 

    } 
} 

```

## 用户定义的作业配置

这给出了特定作业使用的用户定义的作业配置:

`http://localhost:8081/jobs/<jobid>/config`

以下是回应:

```
{ 
    "jid": "cd978489f5e76e5988fa0e5a7c76c09b", 
    "name": "Flink Java Job at Sun Dec 04 16:13:16 IST 2016", 
    "execution-config": { 
        "execution-mode": "PIPELINED", 
        "restart-strategy": "default", 
        "job-parallelism": -1, 
        "object-reuse-mode": false, 
        "user-config": {} 
    } 
} 

```

同样，您可以在自己的设置中探索以下列出的所有 API:

```
/config 
/overview 
/jobs 
/joboverview/running 
/joboverview/completed 
/jobs/<jobid> 
/jobs/<jobid>/vertices 
/jobs/<jobid>/config 
/jobs/<jobid>/exceptions 
/jobs/<jobid>/accumulators 
/jobs/<jobid>/vertices/<vertexid> 
/jobs/<jobid>/vertices/<vertexid>/subtasktimes 
/jobs/<jobid>/vertices/<vertexid>/taskmanagers 
/jobs/<jobid>/vertices/<vertexid>/accumulators 
/jobs/<jobid>/vertices/<vertexid>/subtasks/accumulators 
/jobs/<jobid>/vertices/<vertexid>/subtasks/<subtasknum> 
/jobs/<jobid>/vertices/<vertexid>/subtasks/<subtasknum>/attempts/<attempt> 
/jobs/<jobid>/vertices/<vertexid>/subtasks/<subtasknum>/attempts/<attempt>/accumulators 
/jobs/<jobid>/plan 

```

# 背压监测

背压是 Flink 应用中的一种特殊情况，在这种情况下，下游操作员无法以与推动数据的上游操作员相同的速度消耗数据。这开始在管道上建立压力，数据流从相反的方向开始。通常，如果发生这种情况，Flink 会在日志中向我们发出警告。

在源接收器场景中，如果我们看到对源的警告，那么这意味着接收器消耗数据的速度比源产生数据的速度慢。

监控所有流作业中的背压非常重要，因为高背压作业可能会失败或给出错误的结果。背压可以通过 Flink 仪表板进行监控。

Flink 持续处理背压监控，获取运行任务的样本堆栈轨迹。如果示例显示任务被内部方法卡住，这表明存在背压。

平均而言，作业管理器每 50 毫秒触发 100 次堆栈跟踪。根据卡在内部流程中的任务数量，决定背压警告级别，如下表所示:

<colgroup><col> <col></colgroup> 
| **比值** | **背压水平** |
| 0 至 0.10 | 好的 |
| 0.10 至 0.5 | 低的 |
| 0.5 比 1 | 高的 |

您还可以通过设置以下参数来配置样本数量及其间隔:

<colgroup><col> <col></colgroup> 
| **参数** | **描述** |
| `jobmanager.web.backpressure.refresh-interval` | 重置可用统计信息的刷新间隔。默认为`60,000`，1 分钟。 |
| `jobmanager.web.backpressure.delay-between-samples` | 样本之间的延迟间隔。默认为`50`毫秒 |
| `jobmanager.web.backpressure.num-samples` | 确定背压的样品数量。默认为`100`。 |

# 总结

在这最后一章中，我们看了一些你应该遵循的最佳实践，以实现弗林克最好的表现。我们还研究了各种监控应用程序接口和指标，它们可用于对 Flink 应用程序的详细监控。

对弗林克来说，我想说旅程才刚刚开始，我相信这些年来，社区和支持会变得越来越强，越来越好。毕竟 Flink 被称为大数据的**第四代** ( **4G** )！