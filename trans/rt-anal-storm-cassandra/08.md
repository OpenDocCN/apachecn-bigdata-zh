# 第八章。卡珊德拉管理和维护

在这一章中，我们将了解卡珊德拉的八卦协议。此后，我们将深入研究 Cassandra 的管理和管理，以了解行动中的可伸缩性和可靠性。这将使您具备处理您不希望遇到但确实发生在生产中的情况的能力，例如处理可恢复节点、滚动重启等。

本章将涉及以下主题:

*   卡珊德拉——八卦协议
*   Cassandra 扩展—向群集添加新节点
*   替换节点
*   复制因子变化
*   节点工具命令
*   滚动重启和容错
*   Cassandra 监控工具

因此，本章将帮助您了解 Cassandra 的基础知识，以及维护和管理 Cassandra 活动所需的各种选项。

# 卡珊德拉——八卦协议

流言是一种协议，其中节点周期性地与其他节点交换关于它们知道的节点的信息；这样，所有节点都可以通过这种对等通信机制获得彼此的信息。这与现实世界和社交媒体世界的八卦非常相似。

Cassandra 每秒执行这个机制，一个节点能够与集群中多达三个节点交换八卦信息。所有这些八卦消息都有一个相关的版本来跟踪时间表，旧的八卦互动更新会被新的按时间顺序覆盖。

现在我们在非常高的水平上知道了卡珊德拉的八卦是什么样的，让我们仔细看看，了解一下这个闲聊协议的目的。以下是实施该计划的两个主要目的:

*   拔靴带
*   故障场景处理—检测和恢复

让我们理解他们在行动中的意义，以及他们对卡珊德拉集群的福祉和稳定的贡献。

## 自举

引导是当节点第一次加入环时在集群中触发的过程。正是我们在`Cassandra.yaml`配置文件下定义的种子节点帮助新节点获得关于集群、环、密钥集和分区范围的信息。建议您在整个集群中保持类似的设置；否则，您可能会遇到集群中的分区。一个节点即使在重启后也能记住它和哪些节点聊过天。关于种子节点还有一点需要记住，它们的目的是在引导时为节点服务；除此之外，它既不是单一的失败点，也不服务于任何其他目的。

## 故障场景处理–检测和恢复

嗯，流言蜚语协议是卡珊德拉自己知道失败何时发生的有效方式；也就是说，整个拳台都是通过流言蜚语知道一个被击倒的主持人的。相反，当一个节点加入集群时，采用相同的机制通知环中的所有节点。

一旦卡珊德拉检测到环上的节点出现故障，它将停止向其路由客户端请求，故障肯定会对集群的整体性能产生一些影响。然而，在我们有足够的一致性副本提供给客户端之前，它从来都不是一个阻止程序。

关于八卦的另一个有趣的事实是，它发生在不同的层面——卡珊德拉八卦，像现实世界的八卦一样，可能是二手的或第三方的，等等；这是间接八卦的表现。

节点故障可能是实际的，也可能是虚拟的。这意味着，一个节点可能由于系统硬件泄露而实际发生故障，或者故障可能是虚拟的，其中，在一段时间内，网络延迟如此之高，以至于看起来节点没有响应。后者的场景，大多数时候，都是可恢复的；也就是说，一段时间后，网络恢复正常，节点再次在环中被检测到。活动节点会不断尝试定期 ping 失败的节点并与其闲聊，以查看它们是否正常工作。如果一个节点被声明为永久脱离集群，我们需要一些管理干预来明确地将该节点从环中移除。

当一个节点在相当长的时间后重新加入集群时，它可能会错过几次写入(插入/更新/删除)，因此，节点上的数据与最新数据状态的相差甚远。建议使用`nodetool repair`命令运行维修。

# 卡珊德拉集群扩展–添加新节点

卡珊德拉非常容易缩放，并且没有停机时间。这也是为什么它会比其他竞争者更受青睐的原因之一。步骤非常简单明了:

1.  您需要在要添加的节点上设置 Cassandra。先不要启动卡珊德拉进程；首先，遵循以下步骤:
    1.  更新`seed_provider`下`Cassandra.yaml`中的种子节点。
    2.  确保`tmp`文件夹干净。
    3.  将`auto_bootstrap`添加到`Cassandra.yaml`并将其设置为`true`。
    4.  更新`Cassandra.yaml`中的`cluster_name`。
    5.  在`Cassandra.yaml`更新`listen_address` / `broadcast_address`。
2.  逐个启动所有新节点，在两次连续启动之间至少暂停 5 分钟。
3.  一旦节点启动，它将根据它所拥有的令牌范围来声明它的数据份额，并开始流式传输。这可以使用`nodetoolnetstat`命令来验证，如以下代码所示:

    ```
    mydomain@my-cass1:/home/ubuntu$ /usr/local/cassandra/apache- cassandra-1.1.6/bin/nodetool -h 10.3.12.29 netstats | grep - v 0%
    Mode: JOINING
    Not sending any streams.
    Streaming from: /10.3.12.179
    my_keyspace:  /var/lib/cassandra/data/my_keyspace/mycf/my_keyspace-my-hf- 461279-Data.db sections=1  progress=2382265999194/3079619547748 - 77%
    Pool Name                    Active   Pending      Completed
    Commands                        n/a         0             33
    Responses                       n/a         0       13575829
    mydomain@my-cass1:/home/ubuntu$

    ```

4.  在所有节点加入集群后，严格来说建议您在所有节点上运行`nodetool cleanup`命令。建议这样做，以便他们放弃对以前属于他们但现在属于已加入群集的新节点的密钥的控制。这里是命令和执行输出:

    ```
    mydomain@my-cass3:/usr/local/cassandra/apache-cassandra- 1.1.6/bin$ sudo -bE ./nodetool -h 10.3.12.178 cleanup  my_keyspacemycf_index
    mydomain@my-cass3:/usr/local/cassandra/apache-cassandra- 1.1.6/bin$ du -h   /var/lib/cassandra/data/my_keyspace/mycf_index/
    53G  /var/lib/cassandra/data/my_keyspace/mycf_index/
    mydomain@my-cass3:/usr/local/cassandra/apache-cassandra- 1.1.6/bin$ jps
    27389 Jps
    26893 NodeCmd
    17925 CassandraDaemon

    ```

5.  注意`NodeCmd`进程实际上是卡珊德拉守护进程的清理进程。前一个节点清理后回收的磁盘空间如下所示:

    ```
    Size before cleanup – 57G
    Size after cleanup – 30G

    ```

# 卡珊德拉集群——替换一个死节点

本部分捕获了在 Cassandra 集群中可能发生并导致故障的各种情况和场景。我们还将为您提供相关知识，并讲述处理这些情况的步骤。这些情况特定于 1.1.6 版，但也可以应用于其他版本。

比方说，这就是问题所在:您正在运行一个 n 节点，例如，假设有三个节点集群，一个节点从这个节点向下；这将导致不可恢复的硬件故障。解决方法是这样的:用新节点替换死节点。

以下是实现解决方案的步骤:

1.  使用`nodetool ring`命令

    ```
    bin/nodetool ring -h hostname

    ```

    确认节点故障
2.  死节点将显示为`DOWN`；让我们假设`node3`停机:

    ```
    192.168.1.54 datacenter1rack1 Up  Normal 755.25 MB 50.00% 0
    192.168.1.55 datacenter1rack1 Down Normal 400.62 MB 25.00%  42535295865117307932921825928971026432
    192.168.1.56 datacenter1rack1 Up  Normal 793.06 MB 25.00%  85070591730234615865843651857942052864

    ```

3.  Install and configure Cassandra on the replacement node. Make sure we remove the old installation, if any, from the replaced Cassandra node using the following command:

    ```
    sudorm -rf /var/lib/cassandra/*

    ```

    这里，`/var/lib/cassandra`是卡珊德拉的卡珊德拉数据目录的路径。

4.  配置`Cassandra.yaml`使其保持与预先存在的 Cassandra 集群相同的非默认设置。
5.  将替换节点的`cassandra.yaml`文件中的`initial_token`范围设置为死节点令牌 1 的值，即`42535295865117307932921825928971026431`。
6.  启动新节点将在环中死节点之前的一个位置加入集群:

    ```
    192.168.1.54 datacenter1rack1 Up    Normal 755.25 MB 50.00% 0
    192.168.1.51 datacenter1rack1 Up    Normal 400.62 MB 0.00%  42535295865117307932921825928971026431
    192.168.1.55 datacenter1rack1 Down     Normal 793.06 MB 25.00%  42535295865117307932921825928971026432
    192.168.1.56 datacenter1rack1 Up    Normal 793.06 MB 25.00%  85070591730234615865843651857942052864

    ```

7.  我们快完成了。只需在每个键空间的每个节点上运行【T0:

    ```
    nodetool repair -h 192.168.1.54 keyspace_name -pr
    nodetool repair -h 192.168.1.51 keyspace_name -pr
    nodetool repair -h 192.168.1.56 keyspace_name–pr

    ```

8.  Remove the token of the dead node from the ring using the following command:

    ```
    nodetoolremovetoken 85070591730234615865843651857942052864

    ```

    需要在所有剩余节点上执行此命令，以确保所有活动节点都知道死节点不再可用。

9.  这从集群中移除死节点；现在我们结束了。

# 复制因子

偶尔，我们会遇到这样的情况:我们对复制因子进行了更改。例如，我从一个较小的集群开始，因此我将复制因子保持为 2。后来，我从 4 个节点扩展到 8 个节点，因此为了使我的整个设置更加安全，我将复制因子增加到 4。在这种情况下，应遵循以下步骤:

1.  以下是更新复制因子和/或更改策略的命令。在卡珊德拉命令行界面上执行以下命令:

    ```
    ALTER KEYSPACEmy_keyspace WITH REPLICATION = { 'class' :  'SimpleStrategy', 'replication_factor' : 4 };

    ```

2.  一旦命令被更新，您必须在每个节点上逐个(连续地)执行`nodetool`修复，以便根据新的复制值正确复制所有密钥:

    ```
    sudo -bE ./nodetool -h 10.3.12.29 repair my_keyspacemycf -pr
    6
    mydomain@my-cass3:/home/ubuntu$ sudo -E  /usr/local/cassandra/apache-cassandra-1.1.6/bin/nodetool -h  10.3.21.29 compactionstats
    pending tasks: 1
    compaction type  keyspace         column family bytes  compacted      bytes total  progress
    Validation       my_keyspacemycf  1826902206  761009279707   0.24%
    Active compaction remaining time :        n/a
    mydomain@my-cass3:/home/ubuntu$

    ```

以下`compactionstats`命令用于跟踪`nodetool repair`命令的进度。

# 节点工具命令

卡珊德拉中的`nodetool`命令是卡珊德拉管理员手中最方便的工具。它拥有各种节点的各种情境处理所需的所有工具和命令。让我们仔细看看一些广泛使用的方法:

*   `Ring`: This command depicts the state of nodes (normal, down, leaving, joining, and so on). The ownership of the token range and percentage ownership of the keys along with the data centre and rack details is as follows:

    ```
    bin/nodetool -host 192.168.1.54 ring

    ```

    输出如下所示:

    ```
    192.168.1.54 datacenter1rack1 Up    Normal 755.25 MB 50.00% 0
    192.168.1.51 datacenter1rack1 Up    Normal 400.62 MB 0.00%  42535295865117307932921825928971026431
    192.168.1.55 datacenter1rack1 Down    Normal 793.06 MB 25.00%  42535295865117307932921825928971026432
    192.168.1.56 datacenter1rack1 Up    Normal 793.06 MB 25.00%  85070591730234615865843651857942052864

    ```

*   `Join`:这是可以和`nodetool`一起使用的选项，需要执行将新节点添加到集群中。当新节点加入集群时，它开始从其他节点流式传输数据，直到它根据基于环中令牌的指定所有权接收到所有密钥。可以使用`netsat`命令

    ```
    mydomain@my-cass3:/home/ubuntu$ /usr/local/cassandra/apache- cassandra-1.1.6/bin/nodetool -h 10.3.12.29 netstats | grep - v 0%
    Mode: JOINING
    Not sending any streams.
    Streaming from: /10.3.12.179
    my_keyspace:  /var/lib/cassandra/data/my_keyspace/mycf/my_keyspace-mycf- hf-46129-Data.db sections=1  progress=238226599194/307961954748 - 77%
    Pool Name                    Active   Pending      Completed
    Commands                        n/a         0             33
    Responses                       n/a         0       13575829

    ```

    检查该状态
*   `Info`:此`nodetool`选项获取以下命令中指定节点的所有必需信息:

    ```
    bin/nodetool -host 10.176.0.146 info
    Token(137462771597874153173150284137310597304)
    Load Info        : 0 bytes.
    Generation No    : 1
    Uptime (seconds) : 697595
    Heap Memory (MB) : 28.18 / 759.81

    ```

*   `Cleanup`:这是这个选项，一般是我们缩放集群的时候用的。添加新节点，因此现有节点需要放弃对现在属于集群中新进入者的密钥的控制:

    ```
    mydomain@my-cass3:/usr/local/cassandra/apache-cassandra- 1.1.6/bin$ sudo -bE ./nodetool -h 10.3.12.178 cleanup  my_keyspacemycf_index
    mydomain@my-cass3:/usr/local/cassandra/apache-cassandra- 1.1.6/bin$ du -h  /var/lib/cassandra/data/my_keyspace/mycf_index/
    53G  /var/lib/cassandra/data/my_keyspace/mycf_index/
    aeris@nrt-prod-cass3-C2:/usr/local/cassandra/apache-cassandra- 1.1.6/bin$ sudo `which jps
    27389 Jps
    26893 NodeCmd
    17925 CassandraDaemon
    mydomain@my-cass3:/usr/local/cassandra/apache-cassandra- 1.1.6/bin$ du -h  /var/lib/cassandra/data/my_keyspace/mycf_index/
    53G  /var/lib/cassandra/data/my_keyspace/mycf_index/

    ```

*   `Compaction`: This is one of the most useful tools. It's used to explicitly issue the `compact` command to Cassandra. This can be done on the entire node, key space, or at the column family level:

    ```
    sudo -bE /usr/local/cassandra/apache-cassandra- 1.1.6/bin/nodetool -h 10.3.1.24 compact
    mydomain@my-cass3:/home/ubuntu$ sudo -E  /usr/local/cassandra/apache-cassandra-1.1.6/bin/nodetool -h  10.3.1.24 compactionstats
    pending tasks: 1
    compaction type keyspace column family bytes compacted bytes  total progress
    Compaction my_keyspacemycf 1236772 1810648499806 0.00%
    Active compaction remaining time:29h58m42s
    mydomain@my-cass3:/home/ubuntu$

    ```

    卡珊德拉有两种类型的压缩:小压缩和大压缩。每当新的`sstable`数据被创建以移除所有的墓碑(即，被删除的条目)时，压缩的小循环就被执行。

    主要的压缩是使用前面的`nodetool`命令手动触发的。这可以应用于节点、键空间和柱族级别。

*   `Decommission`:在某种程度上，这与引导相反，当我们想要一个节点离开集群时触发。一个活动节点收到命令，它就停止接受新的权限，刷新`memtables`，并开始将数据从自身流式传输到节点，这些节点将成为其当前拥有的密钥范围的新所有者:

    ```
    bin/nodetool -h 192.168.1.54 decommission

    ```

*   `Removenode`:这个命令在节点失效，即物理不可用时执行。这会通知其他节点该节点不可用。Cassandra 复制通过根据新的环所有权创建数据拷贝来恢复正确的复制:

    ```
    bin/nodetoolremovenode<UUID>
    bin/nodetoolremovenode force

    ```

*   `Repair`:执行此`nodetool repair`命令，固定任意节点上的数据。这个是一个非常重要的工具，可以保证有数据一致性，一段时间后重新加入集群的节点存在。让我们假设一个集群有四个节点，通过风暴拓扑来满足连续写入。在这里，一两个小时后，其中一个节点关闭并再次加入环。现在，在此期间，节点可能会错过一些写入；要修复这个数据，我们应该在节点上执行`repair`命令:

    ```
    bin/nodetool repair

    ```

# 卡珊德拉容错

使用 Cassandra 作为数据存储的主要原因之一是它的容错能力。不是典型的主从架构驱动的，主机的故障成为系统崩溃的单点。相反，它有一个以环形模式运行的概念，因此没有单点故障。无论何时需要，我们都可以重新启动节点，而不必担心会导致整个集群崩溃；在各种情况下，这种能力都会派上用场。

有些情况下，我们需要重新启动 Cassandra，但是 Cassandra 的环形体系结构使管理员能够无缝地做到这一点，集群的宕机时间为零。这意味着，在以下情况下，需要重新启动 Cassandra 集群，Cassandra 管理员可以逐个重新启动节点，而不是关闭整个集群，然后再启动它:

*   通过更改内存配置来启动 Cassandra 守护程序
*   在已经运行的 Cassandra 集群上启用 JMX
*   有时机器需要例行维护，需要重启

# 卡珊德拉监控系统

既然我们已经讨论了 Cassandra 的各种管理方面，让我们探索 Cassandra 集群的各种缓冲和监控选项。我们现在将讨论各种免费和许可的工具。

## JMX 监测

你可以对卡珊德拉使用一种基于`jconsole`的监控。以下是使用`jconsole`连接卡珊德拉的步骤:

1.  In the Command Prompt, execute the `jconsole` command:

    ![JMX monitoring](../images/00051.jpeg)

2.  In the next step, you have to specify the Cassandra node IP and port for connectivity:

    ![JMX monitoring](../images/00052.jpeg)

3.  Once you are connected, JMX provides a variety of graphs and monitoring utilities:

    ![JMX monitoring](../images/00053.jpeg)

开发人员可以使用 jconsole **Memory** 选项卡监控堆内存使用情况。这将帮助您了解节点资源的利用率。

jconsole 的局限性在于它执行特定于节点的监控，而不是基于 Cassandra 环的监控和缓冲。让我们探索一下上下文中的其他工具。

## 数据税选项

这是一个data tax 提供的实用程序，带有一个图形界面，允许用户从一个中央仪表板监控和执行管理活动。请注意，免费版本仅适用于非生产用途。

数据税运营中心为各种重要系统**关键绩效指标** ( **关键绩效指标**)提供了大量的图形表示，如绩效趋势、汇总等。它的用户界面也提供了历史数据分析和对单个数据点的向下钻取功能。OpsCenter 将其所有指标存储在 Cassandra 本身中。OpsCenter 实用程序的主要功能如下:

*   对整个集群进行基于关键绩效指标的监控
*   警报和警报
*   结构管理
*   易于设置

您可以使用以下简单的步骤安装并设置 OpsCenter:

1.  运行以下命令开始:

    ```
    $ sudo service opscenterd start

    ```

2.  在`http://localhost:8888`连接到网络浏览器中的 OpsCenter。
3.  您将看到一个欢迎屏幕，您可以选择生成新集群或连接到现有集群。
4.  接下来，配置代理；一旦完成，OpsCenter 就可以使用了。

下面是应用程序的截图:

![Datastax OpsCenter](../images/00054.jpeg)

这里我们选择要执行的度量，以及操作是在特定的节点上执行还是在所有节点上执行。下面的截图捕捉到 OpsCenter 启动并识别集群中的各个节点:

![Datastax OpsCenter](../images/00055.jpeg)

下面的屏幕截图捕获了群集读写、总体群集延迟、磁盘 I/O 等方面的各种关键绩效指标:

![Datastax OpsCenter](../images/00056.jpeg)

# 测验时间

问题 1 .陈述以下陈述是对还是错。

1.  卡珊德拉只有一个失败点。
2.  卡珊德拉环中会立即检测到一个死节点。
3.  八卦是一种数据交换协议。
4.  `decommission`和`removenode`命令相同。

问题 2 .填空。

1.  _____ 是用于运行压缩的命令。
2.  _____ 是获取活动节点信息的命令。
3.  _____ 是显示整个群集信息的命令。

问题 3 .执行以下用例，查看 Cassandra 高可用性和复制:

1.  创建 4 节点 Cassandra 集群。
2.  创建复制因子为 3 的键空间。
3.  在其中一个节点上关闭卡珊德拉守护进程。
4.  在每个节点上执行`nestat`查看数据流。

# 总结

在这一章中，您学习了 gossip 协议的概念，并调整了用于各种场景的工具，例如扩展集群、替换死节点、压缩以及在 Cassandra 上的修复操作。

在下一章中，我们将讨论风暴集群的维护和操作方面。