# 为建模准备数据

在本章中，我们将介绍如何清理数据并为建模做准备。您将学习以下食谱:

*   处理重复项
*   处理缺失的观察值
*   处理异常值
*   探索描述性统计
*   计算相关性
*   绘制直方图
*   可视化要素之间的交互

# 介绍

既然我们已经彻底了解了关系数据库和数据框架是如何工作的，以及它们能做什么，我们就可以开始为建模准备我们自己和我们的数据了。

某著名人士(阿尔伯特·爱因斯坦)曾说过(转述):

"The universe and the problems with any dataset are infinite, and I am not sure about the former."

前面当然是个笑话。然而，你使用的任何数据集，无论是在工作中获得的、在网上找到的、自己收集的，还是通过任何其他方式获得的，在得到证明之前都是肮脏的；你不应该相信它，你不应该玩它，你甚至不应该看它，直到你向自己证明它足够干净(没有完全干净的东西)。

你的数据集会有什么问题？举几个例子:

*   **重复观察**:这些是由系统和操作员的故障引起的
*   **缺失观察**:这些可能是由于传感器问题、受访者不愿意提供问题的答案，或者只是一些数据损坏而出现的
*   **无观测值**:当你观察它们时，与数据集的其他部分或一个群体相比，它们是突出的观测值
*   **编码**:未规范化的文本字段(例如，单词没有词干或使用同义词)，在不同的语言中，或者您可能会遇到乱码文本输入，日期和日期时间字段可能不会以相同的方式编码
*   **不可信的答案(尤其是对于调查而言为真)**:当被调查者以任何理由撒谎时；这种类型的脏数据更难处理和清理

正如你所看到的，你的数据可能被成千上万的陷阱所困扰，这些陷阱正等着你去上当。清理数据并熟悉它是我们(作为数据科学家)80%的时间所做的事情(剩下的 20%我们花在构建模型和抱怨清理数据上)。所以系好安全带，为*颠簸的旅程*做好准备，这是我们信任现有数据并熟悉它所必需的。

在本章中，我们将使用一个由`22`条记录组成的小数据集:

```
dirty_data = spark.createDataFrame([(1,'Porsche','Boxster S','Turbo',2.5,4,22,None), (2,'Aston Martin','Vanquish','Aspirated',6.0,12,16,None), (3,'Porsche','911 Carrera 4S Cabriolet','Turbo',3.0,6,24,None), (3,'General Motors','SPARK ACTIV','Aspirated',1.4,None,32,None), (5,'BMW','COOPER S HARDTOP 2 DOOR','Turbo',2.0,4,26,None), (6,'BMW','330i','Turbo',2.0,None,27,None), (7,'BMW','440i Coupe','Turbo',3.0,6,23,None), (8,'BMW','440i Coupe','Turbo',3.0,6,23,None), (9,'Mercedes-Benz',None,None,None,None,27,None), (10,'Mercedes-Benz','CLS 550','Turbo',4.7,8,21,79231), (11,'Volkswagen','GTI','Turbo',2.0,4,None,None), (12,'Ford Motor Company','FUSION AWD','Turbo',2.7,6,20,None), (13,'Nissan','Q50 AWD RED SPORT','Turbo',3.0,6,22,None), (14,'Nissan','Q70 AWD','Aspirated',5.6,8,18,None), (15,'Kia','Stinger RWD','Turbo',2.0,4,25,None), (16,'Toyota','CAMRY HYBRID LE','Aspirated',2.5,4,46,None), (16,'Toyota','CAMRY HYBRID LE','Aspirated',2.5,4,46,None), (18,'FCA US LLC','300','Aspirated',3.6,6,23,None), (19,'Hyundai','G80 AWD','Turbo',3.3,6,20,None), (20,'Hyundai','G80 AWD','Turbo',3.3,6,20,None), (21,'BMW','X5 M','Turbo',4.4,8,18,121231), (22,'GE','K1500 SUBURBAN 4WD','Aspirated',5.3,8,18,None)], ['Id','Manufacturer','Model','EngineType','Displacement','Cylinders','FuelEconomy','MSRP'])
```

在后续的食谱中，我们将清理前面的数据集，并了解更多关于它的信息。

# 处理重复项

重复数据出现在数据中有很多原因，但有时很难发现它们。在本食谱中，我们将向您展示如何发现最常见的问题，并使用 Spark 处理它们。

# 准备好

要执行这个食谱，你需要有一个工作的火花环境。如果您没有，您可能想回到[第 1 章](1.html#OPEK0-dc04965c02e747b9b9a057725c821827)、*安装和配置 Spark、*并遵循您将在那里找到的食谱。

我们将研究引言中的数据集。本章中您需要的所有代码都可以在我们为这本书设置的 GitHub 存储库中找到:[http://bit.ly/2ArlBck](http://bit.ly/2ArlBck)。转到`Chapter04`并打开 [4。准备建模数据。ipynb](https://github.com/drabastomek/PySparkCookbook/blob/devel/Chapter04/4.Preparing%20data%20for%20modeling.ipynb) 笔记本。

不需要其他先决条件。

# 怎么做...

重复是数据集中出现多次的记录。这是一个精确的副本。Spark 数据框有一个方便的方法来删除重复的行，`.dropDuplicates()`转换:

1.  检查是否有重复的行，如下所示:

```
dirty_data.count(), dirty_data.distinct().count()
```

2.  如果有重复项，请将其删除:

```
full_removed = dirty_data.dropDuplicates()
```

# 它是如何工作的...

你现在应该知道这个了，但是`.count()`方法计算我们的数据帧中有多少行。第二个命令检查我们有多少不同的行。在我们的`dirty_data`上执行这两个命令。结果数据框产生`(22, 21)`。因此，我们现在知道我们的数据集中有两条记录是彼此的精确副本。让我们看看哪些:

```
(dirty_data.groupby(dirty_data.columns).count().filter('count > 1').show())
```

让我们解开这里发生的事情。首先，我们使用`.groupby(...)`方法定义聚合使用哪些列；在这个例子中，我们基本上使用了所有的列，因为我们希望找到数据集中所有列的所有不同组合。接下来，我们使用`.count()`方法计算这样的值组合出现了多少次；该方法将`count`列添加到我们的数据集。使用`.filter(...)`方法，我们选择数据集中出现多次的所有行，并使用`.show()`操作将它们打印到屏幕上。

这会产生以下结果:

![](../images/00078.jpeg)

所以`Id`等于`16`的行就是重复的行。所以，让我们用`.dropDuplicates(...)`的方法放弃它。最后，运行`full_removed.count()`命令确认我们现在有 21 条记录。

# 还有更多...

嗯，正如你所想象的，还有更多。我们的`full_removed`数据框中还有一些重复的记录。让我们仔细看看。

# 只有身份证不同

如果随着时间的推移收集数据，您可能会记录相同的数据，但使用不同的标识。让我们检查一下我们的数据框是否有这样的记录。下面的代码片段将帮助您做到这一点:

```
(full_removed.groupby([col for col in full_removed.columns if col != 'Id']).count().filter('count > 1').show())
```

就像以前一样，我们首先按所有列分组，但我们排除了`'Id'`列，然后计算我们从这个分组中获得了多少条记录，最后我们用`'count > 1'`提取这些记录并在屏幕上显示。运行了前面的代码后，我们得到了以下结果:

![](../images/00079.jpeg)

如你所见，我们有四个不同 id 的记录，但它们是相同的车:`BMW` `440i Coupe`和`Hyundai` `G80 AWD`。

我们也可以检查计数，就像以前一样:

```
no_ids = (full_removed.select([col for col in full_removed.columns if col != 'Id']))no_ids.count(), no_ids.distinct().count()
```

首先，我们只选择除`'Id'`1 之外的所有列，然后计算总行数和不同行的总数。运行前面的片段后，您应该会看到`(21, 19)`，这表明我们有四条记录被复制，就像我们前面看到的一样。

`.dropDuplicates(...)`方法可以轻松处理这种情况。我们所需要做的就是向`subset`参数传递一个我们希望它在搜索重复项时考虑的所有列的列表。以下是如何:

```
id_removed = full_removed.dropDuplicates(subset = [col for col in full_removed.columns if col != 'Id'])
```

再次，我们选择除`'Id'`列以外的所有列来定义使用哪些列来确定重复项。如果我们现在计算`id_removed`数据帧中的总行数，我们应该得到`19`:

![](../images/00080.jpeg)

这正是我们得到的！

# 身份冲突

您还可以假设，如果有两条记录具有相同的标识，则它们是重复的。嗯，虽然这可能是真的，但我们现在已经在删除基于所有列的记录时删除了它们。因此，在这一点上，任何重复的标识更有可能发生冲突。

重复的标识可能由于多种原因而出现:检测错误或存储标识的数据结构不足，或者如果标识代表记录元素的某个散列函数，则选择散列函数可能会产生冲突。这些只是为什么你可能有重复的身份证，但记录并没有真正重复的几个原因。

让我们检查一下数据集是否如此:

```
import pyspark.sql.functions as fnid_removed.agg(fn.count('Id').alias('CountOfIDs'), fn.countDistinct('Id').alias('CountOfDistinctIDs')).show()
```

在本例中，我们将使用`.agg(...)`方法，而不是对记录进行子集化，然后对记录进行计数，然后对不同的记录进行计数。为此，我们首先从`pyspark.sql.functions`模块导入所有功能。

For a list of all the functions available in `pyspark.sql.functions`, please refer to [https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions).

我们将使用的两个函数将允许我们一次性进行计数:`.count(...)`方法对指定列中具有非空值的所有记录进行计数，而`.countDistinct(...)`则返回这样一列中不同值的计数。`.alias(...)`方法允许我们为计数产生的列指定友好的名称。以下是我们计算后得到的结果:

![](../images/00081.jpeg)

好的，我们有两条相同身份的记录。同样，让我们检查哪些标识是重复的:

```
(id_removed.groupby('Id').count().filter('count > 1').show())
```

和以前一样，我们首先按`'Id'`列中的值分组，然后用大于`1`的`count`显示所有记录。我们得到的是:

![](../images/00082.jpeg)

看起来我们有两张`'Id == 3'`的记录。让我们检查它们是否相同:

![](../images/00083.jpeg)

这些肯定不是相同的记录，但它们共享相同的 ID。在这种情况下，我们可以创建一个唯一的新标识(我们已经确保数据集中没有其他重复的标识)。PySpark 的 SQL 函数模块提供了一种`.monotonically_increasing_id()`方法，可以创建一个唯一的 id 流。

The `.monotonically_increasing_id()`—generated ID is guaranteed to be unique as long as your data lives in less than one billion partitions and with less than eight billion records in each. That's a pretty big number.

这里有一个片段，它将创建一个唯一的 ID 列并替换它:

```
new_id = (id_removed.select([fn.monotonically_increasing_id().alias('Id')] + [col for col in id_removed.columns if col != 'Id']))new_id.show()
```

我们首先创建标识列，然后选择除原始`'Id'`列之外的所有其他列。以下是新身份证的外观:

![](../images/00084.jpeg)

这些数字绝对是独一无二的。我们现在准备处理数据集中的其他问题。

# 处理缺失的观察值

缺失观测值几乎是数据集中第二常见的问题。正如我们在导言中已经提到的那样，出现这些问题有许多原因。在这个食谱中，我们将学习如何处理它们。

# 准备好

要执行这个食谱，你需要有一个工作的火花环境。此外，我们将处理我们在前面的配方中创建的`new_id`数据框，因此我们假设您已经按照步骤删除了重复的记录。

不需要其他先决条件。

# 怎么做...

由于我们的数据有两个维度(行和列)，我们需要检查每行和每列中丢失数据的百分比，以确定保留什么、丢弃什么以及(潜在地)估算什么:

1.  要计算一行中有多少缺失的观察值，请使用以下代码片段:

```
(spark.createDataFrame(new_id.rdd.map(lambda row: (row['Id'], sum([c == None for c in row]))).collect().filter(lambda el: el[1] > 1),['Id', 'CountMissing']).orderBy('CountMissing', ascending=False).show())
```

2.  要计算每列中缺少多少数据，请使用以下代码:

```
for k, v in sorted(merc_out.agg(*[(1 - (fn.count(c) / fn.count('*'))).alias(c + '_miss')for c in merc_out.columns]).collect()[0].asDict().items(), key=lambda el: el[1], reverse=True):print(k, v)
```

让我们一步一步地走完这些。

# 它是如何工作的...

现在让我们看看如何在下面的部分中详细处理行和列中缺失的观察值。

# 每行缺少观察值

要计算一行中丢失了多少数据，使用 RDDs 更容易，因为我们可以遍历 RDD 记录的每个元素，并计算丢失了多少值。因此，我们要做的第一件事就是访问`new_id`数据框中的`.rdd`。使用`.map(...)`变换，我们遍历每一行，提取`'Id'`，并使用`sum([c == None for c in row])`表达式计算一个元素丢失的次数。这些操作的结果是元素的 RDD，每个元素有两个值:行的标识和缺失值的计数。

接下来，我们只选择那些有多个缺失值的记录和`.collect()`那些关于驱动程序的记录。然后，我们创建一个简单的数据框`.orderBy(...)`，按照缺失值的计数降序排列，并显示记录。

结果如下:

![](../images/00085.jpeg)

如您所见，其中一条记录的八个值中有五个丢失。让我们看看记录:

```
(new_id.where('Id == 197568495616').show())
```

前面的代码显示其中一条`Mercedes-Benz`记录的大部分值丢失了:

![](../images/00086.jpeg)

因此，我们可以放弃整个观察，因为在这个记录中并没有包含太多的价值。为了实现这个目标，我们可以使用数据帧的`.dropna(...)`方法:`merc_out = new_id.dropna(thresh=4)`。

If you use `.dropna()` without passing any parameters, any record that has a missing value will be removed.

我们指定`thresh=4`，所以我们只删除最少有四个非缺失值的记录；我们的记录只有三条有用的信息。

让我们确认:运行`new_id.count(), merc_out.count()`产生`(19, 18)`，所以是的，确实，我们删除了其中一个记录。我们真的移除了`Mercedes-Benz`吗？让我们检查一下:

```
(merc_out.where('Id == 197568495616').show())
```

前面的代码片段生成了一个空表，因此它删除了`Id`等于`197568495616`的记录，如下图所示:

![](../images/00087.jpeg)

# 每列缺少观察值

我们还需要检查是否有有用信息发生率特别低的栏目。在我们展示的代码中有很多事情正在发生，所以让我们一步一步地解开它。

让我们从内部列表开始:

```
[(1 - (fn.count(c) / fn.count('*'))).alias(c + '_miss')for c in merc_out.columns]
```

我们循环遍历`merc_out`数据框中的所有列，并计算在每一列中找到多少个非缺失值。然后，我们用所有行的总数除以它，再从 1 中减去它，这样我们就得到缺失值的百分比。

We imported `pyspark.sql.functions` as `fn` earlier in the chapter.

然而，我们在这里实际做的并不是计算什么。此时，Python 存储这些信息的方式只是作为一个对象列表，或者指向某些操作的指针。只有在我们将列表传递给`.agg(...)`方法之后，它才会被翻译成 PySpark 的内部执行图(只有当我们调用`.collect()`动作时才会被执行)。

The `.agg(...)` method accepts a set of parameters, not as a list object, but as a comma-separated list of parameters. Therefore, instead of passing the list itself to the `.agg(...)` method, we included `'*'` in front of the list, which unfolds each element of our list and passes it like a parameter to our method.

`.collect()`方法将返回一个元素的列表——一个带有聚合信息的`Row`对象。我们可以使用`.asDict()`方法将`Row`转换成字典，然后从中提取所有的`items`。这将产生一个元组列表，其中第一个元素是列名(我们使用`.alias(...)`方法将`'_miss'`追加到每个列中)，第二个元素是缺失观测值的百分比。

在遍历排序列表的元素时，我们只需将它们打印到屏幕上:

![](../images/00088.jpeg)

看起来`MSRP`栏的大部分信息都不见了。因此，我们可以放弃它，因为它不会给我们带来任何有用的信息:

```
no_MSRP = merc_out.select([col for col in new_id.columns if col != 'MSRP'])
```

我们还有两栏缺少一些信息。让我们为他们做点什么。

# 还有更多...

PySpark 允许您估算缺失的观测值。您可以传递一个值，数据中的每一个`null`或`None`都将被替换，或者您可以传递一个字典，为缺少观察值的每一列提供不同的值。在这个例子中，我们将使用后一种方法，并将指定燃油经济性和排量之间的比率，以及气缸数量和排量之间的比率。

首先，让我们创建我们的字典:

```
multipliers = (no_MSRP.agg(fn.mean(fn.col('FuelEconomy') / (fn.col('Displacement') * fn.col('Cylinders'))).alias('FuelEconomy'), fn.mean(fn.col('Cylinders') / fn.col('Displacement')).alias('Cylinders'))).toPandas().to_dict('records')[0]
```

在这里，我们有效地计算了我们的乘数。为了替换燃油经济性中缺失的值，我们将使用以下公式:

![](../images/00089.jpeg)

对于圆柱体的数量，我们将使用以下等式:

![](../images/00090.jpeg)

我们前面的代码使用这两个公式来计算每行的乘数，然后取它们的平均值。

This is not going to be totally accurate but given the data we have, it should be accurate enough.

在这里，我们还介绍了另一种用你的(小！)火花数据帧:使用`.toPandas()`方法将火花数据帧转换为熊猫数据帧。熊猫的数据框架有一个`.to_dict(...)`方法，可以让你把我们的数据转换成字典。`'records'`参数指示方法将每一行转换为字典，其中关键字是具有相应记录值的列名。

Check out this link to read more about the `.to_dict(...)` method: [https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_dict.html](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_dict.html).

我们生成的字典如下所示:

![](../images/00091.jpeg)

现在让我们用它来估算我们丢失的数据:

```
imputed = (no_MSRP.withColumn('FuelEconomy', fn.col('FuelEconomy') / fn.col('Displacement') / fn.col('Cylinders')).withColumn('Cylinders', fn.col('Cylinders') / fn.col('Displacement')).fillna(multipliers).withColumn('Cylinders', (fn.col('Cylinders') * fn.col('Displacement')).cast('integer')).withColumn('FuelEconomy', fn.col('FuelEconomy') * fn.col('Displacement') * fn.col('Cylinders')))
```

首先，我们转换原始数据，以便它也反映我们之前指定的比率。接下来，我们使用乘数字典来填充缺失的值，最后我们将列恢复到它们的原始状态。

Note that each time we use the `.withColumn(...)` method, we overwrite the original column names.

生成的数据帧如下所示:

![](../images/00092.jpeg)

如您所见，气缸和燃油经济性的最终值并不完全准确，但可以说仍然比用一些预定义的值替换它们要好。

# 请参见

*   查看 PySpark 关于缺失观察方法的文档:[https://spark . Apache . org/docs/latest/API/python/PySpark . SQL . html # PySpark . SQL . data framenafunctions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameNaFunctions)

# 处理异常值

与其他观测值差异很大的观测值，即位于数据分布的长尾中的观测值，是异常值。在这个食谱中，我们将学习如何定位和处理异常值。

# 准备好

要执行这个食谱，你需要有一个工作的火花环境。此外，我们将处理我们在前面的配方中创建的`imputed`数据帧，因此我们假设您已经按照步骤处理了缺失的观察。

不需要其他先决条件。

# 怎么做...

让我们从离群值的一个流行定义开始。

符合以下标准的点![](../images/00093.jpeg):

![](../images/00094.jpeg)

不被视为异常值；这个范围之外的任何点都是。在上式中， *Q <sup class="calibre70">1</sup>* 为第一个四分位数(25 <sup xmlns:epub="http://www.idpf.org/2007/ops" class="calibre70">第</sup>百分位) *Q <sup class="calibre70">3</sup>* 为第三个四分位数， *IQR* 为**四分位数区间**，定义为 *Q <sup class="calibre70">3</sup>* 与*Q<sup class="calibre70">1</sup>*:IQR =

要标记异常值，请执行以下步骤:

1.  让我们先计算一下我们的范围:

```
features = ['Displacement', 'Cylinders', 'FuelEconomy']quantiles = [0.25, 0.75]cut_off_points = []for feature in features:quants = imputed.approxQuantile(feature, quantiles, 0.05)IQR = quants[1] - quants[0]cut_off_points.append((feature, [quants[0] - 1.5 * IQR,quants[1] + 1.5 * IQR,]))cut_off_points = dict(cut_off_points)
```

2.  接下来，我们标记异常值:

```
outliers = imputed.select(*['id'] + [((imputed[f] < cut_off_points[f][0]) |(imputed[f] > cut_off_points[f][1])).alias(f + '_o') for f in features])
```

# 它是如何工作的...

我们将只关注数字变量:排量、气缸和燃油经济性。

我们循环所有这些特征，并使用`.approxQuantile(...)`方法计算第一个和第三个四分位数。该方法将要素(列)名称作为其第一个参数，要计算的四分位数的浮点数(或浮点数列表)作为第二个参数，第三个参数指定相对目标精度(将该值设置为 0 将找到精确的分位数，但可能非常昂贵)。

该方法返回两个(在我们的例子中)值的列表:*Q<sup class="calibre70">1</sup>T5】和*Q<sup class="calibre70">3</sup>T9】。然后，我们计算四分位数之间的范围，并将`(feature_name, [lower_bound, upper_bound])`元组添加到`cut_off_point`列表中。转换成词典后，我们的分界点如下:**

![](../images/00095.jpeg)

所以，现在我们可以用这些来标记我们的外围观察。我们将只选择标识列，然后遍历我们的特征来检查它们是否超出了我们的计算范围。我们得到的是:

![](../images/00096.jpeg)

因此，我们在燃油经济性一栏中有两个异常值。让我们检查一下记录:

```
with_outliers_flag = imputed.join(outliers, on='Id')(with_outliers_flag.filter('FuelEconomy_o').select('Id', 'Manufacturer', 'Model', 'FuelEconomy').show())
```

首先，我们将`imputed`数据框与`outliers`数据框合并，然后我们过滤`FuelEconomy_o`标志，只选择我们的外围记录。最后，我们只提取最相关的列来显示:

![](../images/00097.jpeg)

所以我们有`SPARK ACTIV`和`CAMRY HYBRID LE`作为异常值。`SPARK ACTIV`由于我们的估算逻辑，我们不得不估算其燃油经济性值，因此成为了一个异常值；考虑到它的发动机排量是 1.4 升，我们的逻辑没有很好地工作。嗯，还有其他方法可以估算这些值。凯美瑞作为一款混合动力车型，在由大型涡轮增压发动机主导的数据集中绝对是个异类；在这里看到它，我们不应该感到惊讶。

试图基于具有异常值的数据构建机器学习模型可能会导致一些不可信的结果，或者模型不能很好地概括，因此我们通常会从数据集中删除这些结果:

```
no_outliers = (with_outliers_flag.filter('!FuelEconomy_o').select(imputed.columns))
```

前面的片段只是过滤掉了所有在我们的`FuelEconomy_o`列中不是异常值的记录。就这样！

# 请参见

*   查看本网站了解更多关于异常值的信息:[http://www . ITL . NIST . gov/div 898/handbook/PRC/section 1/PRC 16 . htm](http://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm)

# 探索描述性统计

描述性统计是最基本的数据度量。在这个食谱中，我们将了解在 PySpark 中熟悉我们的数据集有多容易。

# 准备好了

要执行这个食谱，你需要有一个工作的火花环境。此外，我们将处理我们在*处理异常值*配方中创建的`no_outliers`数据帧，因此我们假设您已经按照步骤处理了重复、缺失观察值和异常值。

不需要其他先决条件。

# 怎么做...

在 PySpark 中，计算数据的描述性统计数据非常容易。以下是如何:

```
descriptive_stats = no_outliers.describe(features)
```

就这样！

# 它是如何工作的...

前面的代码几乎不需要解释。`.describe(...)`方法获取要计算描述性统计的列列表，并返回一个包含基本描述性统计的数据框:计数、平均值、标准偏差、最小值和最大值。

You can specify both numeric and string columns as input parameters to `.describe(...)`. 

以下是我们在列列表`features`上运行`.describe(...)`方法得到的结果:

![](../images/00098.jpeg)

不出所料，我们总共有`16`条记录。我们的数据集似乎向更大的发动机倾斜(这里作为一个松散的术语使用，而不是统计术语)，因为平均排量是 6 缸的`3.44`升。燃油经济性，对于如此大的发动机，似乎是体面的，虽然，在 19 英里/加仑。

# 还有更多...

如果不传递列列表来计算描述性统计信息，PySpark 将返回数据框中每一列的统计信息。查看以下代码片段:

```
descriptive_stats_all = no_outliers.describe()descriptive_stats_all.show()
```

结果如下表所示:

![](../images/00099.jpeg)

正如您所看到的，即使是字符串列也得到了它们的描述性统计数据，然而，这些统计数据很难解释。

# 聚合列的描述性统计信息

有时，您希望计算一组值中的一些描述性统计数据。在本例中，我们将计算不同气缸数量的汽车的一些基本统计数据:

```
(no_outliers.select(features).groupBy('Cylinders').agg(*[fn.count('*').alias('Count'), fn.mean('FuelEconomy').alias('MPG_avg'), fn.mean('Displacement').alias('Disp_avg'), fn.stddev('FuelEconomy').alias('MPG_stdev')
```

```
, fn.stddev('Displacement').alias('Disp_stdev')]).orderBy('Cylinders')).show()
```

首先，我们选择我们的`features`列列表，这样我们就减少了需要分析的数据数量。接下来，我们汇总气缸列的数据，并使用(已经熟悉的)`.agg(...)`方法计算燃油经济性和排量的计数、平均值和标准偏差。

There are more aggregation functions available in the `pyspark.sql.functions` module: `avg(...)`, `count(...)`, `countDistinct(...)`, `first(...)`, `kurtosis(...)`, `max(...)`, `mean(...)`, `min(...)`, `skewness(...)`, `stddev_pop(...)`, `stddev_samp(...)`, `sum(...)`, `sumDistinct(...)`, `var_pop(...)`, `var_samp(...)`, and `variance(...)`.

下面是结果表:

![](../images/00100.jpeg)

从这张表中我们可以看出两件事:

*   我们的估算方法确实不准确，所以下次我们应该想出一个更好的方法。
*   `MPG_avg`六缸车比四缸车高，那就可疑了。这就是为什么你应该与你的数据越来越亲密，因为你可以在你的数据中发现这样的隐藏陷阱。

如何处理这样的发现超出了本书的范围。但是，关键是，这就是为什么数据科学家会花 80%的时间清理数据并熟悉它，因此可以依赖用这些数据构建的模型。

# 请参见

*   你可以根据你的数据计算出许多我们在这里没有涉及的其他统计数据(但是 PySpark 允许你计算)。想要了解更全面的概况，我们建议你去看看这个网站:[https://www.socialresearchmethods.net/kb/statdesc.php](https://www.socialresearchmethods.net/kb/statdesc.php)。

# 计算相关性

与结果相关的特征是可取的，但是那些在它们之间也相关的特征会使模型不稳定。在本食谱中，我们将向您展示如何计算特征之间的相关性。

# 准备好

要执行这个食谱，你需要有一个工作的火花环境。此外，我们将处理我们在*处理异常值*配方中创建的`no_outliers`数据帧，因此我们假设您已经按照步骤处理了重复、缺失观察值和异常值。

不需要其他先决条件。

# 怎么做...

要计算两个特征之间的相关性，您只需提供它们的名称:

```
(no_outliers.corr('Cylinders', 'Displacement'))
```

就这样！

# 它是如何工作的...

`.corr(...)`方法取两个参数，你要计算的两个特征的名称之间的相关系数。

Currently, only the Pearson correlation coefficient is available.

前面的命令将为我们的数据集产生一个等于`0.938`的相关系数。

# 还有更多...

如果你想计算一个相关矩阵，你需要手动完成。以下是我们的解决方案:

```
n_features = len(features)corr = []for i in range(0, n_features):temp = [None] * ifor j in range(i, n_features):temp.append(no_outliers.corr(features[i], features[j]))corr.append([features[i]] + temp)correlations = spark.createDataFrame(corr, ['Column'] + features)
```

前面的代码有效地循环了我们的`features`列表，并计算它们之间的成对相关性，以填充矩阵的上三角部分。

We introduced the `features` list in the *Handling outliers *recipe earlier.

然后，计算出的系数被附加到`temp`列表中，作为回报，该列表被添加到`corr`列表中。最后，我们创建相关数据帧。看起来是这样的:

![](../images/00101.jpeg)

如你所见，`Displacement`和`Cylinders`之间只有很强的相关性，这当然不足为奇。`FuelEconomy`与排量没有真正的关联，因为还有其他因素影响`FuelEconomy`，比如汽车的阻力和重量。然而，如果你试图预测，例如，最大速度，并假设(这是一个公平的假设)`Displacement`和`Cylinders`将与最大速度高度正相关，那么你应该只使用其中之一。

# 绘制直方图

直方图是直观检查数据分布的最简单方法。在本食谱中，我们将向您展示如何在 PySpark 中做到这一点。

# 准备好了

要执行这个食谱，你需要有一个工作的火花环境。此外，我们将处理我们在*处理异常值*配方中创建的`no_outliers`数据帧，因此我们假设您已经按照步骤处理了重复、缺失观察值和异常值。

不需要其他先决条件。

# 怎么做...

在 PySpark 中有两种生成直方图的方法:

*   选择想要可视化的特征，在驱动上`.collect()`它，然后使用 matplotlib 的原生`.hist(...)`方法绘制直方图
*   计算 PySpark 中每个直方图箱中的计数，并将计数返回给驱动程序进行可视化

前一种解决方案适用于小数据集(如本章中的数据集)，但如果数据太大，它会破坏您的驱动程序。此外，我们分发数据是有充分理由的，这样我们就可以并行而不是在一个线程中进行计算。因此，在本食谱中，我们将只向您展示第二种解决方案。下面是为我们做所有计算的片段:

```
histogram_MPG = (no_outliers.select('FuelEconomy').rdd.flatMap(lambda record: record).histogram(5))
```

# 它是如何工作的...

前面的代码非常容易理解。首先，我们选择感兴趣的特征(在我们的例子中，燃料经济性)。

The Spark DataFrames do not have a native histogram method, so that's why we switch to the underlying RDD.

接下来，我们将结果展平成一个长列表(而不是一个`Row`对象)，并使用`.histogram(...)`方法计算我们的直方图。

`.histogram(...)`方法接受一个整数来指定要分配给我们的数据的存储桶数量，或者接受一个具有指定存储桶限制的列表。

Check out PySpark's documentation on the `.histogram(...)` at [https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.histogram](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.histogram).

该方法返回两个元素的元组:第一个元素是容器边界的列表，另一个元素是相应容器中元素的计数。这就是我们燃油经济性的特点:

![](../images/00102.jpeg)

请注意，我们指定希望`.histogram(...)`方法将我们的数据分成五个箱，但是第一个列表中有六个元素。然而，我们的数据集中仍然有五个桶:*【8.97，12.38】【12.38，15.78】【15.78，19.19】【19.19，22.59】*，以及*【22.59，26.0】*。

如果不经过大量的设置，我们无法在 PySpark 中创建任何情节(例如，请参见:[https://plot.ly/python/apache-spark/](https://plot.ly/python/apache-spark/))。更简单的方法是用我们的数据准备一个数据框架，并使用一些*魔法*(嗯，火花魔法，但它仍然有效！)本地安装在驱动程序上。

首先，我们需要提取我们的数据并创建一个临时的`histogram_MPG`表:

```
(spark.createDataFrame([(bins, counts) for bins, counts in zip(histogram_MPG[0], histogram_MPG[1])], ['bins', 'counts']).registerTempTable('histogram_MPG'))
```

我们创建一个两列的 DataFrame，其中第一列包含 bin 下限，第二列包含相应的计数。`.registerTempTable(...)`方法(顾名思义)注册了一个临时表，这样我们就可以使用`%%sql`魔法:

```
%%sql -o hist_MPG -qSELECT * FROM histogram_MPG
```

前面的命令从我们的临时`histogram_MPG`表中选择所有记录，并将其输出到本地可访问的`hist_MPG`变量；`-q`开关在那里，所以没有东西被打印到笔记本上。

借助本地可访问的`hist_MPG`，我们现在可以使用它来制作我们的地块:

```
%%localimport matplotlib.pyplot as plt%matplotlib inlineplt.style.use('ggplot')fig = plt.figure(figsize=(12,9))ax = fig.add_subplot(1, 1, 1)ax.bar(hist_MPG['bins'], hist_MPG['counts'], width=3)ax.set_title('Histogram of fuel economy')
```

`%%local`在本地模式下执行位于该笔记本单元中的任何内容。首先，我们导入`matplotlib`库，并指定它在笔记本内内联生成绘图，而不是每次生成绘图时弹出一个新窗口。`plt.style.use(...)`改变我们图表的风格。

For a full list of available styles, check out [https://matplotlib.org/devdocs/gallery/style_sheets/style_sheets_reference.html](https://matplotlib.org/devdocs/gallery/style_sheets/style_sheets_reference.html).

接下来，我们创建一个图形，并添加一个我们将要绘制的子图。最后，我们使用`.bar(...)`方法绘制直方图并设置标题。图表如下所示:

![](../images/00103.jpeg)

就这样！

# 还有更多...

Matplotlib 不是我们唯一可以用来绘制直方图的库。Bokeh(可在[https://bokeh.pydata.org/en/latest/](https://bokeh.pydata.org/en/latest/)获得)是另一个强大的绘图库，建立在`D3.js`之上，允许你交互式地*使用你的图表玩*。

Check out the gallery of examples at [https://bokeh.pydata.org/en/latest/docs/gallery.html](https://bokeh.pydata.org/en/latest/docs/gallery.html).

以下是你如何与博克策划:

```
%%localfrom bokeh.io import showfrom bokeh.plotting import figurefrom bokeh.io import output_notebookoutput_notebook()labels = [str(round(e, 2)) for e in hist_MPG['bins']]p = figure(x_range=labels, plot_height=350, title='Histogram of fuel economy')p.vbar(x=labels, top=hist_MPG['counts'], width=0.9)show(p)
```

首先，我们加载 Bokeh 的所有必要组件；`output_notebook()`方法确保我们在笔记本中内联生成图表，而不是每次都打开一个新窗口。接下来，我们制作标签放在图表上。然后，我们定义我们的图形:`x_range`参数指定 *x* 轴上的点数，`plot_height`设置我们的图的高度。最后，我们使用`.vbar(...)`方法绘制直方图的条；`x`参数是放在我们的绘图上的标签，`top`参数指定计数。

结果如下:

![](../images/00104.jpeg)

这是相同的信息，但您可以在浏览器中与此图表交互。

# 请参见

*   如果你想进一步定制你的直方图，这里有一个可能有用的页面:[https://plot.ly/matplotlib/histograms/](https://plot.ly/matplotlib/histograms/)

# 可视化要素之间的交互

绘制要素之间的交互不仅可以加深您对数据分布的理解，还可以加深您对要素之间相互关系的理解。在本食谱中，我们将向您展示如何根据您的数据创建散点图。

# 准备好

要执行这个食谱，你需要有一个工作的火花环境。此外，我们将处理我们在*处理异常值*配方中创建的`no_outliers`数据帧，因此我们假设您已经按照步骤处理了重复、缺失观察值和异常值。

不需要其他先决条件。

# 怎么做...

我们将再次从数据框中选择数据，并在本地公开:

```
scatter = (no_outliers.select('Displacement', 'Cylinders'))scatter.registerTempTable('scatter')%%sql -o scatter_source -qSELECT * FROM scatter
```

# 它是如何工作的...

首先，我们选择两个我们想了解更多的特性，看看它们是如何相互作用的；在我们的例子中，它们是位移和圆柱体特征。

Our example here is small so we can work with all our data. However, in the real world, you should sample your data first before attempting to plot billions of data points.

注册临时表后，我们使用`%%sql`魔法从`scatter`表中选择所有数据，并在本地将其公开为`scatter_source`。现在，我们可以开始绘制:

```
%%localimport matplotlib.pyplot as plt%matplotlib inlineplt.style.use('ggplot')fig = plt.figure(figsize=(12,9))ax = fig.add_subplot(1, 1, 1)ax.scatter(list(scatter_source['Cylinders']), list(scatter_source['Displacement']), s = 200, alpha = 0.5)ax.set_xlabel('Cylinders')ax.set_ylabel('Displacement')ax.set_title('Relationship between cylinders and displacement')
```

首先，我们加载 Matplotlib 库并设置它。

See the *Drawing histograms* recipe for a more detailed explanation of what these Matplotlib commands do.

接下来，我们创建一个图形，并向其中添加一个子场景。然后，我们使用我们的数据绘制散点图； *x* 轴代表气缸数量， *y* 轴代表排量。最后，我们设置坐标轴标签和图表标题。

以下是最终结果:

![](../images/00105.jpeg)

# 还有更多...

您可以使用`bokeh`创建上述图表的交互式版本:

```
%%local from bokeh.io import showfrom bokeh.plotting import figurefrom bokeh.io import output_notebookoutput_notebook()p = figure(title = 'Relationship between cylinders and displacement')p.xaxis.axis_label = 'Cylinders'p.yaxis.axis_label = 'Displacement'p.circle( list(scatter_source['Cylinders']), list(scatter_source['Displacement']), fill_alpha=0.2, size=10)show(p)
```

首先，我们创建画布，我们将要绘制的图形。接下来，我们设置我们的标签。最后，我们使用`.circle(...)`方法在画布上绘制点。

最终结果如下:

![](../images/00106.jpeg)