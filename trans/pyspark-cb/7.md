# 使用 PySpark 的结构化流

在本章中，我们将介绍如何在 PySpark 中使用 Apache Spark 结构化流。您将学习以下食谱:

*   理解数据流
*   了解全局聚合
*   结构化流的连续聚合

# 介绍

随着机器生成的*实时数据*的普及，包括但不限于物联网传感器、设备和信标，在创建数据的同时快速了解这些数据变得越来越重要。无论您是检测欺诈交易、实时检测传感器异常，还是分析下一个卡特彼勒视频，流分析都是一个越来越重要的优势和业务优势。

随着这些方法的进展，我们将结合*批处理*和*实时*处理的结构来创建连续的应用程序。借助 Apache Spark，数据科学家和数据工程师可以使用 Spark SQL 批量实时分析他们的数据，使用 MLlib 训练机器学习模型，并通过 Spark Streaming 对这些模型进行评分。

快速采用 Apache Spark 的一个重要原因是它统一了所有这些不同的数据处理范例(通过 ML 和 MLlib 的机器学习、Spark SQL 和流)。如前所述，在*Spark Streaming:It 是什么和谁在使用它*([https://www . datanami . com/2015/11/30/Spark-Streaming-It 是什么和谁在使用它/](https://www.datanami.com/2015/11/30/spark-streaming-what-is-it-and-whos-using-it/) )中，优步、网飞和 Pinterest 等公司经常通过 Spark Streaming 展示他们的使用案例:

*   *优步如何利用 Spark 和 Hadoop 优化客户体验*网址为
*   火花和火花流在网飞[https://Spark-summit . org/2015/events/Spark-and-Spark-Streaming-at-网飞/](https://spark-summit.org/2015/events/spark-and-spark-streaming-at-netflix/)
*   火花流能在混沌猴子中生存吗？在[http://tech blog . Netflix . com/2015/03/can-spark-streaming-survive-chaos-monkey . html](http://techblog.netflix.com/2015/03/can-spark-streaming-survive-chaos-monkey.html)
*   实时分析在 Pinterest[https://engineering . Pinterest . com/blog/real-time-analytics-Pinterest](https://engineering.pinterest.com/blog/real-time-analytics-pinterest)

# 理解火花流

对于 Apache Spark 中的实时处理，当前的重点是结构化流，它构建在数据帧/数据集基础架构之上。数据框架抽象的使用允许在 Spark SQL 引擎 Catalyst Optimizer 及其常规改进(例如，钨项目)中优化流、机器学习和 Spark SQL。然而，为了更容易理解火花流，了解其火花流前身的基本原理是值得的。下图显示了火花流应用程序数据流，包括火花驱动程序、工作人员、流源和流目标:

![](../images/00164.jpeg)

上图描述如下:

1.  从**火花流上下文** ( **SSC** )开始，驱动程序将在执行器(即火花工作人员)上执行长时间运行的任务。
2.  驱动程序中定义的代码(从`ssc.start()`开始)，执行器上的**接收器**(**执行器 1【本图中的 T4】)从**流源**接收数据流。Spark Streaming 可以接收**卡夫卡**或**推特**，和/或你可以建立自己的定制接收器。对于传入的数据流，接收器将数据流分成块，并将这些块保存在内存中。**
3.  这些数据块被复制到另一个执行器以获得高可用性。
4.  块标识信息被传输到驱动程序上的块管理器主机，从而确保跟踪和记录内存中的每个数据块。
5.  对于在 SSC 中配置的每个批处理间隔(通常是每 1 秒)，驱动程序将启动 Spark 任务来处理这些块。然后，这些块被保存到任意数量的目标数据存储中，包括云存储(例如，S3、ISMB)、关系数据存储(例如，MySQL、PostgreSQL 等)和 NoSQL 存储。

在接下来的部分中，我们将使用**离散流**或**数据流**(基本的数据流构建模块)来查看配方，然后通过对数据流执行状态计算来执行全局聚合。然后，我们将通过使用结构化流来简化我们的流应用程序，同时获得性能优化。

# 理解数据流

在我们深入研究结构化流之前，让我们先来谈谈数据流。数据流是建立在关系数据库之上的，代表一个分成小块的数据流。下图以毫秒到秒的微批次表示这些数据块。在本例中，数据流的行被微批处理成秒，其中每个方块代表在第二个窗口内发生的事件的微批处理:

*   在 1 秒的时间间隔内，出现了 5 次蓝色的**事件和 3 次绿色的**事件****
*   在 2 秒的时间间隔内，**悟空**出现一次
*   在 4 秒的时间间隔内，事件**绿色**出现两次

![](../images/00165.jpeg)

Because DStreams are built on top of RDDs, Apache Spark's core data abstraction, this allows Spark Streaming to easily integrate with other Spark components such as MLlib and Spark SQL.

# 准备好

对于这些 Apache Spark Streaming 示例，我们将通过 bash 终端创建并执行一个控制台应用程序。为了使事情变得更容易，您需要打开两个终端窗口。

# 怎么做...

如前一节所述，我们将使用两个终端窗口:

*   传输事件的一个终端窗口
*   接收这些事件的另一个终端

注意，这个的源代码可以在 Apache Spark 1.6 流媒体编程指南中找到，网址为:[https://Spark . Apache . org/docs/1 . 6 . 0/Streaming-Programming-Guide . html](https://spark.apache.org/docs/1.6.0/streaming-programming-guide.html)。

# 终端 1–网猫窗口

对于第一个窗口，我们将使用 Netcat(或 nc)手动发送蓝色、绿色和 gohawks 等事件。要启动 Netcat，请使用以下命令；我们将把我们的事件导向端口`9999`，在这里我们的火花流作业将检测:

```py
nc -lk 9999
```

为了与前面的图表相匹配，我们将输入事件，以便控制台屏幕如下所示:

```py
$nc -lk 9999blue blue blue blue blue green green greengohawksgreen green 
```

# 终端 2–火花流窗口

我们将使用以下名为`streaming_word_count.py`的代码创建一个简单的 PySpark Streaming 应用程序:

```py
## streaming_word_count.py## Import the necessary classes and create a local SparkContext and Streaming Contextsfrom pyspark import SparkContextfrom pyspark.streaming import StreamingContext# Create Spark Context with two working threads (note, `local[2]`)sc = SparkContext("local[2]", "NetworkWordCount")# Create local StreamingContextwith batch interval of 1 secondssc = StreamingContext(sc, 1)# Create DStream that will connect to the stream of input lines from connection to localhost:9999lines = ssc.socketTextStream("localhost", 9999)# Split lines into wordswords = lines.flatMap(lambda line: line.split(" "))# Count each word in each batchpairs = words.map(lambda word: (word, 1))wordCounts = pairs.reduceByKey(lambda x, y: x + y)# Print the first ten elements of each RDD generated in this DStream to the consolewordCounts.pprint()# Start the computationssc.start()# Wait for the computation to terminatessc.awaitTermination()
```

要运行这个 PySpark Streaming 应用程序，请从您的`$SPARK_HOME`文件夹中执行以下命令:

```py
./bin/spark-submit streaming_word_count.py localhost 9999
```

就如何计时而言，您应该:

1.  首先从`nc -lk 9999`开始。
2.  然后，启动你的 PySpark Streaming 应用程序:`/bin/spark-submit streaming_word_count.py localhost 9999`。

3.  然后，开始键入您的事件，例如:
    1.  第一秒，输入`blue blue blue blue blue green green green`
    2.  第二秒，输入`gohawks`
    3.  等一下；第四秒，输入`green green`

PySpark 流应用程序的控制台输出看起来类似于这样:

```py
$ ./bin/spark-submit streaming_word_count.py localhost 9999-------------------------------------------Time: 2018-06-21 23:00:30-------------------------------------------(u'blue', 5)(u'green', 3)-------------------------------------------Time: 2018-06-21 23:00:31-------------------------------------------(u'gohawks', 1)-------------------------------------------Time: 2018-06-21 23:00:32--------------------------------------------------------------------------------------Time: 2018-06-21 23:00:33-------------------------------------------(u'green', 2)------------------------------------------- 
```

要结束流应用程序(以及`nc`窗口)，请执行终止命令(例如， *Ctrl* + *C* )。

# 它是如何工作的...

如前几小节所述，该配方由一个使用`nc`传输事件数据的终端窗口组成。第二个窗口运行我们的 Spark Streaming 应用程序，从第一个窗口传输到的端口读取数据。

此处记录了该代码的重要提示:

*   我们使用两个工作线程创建了一个 Spark 上下文，因此使用了`local[2]`。
*   正如在网猫窗口中注意到的，我们正在使用`ssc.socketTextStream`来监听`localhost`端口`9999`的本地插座。
*   回想一下，对于每一个 1 秒的批次，我们不仅读取了一行(例如`blue blue blue blue blue green green green`，还通过`split`将其拆分为单个`words`。
*   我们使用 Python `lambda`函数和 PySpark `map`和`reduceByKey`函数来快速计算 1 秒钟内单词的出现次数。例如`blue blue blue blue blue green green green`事件，有五个蓝色事件和三个绿色事件，如我们的流媒体应用的 *2018-06-21 23:00:30* 所报道的。
*   `ssc.start()`是指启动火花流上下文的应用程序。
*   `ssc.awaitTermination()`正在等待终止命令停止流媒体应用(例如*Ctrl*+*C*)；否则，应用程序将继续运行。

# 还有更多...

使用 PySpark 控制台时，通常会有大量消息发送到控制台，这可能会使读取流式输出变得困难。为了便于阅读，请确保您已经在`$SPARK_HOME/conf`文件夹中创建并修改了`log4j.properties`文件。为此，请遵循以下步骤:

1.  转到`$SPARK_HOME/conf`文件夹。
2.  默认情况下，有一个`log4j.properties.template`文件。复制同名，去掉`.template`，即:

```py
cp log4j.properties.template log4j.properties
```

3.  在自己喜欢的编辑器中编辑`log4j.properties`(例如威震、vi 等)。在文件的第 19 行，更改这一行:

```py
log4j.rootCategory=INFO, console
```

对此:

```py
log4j.rootCategory=ERROR, console
```

这样，所有日志信息(即`INFO`)不会被定向到控制台，只有错误(即`ERROR`)会被定向到控制台。

# 了解全局聚合

在前一节中，我们的食谱提供了事件的快照计数。也就是说，它提供了该时间点的事件计数。但是，如果您想了解某个时间窗口的一系列事件，该怎么办？这就是全局聚合的概念:

![](../images/00166.jpeg)

如果我们想要全局聚合，与之前相同的示例(时间 1: 5 蓝色，3 绿色，时间 2: 1 gohawks，时间 4: 2 绿色)将计算如下:

*   时间 1: 5 蓝色，3 绿色
*   时间 2: 5 蓝色，3 绿色，1 天鹰
*   时间 4: 5 蓝色，5 绿色，1 天鹰

在传统的批量计算中，这类似于`groupbykey`或`GROUP BY`语句。但是在流式应用的情况下，该计算需要在毫秒内完成，这对于执行`GROUP BY`计算来说通常是太短的时间窗口。但是，使用 Spark Streaming 全局聚合，可以通过执行有状态流计算来快速完成此计算。也就是说，使用 Spark Streaming 框架，执行聚合的所有信息都保存在内存中(即保持数据处于*状态*，以便在其小时间窗口中进行计算)。

# 准备好

对于这些 Apache Spark Streaming 示例，我们将通过 bash 终端创建并执行一个控制台应用程序。为了使事情变得更容易，您需要打开两个终端窗口。

# 怎么做...

如前一节所述，我们将使用两个终端窗口:

*   传输事件的一个终端窗口
*   接收这些事件的另一个终端

这方面的源代码可以在 Apache Spark 1.6 流媒体编程指南中找到，网址为:[https://Spark . Apache . org/docs/1 . 6 . 0/Streaming-Programming-Guide . html](https://spark.apache.org/docs/1.6.0/streaming-programming-guide.html)。

# 终端 1–网猫窗口

对于第一个窗口，我们将使用 Netcat(或`nc`)手动发送蓝色、绿色和 gohawks 等事件。要启动 Netcat，请使用以下命令；我们将把我们的事件导向端口`9999`，在那里我们的火花流作业将检测:

```py
nc -lk 9999
```

为了与前面的图表相匹配，我们将输入事件，以便控制台屏幕如下所示:

```py
$nc -lk 9999blue blue blue blue blue green green greengohawksgreen green 
```

# 终端 2–火花流窗口

我们将使用以下名为`streaming_word_count.py`的代码创建一个简单的 PySpark Streaming 应用程序:

```py
## stateful_streaming_word_count.py## Import the necessary classes and create a local SparkContext and Streaming Contextsfrom pyspark import SparkContextfrom pyspark.streaming import StreamingContext# Create Spark Context with two working threads (note, `local[2]`)sc = SparkContext("local[2]", "StatefulNetworkWordCount")# Create local StreamingContextwith batch interval of 1 secondssc = StreamingContext(sc, 1)# Create checkpoint for local StreamingContextssc.checkpoint("checkpoint")# Define updateFunc: sum of the (key, value) pairsdef updateFunc(new_values, last_sum):return sum(new_values) + (last_sum or 0)# Create DStream that will connect to the stream of input lines from connection to localhost:9999lines = ssc.socketTextStream("localhost", 9999)# Calculate running counts# Line 1: Split lines in to words# Line 2: count each word in each batch# Line 3: Run `updateStateByKey` to running countrunning_counts = lines.flatMap(lambda line: line.split(" "))\.map(lambda word: (word, 1))\.updateStateByKey(updateFunc)# Print the first ten elements of each RDD generated in this stateful DStream to the consolerunning_counts.pprint()# Start the computationssc.start() # Wait for the computation to terminatessc.awaitTermination() 
```

要运行这个 PySpark Streaming 应用程序，请从您的`$SPARK_HOME`文件夹中执行以下命令:

```py
./bin/spark-submit stateful_streaming_word_count.py localhost 9999
```

就如何计时而言，您应该:

1.  首先从`nc -lk 9999`开始。
2.  然后，启动你的 PySpark Streaming 应用程序:`./bin/spark-submit stateful_streaming_word_count.py localhost 9999`。
3.  然后，开始键入您的事件，例如:
    1.  第一秒，输入`blue blue blue blue blue green green green`
    2.  第二秒，输入`gohawks`
    3.  等一下；第四秒，输入`green green`

PySpark 流应用程序的控制台输出看起来类似于以下输出:

```py
$ ./bin/spark-submit stateful_streaming_word_count.py localhost 9999-------------------------------------------Time: 2018-06-21 23:00:30-------------------------------------------(u'blue', 5)(u'green', 3)-------------------------------------------Time: 2018-06-21 23:00:31-------------------------------------------(u'blue', 5)(u'green', 3)(u'gohawks', 1)-------------------------------------------Time: 2018-06-21 23:00:32--------------------------------------------------------------------------------------Time: 2018-06-21 23:00:33-------------------------------------------(u'blue', 5)(u'green', 5)(u'gohawks', 1)------------------------------------------- 
```

要结束流应用程序(以及`nc`窗口)，请执行终止命令(例如， *Ctrl* + *C* )。

# 它是如何工作的...

如前几小节所述，该配方由一个使用`nc`传输事件数据的终端窗口组成。第二个窗口运行我们的 Spark Streaming 应用程序，从第一个窗口传输到的端口读取数据。

此处记录了该代码的重要提示:

*   我们使用两个工作线程创建了一个 Spark 上下文，因此使用了`local[2]`。
*   正如在网猫窗口中注意到的，我们正在使用`ssc.socketTextStream`来监听`localhost`端口`9999`的本地插座。
*   我们已经创建了一个`updateFunc`，它执行将先前值与当前聚合值聚合的任务。
*   回想一下，对于每一个 1 秒的批次，我们不仅读取了一行(例如`blue blue blue blue blue green green green`)，还通过`split`将其拆分为单个`words`。
*   我们使用 Python `lambda`函数和 PySpark `map`和`reduceByKey`函数来快速计算 1 秒钟内单词的出现次数。例如`blue blue blue blue blue green green green`的情况，有 5 个蓝色事件和 3 个绿色事件，如我们的流媒体应用的 *2018-06-21 23:00:30* 所报道的。
*   以前的流应用程序与当前有状态版本之间的区别在于，我们使用`updateStateByKey`计算当前聚合(例如，五个蓝色和三个绿色事件)的运行计数(`running_counts`)。这允许火花流将当前聚合的状态保持在先前定义的`updateFunc`的上下文中。
*   `ssc.start()`是指启动火花流上下文的应用程序。
*   `ssc.awaitTermination()`正在等待终止命令停止流媒体应用(例如*Ctrl*+*C*)；否则，应用程序将继续运行。

# 结构化流的连续聚合

如前几章所述，Spark SQL 或 DataFrame 查询的执行围绕着构建一个逻辑计划，根据其成本优化器选择一个物理计划(在许多生成的物理计划中)，然后通过 Spark SQL Engine Catalyst Optimizer 生成代码(即代码生成)。结构化流引入的是*增量*执行计划的概念。也就是说，结构化流为它接收的每个新数据块重复应用执行计划。这样，火花 SQL 引擎可以利用火花数据帧中包含的优化，并将它们应用于传入的数据流。因为结构化流是建立在 Spark 数据帧之上的，这意味着集成其他数据帧优化组件也会更容易，包括 MLlib、GraphFrames、TensorFrames 等:

![](../images/00167.jpeg)

# 准备好

对于这些 Apache Spark Streaming 示例，我们将通过 bash 终端创建并执行一个控制台应用程序。为了使事情变得更容易，您需要打开两个终端窗口。

# 怎么做...

如前一节所述，我们将使用两个终端窗口:

*   传输事件的一个终端窗口
*   接收这些事件的另一个终端

这方面的源代码可以在 Apache Spark 2.3.1 结构化流编程指南中找到，网址为:[https://Spark . Apache . org/docs/latest/Structured-Streaming-Programming-Guide . html](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)。

# 终端 1–网猫窗口

对于第一个窗口，我们将使用 Netcat(或`nc`)手动发送蓝色、绿色和 gohawks 等事件。要启动 Netcat，请使用以下命令；我们将把事件导向端口`9999`，在这里我们的火花流作业将检测:

```py
nc -lk 9999
```

为了与前面的图表相匹配，我们将输入事件，以便控制台屏幕如下所示:

```py
$nc -lk 9999blue blue blue blue blue green green greengohawksgreen green 
```

# 终端 2–火花流窗口

我们将使用以下名为`structured_streaming_word_count.py`的代码创建一个简单的 PySpark Streaming 应用程序:

```py
## structured_streaming_word_count.py## Import the necessary classes and create a local SparkSessionfrom pyspark.sql import SparkSessionfrom pyspark.sql.functions import explodefrom pyspark.sql.functions import splitspark = SparkSession \.builder \.appName("StructuredNetworkWordCount") \.getOrCreate()# Create DataFrame representing the stream of input lines from connection to localhost:9999lines = spark\.readStream\.format('socket')\.option('host', 'localhost')\.option('port', 9999)\.load()# Split the lines into wordswords = lines.select(explode(split(lines.value, ' ')).alias('word'))# Generate running word countwordCounts = words.groupBy('word').count()# Start running the query that prints the running counts to the consolequery = wordCounts\.writeStream\.outputMode('complete')\.format('console')\.start()# Await Spark Streaming terminationquery.awaitTermination()
```

要运行这个 PySpark Streaming 应用程序，请从您的`$SPARK_HOME`文件夹中执行以下命令:

```py
./bin/spark-submit structured_streaming_word_count.py localhost 9999
```

就如何计时而言，您应该:

1.  首先从`nc -lk 9999`开始。
2.  然后，启动你的 PySpark Streaming 应用程序:`./bin/spark-submit stateful_streaming_word_count.py localhost 9999`。

3.  然后，开始键入您的事件，例如:
    1.  第一秒，输入`blue blue blue blue blue green green green`
    2.  第二秒，输入`gohawks`
    3.  等一下；第四秒，输入`green green`

PySpark 流应用程序的控制台输出看起来类似于以下内容:

```py
$ ./bin/spark-submit structured_streaming_word_count.py localhost 9999-------------------------------------------Batch: 0-------------------------------------------+-----+-----+| word|count|+-----+-----+|green|    3|| blue|    5|+-----+-----+-------------------------------------------Batch: 1-------------------------------------------+-------+-----+|   word|count|+-------+-----+|  green|    3||   blue|    5||gohawks|    1|+-------+-----+-------------------------------------------Batch: 2-------------------------------------------+-------+-----+|   word|count|+-------+-----+|  green|    5||   blue|    5||gohawks|    1|+-------+-----+
```

要结束流应用程序(以及`nc`窗口)，请执行终止命令(例如， *Ctrl* + *C* )。

Similar to global aggregations with DStreams, with structured streaming, you can easily perform stateful global aggregations within the context of a DataFrame. Another optimization you'll notice with structured streaming is that the streaming aggregations will only appear whenever there are new events. Specifically notice how when we delayed between time = 2s and time = 4s, there is not an extra batch being reported to the console.

# 它是如何工作的...

如前几小节所述，该配方由一个使用`nc`传输事件数据的终端窗口组成。第二个窗口运行我们的 Spark Streaming 应用程序，从第一个窗口传输到的端口读取数据。

此处注明了该代码的重要标注:

*   我们不是创建一个火花上下文，而是创建一个`SparkSession`
*   使用 SparkSession，我们可以使用`readStream`指定`socket` *格式*来指定我们正在港口`9999`收听`localhost`
*   我们使用 PySpark SQL 函数`split`和`explode`来获取我们的`line`并将其分解为`words`
*   为了生成我们的运行字数，我们只需要创建`wordCounts`来运行`groupBy`语句并在`words`上运行`count()`
*   最后，我们将使用`writeStream`将`query`数据的`complete`集写到`console`(相对于其他一些数据接收器)
*   因为我们使用的是 Spark 会话，所以应用程序正在等待终止命令，以便通过`query.awaitTermination()`停止流式应用程序(例如， <ctrl><c>)</c></ctrl>

因为结构化流使用数据帧，所以它更简单、更容易阅读，因为我们使用了熟悉的数据帧抽象，同时还获得了数据帧的所有性能优化。