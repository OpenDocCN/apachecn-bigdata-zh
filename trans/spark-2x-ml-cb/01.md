# 基于 Scala 的火花实用机器学习

在本章中，我们将介绍:

*   下载并安装 JDK
*   下载和安装 IntelliJ
*   下载并安装 Spark
*   将 IntelliJ 配置为使用 Spark 并运行 Spark ML 示例代码
*   从 Spark 运行一个样本 ML 代码
*   识别实际机器学习的数据源
*   使用 Apache Spark 2.0 和 IntelliJ 集成开发环境运行您的第一个程序
*   如何向您的 Spark 程序添加图形

# 介绍

随着最近集群计算的发展和大数据的兴起，机器学习领域被推到了计算的前沿。长期以来，对能够大规模实现数据科学的互动平台的需求一直是一个梦想，现在已经成为现实。

以下三个领域共同促成并加速了交互式数据科学的大规模发展:

*   **Apache Spark** :数据科学的统一技术平台，将快速计算引擎和容错数据结构结合到精心设计和集成的产品中
*   **机器学习**:人工智能的一个领域，使机器能够模仿一些最初只留给人脑的任务
*   **Scala** :一种基于 JVM 的现代语言，它建立在传统语言的基础上，但是将功能性和面向对象的概念结合在一起，没有其他语言的冗长

首先，我们需要设置开发环境，它将由以下组件组成:

*   火花
*   IntelliJ 社区版 IDE
*   斯卡拉

本章中的方法将为您提供安装和配置 IntelliJ IDE、Scala 插件和 Spark 的详细说明。开发环境设置好之后，我们将继续运行其中一个 Spark ML 示例代码来测试设置。

# 阿帕奇火花

Apache Spark 正在成为大数据分析事实上的平台和交易语言，并作为 **Hadoop** 范例的补充。Spark 使数据科学家能够以最有利于其工作流程的方式开箱即用地工作。Spark 的方法是以完全分布式的方式处理工作负载，而不需要 **MapReduce** ( **MR** )或将中间结果重复写入磁盘。

Spark 在统一的技术堆栈中提供了一个易于使用的分布式框架，这使得它成为数据科学项目的首选平台，这些项目通常需要迭代算法，最终合并成一个解决方案。这些算法由于其内部工作原理，会产生大量中间结果，这些结果需要在中间步骤中从一个阶段进入下一个阶段。对具有强大的本地分布式机器学习库的交互工具的需求排除了大多数数据科学项目的基于磁盘的方法。

Spark 对集群计算有不同的方法。它作为技术堆栈而不是生态系统来解决问题。大量集中管理的库与能够支持容错数据结构的闪电般的计算引擎相结合，使得 Spark 有望取代 Hadoop，成为首选的大数据分析平台。

Spark 采用模块化方法，如下图所示:

![](../images/00005.jpeg)

# 机器学习

机器学习的目的是生产能够模仿人类智能的机器和设备，并自动完成传统上留给人脑的一些任务。机器学习算法被设计成在相对较短的时间内处理非常大的数据集，并获得人类需要更长时间才能处理的近似答案。

机器学习领域可以分为多种形式，在高层次上，可以分为监督学习和非监督学习。监督学习算法是一类 ML 算法，它使用训练集(即标记数据)来计算概率分布或图形模型，进而允许它们在没有进一步人工干预的情况下对新数据点进行分类。无监督学习是一种机器学习算法，用于从由输入数据组成的数据集得出推论，而无需标记响应。

开箱即用，Spark 提供了一套丰富的 ML 算法，可以部署在大型数据集上，无需任何进一步的编码。下图将 Spark 的 MLlib 算法描述为思维导图。Spark 的 MLlib 旨在利用并行性，同时具有容错分布式数据结构。Spark 指的是**弹性分布式数据集**或**关系数据库**这样的数据结构:

![](../images/00006.jpeg)

# 斯卡拉

**Scala** 是一种现代编程语言，正在成为传统编程语言的替代品，如 **Java** 和 **C++** 。Scala 是一种基于 JVM 的语言，它不仅提供了简洁的语法，而没有传统的样板代码，而且将面向对象和函数式编程结合到一种非常简洁和非常强大的类型安全语言中。

Scala 采用了一种灵活且富有表现力的方法，这使得它非常适合与 Spark 的 MLlib 进行交互。Spark 本身是用 Scala 编写的这一事实提供了一个强有力的证据，证明 Scala 语言是一种全服务的编程语言，可以用来创建具有繁重性能需求的复杂系统代码。

Scala 在 Java 传统的基础上解决了它的一些缺点，同时避免了全有或全无的方法。Scala 代码编译成 Java 字节码，这反过来使得它可以与丰富的 Java 库交替共存。能够将 Java 库与 Scala 一起使用，反之亦然，这为软件工程师构建现代复杂的机器学习系统提供了连续性和丰富的环境，而不会完全脱离 Java 传统和代码库。

Scala 完全支持功能丰富的函数式编程范式，标准支持 lambda、currying、类型接口、不变性、惰性评估，以及模式匹配范式，让人想起没有神秘语法的 Perl。Scala 非常适合机器学习编程，因为它支持代数友好的数据类型、匿名函数、协方差、反方差和高阶函数。

这是 Scala 中的 hello world 程序:

```
object HelloWorld extends App { 
   println("Hello World!") 
 } 
```

在 Scala 中编译和运行`HelloWorld`如下所示:

![](../images/00007.jpeg)

Apache Spark 机器学习食谱采用了一种实用的方法，为开发人员提供了一个多学科的视角。本书重点介绍了**机器学习**、**阿帕奇火花**和**斯卡拉**的互动和凝聚力。我们还采取了额外的步骤，教您如何设置和运行开发人员熟悉的全面开发环境，并提供代码片段，您必须在交互式外壳中运行这些代码片段，而不需要 ide 提供的现代设施:

![](../images/00008.jpeg)

# 本书使用的软件版本和库

下表提供了本书中使用的软件版本和库的详细列表。如果您遵循本章中的安装说明，它将包括此处列出的大多数项目。特定配方可能需要的任何其他 JAR 或库文件将通过相应配方中的附加安装说明进行介绍:

| **核心系统** | **版本** |
| 火花 | 2.0.0 |
| Java 语言(一种计算机语言，尤用于创建网站) | One point eight |
| IntelliJ IDEA | 2016.2.4 |
| Scala-sdk | 2.11.8 |

需要的其他 JARs 如下:

| **杂坛** | **版本** |
| `bliki-core` | 3.0.19 |
| `breeze-viz` | Zero point one two |
| `Cloud9` | 1.5.0 |
| `Hadoop-streaming` | 2.2.0 |
| `JCommon` | 1.0.23 |
| `JFreeChart` | 1.0.19 |
| `lucene-analyzers-common` | 6.0.0 |
| `Lucene-Core` | 6.0.0 |
| `scopt` | 3.3.0 |
| `spark-streaming-flume-assembly` | 2.0.0 |
| `spark-streaming-kafka-0-8-assembly` | 2.0.0 |

我们已经在 Spark 2.1.1 上额外测试了本书中的所有食谱，并发现程序按预期执行。出于学习目的，建议您使用这些表中列出的软件版本和库。

为了跟上快速变化的 Spark 环境和文档，本书中提到的 Spark 文档的 API 链接指向最新版本的 Spark 2.x.x，但是配方中的 API 引用明确是针对 Spark 2.0.0 的。

本书提供的所有 Spark 文档链接都指向 Spark 网站上的最新文档。如果您更喜欢查找特定版本的 Spark 的文档(例如，Spark 2.0.0)，请使用以下网址在 Spark 网站上查找相关文档:

[https://spark.apache.org/documentation.html](https://spark.apache.org/documentation.html)

为了清晰起见，我们尽可能地简化了代码，而不是演示 Scala 的高级特性。

# 下载并安装 JDK

第一步是下载 Scala/Spark 开发所需的 JDK 开发环境。

# 准备好

当您准备下载并安装 JDK 时，请访问以下链接:

[http://www . Oracle . com/tech network/Java/javase/downloads/index . html](http://www.oracle.com/technetwork/java/javase/downloads/index.html)

# 怎么做...

成功下载后，按照屏幕上的说明安装 JDK。

# 下载和安装 IntelliJ

IntelliJ 社区版是一个面向 Java SE、Groovy、Scala 和 Kotlin 开发的轻量级 IDE。要使用 Spark 开发环境完成机器学习的设置，需要安装 IntelliJ IDE。

# 准备好

当您准备下载并安装 IntelliJ 时，请访问以下链接:

[https://www.jetbrains.com/idea/download/](https://www.jetbrains.com/idea/download/)

# 怎么做...

在撰写本文时，我们正在使用 IntelliJ 15 . x 版或更高版本(例如 2016.2.4 版)来测试书中的示例，但请随意下载最新版本。下载安装文件后，双击下载的文件(`.exe`)开始安装 IDE。如果不想进行任何更改，请将所有安装选项保留为默认设置。按照屏幕上的说明完成安装:

![](../images/00009.jpeg)

# 下载并安装 Spark

我们现在开始下载并安装 Spark。

# 准备好

当您准备好下载和安装 Spark 时，请通过以下链接访问 Apache 网站:

[http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html)

# 怎么做...

转到 Apache 网站，选择所需的下载参数，如下图所示:

![](../images/00010.jpeg)

确保接受默认选择(单击下一步)并继续安装。

# 将 IntelliJ 配置为使用 Spark 并运行 Spark ML 示例代码

我们需要运行一些配置来确保项目设置是正确的，然后才能运行由 Spark 或本书中列出的任何程序提供的示例。

# 准备好了

在配置项目结构和全局库时，我们需要特别小心。在我们设置好一切之后，我们继续运行 Spark 团队提供的示例 ML 代码来验证设置。示例代码可以在 Spark 目录下找到，也可以通过下载带有示例的 Spark 源代码获得。

# 怎么做...

以下是将 IntelliJ 配置为使用 Spark MLlib 以及在示例目录中运行 Spark 提供的示例 ML 代码的步骤。示例目录可以在您的 Spark 主目录中找到。使用 Scala 示例继续:

1.  单击项目结构...选项来配置项目设置，如下图所示:

![](../images/00011.jpeg)

2.  验证设置:

![](../images/00012.jpeg)

3.  配置全局库。选择 Scala SDK 作为您的全局库:

![](../images/00013.jpeg)

4.  为新的 Scala SDK 选择 JARs，并完成下载:

![](../images/00014.jpeg)

5.  选择项目名称:

![](../images/00015.jpeg)

6.  验证设置和其他库:

![](../images/00016.jpeg)

7.  添加依赖项 JARs。在左侧窗格的项目设置下选择模块，然后单击依赖项来选择所需的 JARs，如下图所示:

![](../images/00017.jpeg)

8.  选择 Spark 提供的 JAR 文件。选择 Spark 的默认安装目录，然后选择`lib`目录:

![](../images/00018.jpeg)

9.  然后，我们选择为 Spark 提供的示例的 JAR 文件。

![](../images/00019.jpeg)

10.  通过验证您选择并导入了左侧窗格中`External Libraries`下列出的所有 jar，添加所需的 jar:

![](../images/00020.jpeg)

11.  Spark 2.0 使用 Scala 2.11。运行示例需要两个新的流 JARs，Flume 和 Kafka，可以从以下网址下载:
    *   [https://repo 1 . maven . org/maven 2/org/Apache/spark/spark-streaming-flu-assembly _ 2.11/2 . 0 . 0/spark-streaming-flu-assembly _ 2.11-2 . 0 . 0 . jar](https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-flume-assembly_2.11/2.0.0/spark-streaming-flume-assembly_2.11-2.0.0.jar)
    *   [https://repo 1 . maven . org/maven 2/org/Apache/spark/spark-streaming-Kafka-0-8-assembly _ 2.11/2 . 0 . 0/spark-streaming-Kafka-0-8-assembly _ 2.11-2 . 0 . 0 . jar](https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8-assembly_2.11/2.0.0/spark-streaming-kafka-0-8-assembly_2.11-2.0.0.jar)

下一步是下载并安装 Flume 和 Kafka JARs。出于本书的目的，我们使用了 Maven repo:

![](../images/00021.jpeg)

12.  下载并安装卡夫卡组件:

![](../images/00022.jpeg)

13.  下载并安装水槽组件:

![](../images/00023.jpeg)

14.  下载完成后，将下载的 JAR 文件移动到 Spark 的`lib`目录下。我们在安装 Spark 时使用了`C`驱动器:

![](../images/00024.jpeg)

15.  打开您的 IDE，并验证左侧`External Libraries`文件夹下的所有 JARs，如下图所示，都存在于您的设置中:

![](../images/00025.jpeg)

16.  在 Spark 中构建示例项目以验证设置:

![](../images/00026.jpeg)

17.  验证生成是否成功:

![](../images/00027.jpeg)

# 还有更多...

在 Spark 2.0 之前，我们需要谷歌的另一个名为**番石榴**的库，以方便输入/输出，并提供一套丰富的方法来定义表，然后让 Spark 在集群中广播它们。由于难以解决的依赖性问题，Spark 2.0 不再使用番石榴库。如果您使用的是 2.0 之前的 Spark 版本(1.5.2 版本中需要)，请确保使用番石榴库。可以通过以下网址访问番石榴图书馆:

[https://github . com/Google/guava/wiki](https://github.com/google/guava/wiki)

您可能想使用番石榴 15.0 版本，可以在这里找到:

[https://mvnrepository . com/artifact/com . Google . guava/guava/15.0](https://mvnrepository.com/artifact/com.google.guava/guava/15.0)

如果您使用的是以前博客中的安装说明，请确保从安装集中排除番石榴库。

# 请参见

如果完成 Spark 安装需要其他第三方库或 JARs，您可以在以下 Maven 存储库中找到这些库或 JARs:

[https://repo1.maven.org/maven2/org/apache/spark/](https://repo1.maven.org/maven2/org/apache/spark/)

# 从 Spark 运行一个样本 ML 代码

我们可以通过简单地从 Spark 源代码树中下载示例代码并将其导入 IntelliJ 以确保其运行来验证设置。

# 准备好

我们将首先运行样本中的逻辑回归代码来验证安装。在下一节中，我们继续编写相同程序的自己的版本，并检查输出，以便理解它是如何工作的。

# 怎么做...

1.  转到源目录，选择一个要运行的 ML 示例代码文件。我们选择了逻辑回归的例子。

If you cannot find the source code in your directory, you can always download the Spark source, unzip, and then extract the examples directory accordingly.

2.  选择示例后，选择编辑配置...，如下图所示:

![](../images/00028.jpeg)

3.  在“配置”选项卡中，定义以下选项:
    *   虚拟机选项:显示的选项允许您运行独立的 Spark 集群
    *   程序参数:我们应该传递到程序中的内容

![](../images/00029.jpeg)

4.  运行逻辑回归，方法是运行“逻辑回归示例”，如下图所示:

![](../images/00030.jpeg)

5.  验证退出代码，并确保它如下面的屏幕截图所示:

![](../images/00031.jpeg)

# 识别实际机器学习的数据源

在过去，为机器学习项目获取数据是一项挑战。然而，现在有一套丰富的公共数据源专门适用于机器学习。

# 准备好

除了大学和政府来源，还有许多其他开放的数据来源，可以用来学习和编写自己的例子和项目。我们将列出数据源，并向您展示如何最好地获取和下载每章的数据。

# 怎么做...

如果您想开发该领域的应用程序，以下是值得探索的开源数据列表:

*   *UCI 机器学习资源库*:这是一个具有搜索功能的扩展库。在撰写本报告时，共有 350 多个数据集。您可以点击[https://archive.ics.uci.edu/ml/index.html](https://archive.ics.uci.edu/ml/index.html)链接查看所有数据集，或者使用简单的搜索来查找特定的数据集( *Ctrl* + *F* )。
*   *Kaggle 数据集*:你需要创建一个账号，但是你可以下载任何一套用于学习以及参加机器学习比赛。[https://www.kaggle.com/competitions](https://www.kaggle.com/competitions)链接提供了探索和学习更多关于卡格尔和机器学习竞赛内部工作的细节。
*   *MLdata.org*:面向所有人开放的公共网站，为机器学习爱好者提供数据集存储库。
*   *谷歌趋势*:你可以在[http://www.google.com/trends/explore](http://www.google.com/trends/explore)上找到自 2004 年以来任何给定术语的搜索量统计数据(占总搜索量的比例)。
*   *美国中央情报局世界概况介绍*:[https://www . CIA . gov/library/publications/The-World-Factbook/](https://www.cia.gov/library/publications/the-world-factbook/)链接提供了 267 个国家的历史、人口、经济、政府、基础设施和军事方面的信息。

# 请参见

机器学习数据的其他来源:

*   短信垃圾邮件数据:[http://www . dt . fee . uniamp . br/~ tiago/smtphost collection/](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/)
*   来自借贷俱乐部[https://www.lendingclub.com/info/download-data.action](https://www.lendingclub.com/info/download-data.action)的金融数据集
*   来自雅虎[http://webscope.sandbox.yahoo.com/index.php](http://webscope.sandbox.yahoo.com/index.php)的研究数据
*   亚马逊 AWS 公共数据集[http://aws.amazon.com/public-data-sets/](http://aws.amazon.com/public-data-sets/)
*   来自影像网[http://www.image-net.org](http://www.image-net.org)的标注视觉数据
*   人口普查数据集[http://www.census.gov](http://www.census.gov)
*   整理了 YouTube 数据集[http://netsg.cs.sfu.ca/youtubedata/](http://netsg.cs.sfu.ca/youtubedata/)
*   从电影频道网站[http://grouplens.org/datasets/movielens/](http://grouplens.org/datasets/movielens/)收集评级数据
*   公众可获得的安然数据集[http://www.cs.cmu.edu/~enron/](http://www.cs.cmu.edu/~enron/)
*   统计学习经典书籍元素数据集
*   电影数据集[http://www.imdb.com/interfaces](http://www.imdb.com/interfaces)
*   百万歌曲数据集[http://labrosa.ee.columbia.edu/millionsong/](http://labrosa.ee.columbia.edu/millionsong/)
*   语音和音频数据集[http://labrosa.ee.columbia.edu/projects/](http://labrosa.ee.columbia.edu/projects/)
*   人脸识别数据[http://www.face-rec.org/databases/](http://www.face-rec.org/databases/)
*   社会科学数据[http://www.icpsr.umich.edu/icpsrweb/ICPSR/studies](http://www.icpsr.umich.edu/icpsrweb/ICPSR/studies)
*   来自康乃尔大学[http://arxiv.org/help/bulk_data_s3](http://arxiv.org/help/bulk_data_s3)的大量数据集
*   古腾堡项目数据集[http://www.gutenberg.org/wiki/Gutenberg:Offline_Catalogs](http://www.gutenberg.org/wiki/Gutenberg:Offline_Catalogs)
*   来自世界银行[http://data.worldbank.org](http://data.worldbank.org)的数据集
*   来自世界网[http://wordnet.princeton.edu](http://wordnet.princeton.edu)的词汇数据库
*   来自 NYPD[http://nypd.openscrape.com/#/](http://nypd.openscrape.com/#/)的碰撞数据
*   国会行呼叫和其他的数据集[http://voteview.com/dwnl.htm](http://voteview.com/dwnl.htm)
*   来自斯坦福[http://snap.stanford.edu/data/index.html](http://snap.stanford.edu/data/index.html)的大型图形数据集
*   来自数据中心[https://datahub.io/dataset](https://datahub.io/dataset)的丰富数据集
*   Yelp 的学术数据集[https://www.yelp.com/academic_dataset](https://www.yelp.com/academic_dataset)
*   数据来源于 GitHub[https://github.com/caesar0301/awesome-public-datasets](https://github.com/caesar0301/awesome-public-datasets)
*   来自 Reddit[https://www.reddit.com/r/datasets/](https://www.reddit.com/r/datasets/)的数据集档案

您可能会对一些专门的数据集(例如，西班牙语的文本分析以及 gene 和 IMF 数据)感兴趣:

*   来自哥伦比亚的数据集(西班牙语):[http://www.datos.gov.co/frm/buscador/frmBuscador.aspx](http://www.datos.gov.co/frm/buscador/frmBuscador.aspx)
*   癌症研究数据集[http://www.broadinstitute.org/cgi-bin/cancer/datasets.cgi](http://www.broadinstitute.org/cgi-bin/cancer/datasets.cgi)
*   来自皮尤[http://www.pewinternet.org/datasets/](http://www.pewinternet.org/datasets/)的研究数据
*   来自美国伊利诺伊州[https://data.illinois.gov](https://data.illinois.gov)的数据
*   数据来自 freebase.com[http://www.freebase.com](http://www.freebase.com)
*   来自联合国及其相关机构的数据集[http://data.un.org](http://data.un.org)
*   国际货币基金组织数据集[http://www.imf.org/external/data.htm](http://www.imf.org/external/data.htm)
*   英国政府数据[https://data.gov.uk](https://data.gov.uk)
*   爱沙尼亚[http://pub.stat.ee/px-web.2001/Dialog/statfile1.asp](http://pub.stat.ee/px-web.2001/Dialog/statfile1.asp)公开数据
*   R 中的很多 ML 库包含可以导出为 CSV 的数据[https://www.r-project.org](https://www.r-project.org)
*   基因表达数据集[http://www.ncbi.nlm.nih.gov/geo/](http://www.ncbi.nlm.nih.gov/geo/)

# 使用 Apache Spark 2.0 和 IntelliJ 集成开发环境运行您的第一个程序

该程序的目的是让您能够使用刚刚设置的 Spark 2.0 开发环境轻松编译和运行配方。我们将在后面的章节中探讨组件和步骤。

我们将编写我们自己版本的 Spark 2.0.0 程序，并检查输出，以便了解它是如何工作的。需要强调的是，这个简短的食谱只是一个简单的 RDD 程序，带有 Scala sugar 语法，以确保您在开始处理更复杂的食谱之前已经正确设置了您的环境。

# 怎么做...

1.  在 IntelliJ 或您选择的 IDE 中启动一个新项目。确保包含必要的 JAR 文件。
2.  下载该书的示例代码，找到`myFirstSpark20.scala`文件，将代码放入以下目录。

我们在 Windows 机器上的`C:\spark-2.0.0-bin-hadoop2.7\`目录中安装了 Spark 2.0。

3.  将`myFirstSpark20.scala`文件放入`C:\spark-2.0.0-bin-hadoop2.7\examples\src\main\scala\spark\ml\cookbook\chapter1`目录:

![](../images/00032.jpeg)

Mac 用户注意到，我们在一台 Mac 机器上的`/Users/USERNAME/spark/spark-2.0.0-bin-hadoop2.7/`目录下安装了 Spark 2.0。

将`myFirstSpark20.scala`文件放入`/Users/USERNAME/spark/spark-2.0.0-bin-hadoop2.7/examples/src/main/scala/spark/ml/cookbook/chapter1`目录。

4.  设置程序将驻留的包位置:

```
package spark.ml.cookbook.chapter1 
```

5.  导入火花会话所需的包以访问集群并`log4j.Logger`减少火花产生的输出量:

```
import org.apache.spark.sql.SparkSession 
import org.apache.log4j.Logger 
import org.apache.log4j.Level 
```

6.  将输出级别设置为`ERROR`以减少 Spark 的日志输出:

```
Logger.getLogger("org").setLevel(Level.ERROR) 
```

7.  通过使用构建器模式指定配置来初始化火花会话，从而为火花集群提供入口点:

```
val spark = SparkSession 
.builder 
.master("local[*]").appName("myFirstSpark20") 
.config("spark.sql.warehouse.dir", ".") 
.getOrCreate() 
```

`myFirstSpark20`对象将以本地模式运行。前面的代码块是开始创建`SparkSession`对象的典型方式。

8.  然后我们创建两个数组变量:

```
val x = Array(1.0,5.0,8.0,10.0,15.0,21.0,27.0,30.0,38.0,45.0,50.0,64.0) 
val y = Array(5.0,1.0,4.0,11.0,25.0,18.0,33.0,20.0,30.0,43.0,55.0,57.0) 
```

9.  然后，我们让 Spark 基于之前创建的阵列创建两个 rdd:

```
val xRDD = spark.sparkContext.parallelize(x) 
val yRDD = spark.sparkContext.parallelize(y) 
```

10.  接下来，我们让 Spark 对`RDD`进行操作；`zip()`功能将从前面提到的两个 rdd 创建一个新的`RDD`:

```
val zipedRDD = xRDD.zip(yRDD) 
zipedRDD.collect().foreach(println) 
```

在运行时的控制台输出中(有关如何在 IntelliJ IDE 中运行程序的更多详细信息，请参见以下步骤)，您将看到:

![](../images/00033.gif)

11.  现在，我们对`xRDD`和`yRDD`的值求和，并计算新的`zipedRDD`和值。我们还计算了`zipedRDD`的项目数:

```
val xSum = zipedRDD.map(_._1).sum() 
val ySum = zipedRDD.map(_._2).sum() 
val xySum= zipedRDD.map(c => c._1 * c._2).sum() 
val n= zipedRDD.count() 
```

12.  我们在控制台中打印出之前计算的值:

```
println("RDD X Sum: " +xSum) 
println("RDD Y Sum: " +ySum) 
println("RDD X*Y Sum: "+xySum) 
println("Total count: "+n) 
```

控制台输出如下:

![](../images/00034.gif)

13.  我们通过停止火花会话来关闭程序:

```
spark.stop() 
```

14.  一旦程序完成，IntelliJ 项目浏览器中`myFirstSpark20.scala`的布局将如下所示:

![](../images/00035.jpeg)

15.  确保没有编译错误。您可以通过重建项目来测试这一点:

![](../images/00036.jpeg)

重建完成后，控制台上将显示一条构建完成消息:

```
Information: November 18, 2016, 11:46 AM - Compilation completed successfully with 1 warning in 55s 648ms
```

16.  您可以通过右键单击项目浏览器中的`the myFirstSpark20`对象并选择名为`Run myFirstSpark20`的上下文菜单选项(如下一个截图所示)来运行上一个程序。

You can also use the Run menu from the menu bar to perform the same action.

![](../images/00037.jpeg)

17.  程序成功执行后，您将看到以下消息:

```
Process finished with exit code 0
```

这也显示在下面的截图中:

![](../images/00038.jpeg)

18.  使用 IntelliJ 的 Mac 用户将能够使用相同的上下文菜单执行此操作。

Place the code in the correct path.

# 它是如何工作的...

在这个例子中，我们编写了第一个 Scala 程序`myFirstSpark20.scala`，并显示了在 IntelliJ 中执行该程序的步骤。我们将代码放在 Windows 和 Mac 的步骤中描述的路径中。

在`myFirstSpark20`代码中，我们看到了一种创建`SparkSession`对象的典型方法，以及如何使用`master()`功能将其配置为在本地模式下运行。我们从数组对象中创建了两个关系数据库，并使用一个简单的`zip()`函数创建了一个新的 RDD。

我们还对创建的 rdd 进行了简单的求和计算，然后在控制台中显示结果。最后，我们通过调用`spark.stop()`退出并释放资源。

# 还有更多...

火花可以从[http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html)下载。

与 RDD 相关的 Spark 2.0 文档可在[http://Spark . Apache . org/docs/latest/programming-guide . html # rdd-operations](http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations)上找到。

# 请参见

*   更多关于捷脑智能的信息可以在[https://www.jetbrains.com/idea/](https://www.jetbrains.com/idea/)找到。

# 如何向您的 Spark 程序添加图形

在本食谱中，我们将讨论如何使用 JFreeChart 向您的 Spark 2.0.0 程序中添加图形图表。

# 怎么做...

1.  设置 JFreeChart 库。JFreeChart JARs 可以从[https://sourceforge.net/projects/jfreechart/files/](https://sourceforge.net/projects/jfreechart/files/)网站下载。

2.  我们在本书中介绍的 JFreeChart 版本是 JFreeChart 1.0.19，可以在下面的截图中看到。可以从[https://sourceforge.net/projects/jfreechart/files/1.下载% 20 jfreechart/1 . 0 . 19/jfreechart-1 . 0 . 19 . zip/下载](https://sourceforge.net/projects/jfreechart/files/1.%20JFreeChart/1.0.19/jfreechart-1.0.19.zip/download)网站:

![](../images/00039.jpeg)

3.  下载完 ZIP 文件后，提取它。我们为一台 Windows 机器提取了`C:\`下的 ZIP 文件，然后继续在提取的目标目录下找到`lib`目录。
4.  然后我们找到我们需要的两个库(JFreeChart 需要 JCommon)`JFreeChart-1.0.19.jar`和`JCommon-1.0.23`:

![](../images/00040.jpeg)

5.  现在我们将前面提到的两个 jar 复制到`C:\spark-2.0.0-bin-hadoop2.7\examples\jars\`目录中。

6.  如前一个设置部分所述，该目录位于 IntelliJ IDE 项目设置的类路径中:

![](../images/00041.jpeg)

In macOS, you need to place the previous two JARs in the `/Users/USERNAME/spark/spark-2.0.0-bin-hadoop2.7/examples\jars\` directory.

7.  在 IntelliJ 或您选择的 IDE 中启动一个新项目。确保包含必要的 JAR 文件。
8.  下载该书的示例代码，找到`MyChart.scala`，将代码放入以下目录。
9.  我们在 Windows 的`C:\spark-2.0.0-bin-hadoop2.7\`目录下安装了 Spark 2.0。将`MyChart.scala`放入`C:\spark-2.0.0-bin-hadoop2.7\examples\src\main\scala\spark\ml\cookbook\chapter1`目录。
10.  设置程序将驻留的包位置:

```
  package spark.ml.cookbook.chapter1
```

11.  导入火花会话所需的包，以访问集群并`log4j.Logger`减少火花产生的输出量。
12.  为图形导入必要的 JFreeChart 包:

```
import java.awt.Color 
import org.apache.log4j.{Level, Logger} 
import org.apache.spark.sql.SparkSession 
import org.jfree.chart.plot.{PlotOrientation, XYPlot} 
import org.jfree.chart.{ChartFactory, ChartFrame, JFreeChart} 
import org.jfree.data.xy.{XYSeries, XYSeriesCollection} 
import scala.util.Random 
```

13.  将输出级别设置为`ERROR`以减少 Spark 的日志输出:

```
Logger.getLogger("org").setLevel(Level.ERROR) 
```

14.  使用构建器模式初始化指定配置的火花会话，从而为火花集群提供入口点:

```
val spark = SparkSession 
  .builder 
  .master("local[*]") 
  .appName("myChart") 
  .config("spark.sql.warehouse.dir", ".") 
  .getOrCreate() 
```

15.  `myChart`对象将以本地模式运行。前面的代码块是创建`SparkSession`对象的典型开始。
16.  然后我们用一个随机数创建一个 RDD，并用它的索引压缩这个数字:

```
val data = spark.sparkContext.parallelize(Random.shuffle(1 to 15).zipWithIndex) 
```

17.  我们在控制台上打印出 RDD:

```
data.foreach(println) 
```

以下是控制台输出:

![](../images/00042.gif)

18.  然后，我们为 JFreeChart 创建一个数据系列来显示:

```
val xy = new XYSeries("") 
data.collect().foreach{ case (y: Int, x: Int) => xy.add(x,y) } 
val dataset = new XYSeriesCollection(xy) 
```

19.  接下来，我们从 JFreeChart 的`ChartFactory`创建一个图表对象，并设置基本配置:

```
val chart = ChartFactory.createXYLineChart( 
  "MyChart",  // chart title 
  "x",               // x axis label 
  "y",                   // y axis label 
  dataset,                   // data 
  PlotOrientation.VERTICAL, 
  false,                    // include legend 
  true,                     // tooltips 
  false                     // urls 
)
```

20.  我们从图表中获取绘图对象，并准备好显示图形:

```
val plot = chart.getXYPlot() 
```

21.  我们首先配置图:

```
configurePlot(plot) 
```

22.  `configurePlot`功能定义如下；它为图形部分设置了一些基本的颜色模式:

```
def configurePlot(plot: XYPlot): Unit = { 
  plot.setBackgroundPaint(Color.WHITE) 
  plot.setDomainGridlinePaint(Color.BLACK) 
  plot.setRangeGridlinePaint(Color.BLACK) 
  plot.setOutlineVisible(false) 
} 
```

23.  我们现在显示`chart`:

```
show(chart) 
```

24.  `show()`功能定义如下。这是一个非常标准的基于框架的图形显示功能:

```
def show(chart: JFreeChart) { 
  val frame = new ChartFrame("plot", chart) 
  frame.pack() 
  frame.setVisible(true) 
}
```

25.  一旦`show(chart)`执行成功，将弹出如下画面:

![](../images/00043.jpeg)

26.  我们通过停止火花会话来关闭程序:

```
spark.stop() 
```

# 它是如何工作的...

在这个例子中，我们写了`MyChart.scala`并看到了在 IntelliJ 中执行程序的步骤。我们在 Windows 和 Mac 的步骤中描述的路径中放置了代码。

在代码中，我们看到了创建`SparkSession`对象的典型方式以及如何使用`master()`功能。我们用 1 到 15 范围内的随机整数数组创建了一个 RDD，并用索引压缩了它。

然后，我们使用 JFreeChart 构建了一个包含简单的 *x* 和 *y* 轴的基本图表，并为该图表提供了我们在前面步骤中从原始 RDD 生成的数据集。

我们为图表设置了模式，并在 JFreeChart 中调用`show()`函数来显示一个框架，其中 *x* 和 *y* 轴显示为线性图形图表。

最后，我们通过调用`spark.stop()`退出并释放资源。

# 还有更多...

有关 JFreeChart 的更多信息，请访问:

*   [http://www.jfree.org/jfreechart/](http://www.jfree.org/jfreechart/)
*   [http://www.jfree.org/jfreechart/api/javadoc/index.html](http://www.jfree.org/jfreechart/api/javadoc/index.html)

# 请参见

关于 JFreeChart 特性和功能的其他示例可在以下网站上找到:

[http://www.jfree.org/jfreechart/samples.html](http://www.jfree.org/jfreechart/samples.html)