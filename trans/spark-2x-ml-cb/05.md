# Spark 2.0 中回归和分类的实用机器学习——第一部分

在本章中，我们将介绍以下食谱:

*   用传统方法拟合线性回归线
*   Spark 2.0 中的广义线性回归
*   火花 2.0 中带有套索和 BFGS 的线性回归应用编程接口
*   火花 2.0 中带有套索和自动优化选择的线性回归应用编程接口
*   Spark 2.0 中带有岭回归和自动优化选择的线性回归 API
*   Apache Spark 2.0 中的等渗回归
*   Apache Spark 2.0 中的多层感知器分类器
*   Apache Spark 2.0 中的一对多分类器(一对多)
*   Apache Spark 2.0 中的生存回归-参数 AFT 模型

# 介绍

本章和下一章将介绍 Spark 2.0 ML 和 MLlib 库中可用的回归和分类的基本技术。Spark 2.0 突出了一个新的方向，将基于 RDD 的回归(见下一章)移至维护模式，同时强调**线性回归**和**广义回归**向前发展。

在高层次上，新的应用编程接口设计支持弹性网的参数化，以产生脊线与套索回归以及两者之间的一切，而不是命名的应用编程接口(例如，`LassoWithSGD`)。新的应用编程接口方法是一种更干净的设计，当涉及到仍然是数据科学艺术的特征工程时，它迫使你学习弹性网络及其力量。我们提供了足够的例子、变体和注释来指导您理解这些技术的复杂性。

下图描述了本章中的回归和分类范围(第 1 部分):

![](../images/00104.jpeg)

首先，您将通过 Scala 代码和 RDDs 从头开始学习如何使用代数方程实现线性回归，以深入了解数学以及为什么我们需要迭代优化方法来估计大型回归系统的解。其次，我们探索了**广义线性模型**(**【GLM】**)及其各种统计分布族和链接函数，同时强调其仅限于当前实现中的 4，096 个参数。第三，我们处理**线性回归模型** ( **LRM** )以及如何使用弹性网参数化来混合和匹配 L1 和 L2 罚函数，以实现逻辑、岭、拉索以及它们之间的一切。我们还探讨了求解器(即优化器)方法，以及如何将其设置为使用 L-BFGS 优化、自动优化器选择等。

在探索了 GLM 和线性回归方法之后，我们继续提供更多奇异回归/分类方法的方法，例如等渗回归、多层感知器(即神经元网络的形式)、One-vs-Rest 和生存回归，以展示 Spark 2.0 处理线性技术无法解决的情况的能力和完整性。随着 21 世纪初金融世界风险的增加和基因组的新进展，Spark 2.0 还将四种重要的方法(等渗回归、多层感知器、One-vs-Rest 和生存回归或参数 ATF)汇集在一个易于使用的机器学习库中。金融、数据科学家或精算专业人士应该对大规模的参数 ATF 方法特别感兴趣。

即使这些方法中的一些，比如`LinearRegression()` API，从 1.3x+开始理论上就已经可用了，但是需要注意的是，Spark 2.0 在将基于 RDD 的回归 API 移动到维护模式时，以一种易于使用和维护的 API(即向后兼容)的方式将所有这些方法拉在了一起。L-BFGS 优化器和正规方程占据了主导地位，而 SGD 在基于 RDD 的 API 中提供了向后兼容性。

弹性网是首选的方法，它不仅可以绝对地处理 L1(套索回归)和 L2(岭回归)正则化的首选方法，而且还提供了一种类似拨号盘的机制，使用户能够微调罚函数(参数收缩与选择)。虽然我们记得在 1.4.2 中使用了弹性网络函数，但 Spark 2.0 将所有这些都整合在一起，而不需要处理每个单独的应用编程接口来进行参数调整(根据最新数据动态选择模型时很重要)。当我们开始钻研配方时，我们强烈鼓励用户探索各种参数设置`setElasticNetParam()`和`setSolver()`配置，以掌握这些强大的 API。重要的是不要混合罚函数`setElasticNetParam(value: Double)` (L1、L2、OLs、弹性网:线性混合的 L1/L2)，这是具有优化(normal、L-BFGS、auto 等)技术的常规或模型罚方案，这些技术与成本函数优化技术相关。

需要注意的是，基于 RDD 的回归仍然非常重要，因为当前有很多 ML 实现系统严重依赖于以前的 API 机制及其 SGD 优化器。请参阅下一章，以获得包含基于 RDD 回归的完整教学笔记。

# 用传统方法拟合线性回归线

在这个配方中，我们使用 RDDs 和一个封闭形式的公式从头开始编码一个简单的线性方程。我们将此作为第一个方法的原因是为了证明您总是可以通过 RDDs 实现任何给定的统计学习算法，从而使用 Apache Spark 实现计算规模。

# 怎么做...

1.  在 IntelliJ 或您选择的 IDE 中启动一个新项目。确保包含必要的 JAR 文件。

2.  设置程序将驻留的包位置:

```scala
package spark.ml.cookbook.chapter5
```

3.  为`SparkSession`导入必要的包以访问集群，为`log4j.Logger`导入必要的包以减少 Spark 产生的输出量:

```scala
import org.apache.spark.sql.SparkSessionimport scala.math._import org.apache.log4j.Loggerimport org.apache.log4j.Level
```

4.  使用构建器模式初始化指定配置的`SparkSession`，从而为火花簇提供入口点:

```scala
val spark = SparkSession.builder.master("local[4]").appName("myRegress01_20").config("spark.sql.warehouse.dir", ".").getOrCreate()
```

5.  将输出电平设置为`ERROR`以减少火花的输出:

```scala
Logger.getLogger("org").setLevel(Level.ERROR)Logger.getLogger("akka").setLevel(Level.ERROR)
```

6.  我们创建了两个数组，分别代表因变量(即`y`)和自变量(即`x`):

```scala
val x = Array(1.0,5.0,8.0,10.0,15.0,21.0,27.0,30.0,38.0,45.0,50.0,64.0)val y = Array(5.0,1.0,4.0,11.0,25.0,18.0,33.0,20.0,30.0,43.0,55.0,57.0)
```

7.  我们使用`sc.parallelize(x)`将两个数组转换为 RDDs:

```scala
val xRDD = sc.parallelize(x)val yRDD = sc.parallelize(y)
```

8.  在这一步中，我们演示了 RDD 的`zip()`方法，该方法从两个 rdd 创建依赖/独立元组对 *(y，x)* 。我们引入这个函数是因为你必须经常学习在机器学习算法中使用成对运算:

```scala
val zipedRDD = xRDD.zip(yRDD)
```

9.  为了确保我们理解`zip()`功能，让我们看一下输出，但是确保您包括`collect()`或一些其他形式的动作，以确保数据按顺序呈现。如果我们不使用操作方法，RDDs 的输出将是随机的:

![](../images/00105.gif)

10.  这是一个重要的步骤，演示了如何迭代、访问和计算该对中的每个成员。为了计算回归线，我们需要计算总和、乘积和平均值(即*总和(x)* 、*总和(y)* 、*总和(x * y)* )。`map(_._1).sum()`函数是一种迭代 RDD 对的机制，但只考虑第一个元素:

```scala
val xSum = zipedRDD.map(_._1).sum()val ySum = zipedRDD.map(_._2).sum()val xySum= zipedRDD.map(c => c._1 * c._2).sum()
```

11.  在这一步中，我们继续计算单个 RDD 对成员及其产品的平均值。这些单独的计算(即，*平均值(x)* 、*平均值(y)* 、*平均值(x*y)* )以及均方值将用于计算回归线的斜率和截距。虽然我们可以在前面的步骤中根据前面的统计数据手动计算平均值，但我们应该确保熟悉通过 RDD 内在可用的方法:

```scala
val n= zipedRDD.count() 
val xMean = zipedRDD.map(_._1).mean()val yMean = zipedRDD.map(_._2).mean()val xyMean = zipedRDD.map(c => c._1 * c._2).mean()
```

12.  这是最后一步，我们计算`x`和`y`平方的平均值:

```scala
val xSquaredMean = zipedRDD.map(_._1).map(x => x * x).mean()val ySquaredMean = zipedRDD.map(_._2).map(y => y * y).mean()
```

13.  我们打印统计数据以确保完整性:

```scala
println("xMean yMean xyMean", xMean, yMean, xyMean) xMean yMean xyMean ,26.16,25.16,989.08 
```

14.  我们计算公式的`numerator`和`denominator`:

```scala
val numerator = xMean * yMean  - xyMeanval denominator = xMean * xMean - xSquaredMean
```

15.  我们最终计算出回归线的斜率:

```scala
val slope = numerator / denominatorprintln("slope %f5".format(slope))slope 0.9153145 
```

16.  我们现在计算截距并打印。如果不想要截距(截距设置为`0`，那么斜率的公式需要稍微修改。您可以在其他来源(如互联网)中查找更多详细信息，并找到所需的等式:

```scala
val b_intercept = yMean - (slope*xMean)println("Intercept", b_intercept) 

Intercept,1.21
```

17.  利用斜率和截距，我们写出回归线方程如下:

```scala
Y = 1.21 + .9153145 * X
```

# 它是如何工作的...

我们声明了两个 Scala 数组，将它们并行化为两个 RDD，它们是`x()`和`y()`的独立向量。然后，我们使用 RDD 原料药中的`zip()`方法生产成对的(即压缩的)RDD。这就产生了一个 RDD，每个成员都是一对 *(x，y)* 。然后，我们继续计算平均值、总和等，并应用所述的封闭形式公式来找到回归线的截距和斜率。

在 Spark 2.0 中，另一种选择是使用开箱即用的 GLM 应用编程接口。值得一提的是，GLM 支持的闭范式方案的最大参数数被限制为 4，096。

我们使用一个封闭形式的公式来证明一条回归线与一组数字 *(Y1，X1)相关联，...，(Yn，Xn)* 就是最小化平方误差之和的直线。在一个简单的回归方程中，直线如下:

*   回归线的斜率![](../images/00106.jpeg)
*   回归线的偏移量![](../images/00107.jpeg)
*   回归线的方程式![](../images/00108.jpeg)

回归线只是最小化平方误差之和的最佳拟合线。对于一组点(因变量、自变量)，有许多线可以穿过这些点并捕捉一般的线性关系，但只有其中一条线是最小化这种拟合的所有误差的线。

例如，我们给出了线 *Y = 1.21 + .9153145 * X* 。下图中显示了这样一条线，我们用封闭形式的公式计算了斜率和偏移量。由直线的线性方程描述的线性模型代表我们对于给定数据的最佳线性模型(*斜率=.915345* ，*截距= 1.21* ),使用封闭形式的公式:

![](../images/00109.gif)

上图中绘制的数据点如下:

```scala
(Y, X)(5.0,    1.0) 
(8.0,    4.0) 
(10.0,   11.0) 
(15.0,   25.0) 
(21.0,   18.0) 
(27.0,   33.0) 
(30.0,   20.0) 
(38.0,   30.0) 
(45.0,   43.0) 
(50.0,   55.0) 
(64.0,   57.0) 
```

# 还有更多...

需要注意的是，并不是所有的回归形式都有封闭形式的公式，或者在大数据集上有大量参数的情况下变得非常低效(即不切实际)——这就是我们使用优化技术(如 SGD 或 L-BFGS)的原因。

从前面的食谱中回忆起这一点很重要，您应该确保缓存与机器学习算法相关联的任何 RDD 或数据结构，以避免由于 Spark 优化和维护沿袭(即惰性实例化)的方式而导致的惰性实例化。

# 请参见

我们推荐一本斯坦福大学的书，可以从以下网站免费下载。无论您是该领域的新从业者还是高级从业者，这都是一本经典且必读的书:

《统计学习、数据挖掘、推理和预测的要素》，第二版，作者:哈斯蒂、蒂比什拉尼和弗里德曼(2009)。斯普林格-弗拉格([http://web.stanford.edu/~hastie/ElemStatLearn/](http://web.stanford.edu/~hastie/ElemStatLearn/))。

# Spark 2.0 中的广义线性回归

本食谱涵盖了 Spark 2.0 中的**广义回归模型** ( **GLM** )实现。Spark 2.0 中的这个`GeneralizedLinearRegression`和 r 中的`glmnet`实现之间有很大的相似性。这个 API 是一个受欢迎的添加，它允许您使用一个连贯且设计良好的 API 来选择和设置分布族(例如高斯)和链接函数(例如反向对数)。

# 怎么做...

1.  我们使用来自 UCI 机器图书馆托存处的房屋数据集。

2.  从以下网址下载整个数据集:
    *   [https://archive.ics.uci.edu/ml/datasets/Housing](https://archive.ics.uci.edu/ml/datasets/Housing)
    *   [https://archive . ics . UCI . edu/ml/机器学习-数据库/房屋/](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/)

数据集由 14 列组成，前 13 列为自变量(即特征)，试图解释美国波士顿一套自住房屋的中位数价格(即最后一列)。

我们选择并清理了前八列作为特征。我们使用前 200 行来训练和预测中间价格:

3.  请使用`housing8.csv`文件，并确保将其移动到以下目录:

```scala
../data/sparkml2/chapter5/housing8.csv
```

4.  在 IntelliJ 或您选择的 IDE 中启动一个新项目。确保包含必要的 JAR 文件。
5.  设置程序将驻留的包位置:

```scala
package spark.ml.cookbook.chapter5.
```

6.  为`SparkSession`导入必要的包以访问集群，为`log4j.Logger`导入必要的包以减少 Spark 产生的输出量:

```scala
import org.apache.spark.ml.feature.LabeledPointimport org.apache.spark.ml.linalg.Vectorsimport org.apache.spark.ml.regression.GeneralizedLinearRegressionimport org.apache.spark.sql.SparkSessionimport org.apache.log4j.{Level, Logger}
```

7.  将输出级别设置为`ERROR`以减少 Spark 的日志输出:

```scala
Logger.getLogger("org").setLevel(Level.ERROR)Logger.getLogger("akka").setLevel(Level.ERROR)
```

8.  初始化指定配置的`SparkSession`以访问火花簇:

```scala
val spark = SparkSession.builder.master("local[*]").appName("GLR").config("spark.sql.warehouse.dir", ".").getOrCreate()
```

9.  我们需要为数据转换例程导入隐式:

```scala
import spark.implicits._
```

10.  接下来，我们将住房数据加载到一个数据集中:

```scala
val data = spark.read.textFile( "../data/sparkml2/ /chapter5/housing8.csv" ).as[ String ]
```

11.  让我们解析房屋数据并将其转换为标签点:

```scala
val regressionData = data.map { line =>val columns = line.split(',')LabeledPoint(columns(13).toDouble , Vectors.dense(columns(0).toDouble,columns(1).toDouble, columns(2).toDouble, columns(3).toDouble,columns(4).toDouble,columns(5).toDouble,columns(6).toDouble, columns(7).toDouble))}
```

12.  现在使用以下代码显示加载的数据:

```scala
regressionData.show(false)
```

输出如下所示:

![](../images/00110.jpeg)

13.  接下来，我们配置一个广义线性回归算法来生成一个新模型:

```scala
val glr = new GeneralizedLinearRegression().setMaxIter(1000).setRegParam(0.03) //the value ranges from 0.0 to 1.0\. Experimentation required to identify the right value..setFamily("gaussian").setLink( "identity" )
```

请随意尝试不同的参数，以获得更好的契合度。

14.  我们将模型与住房数据相匹配:

```scala
val glrModel = glr.fit(regressionData)
```

15.  接下来，我们检索汇总数据来判断模型的准确性:

```scala
val summary = glrModel.summary
```

16.  最后，我们打印出汇总统计:

```scala
val summary = glrModel.summarysummary.residuals().show()println("Residual Degree Of Freedom: " + summary.residualDegreeOfFreedom)println("Residual Degree Of Freedom Null: " + summary.residualDegreeOfFreedomNull)println("AIC: " + summary.aic)println("Dispersion: " + summary.dispersion)println("Null Deviance: " + summary.nullDeviance)println("Deviance: " +summary.deviance)println("p-values: " + summary.pValues.mkString(","))println("t-values: " + summary.tValues.mkString(","))println("Coefficient Standard Error: " + summary.coefficientStandardErrors.mkString(","))}
```

17.  我们通过停止`SparkSession`来关闭程序:

```scala
spark.stop()
```

# 它是如何工作的...

在这个配方中，我们展示了一个正在运行的广义线性回归算法。我们首先将一个 CSV 文件加载并解析到一个数据集中。接下来，我们创建了一个广义线性回归算法，并通过将数据集传递给`fit()`方法来生成一个新模型。一旦拟合操作完成，我们从模型中检索汇总统计数据，并显示计算值以调节精度。

在本例中，我们探索了用*高斯*分布和*恒等式*拟合数据，但是我们可以使用更多的配置来解决特定的回归拟合，这将在下一节中解释。

# 还有更多...

Spark 2.0 中的 GLM 是一个通用回归模型，可以支持多种配置。我们对 Spark 2.0.0 最初发布时可用的家庭数量印象深刻。

值得注意的是，从 Spark 2.0.2 开始:

*   回归的最大参数数量目前被限制为最多 4，096 个。
*   目前唯一支持的优化(即求解器)是**迭代重加权最小二乘** ( **IRLS** ，也是默认求解。
*   当您将解算器设置为*自动*时，它默认为 IRLS。
*   `setRegParam()`设置 L2 正则化的正则化参数。根据 Spark 2.0 文档，正则化术语是*0.5 * regparam * l2norm(coefficients)^2*-确保您理解其中的含义。

如果你不确定如何处理分布拟合，我们强烈推荐我们最喜欢的一本书，*用 R* 拟合统计分布手册，它在对 CBOT 小麦等农产品建模时很好地服务了我们，它有一个反向波动微笑曲线(与股票非常不同)。

配置和可用选项如下:

![](../images/00111.jpeg)

请务必尝试不同的系列和链接函数，以确保您对基础分布的假设是正确的。

# 请参见

`GeneralizedLinearRegression()`的文档可通过以下链接获得:

[http://spark . Apache . org/docs/latest/API/Scala/index . html # org . Apache . spark . ml . revolution . generated linearregression](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.GeneralizedLinearRegression)

`GeneralizedLinearRegression`内一些重要的 API 调用:

*   `def **setFamily**(value: String): GeneralizedLinearRegression.this.type`
*   `def **setLink**(value: String): GeneralizedLinearRegression.this.type`
*   `def **setMaxIter**(value: Int): GeneralizedLinearRegression.this.type`
*   `def **setRegParam**(value: Double): GeneralizedLinearRegression.this.type`
*   `def **setSolver**(value: String): GeneralizedLinearRegression.this.type`
*   `def **setFitIntercept**(value: Boolean): GeneralizedLinearRegression.this.type`

解算器目前是 IRLS；可在以下链接找到快速参考:

[https://en . Wikipedia . org/wiki/迭代重加权最小二乘](https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares)

要全面了解 Spark 2.0+中的 GLM 和线性回归新方法，请务必参考并理解 R:

*   主页位于[https://cran.r-project.org/web/packages/glmnet/index.html](https://cran.r-project.org/web/packages/glmnet/index.html)
*   用户指南可在[https://cran.r-project.org/web/packages/glmnet/glmnet.pdf](https://cran.r-project.org/web/packages/glmnet/glmnet.pdf)获得

# 火花 2.0 中带有套索和 BFGS 的线性回归应用编程接口

在本食谱中，我们将演示如何使用 Spark 2.0 的`LinearRegression()`应用编程接口来展示一个统一的/参数化的应用编程接口，以一种全面的方式处理线性回归，这种方式能够扩展，而不会出现基于 RDD 的命名应用编程接口的向后兼容性问题。我们展示了如何使用`setSolver()`将优化方法设置为一阶记忆高效的 L-BFGS，它可以轻松处理大量参数(即，尤其是在稀疏配置中)。

In this recipe, the `.setSolver()` is set to `lbgfs`, which makes the L-BFGS (see RDD-based regression for more detail) the selected optimization method. The `.setElasticNetParam()` is not set, so the default of `0` remains in effect, which makes this a Lasso regression.

# 怎么做...

1.  我们使用来自 UCI 机器图书馆托存处的房屋数据集。

2.  从以下网址下载整个数据集:
    *   [https://archive.ics.uci.edu/ml/datasets/Housing](https://archive.ics.uci.edu/ml/datasets/Housing)
    *   [https://archive . ics . UCI . edu/ml/机器学习-数据库/房屋/](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/)

数据集由 14 列组成，前 13 列为自变量(即特征)，试图解释美国波士顿一套自住房屋的中位数价格(即最后一列)。

我们选择并清理了前八列作为特征。我们使用前 200 行来训练和预测中间价格:

3.  请使用`housing8.csv`文件，并确保将其移动到以下目录:

```scala
../data/sparkml2/chapter5/housing8.csv
```

4.  在 IntelliJ 或您选择的 IDE 中启动一个新项目。确保包含必要的 JAR 文件。
5.  设置程序将驻留的包位置:

```scala
package spark.ml.cookbook.chapter5.
```

6.  为`SparkSession`导入必要的包以访问集群，并为`log4j.Logger`导入必要的包以减少 Spark 产生的输出量:

```scala
import org.apache.spark.ml.regression.LinearRegressionimport org.apache.spark.ml.feature.LabeledPointimport org.apache.spark.sql.SparkSessionimport org.apache.spark.ml.linalg.Vectorsimport org.apache.log4j.{Level, Logger}
```

7.  将输出级别设置为`ERROR`以减少 Spark 的日志输出:

```scala
Logger.getLogger("org").setLevel(Level.ERROR)Logger.getLogger("akka").setLevel(Level.ERROR)
```

8.  初始化指定配置的`SparkSession`以访问火花簇:

```scala
val spark = SparkSession.builder.master("local[*]").appName("myRegress02").config("spark.sql.warehouse.dir", ".").getOrCreate()
```

9.  我们需要为数据转换例程导入隐式:

```scala
import spark.implicits._
```

10.  接下来，我们将住房数据加载到一个数据集中:

```scala
val data = spark.read.text(
  "../data/sparkml2/chapter5/housing8.csv"
).as[
  String
]

```

11.  让我们解析房屋数据并将其转换为标签点:

```scala
val RegressionDataSet = data.map { line =>val columns = line.split(',')LabeledPoint(columns(13).toDouble , Vectors.dense(columns(0).toDouble,columns(1).toDouble, columns(2).toDouble, columns(3).toDouble,columns(4).toDouble,columns(5).toDouble,columns(6).toDouble, columns(7).toDouble))}
```

12.  现在显示加载的数据:

```scala
RegressionDataSet.show(false)
```

输出如下所示:

![](../images/00112.jpeg)

13.  接下来，我们配置一个线性回归算法来生成模型:

```scala
val numIterations = 10val lr = new LinearRegression().setMaxIter(numIterations).setSolver("l-bfgs")
```

14.  现在，我们将模型与住房数据相匹配:

```scala
val myModel = lr.fit(RegressionDataSet)
```

15.  接下来，我们检索汇总数据来协调模型的准确性:

```scala
val summary = myModel.summary
```

16.  最后，我们打印出汇总统计:

```scala
println ( "training Mean Squared Error = " + summary. meanSquaredError )println("training Root Mean Squared Error = " + summary.rootMeanSquaredError) }training Mean Squared Error = 13.608987362865541training Root Mean Squared Error = 3.689036102136375
```

17.  我们通过停止`SparkSession`来关闭程序:

```scala
spark.stop()
```

# 它是如何工作的...

在这个配方中，我们再次使用房屋数据来演示使用 L-BFGS 优化选项的 Spark 2.0 `LinearRegression()` API。我们读入文件，解析数据，并为回归选择特定的列。我们通过接受默认参数来缩短配方，但是在运行`.fit()`方法之前将迭代次数(为了收敛到一个解)和优化方法设置为`lbfgs`。然后，我们继续输出几个快速指标(即，均方误差和 RMSE)，仅供演示。我们向您展示了如何与 RDD 一起实施/计算这些指标。使用 Spark 2.0 原生设施/指标和基于 RDDs 的回归食谱，我们展示了 Spark 现在如何开箱即用地完成这些指标，这证明了我们离 Spark 1.0.1 有多远！

对少量的列使用牛顿的优化技术(例如`lbfgs`)是一种矫枉过正的做法，这将在本书的后面演示，以使读者能够在现实环境中的大型数据集上使用这些方法(例如，典型的癌症/基因组数据可从[第 1 章](01.html#PNV60-4d291c9fed174a6992fd24938c2f9c77)、*使用 Scala 的 Spark 实用机器学习*中提到的来源轻松获得)。

# 还有更多...

从 Spark 1.4 和 1.5 开始，弹性网(由 DB Tsai 和其他人贡献)和 Alpine Labs 传播出现在我们的雷达中，这是 Spark 2.0 中事实上的技术。

对于水平集，弹性网是 L1 和 L2 罚的线性组合。它可以在概念上建模为一个刻度盘，可以决定罚多少 L1 和多少 L2(收缩对选择)。

我们想强调的是，我们现在可以通过参数设置而不是命名的 API 在回归类型之间进行选择。这与我们在本章后面演示的基于 RDD 的 API(即现在处于维护模式)有很大不同。

下表提供了一个快速备忘单，用于设置参数以在套索、脊、OLS 和弹性网之间进行选择。

请看下表`setElasticNetParam(value: Double)`:

| **回归类型** | **处罚** | **参数** |
| 套索 | 腰神经 2 | Zero |
| 山脉 | L2 | one |
| 弹性网 | L1 + L2 | 0.0 |
| 内源性阿片样物质 | 普通最小二乘法 | 没有人 |

理解如何通过弹性网络参数(对应于α)控制正则化是至关重要的，这在下面的简要论述中有所描述:

*   简单:[https://en.wikipedia.org/wiki/Elastic_net_regularization](https://en.wikipedia.org/wiki/Elastic_net_regularization)
*   完成:[http://www.stat.purdue.edu/~tlzhang/mathstat/ElasticNet.pdf](http://www.stat.purdue.edu/~tlzhang/mathstat/ElasticNet.pdf)
*   使用基因组数据:[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3232376/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3232376/)

# 请参见

*   `LinearRegression()` : [文档](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegression)
*   一定要参考实际的源代码，因为它扩展了*回归器*本身:[https://github . com/Apache/spark/blob/v 2 . 0 . 2/mllib/src/main/Scala/org/Apache/spark/ml/revolution/linear revolution . Scala](https://github.com/apache/spark/blob/v2.0.2/mllib/src/main/scala/org/apache/spark/ml/regression/LinearRegression.scala)
*   `LinearRegression`内一些重要的 API 调用:
    *   `def setElasticNetParam(value: Double): LinearRegression.this.type`
    *   `def **setRegParam**(value: Double): LinearRegression.this.type`
    *   `def **setSolver**(value: String): LinearRegression.this.type`
    *   `def **setMaxIter**(value: Int): LinearRegression.this.type`
    *   `def **setFitIntercept**(value: Boolean): LinearRegression.this.type`

Spark ML 的一个重要方面是其简单但异常强大的 API 集，允许开发人员在现有集群上以很少的额外努力扩展到数十亿行。当 L-BFGS 优化(不需要直接的 hessian 矩阵)可以轻松处理大量特征时，Lasso 可以用来发现相关特征集的规模会让你感到惊讶。Spark 2.0 源代码中 LBFGS 的`updater`实现的细节超出了本书的范围。

由于它们的复杂性，我们将在后续章节中介绍与这些 ML 算法相关的优化。

# 火花 2.0 中带有套索和“自动”优化选择的线性回归应用编程接口

在本食谱中，我们在之前食谱`LinearRegression`的基础上，通过`setElasticNetParam(0.0)`明确选择 LASSO 回归，同时让 Spark 2.0 使用`setSolver('auto')` *自行选择优化。*我们再次提醒，基于 RDD 的回归 API 现在处于维护模式，这是未来的首选方法。

# 怎么做...

1.  我们使用来自 UCI 机器图书馆托存处的房屋数据集..

2.  从以下网址下载整个数据集:

*   [https://archive.ics.uci.edu/ml/datasets/Housing](https://archive.ics.uci.edu/ml/datasets/Housing)
*   [https://archive . ics . UCI . edu/ml/机器学习-数据库/房屋/](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/)

数据集由 14 列组成，前 13 列为自变量(即特征)，试图解释美国波士顿一套自住房屋的中位数价格(即最后一列)。

我们选择并清理了前八列作为特征。我们使用前 200 行来训练和预测中间价格:

3.  请使用`housing8.csv`文件，并确保将其移动到以下目录:

```scala
 ../data/sparkml2/chapter5/housing8.csv
```

4.  在 IntelliJ 或您选择的 IDE 中启动一个新项目。确保包含必要的 JAR 文件。

5.  设置程序将驻留的包位置:

```scala
package spark.ml.cookbook.chapter5.
```

6.  为`SparkSession`导入必要的包以访问集群，并为`Log4j.Logger`导入必要的包以减少 Spark 产生的输出量:

```scala
import org.apache.spark.ml.regression.LinearRegressionimport org.apache.spark.ml.feature.LabeledPointimport org.apache.spark.sql.SparkSessionimport org.apache.spark.ml.linalg.Vectorsimport org.apache.log4j.{Level, Logger}
```

7.  将输出级别设置为`ERROR`以减少 Spark 的日志输出:

```scala
Logger.getLogger("org").setLevel(Level.ERROR)Logger.getLogger("akka").setLevel(Level.ERROR)
```

8.  初始化指定配置的`SparkSession`以访问火花簇:

```scala
val spark = SparkSession.builder.master("local[*]").appName("myRegress03").config("spark.sql.warehouse.dir", ".").getOrCreate()
```

9.  我们需要为数据转换例程导入隐式:

```scala
import spark.implicits._
```

10.  接下来，我们将住房数据加载到一个数据集中:

```scala
val data = spark.read.text( "../data/sparkml2/chapter5/housing8.csv" ).as[ String ]
```

11.  让我们解析房屋数据并将其转换为标签点:

```scala
val RegressionDataSet = data.map { line =>val columns = line.split(',')LabeledPoint(columns(13).toDouble , Vectors.dense(columns(0).toDouble,columns(1).toDouble, columns(2).toDouble, columns(3).toDouble,columns(4).toDouble,columns(5).toDouble,columns(6).toDouble, columns(7).toDouble))}
```

12.  现在显示加载的数据:

![](../images/00113.jpeg)

13.  接下来，我们配置一个线性回归算法来生成模型:

```scala
val lr = new LinearRegression().setMaxIter(1000).setElasticNetParam(0.0).setRegParam(0.01).setSolver( "auto" )
```

14.  现在，我们将模型与住房数据相匹配:

```scala
val myModel = lr.fit(RegressionDataSet)
```

15.  接下来，我们检索汇总数据来协调模型的准确性:

```scala
val summary = myModel.summary
```

16.  最后，我们打印出汇总统计:

```scala
println ( "training Mean Squared Error = " + summary. meanSquaredError )println("training Root Mean Squared Error = " + summary.rootMeanSquaredError) }training Mean Squared Error = 13.609079490110766training Root Mean Squared Error = 3.6890485887435482
```

17.  我们通过停止`SparkSession`来关闭程序:

```scala
spark.stop()
```

# 它是如何工作的...

我们读取住房数据，加载选定的列，并使用它们来预测住房单元的价格。我们使用下面的代码片段选择回归作为 LASSO，让 Spark 自己进行优化:

```scala
val lr = new LinearRegression().setMaxIter(1000).setElasticNetParam(0.0).setRegParam(0.01).setSolver( "auto" )
```

出于演示目的，我们将`setMaxIter()`改为`1000`。默认设置是`100`开箱即用。

# 还有更多...

虽然 Spark 很好地实现了 L-BFGS，但请查看以下链接，以快速了解 BFGS 及其内部工作原理，因为它与本食谱有关:

*   BFGS 的简单处理:[https://en . Wikipedia . org/wiki/Broyden-Fletcher-Goldfarb-Shanno _ algorithm](https://en.wikipedia.org/wiki/Broyden-Fletcher-Goldfarb-Shanno_algorithm)
*   来自*机器学习研究杂志*以及从数学编程的角度来看不错的有限内存 BGFS 处理:[http://www.jmlr.org/papers/volume14/hennig13a/hennig13a.pdf](http://www.jmlr.org/papers/volume14/hennig13a/hennig13a.pdf)

另请参阅基于 RDD 的回归食谱，了解关于 LBGFS 的更多细节。如果您需要了解 BFGS 技术，以下链接提供了实现细节。

这种 C 语言的实现有助于我们在代码层面对一阶优化有一个坚实的理解:[http://www.chokkan.org/software/liblbfgs/](http://www.chokkan.org/software/liblbfgs/)

*   *cctbx* 也提供了很好的实现细节，如果你需要看更多:[http://cctbx.sourceforge.net](http://cctbx.sourceforge.net)
*   哈佛大学对 BFGS 的待遇相当不错

# 请参见

*   `LinearRegression()` : [文档](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegression)
*   BFGS 和 BFGS 的文件:
    *   [https://en . Wikipedia . org/wiki/Broyden-Fletcher-Goldfarb-Shanno _ algorithm](https://en.wikipedia.org/wiki/Broyden-Fletcher-Goldfarb-Shanno_algorithm)
    *   [https://en.wikipedia.org/wiki/Limited-memory_BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS)

# Spark 2.0 中带有岭回归和“自动”优化选择的线性回归 API

在这个配方中，我们使用`LinearRegression`界面实现了岭回归。我们使用弹性网络参数将适当的值设置为全 L2 惩罚，这又相应地选择了岭回归。

# 怎么做...

1.  我们使用来自 UCI 机器图书馆托存处的房屋数据集。

2.  从以下网址下载整个数据集:
    *   [https://archive.ics.uci.edu/ml/datasets/Housing](https://archive.ics.uci.edu/ml/datasets/Housing)
    *   [https://archive . ics . UCI . edu/ml/机器学习-数据库/房屋/](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/)

数据集由 14 列组成，前 13 列为自变量(即特征)，试图解释美国波士顿一套自住房屋的中位数价格(即最后一列)。

我们选择并清理了前八列作为特征。我们使用前 200 行来训练和预测中间价格:

3.  请使用`housing8.csv`文件，并确保将其移动到以下目录:

```scala
 ../data/sparkml2/chapter5/housing8.csv
```

4.  在 IntelliJ 或您选择的 IDE 中启动一个新项目。确保包含必要的 JAR 文件。

5.  设置程序将驻留的包位置:

```scala
package spark.ml.cookbook.chapter5.
```

6.  为`SparkSession`导入必要的包以访问集群，为`Log4j.Logger`导入必要的包以减少 Spark 产生的输出量:

```scala
import org.apache.spark.ml.feature.LabeledPointimport org.apache.spark.ml.linalg.Vectorsimport org.apache.spark.ml.regression.LinearRegressionimport org.apache.spark.sql.SparkSessionimport org.apache.log4j.{Level, Logger}
```

7.  将输出级别设置为`ERROR`以减少 Spark 的日志输出:

```scala
Logger.getLogger("org").setLevel(Level.ERROR)Logger.getLogger("akka").setLevel(Level.ERROR)
```

8.  初始化指定配置的`SparkSession`以访问火花簇:

```scala
val spark = SparkSession.builder.master("local[*]").appName("myRegress04").config("spark.sql.warehouse.dir", ".").getOrCreate()
```

9.  我们需要为数据转换例程导入隐式:

```scala
import spark.implicits._
```

10.  接下来，我们将住房数据加载到一个数据集中:

```scala
val data = spark.read.text( "../data/sparkml2/chapter5/housing8.csv" ).as[ String ]
```

11.  让我们解析房屋数据并将其转换为标签点:

```scala
val RegressionDataSet = data.map { line =>val columns = line.split(',')LabeledPoint(columns(13).toDouble , Vectors.dense(columns(0).toDouble,columns(1).toDouble, columns(2).toDouble, columns(3).toDouble,columns(4).toDouble,columns(5).toDouble,columns(6).toDouble, columns(7).toDouble))}
```

12.  现在显示加载的数据:

![](../images/00114.jpeg)

13.  接下来，我们配置一个线性回归算法来生成模型:

```scala
val lr = new LinearRegression().setMaxIter(1000).setElasticNetParam(1.0).setRegParam(0.01).setSolver( "auto" )
```

14.  现在，我们将模型与住房数据进行拟合:

```scala
val myModel = lr.fit(RegressionDataSet)
```

15.  接下来，我们检索汇总数据来协调模型的准确性:

```scala
val summary = myModel.summary
```

16.  最后，我们打印出汇总统计:

```scala
println ( "training Mean Squared Error = " + summary. meanSquaredError )println("training Root Mean Squared Error = " + summary.rootMeanSquaredError) }training Mean Squared Error = 13.61187856748311training Root Mean Squared Error = 3.6894279458315906
```

17.  我们通过停止`SparkSession`来关闭程序:

```scala
spark.stop()
```

# 它是如何工作的...

我们通过读取房屋数据并加载适当的列来加载数据。然后，我们继续设置参数，强制`LinearRegression()`执行岭回归，同时保持优化为“自动”。下面的代码显示了如何使用线性回归 API 将期望的回归类型设置为岭型:

```scala
val lr = new LinearRegression().setMaxIter(1000).setElasticNetParam(1.0).setRegParam(0.01).setSolver( "auto" )
```

然后我们使用`.fit()`将模型拟合到数据。最后用`.summary`提取模型概要，打印模型的 MSE 和 RMSE。

# 还有更多...

为了确保我们清楚了解 ridge 和 Lasso 回归之间的区别，我们必须首先突出参数收缩(即，我们使用平方根函数挤压权重，但从不将其设置为零)和特征工程或参数选择(即，我们将参数一直收缩到`0`，从而导致一些参数从模型中完全消失)之间的区别:

*   岭回归:[https://en.wikipedia.org/wiki/Tikhonov_regularization](https://en.wikipedia.org/wiki/Tikhonov_regularization)
*   套索回归:[https://en . Wikipedia . org/wiki/Lasso _(统计)](https://en.wikipedia.org/wiki/Lasso_(statistics))
*   弹性网-斯坦福大学:[http://web.stanford.edu/~hastie/TALKS/enet_talk.pdf](http://web.stanford.edu/~hastie/TALKS/enet_talk.pdf)

# 请参见

线性回归文档:[http://spark . Apache . org/docs/latest/API/Scala/index . html # org . Apache . spark . ml . revolution . Linear revolution](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegression)

# Apache Spark 2.0 中的等渗回归

在本食谱中，我们演示了 Spark 2.0 中的`IsotonicRegression()`功能。当数据中的顺序是预期的，并且我们希望将一条递增的有序线(也就是说，表现为阶跃函数)拟合到一系列观察值时，使用等渗或单调回归。术语**等张回归** ( **IR** )和**单调回归** ( **MR** )在文献中是同义的，可以互换使用。

简而言之，我们试图用`IsotonicRegression()`配方做的是提供一个更好的匹配，而不是朴素贝叶斯和 SVM 的一些缺点。虽然它们都是强大的分类器，但朴素贝叶斯缺乏对 P (C | X)的良好估计，而**支持向量机** ( **SVM** )充其量只能提供一个代理(可以使用超平面距离)，在某些情况下不是一个准确的估计器。

# 怎么做...

1.  去网站下载文件，并将文件保存到下面代码块中提到的数据路径中。我们使用著名的 Iris 数据，并为观察值拟合一条阶梯线。我们使用库中`LIBSVM`格式的 Iris 数据来演示 IR。

我们选择的文件名是`iris.scale.txt` [。](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/iris.scale)

2.  在 IntelliJ 或您选择的 IDE 中启动一个新项目。确保包含必要的 JAR 文件。

3.  设置程序将驻留的包位置:

```scala
package spark.ml.cookbook.chapter5
```

4.  为`SparkSession`导入必要的包以访问集群，为`Log4j.Logger`导入必要的包以减少 Spark 产生的输出量:

```scala
import org.apache.spark.sql.SparkSessionimport org.apache.spark.ml.regression.IsotonicRegression
```

5.  将输出级别设置为`ERROR`以减少 Spark 的日志输出:

```scala
Logger.getLogger("org").setLevel(Level.ERROR)Logger.getLogger("akka").setLevel(Level.ERROR)
```

6.  使用构建器模式初始化指定配置的`SparkSession`，从而为火花簇提供入口点:

```scala
val spark = SparkSession.builder.master("local[4]").appName("myIsoTonicRegress").config("spark.sql.warehouse.dir", ".").getOrCreate()
```

7.  然后，我们读入数据文件，打印出数据模式，并在控制台中显示数据:

```scala
val data = spark.read.format("libsvm").load("../data/sparkml2/chapter5/iris.scale.txt")data.printSchema()data.show(false)
```

我们得到以下控制台输出:

![](../images/00115.jpeg)

8.  然后我们按照 *0.7:0.3* 的比例将数据集拆分为训练集和测试集:

```scala
val Array(training, test) = data.randomSplit(Array(0.7, 0.3), seed = System.currentTimeMillis())
```

9.  接下来，我们创建`IsotonicRegression`对象，并将其拟合到训练数据中:

```scala
val itr = new IsotonicRegression()val itrModel = itr.fit(training)
```

10.  现在，我们在控制台中打印出模型边界和预测:

```scala
println(s"Boundaries in increasing order: ${itrModel.boundaries}")println(s"Predictions associated with the boundaries: ${itrModel.predictions}")
```

我们得到以下控制台输出:

```scala
Boundaries in increasing order: [-1.0,-0.666667,-0.666667,-0.5,-0.5,-0.388889,-0.388889,-0.333333,-0.333333,-0.222222,-0.222222,-0.166667,-0.166667,0.111111,0.111111,0.333333,0.333333,0.5,0.555555,1.0]Predictions associated with the boundaries: [1.0,1.0,1.1176470588235294,1.1176470588235294,1.1666666666666663,1.1666666666666663,1.3333333333333333,1.3333333333333333,1.9,1.9,2.0,2.0,2.3571428571428577,2.3571428571428577,2.5333333333333314,2.5333333333333314,2.7777777777777786,2.7777777777777786,3.0,3.0]
```

11.  我们让模型转换测试数据并显示结果:

```scala
itrModel.transform(test).show()
```

我们得到以下控制台输出:

![](../images/00116.jpeg)

12.  我们通过停止`SparkSession`来关闭程序:

```scala
spark.stop()
```

# 它是如何工作的...

在这个例子中，我们探索了等渗回归模型的特性。我们首先以`libsvm`格式将数据集文件读入 Spark。然后我们分割数据( *70/30* )并继续进行。接下来，我们通过调用`.show()`函数在控制台中显示数据框。然后我们创建了`IsotonicRegression()`对象，并通过调用`fit(data)`函数让模型自己运行。在这个配方中，我们保持简单，没有改变任何默认参数，但是读者应该实验并使用 JChart 包来绘制线，并看到对增加和步进线结果的影响。

最后，我们在控制台中显示模型边界和预测，并使用模型转换测试数据集，在控制台中显示结果数据框，其中包含预测字段。所有火花最大似然算法对超参数值敏感。虽然设置这些参数没有硬性规定，但在投入生产之前，需要使用科学方法进行大量实验。

在前几章中，我们已经介绍了 Spark 提供的大量模型评估工具，并在整本书中讨论了评估指标，没有多余的内容。Spark 提供了以下模型评估方法。开发人员必须根据被评估的算法类型(例如，离散、连续、二进制、多类等)挑选特定的评估度量工具。

我们将使用菜谱单独涵盖评估指标，但请参见 Spark 模型评估覆盖的以下链接:[http://Spark . Apache . org/docs/latest/mllib-evaluation-metrics . html](http://spark.apache.org/docs/latest/mllib-evaluation-metrics.html)。

# 还有更多...

在撰写本文时，Spark 2.0 实现有以下限制:

*   仅支持单一特征(即单变量)算法:

```scala
def setFeaturesCol(value: String): IsotonicRegression.this.type
```

*   该实现目前设置为**并行池相邻违规者算法** ( **PAVA** ):
*   从 Spark 2.1.0 开始，它是一个单变量单调实现
*   参见类似 Spark 2.0 的 CRAN 实现:[https://CRAN . r-project . org/web/packages/isotone/渐晕/isotone.pdf](https://cran.r-project.org/web/packages/isotone/vignettes/isotone.pdf)
*   见加州大学洛杉矶分校论文(PAVA):[http://gifi . stat . UCLA . edu/janspubs/2009/reports/deleeuw _ hornik _ mair _ R _ 09 . pdf](http://gifi.stat.ucla.edu/janspubs/2009/reports/deleeuw_hornik_mair_R_09.pdf)
*   参见威斯康星大学:https://www.biostat.wisc.edu/sites/default/files/tr_116.pdf
*   等渗回归的文档:
    *   [https://spark . Apache . org/docs/latest/ml-分类-回归. html #等渗-回归](https://spark.apache.org/docs/latest/ml-classification-regression.html#isotonic-regression)
    *   [https://spark . Apache . org/docs/latest/API/Scala/index . html # org . Apache . spark . ml . rejection . isotoncregression](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.IsotonicRegression)
    *   [https://spark . Apache . org/docs/latest/API/Scala/index . html # org . Apache . spark . ml . revolution . isotoncregressionmodel](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.IsotonicRegressionModel)

# 请参见

有关等渗回归的更多信息，请访问:

[https://en.wikipedia.org/wiki/Isotonic_regression](https://en.wikipedia.org/wiki/Isotonic_regression)

等渗回归线最终是一个阶跃函数，而不是直线的线性回归。下图(来源:维基百科)提供了很好的参考:

![](../images/00117.jpeg)

# Apache Spark 2.0 中的多层感知器分类器

在这个食谱中，我们探索了 Spark 的 2.0 **多层感知器分类器** ( **MLPC** )，这是前馈神经网络的另一个名称。我们使用虹膜数据集来预测描述输入的特征向量的二进制结果。要记住的关键一点是，尽管这个名字听起来有点复杂，但从本质上来说，MLP 只是一个非线性的数据分类器，不能通过简单的线性线或超平面来分离。

# 怎么做...

1.  转到`LIBSVM`数据:分类(多类)资源库，从以下网址下载文件:[https://www . csie . NTU . edu . tw/~ cjlin/libsvmtols/datasets/Multi class/iris . scale](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/iris.scale)

2.  在 IntelliJ 或您选择的 IDE 中启动一个新项目。确保包含必要的 JAR 文件。

3.  设置程序将驻留的包位置:

```scala
package spark.ml.cookbook.chapter5
```

4.  为`SparkSession`导入必要的包以访问集群，为`Log4j.Logger`导入必要的包以减少 Spark 产生的输出量:

```scala
import org.apache.spark.ml.classification.MultilayerPerceptronClassifierimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluatorimport org.apache.spark.sql.SparkSessionimport org.apache.log4j.{ Level, Logger}
```

5.  将输出级别设置为`ERROR`以减少 Spark 的日志输出:

```scala
 Logger.getLogger("org").setLevel(Level.ERROR)Logger.getLogger("akka").setLevel(Level.ERROR)
```

6.  初始化指定配置的`SparkSession`以访问火花簇:

```scala
val spark = SparkSession.builder.master("local[*]").appName("MLP").getOrCreate()
```

7.  我们首先将`libsvm`格式的数据文件加载到内存中:

```scala
val data = spark.read.format( "libsvm" ).load("../data/sparkml2/chapter5/iris.scale.txt")
```

8.  现在显示加载的数据:

从控制台，这是输出:

```scala
data.show(false)
```

![](../images/00118.jpeg)

9.  接下来，我们利用数据集`randomSplit`方法将数据分成两个桶，每个桶分配 80%和 20%:

```scala
val splitData = data.randomSplit(Array( 0.8 , 0.2 ), seed = System.currentTimeMillis())
```

10.  `randomSplit`方法返回一个包含两组数据的数组，训练集是 80%的部分，测试集是 20%的部分:

```scala
val train = splitData(0)val test = splitData(1)
```

11.  接下来，我们配置多层感知器分类器，其输入层由四个节点组成，隐藏层由五个节点组成，输出层由四个节点组成:

```scala
val layers = Array[Int](4, 5, 4)val mlp = new MultilayerPerceptronClassifier().setLayers(layers).setBlockSize(110).setSeed(System.currentTimeMillis()).setMaxIter(145)
```

GitHub 上的 Spark 源代码中的以下两行显示了代码中的默认设置:

```scala
setDefault(maxIter->100, tol -> 1e-6, blockSize ->128, solver -> MultilayerPerceptronClassifier.LBFGS, stepSize ->0.03)
```

为了更好地理解参数和播种，请参见位于[的 MLP 源代码。](https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/classification/MultilayerPerceptronClassifier.scala)

12.  我们通过调用 fit 方法生成一个模型:

```scala
val mlpModel = mlp.fit(train)
```

13.  接下来，我们将训练好的模型用于转换测试数据和显示预测结果:

```scala
val result = mlpModel.transform(test)result.show(false)
```

结果将显示在控制台中，如下所示:

![](../images/00119.gif)

14.  最后，我们从结果中提取预测和标签，并将它们传递给多类分类评估器，以生成准确性值:

```scala
val predictions = result.select("prediction", "label")val eval = new MulticlassClassificationEvaluator().setMetricName("accuracy")println("Accuracy: " + eval.evaluate(predictions))Accuracy: 0.967741935483871
```

15.  我们通过停止`SparkSession`来关闭程序:

```scala
spark.stop()
```

# 它是如何工作的...

在本食谱中，我们演示了多层感知器分类器的使用。我们首先以`libsvm`格式加载经典的 Iris 数据集。接下来，我们以 80%的训练集数据和 20%的测试集数据的比率分割数据集。在定义阶段，我们为多层感知器分类器配置了一个四节点输入层、一个五节点隐藏层和一个四节点输出层。我们通过调用`fit()`方法生成一个训练好的模型，然后利用训练好的模型产生预测。

最后，我们检索预测和标签，将它们传递给计算精度值的多类分类评估器。

无需太多实验和拟合，对预测值和实际值进行简单的视觉检查似乎非常令人印象深刻，并证明了为什么神经网络(与 20 世纪 90 年代早期的版本有很大不同)重新受到青睐。它们在捕捉非线性表面方面做得很好。以下是一些非线性表面的例子(来源:苹果应用商店的图形计算器 4)。

下图显示了一个非线性示例的 2D 描述:

![](../images/00120.jpeg)

下图显示了一个示例非线性情况的三维描述。

![](../images/00121.jpeg)

一般来说，神经网络首先由如下代码样本定义:

```scala
val layers = Array[Int](4, 5, 4)val mlp = new MultilayerPerceptronClassifier().setLayers(layers).setBlockSize(110).setSeed(System.currentTimeMillis()).setMaxIter(145)
```

它定义了网络的物理配置。在这种情况下，我们有一个 *4 x 5 x 4* MLP，意思是四个输入层，五个隐藏层和四个输出层。出于演示目的，使用`setBlockSize(110)`方法将`BlockSize`设置为 110，但默认为 128。重要的是要有一个好的随机函数来初始化权重，在这种情况下是当前的系统时间`setSeed(System.*currentTimeMillis*()`。`setMaxIter(145)`是`setSolver()`方法使用的最大迭代次数，默认为`l-bfgs`求解器。

# 还有更多...

**多层感知器**(**【MLP】**)或**前馈网络** ( **FFN** )通常是人们在毕业前了解到的第一类神经元网络，它们是深度学习中常见的**受限玻尔兹曼机器** ( **RBM** )和**递归神经网络** ( **RRN** )。虽然 MLP 在技术上可以被配置/称为深度网络，但是我们必须稍微调查一下，并理解为什么它被认为是走向深度学习网络的第一步。

在 Spark 的 2.0 实现中，sigmoid 函数(非线性激活)用于深度可堆叠网络配置(超过三层)中，将输出映射到`Softmax`函数，以创建可捕获数据极端非线性行为的非平凡映射表面。

Spark 使用 sigmoid 函数，通过易于使用的 API，在可堆叠配置中实现非线性映射。下图描述了一个 Sigmoid 函数及其图形，使用了 Mac 上的图形计算器软件

![](../images/00122.jpeg)

# 请参见

*   Spark 2.0 MLP 的文档资料
*   要快速了解 MLP，请参阅以下内容:
    *   [https://en.wikipedia.org/wiki/Multilayer_perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron)
    *   [https://en . Wikipedia . org/wiki/sensors](https://en.wikipedia.org/wiki/Perceptron)
    *   [http://www . di . ubi . pt/~ lfbaa/pubs/nn 2008 . pdf](http://www.di.ubi.pt/~lfbaa/pubs/NN2008.pdf)

*   参见[上的 Spark MLP 源代码](https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/classification/MultilayerPerceptronClassifier.scala)

经典论文需要理解深层信念网络(绝对最小值)及其与简单 MLP 的对比:

*   深度信仰网络:[https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf)
*   堆叠式自动编码器:[http://papers . nips . cc/paper/3048-贪婪逐层训练深度网络. pdf](http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf)
*   稀疏表示:[http://www . cs . NYU . edu/~ ranz ATO/publications/ranz ATO-nip s06 . pdf](http://www.cs.nyu.edu/~ranzato/publications/ranzato-nips06.pdf)

`MultilayerPerceptronClassifier` **内的一些重要 API 调用:**

`BlockSize`默认设置为 128 -只有当你觉得你已经完全掌握了 MLP 时，你才应该开始调整这个参数:

*   `def **setLayers**(value: Array[Int]): MultilayerPerceptronClassifier.this.type`
*   `def **setFeaturesCol**(value: String): MultilayerPerceptronClassifier`
*   `def **setLabelCol**(value: String): MultilayerPerceptronClassifier`
*   ``def **setSeed**(value: Long): MultilayerPerceptronClassifier.this.type``
*   `def **setBlockSize**(value: Int): MultilayerPerceptronClassifier.this.type`
*   `def **setSolver**(value: String): MultilayerPerceptronClassifier.this.type`

# Apache Spark 2.0 中的一对多分类器(一对多)

在这个食谱中，我们在 Apache Spark 2.0 中演示了一对多。我们试图通过`OneVsRest()`分类器实现的是进行二元逻辑回归来解决多类/多标签分类问题。配方是一个两步方法，其中我们首先配置一个`LogisticRegression()`对象，然后在`OneVsRest()`分类器中使用它来解决使用逻辑回归的多类分类问题。

# 怎么做...

1.  转到`LIBSVM`数据:分类(多类)资源库，下载文件:[https://www . csie . NTU . edu . tw/~ cjlin/libsvmtols/datasets/Multi class/iris . scale](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/iris.scale)

2.  在 IntelliJ 或您选择的 IDE 中启动一个新项目。确保包含必要的 JAR 文件。
3.  设置程序将驻留的包位置:

```scala
package spark.ml.cookbook.chapter5
```

4.  为`SparkSession`导入必要的包以访问集群，并为`Log4j.Logger`导入必要的包以减少 Spark 产生的输出量:

```scala
import org.apache.spark.sql.SparkSessionimport org.apache.spark.ml.classification.{LogisticRegression, OneVsRest}import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluatorimport org.apache.log4j.{ Level, Logger}
```

5.  将输出级别设置为`ERROR`以减少 Spark 的日志输出:

```scala
Logger.getLogger("org").setLevel(Level.ERROR)Logger.getLogger("akka").setLevel(Level.ERROR)
```

6.  初始化一个`SparkSession`指定构造火花簇入口点的配置:

```scala
val spark = SparkSession.builder.master("local[*]").appName("One-vs-Rest").getOrCreate()
```

7.  我们首先将`libsvm`格式的数据文件加载到内存中:

```scala
 val data = spark.read.format("libsvm")
 .load("../data/sparkml2/chapter5/iris.scale.txt")
```

8.  现在显示加载的数据:

```scala
data.show(false)
```

![](../images/00123.jpeg)

9.  接下来，我们利用数据集`randomSplit`方法，以 80%的训练数据和 20%的测试数据的比率分割数据集:

```scala
val Array (train, test) = data.randomSplit(Array( 0.8 , 0.2 ), seed = System.currentTimeMillis())
```

10.  让我们配置一个物流回归算法，用作一对多算法的分类器:

```scala
val lrc = new LogisticRegression().setMaxIter(15).setTol(1E-3).setFitIntercept(true)
```

11.  接下来，我们创建 one vs rest 对象，传递我们新创建的 logistic 回归对象作为参数:

```scala
val ovr = new OneVsRest().setClassifier(lrc)
```

12.  我们通过调用 one-vs-rest 对象上的 fit 方法来生成模型:

```scala
val ovrModel = ovr.fit(train)
```

13.  Now, we will use the trained model to generate predictions for the test data and display the results:

    ![](../images/00124.jpeg)

14.  最后，我们将预测传递给多类别分类评估器，以生成一个准确性值:

```scala
val eval = new MulticlassClassificationEvaluator().setMetricName("accuracy")val accuracy = eval.evaluate(predictions)println("Accuracy: " + eval.evaluate(predictions))Accuracy: 0.9583333333333334
```

15.  我们通过停止`SparkSession`来关闭程序:

```scala
spark.stop()
```

# 它是如何工作的...

在这个例子中，我们演示了一对多分类器的用法。我们首先以`libsvm`格式加载经典的 Iris 数据集。接下来，我们以训练数据集 80%和测试数据集 20%的比率分割数据集。我们提请用户注意我们如何使用系统时间进行随机分割，如下所示:

```scala
data.randomSplit(Array( 0.8 , 0.2 ), seed = System.currentTimeMillis())
```

该算法可以最好地描述为一个三步过程:

1.  我们首先配置回归对象，而不需要手边有一个基本的逻辑模型，这样它就可以输入到我们的分类器中:

```scala
LogisticRegression().setMaxIter(15).setTol(1E-3).setFitIntercept(true)
```

2.  在下一步中，我们将配置好的回归模型输入到我们的分类器中，并调用`fit()`函数来完成相应的工作:

```scala
val ovr = new OneVsRest().setClassifier(lrc)
```

3.  我们生成一个训练好的模型，并通过模型转换测试数据。最后，我们将预测传递给多类分类评估器，生成一个准确性值。

# 还有更多...

这个算法的典型用法是将一个人感兴趣的不同新闻项目标记和打包成不同的类别(例如友好对敌对、冷淡对兴高采烈等等)。医疗记账的另一种用途是将患者诊断分类为不同的医疗代码，用于自动记账和收入周期最大化。

One-vs-Rest:如下图所示，这通过二元逻辑回归解决了一个 *n* 标签分类问题:

![](../images/00125.jpeg)

# 请参见

Spark 针对`OneVsRest()`的 2.0 文档可在以下网址找到:

[http://spark . Apache . org/docs/latest/API/Scala/index . html # org . Apache . spark . ml . classification . onevsrest](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.classification.OneVsRest)

可视化的另一种方法是，评估对于给定的二进制分类器，我们是否可以将 N 类输入分解为 N 个逻辑回归，然后选择最能描述数据的一个。在 Python 中使用 Scikit Learn 库的这个分类器有很多例子，如下所示:

[http://scikit-learn . org/stability/modules/generate/硬化. multicclass . onevsoneclassifier . html #硬化. multicclass . onevsoneclassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html#sklearn.multiclass.OneVsOneClassifier)

但是我们建议您在 GitHub 上快速浏览一下实际的 Scala 源代码(只有不到 400 行):

[https://github . com/Apache/spark/blob/v 2 . 0 . 2/mllib/src/main/scale/org/Apache/spark/ml/class ification/onevsrest . scale](https://github.com/apache/spark/blob/v2.0.2/mllib/src/main/scala/org/apache/spark/ml/classification/OneVsRest.scala)

# 生存回归 Apache Spark 2.0 中的参数 AFT 模型

在这个食谱中，我们探索了 Spark 2.0 的生存回归实现，它不是典型的比例风险模型，而是**加速失效时间** ( **AFT** )模型。这是一个重要的区别，在运行这个配方时应该记住，否则结果将没有意义。

生存回归分析考虑的是*时间到事件*性质的模型，这在医学、保险和受试者的任何时间生存性都是常见的。我的一个合著者恰好是一个训练有素的医学博士(除了是一个计算机科学家之外)，所以我们使用了一个真实的数据集 HMO-HIM+来自该领域一本备受尊敬的书的研究，这样我们就可以获得合理的输出。

目前，我们正在使用这种技术进行大规模干旱建模，以在长期时间框架和预测中预测价格对农产品的影响。

# 怎么做...

1.  去加州大学洛杉矶分校网站下载文件:

[https://stats . IDR . UCLA . edu/stat/r/examples/asa/hmohiv . CSV](https://stats.idre.ucla.edu/stat/r/examples/asa/hmohiv.csv)

我们使用的数据集是大卫·霍斯默和斯坦利·莱梅肖(1999)在《应用生存分析:时间到事件数据的回归建模》一书中的实际数据。数据来自健康维护组织-健康维护组织+研究，数据包含以下字段:

![](../images/00126.jpeg)

2.  在 IntelliJ 或您选择的 IDE 中启动一个新项目。确保包含必要的 JAR 文件。
3.  设置程序将驻留的包位置:

```scala
package spark.ml.cookbook.chapter5
```

4.  为`SparkSession`导入必要的包以访问集群，为`Log4j.Logger`导入必要的包以减少 Spark 产生的输出量:

```scala
import org.apache.log4j.{Level, Logger}import org.apache.spark.ml.linalg.Vectorsimport org.apache.spark.ml.regression.AFTSurvivalRegressionimport org.apache.spark.sql.SparkSession
```

5.  将输出级别设置为`ERROR`以减少 Spark 的日志输出:

```scala
Logger.getLogger("org").setLevel(Level.ERROR)Logger.getLogger("akka").setLevel(Level.ERROR)
```

6.  使用构建器模式初始化指定配置的`SparkSession`，从而为火花簇提供一个入口点:

```scala
val spark = SparkSession.builder.master("local[4]").appName("myAFTSurvivalRegression").config("spark.sql.warehouse.dir", ".").getOrCreate()
```

7.  然后我们读入`csv`文件，跳过第一行(表头)。

注意:有多种方法可以将`csv`文件读入火花数据帧:

```scala
val file = spark.sparkContext.textFile("../data/sparkml2/chapter5/hmohiv.csv")val headerAndData = file.map(line => line.split(",").map(_.trim))val header = headerAndData.firstval rawData = headerAndData.filter(_(0) != header(0))
```

8.  我们将字段从字符串转换为双精度。我们只对身份、时间、年龄和审查领域感兴趣。这四个字段形成一个数据帧:

```scala
val df = spark.createDataFrame(rawData.map { line =>val id = line(0).toDoubleval time =line(1).toDoubleval age = line(2).toDoubleval censor = line(4).toDouble(id, censor,Vectors.dense(time,age))}).toDF("label", "censor", "features")
```

新的`features`场是由`time`场和`age`场组成的向量。

9.  接下来，我们在控制台中显示数据框:

```scala
df.show()
```

从控制台，这是输出:

![](../images/00127.jpeg)

10.  现在我们将创建`AFTSurvivalRegression`对象，并设置参数。

对于这个特定的配方，分位数概率被设置为 0.3 和 0.6。数值描绘了分位数的边界，分位数是数值在 *0.0* 到 *1.0* [ *0.0，1.0* 范围内的概率的数值向量。例如，对分位数概率向量使用(0.25，0.5，0.75)是一个常见的主题。

分位数列名设置为*分位数*。

在下面的代码中，我们创建了`AFTSurvivalRegression()`对象并设置了列名和分位数概率向量。

下面来自 GitHub 上 Spark 源代码的代码显示了默认值:

```scala
 @Since("1.6.0")def getQuantileProbabilities: Array[Double] = $(quantileProbabilities)setDefault(quantileProbabilities -> Array(0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99)) 
```

为了理解参数化和播种，*生存回归的 Spark 源代码*可以在[的 GitHub 上参考。](https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/regression/AFTSurvivalRegression.scala)

```scala
val aft = new AFTSurvivalRegression().setQuantileProbabilities(Array(0.3, 0.6)).setQuantilesCol("quantiles")
```

11.  我们让模型运行:

```scala
val aftmodel = aft.fit(df)
```

12.  我们将模型数据打印到控制台中:

```scala
println(s"Coefficients: ${aftmodel.coefficients} ")println(s"Intercept: ${aftmodel.intercept}" )println(s"Scale: ${aftmodel.scale}")
```

控制台中将显示以下输出:

```scala
Coefficients: [6.601321816135838E-4,-0.02053601452465816]Intercept: 4.887746420937845Scale: 0.572288831706005
```

13.  我们使用前面的模型来转换数据集，并在控制台中显示结果:

```scala
aftmodel.transform(df).show(false)
```

控制台中将显示以下输出:

![](../images/00128.gif)

14.  我们通过停止`SparkSession`来关闭程序:

```scala
spark.stop()
```

# 它是如何工作的...

我们探索了**加速故障时间** ( **AFT** )模型的特点。我们首先使用`sparkContext.textFile()`将数据集文件读入 Spark。读取`csv`格式文件有多种方式。我们只是选了一个展示更详细步骤的。

接下来，我们过滤掉头行，将感兴趣的字段从字符串转换为 double，然后将 double 数据集转换为带有新`features`字段的新 DataFrame。

然后我们创建`AFTSurvivalRegression`对象并设置分位数参数，通过调用`fit(data)`函数让模型自己运行。

最后，我们显示了模型摘要，并使用该模型转换数据集，显示了包含预测和分位数字段的结果数据帧。

# 还有更多...

生存回归的火花实现(`AFTSurvivalRegression` ) *:*

*   **型号**:加速失效时间(AFT)。
*   **参数:**采用威布尔分布。
*   **优化:** Spark 选择 AFT 是因为更容易并行化，将问题视为凸优化问题，选择 L-BFGS 作为优化方法。
*   **R/SparkR 用户:**在对具有常数非零列的数据集进行无截距拟合`AFTSurvivalRegressionModel`时，Spark MLlib 输出常数非零列的零系数。这种行为与 R `survival::survreg`不同。(来自 Spark 2.0.2 文档)

你应该把结果想成是一个利益事件发生之前的时间，比如一个疾病的发生、输赢、房贷违约的时间、结婚、离婚、毕业后找工作等等。这些模型的独特之处在于*时间事件*是一个持续时间，不一定有解释变量(即只是以天、月或年为单位的持续时间)。

您可能使用生存模型而不是简单回归(即诱人)的原因如下:

*   需要将结果变量建模为时间事件
*   审查-不是所有的数据都是已知的或使用过的(在使用过去几个世纪的长期商品数据时很常见)
*   非正态分布的结果——通常是时间的情况
*   这可能是也可能不是多元分析的案例

虽然这里概述了生存回归的两种方法，但在撰写本文时，Spark 2.0 仅支持 AFT 模型，而不支持最受关注的版本，即比例风险模型:

*   **比例危险模型** ( **PH** ):
    *   随着时间推移假设的比例
    *   在考虑时间内将常数乘以协方差
    *   例子:考克斯比例风险模型
    *   **hx(y) = h0(y)*g(X)**
*   加速时间故障(ATF) **-** 火花 2.0 实施:
    *   比例假设可以假设，也可以违反
    *   常数值乘以协方差得到的回归系数值可以是:
        *   加速
        *   加速
    *   考虑到回归展开的阶段:
        *   疾病阶段
        *   生存阶段
    *   *Yx * g(X) = Y0*

*Sx(y) = S0(yg(X))*

*在哪里，*

*Y* :存活时间，

*X* :协变量向量，

*hx(y)* :危险功能，

*Sx(Y)*:*Y*给 *X* 的生存函数，

*Yx* : *Y* 给定 *X*

*   参数建模-时间变量的基本分布；
    *   指数的
    *   威布尔-火花 2.0 实现
    *   对数逻辑
    *   常态
    *   微克
*   另请参阅-在 R 中非常流行-我们使用了这两个包:
    *   图书馆(生存):标准生存分析
    *   库(eha):用于 AFT 建模

`SurvivalRegression`的文档可在以下网址获得:

*   [http://spark . Apache . org/docs/latest/API/Scala/index . html # org . Apache . spark . ml . rejection . aftservarilgressionmodel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.AFTSurvivalRegressionModel)
*   [https://spark . Apache . org/docs/latest/ml-分类-回归. html #生存-回归](https://spark.apache.org/docs/latest/ml-classification-regression.html#survival-regression)

`HMOHIV`数据集的原始格式可以在- connect as guest 中找到:

[ftp://ftp.wiley.com/public/sci_tech_med/survival](ftp://ftp.wiley.com/public/sci_tech_med/survival)

比例与后效(火花 2.0)危险模型的深入而完整的比较可以在以下网址找到:

[https://eco mons . usask . ca/bit stream/handle/10388/etd-03302009-140638/jiezhightiscust . pdf](https://ecommons.usask.ca/bitstream/handle/10388/etd-03302009-140638/JiezhiQiThesis.pdf)

带图表的端到端真实医学研究:

[https://www . research gate . net/profile/Richard _ Kay 2/publication/254087561 _ On _ Use _ the _ Accelerated _ Failure _ Time _ Model _ as _ Alternative _ the _ Proportional _ Hazards _ Model _ in _ the _ Treatment _ Time _ to _ Event _ Data _ A _ Case _ Study _ in _ 流感/links/548 ed 67 e0cf 225 BF 66 A 710 . pdf](https://www.researchgate.net/profile/Richard_Kay2/publication/254087561_On_the_Use_of_the_Accelerated_Failure_Time_Model_as_an_Alternative_to_the_Proportional_Hazards_Model_in_the_Treatment_of_Time_to_Event_Data_A_Case_Study_in_Influenza/links/548ed67e0cf225bf66a710ce.pdf)

# 请参见

*   AFT 生存实施文件:
    *   [https://spark . Apache . org/docs/latest/API/Scala/index . html # org . Apache . spark . ml . feature . minmax scaler](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.MinMaxScaler)
    *   [https://spark . Apache . org/docs/latest/API/Scala/index . html # org . Apache . spark . ml . rejection . aftserviRegression](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.AFTSurvivalRegression)
    *   [https://spark . Apache . org/docs/latest/API/Scala/index . html # org . Apache . spark . ml . rejection . aftservarilgressionmodel](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.AFTSurvivalRegressionModel)
*   生存回归的 Spark 源代码可以在 GitHub 上参考:[https://GitHub . com/Apache/spark/blob/master/mllib/src/main/Scala/org/Apache/spark/ml/revolution/aftservirercression . Scala](https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/regression/AFTSurvivalRegression.scala)