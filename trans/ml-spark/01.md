# 启动并运行 Spark

**Apache Spark**是一个用于分布式计算的框架；该框架旨在使编写跨计算机或虚拟机集群中的许多节点并行运行的程序变得更简单。 它试图抽象资源调度、作业提交、执行、跟踪和节点之间的通信任务，以及并行数据处理中固有的低级操作。 它还提供了更高级别的 API 来处理分布式数据。 在这种情况下，它类似于 Apache Hadoop 等其他分布式处理框架；但是，底层架构略有不同。

Spark 最初是加州大学伯克利分校 AMP 实验室的一个研究项目([https://amplab.cs.berkeley.edu/projects/spark-lightning-fast-cluster-computing/](https://amplab.cs.berkeley.edu/projects/spark-lightning-fast-cluster-computing/))。 该大学专注于分布式机器学习算法的用例。 因此，它完全是为迭代性质的应用程序的高性能而设计的，在迭代性质的应用程序中，相同的数据被多次访问。 这种性能主要是通过在内存中缓存数据集以及启动并行计算任务的低延迟和低开销来实现的。 与容错、灵活的分布式内存数据结构和强大的功能 API 等其他功能一起，Spark 已被证明广泛适用于除机器学习和迭代分析之外的各种大规模数据处理任务。

有关更多信息，请访问：

*   [http：//spark.apache.org/community.html](http://spark.apache.org/community.html)
*   [http：//spark.apache.org/community.html#history](http://spark.apache.org/community.html#history)

在性能方面，Spark 在相关工作负载方面比 Hadoop 快得多。 请参阅下图：

![](images/image_01_001.png)

Source: https://amplab.cs.berkeley.edu/wp-content/uploads/2011/11/spark-lr.png

Spark 以四种模式运行：

*   独立本地模式，其中所有 Spark 进程都在相同的**Java 虚拟机**(**JVM**)进程中运行
*   独立集群模式，使用 Spark 自己的内置作业调度框架
*   使用**Mesos**，一个流行的开源集群计算框架
*   使用纱线(通常称为 NextGen MapReduce)，Hadoop

在本章中，我们将执行以下操作：

*   下载 Spark 二进制文件并设置一个在 Spark 的独立本地模式下运行的开发环境。 本书中将使用此环境来运行示例代码。
*   使用 Spark 的交互控制台探索 Spark 的编程模型和 API。
*   用 Scala、Java、R 和 Python 编写我们的第一个 Spark 程序。
*   使用 Amazon 的**Elastic Cloud Compute**(**EC2**)平台设置 Spark 集群，该平台可用于大数据和更重的计算需求，而不是以本地模式运行。
*   使用 Amazon Elastic Map Reduce 设置星火集群

如果您以前有过设置 Spark 的经验，并且熟悉编写 Spark 程序的基础知识，请随意跳过本章。

# 在本地安装和设置 Spark

Spark 可以在本地模式下使用内置的独立集群调度程序运行。 这意味着所有 Spark 进程都在同一个 JVM 中运行--实际上就是 Spark 的一个多线程实例。 本地模式非常适用于原型制作、开发、调试和测试。 但是，此模式在实际方案中也很有用，可以跨单台计算机上的多个核心执行并行计算。

因为 Spark 的本地模式与集群模式完全兼容；只需几个额外步骤，本地编写和测试的程序就可以在集群上运行。

在本地设置 Spark 的第一步是下载最新版本[http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html)，其中包含下载 Spark 各种版本以及通过 giHub 获取最新源代码的链接。

The documents/docs available at [http://spark.apache.org/docs/latest/](http://spark.apache.org/docs/latest/) are a comprehensive resource to learn more about Spark. We highly recommend that you explore it!

Spark 需要针对特定版本的 Hadoop 构建，以便访问**Hadoop 分布式文件系统**(**HDFS**)以及标准和自定义 Hadoop 输入源 Cloudera 的 Hadoop 分发版、MapR 的 Hadoop 分发版和 Hadoop 2(纱线)。 除非您希望针对特定的 Hadoop 版本构建 Spark，否则我们建议您从[http://d3kbcqa49mib13.cloudfront.net/spark-2.0.2-bin-hadoop2.7.tgz](http://d3kbcqa49mib13.cloudfront.net/spark-2.0.2-bin-hadoop2.7.tgz)的 Apache 镜像下载预构建的 Hadoop2.7 包。

Spark 需要 Scala 编程语言(撰写本书时的版本 2.10.x 或 2.11.x)才能运行。 幸运的是，预构建的二进制软件包附带了 Scala 运行时软件包，因此您不需要单独安装 Scala 就可以开始使用。 但是，您需要有**Java Runtime Environment**(**JRE**)或**Java Development Kit**(**JDK**)。

Refer to the software and hardware list in this book's code bundle for installation instructions. R 3.1+ is needed.

下载 Spark 二进制软件包后，通过运行以下命令将该软件包的内容解压并将其更改为新创建的目录：

```
 $ tar xfvz spark-2.0.0-bin-hadoop2.7.tgz
 $ cd spark-2.0.0-bin-hadoop2.7

```

Spark 将运行 Spark 的用户脚本放在`bin`目录中。 您可以通过运行 Spark 中包含的示例程序之一来测试一切是否正常工作。 运行以下命令：

```
 $ bin/run-example SparkPi 100

```

这将在 Spark 的本地独立模式下运行该示例。 在这种模式下，所有 Spark 进程都在同一个 JVM 中运行，Spark 使用多个线程进行并行处理。 默认情况下，上面的示例使用的线程数量等于系统上可用的核心数量。 执行该程序后，您应该会在输出末尾看到类似以下行的内容：

```
...
16/11/24 14:41:58 INFO Executor: Finished task 99.0 in stage 0.0 
    (TID 99). 872 bytes result sent to driver
16/11/24 14:41:58 INFO TaskSetManager: Finished task 99.0 in stage 
    0.0 (TID 99) in 59 ms on localhost (100/100)
16/11/24 14:41:58 INFO DAGScheduler: ResultStage 0 (reduce at 
    SparkPi.scala:38) finished in 1.988 s
16/11/24 14:41:58 INFO TaskSchedulerImpl: Removed TaskSet 0.0, 
    whose tasks have all completed, from pool 
16/11/24 14:41:58 INFO DAGScheduler: Job 0 finished: reduce at 
    SparkPi.scala:38, took 2.235920 s
Pi is roughly 3.1409527140952713

```

前面的命令调用 class`org.apache.spark.examples.SparkPi`类。

该类接受`local[N]`形式的参数，其中`N`是要使用的线程数。 例如，要只使用两个线程，请运行以下命令`instead:N`是要使用的线程数。 给定`local[*]`将使用本地计算机上的所有内核--这是一种常见用法。

要仅使用两个线程，请改为运行以下命令：

```
 $ ./bin/spark-submit  --class org.apache.spark.examples.SparkPi 
 --master local[2] ./examples/jars/spark-examples_2.11-2.0.0.jar 100 

```

# 星火团

Spark 集群由两种类型的进程组成：一个驱动程序和多个执行器。 在本地模式下，所有这些进程都在同一个 JVM 中运行。 在集群中，这些进程通常在不同的节点上运行。

例如，在 Spark 的独立模式下运行的典型群集(即，使用 Spark 的内置群集管理模块)将具有以下内容：

*   运行 Spark 独立主进程和驱动程序的主节点
*   多个工作节点，每个工作节点运行一个执行器进程

虽然我们将在本书中使用 Spark 的本地独立模式来说明概念和示例，但我们编写的相同 Spark 代码可以在 Spark 集群上运行。 在前面的示例中，如果我们在 Spark 独立群集上运行代码，我们只需传入主节点的 URL，如下所示：

```
 $ MASTER=spark://IP:PORT --class org.apache.spark.examples.SparkPi 
 ./examples/jars/spark-examples_2.11-2.0.0.jar 100

```

这里，`IP`是 Spark Master 的 IP 地址，`PORT`是端口。 这告诉 Spark 在运行 Spark 主进程的集群上运行该程序。

全面介绍 Spark 的集群管理和部署超出了本书的范围。 不过，我们将在本章后面简要介绍如何设置和使用 Amazon EC2 群集。

有关 Spark 集群应用程序部署的概述，请查看以下链接：

*   [http：//spark.apache.org/docs/latest/cluster-overview.html](http://spark.apache.org/docs/latest/cluster-overview.html)

*   [http：//spark.apache.org/docs/latest/submitting-applications.html](http://spark.apache.org/docs/latest/submitting-applications.html)

# Spark 编程模型

在我们深入研究 Spark 设计的高级概述之前，我们将介绍`SparkContext`对象和 Spark shell，我们将使用它们交互地探索 Spark 编程模型的基础知识。

虽然本节提供了使用 Spark 的简要概述和示例，但我们建议您阅读以下文档以获得详细了解：

请参阅以下 URL：

*   有关火花快速入门的信息，请参阅[http://spark.apache.org/docs/latest/quick-start](http://spark.apache.org/docs/latest/quick-start)
*   有关 Spark 编程指南(涵盖 scala、JAVA、Python 和 R--)，请参阅[http://spark.apache.org/docs/latest/programming-guide.html](http://spark.apache.org/docs/latest/programming-guide.html)

# SparkContext 和 SparkConf

编写任何 Spark 程序的起点都是`SparkContext`(或者 Java 中的`JavaSparkContext`)。 `SparkContext`使用`SparkConf`对象的实例进行初始化，该实例包含各种 Spark 集群配置设置(例如，主节点的 URL)。

它是 Spark 功能的主要入口点。 `SparkContext`是到星火星团的连接。 它可用于在群集上创建 RDDS、累加器和广播变量。

每个 JVM 只有一个`SparkContext`处于活动状态。 在创建新的`SparkContext`之前，您必须调用`stop()`，它是活动的`SparkContext`。

初始化后，我们将使用`SparkContext`对象中的各种方法来创建和操作分布式数据集和共享变量。 Spark shell(在 Scala 和 Python 中都有，遗憾的是 Java 不支持)为我们处理上下文初始化，但是下面几行代码显示了在 Scala 中创建在本地模式下运行的上下文的示例：

```
val conf = new SparkConf() 
.setAppName("Test Spark App") 
.setMaster("local[4]") 
val sc = new SparkContext(conf)

```

这将创建一个在本地模式下运行的上下文，其中有四个线程，应用程序的名称设置为`Test Spark App`。 如果我们希望使用默认配置值，还可以为我们的`SparkContext`对象调用以下简单的构造函数，它的工作方式与此完全相同：

```
val sc = new SparkContext("local[4]", "Test Spark App")

```

Downloading the example code
You can download the example code files for all Packt books you have purchased from your account at [http://www.packtpub.com](http://www.packtpub.com). If you purchased this book from any other source, you can visit[http://www.packtpub.com/support](http://www.packtpub.com/support) and register to have the files e-mailed directly to you.

# SparkSession

`SparkSession`允许使用`DataFrame`和数据集 API 进行编程。 它是这些 API 的单一入口点。

首先，我们需要创建`SparkConf`类的一个实例，并使用它创建`SparkSession`实例。 请考虑以下示例：

```
val spConfig = (new SparkConf).setMaster("local").setAppName("SparkApp")
 val spark = SparkSession
   .builder()
   .appName("SparkUserData").config(spConfig)
   .getOrCreate()

```

接下来，我们可以使用 Spark 对象创建`DataFrame`：

```
val user_df = spark.read.format("com.databricks.spark.csv")
   .option("delimiter", "|").schema(customSchema)
   .load("/home/ubuntu/work/ml-resources/spark-ml/data/ml-100k/u.user")
val first = user_df.first()

```

# 火花壳

Spark 支持使用 Scala、Python 或 R**REPL**(即**Read-Eval-Print-Loop**或交互式 shell)以交互方式编写程序。 当我们输入代码时，外壳会提供即时反馈，因为该代码会被立即评估。 在 Scala shell 中，运行一段代码后还会显示返回结果和类型。

要在 Scala 中使用 Spark shell，只需从 Spark 基本目录运行`./bin/spark-shell`即可。 这将启动 Scala shell 并初始化`SparkContext`，我们可以将其作为 Scala 值`sc`使用。 在 Spark 2.0 中，控制台中还提供了 Spark 变量形式的`SparkSession`实例。

您的控制台输出应与以下内容类似：

```
$ ~/work/spark-2.0.0-bin-hadoop2.7/bin/spark-shell 
Using Spark's default log4j profile: org/apache/spark/log4j-
    defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel).
16/08/06 22:14:25 WARN NativeCodeLoader: Unable to load native-
    hadoop library for your platform... using builtin-java classes 
    where applicable
16/08/06 22:14:25 WARN Utils: Your hostname, ubuntu resolves to a 
    loopback address: 127.0.1.1; using 192.168.22.180 instead (on 
    interface eth1)
16/08/06 22:14:25 WARN Utils: Set SPARK_LOCAL_IP if you need to 
    bind to another address
16/08/06 22:14:26 WARN Utils: Service 'SparkUI' could not bind on 
    port 4040\. Attempting port 4041.
16/08/06 22:14:27 WARN SparkContext: Use an existing SparkContext, 
    some configuration may not take effect.
Spark context Web UI available at http://192.168.22.180:4041
Spark context available as 'sc' (master = local[*], app id = local-
    1470546866779).
Spark session available as 'spark'.
Welcome to
 ____              __
 / __/__  ___ _____/ /__
 _ / _ / ______/ __/  '_/
 /___/ .__/_,_/_/ /_/_   version 2.0.0
 /_/

Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, 
    Java 1.7.0_60)
Type in expressions to have them evaluated.
Type :help for more information.

scala> 

```

要将 Python shell 与 Spark 一起使用，只需运行`./bin/pyspark`命令。 与 Scala shell 一样，Python`SparkContext`对象应该可以作为 Python 变量`sc`使用。 您的输出应该与以下内容类似：

```
~/work/spark-2.0.0-bin-hadoop2.7/bin/pyspark 
Python 2.7.6 (default, Jun 22 2015, 17:58:13) 
[GCC 4.8.2] on linux2
Type "help", "copyright", "credits" or "license" for more 
    information.
Using Spark's default log4j profile: org/apache/spark/log4j-
    defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel).
16/08/06 22:16:15 WARN NativeCodeLoader: Unable to load native-
    hadoop library for your platform... using builtin-java classes 
    where applicable
16/08/06 22:16:15 WARN Utils: Your hostname, ubuntu resolves to a 
    loopback address: 127.0.1.1; using 192.168.22.180 instead (on 
    interface eth1)
16/08/06 22:16:15 WARN Utils: Set SPARK_LOCAL_IP if you need to 
    bind to another address
16/08/06 22:16:16 WARN Utils: Service 'SparkUI' could not bind on 
    port 4040\. Attempting port 4041.
Welcome to
 ____              __
 / __/__  ___ _____/ /__
 _ / _ / ______/ __/  '_/
 /__ / .__/_,_/_/ /_/_   version 2.0.0
 /_/

Using Python version 2.7.6 (default, Jun 22 2015 17:58:13)
SparkSession available as 'spark'.
>>> 

```

**R**是一种语言，具有用于统计计算和图形的运行时环境。 这是一个 GNU 项目。 R 是**S**(贝尔实验室开发的一种语言)的不同实现。

R 提供统计(线性和非线性建模、经典统计检验、时间序列分析、分类和聚类)和图形技术。 它被认为是高度可扩展的。

要使用 R 来使用 Spark，请运行以下命令打开 Spark-R shell：

```
$ ~/work/spark-2.0.0-bin-hadoop2.7/bin/sparkR
R version 3.0.2 (2013-09-25) -- "Frisbee Sailing"
Copyright (C) 2013 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

 Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

Launching java with spark-submit command /home/ubuntu/work/spark- 
    2.0.0-bin-hadoop2.7/bin/spark-submit   "sparkr-shell" 
    /tmp/RtmppzWD8S/backend_porta6366144af4f 
Using Spark's default log4j profile: org/apache/spark/log4j-
    defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel).
16/08/06 22:26:22 WARN NativeCodeLoader: Unable to load native-
    hadoop library for your platform... using builtin-java classes 
    where applicable
16/08/06 22:26:22 WARN Utils: Your hostname, ubuntu resolves to a 
    loopback address: 127.0.1.1; using 192.168.22.186 instead (on 
    interface eth1)
16/08/06 22:26:22 WARN Utils: Set SPARK_LOCAL_IP if you need to 
    bind to another address
16/08/06 22:26:22 WARN Utils: Service 'SparkUI' could not bind on 
    port 4040\. Attempting port 4041.

 Welcome to
 ____              __ 
 / __/__  ___ _____/ /__ 
 _ / _ / _ ____/ __/  '_/ 
 /___/ .__/_,_/_/ /_/_   version  2.0.0 
 /_/ 
 SparkSession available as 'spark'.
During startup - Warning message:
package 'SparkR' was built under R version 3.1.1 
> 

```

# 弹性分布式数据集

Spark 的核心是一个概念，称为**弹性分布式数据集**(**RDD**)。 RDD 是跨集群中的多个节点分布或分区的多个*记录*(严格地说，是某种类型的对象)的集合(对于 Spark 本地模式，可以用同样的方式将单个多线程进程看作是单个多线程进程)。 Spark 中的 RDD 是容错的；这意味着如果给定节点或任务失败(由于除错误用户代码以外的某些原因，如硬件故障、通信中断等)，RDD 可以在其余节点上自动重建，并且作业仍将完成。

# 正在创建 RDDS

RDDS 可以是您之前启动的 Scala Spark shell：

```
val collection = List("a", "b", "c", "d", "e") 
val rddFromCollection = sc.parallelize(collection)

```

还可以从基于 Hadoop 的输入源(包括本地文件系统、HDFS 和 Amazon S3)创建 RDDS。 基于 Hadoop 的 RDD 可以利用实现 Hadoop`InputFormat`接口的任何输入格式，包括文本文件、其他标准 Hadoop 格式、HBase、Cassandra、tachyon 等等。

以下代码是从本地文件系统上的文本文件创建 RDD 的示例：

```
val rddFromTextFile = sc.textFile("LICENSE")

```

前面的`textFile`方法返回一个 RDD，其中每条记录都是一个表示文本文件一行的`String`对象。 前面命令的输出如下所示：

```
rddFromTextFile: org.apache.spark.rdd.RDD[String] = LICENSE   
MapPartitionsRDD[1] at textFile at <console>:24

```

以下代码是如何使用`hdfs://`协议从 HDFS 上的文本文件创建 RDD 的示例：

```
val rddFromTextFileHDFS = sc.textFile("hdfs://input/LICENSE ")

```

以下代码是如何使用`s3n://`协议从 Amazon S3 上的文本文件创建 RDD 的示例：

```
val rddFromTextFileS3 = sc.textFile("s3n://input/LICENSE ")

```

# 火花作业

一旦我们创建了 RDD，我们就有了可以操作的分布式记录集合。 在 Spark 的编程模型中，操作分为转换和操作。 一般而言，转换操作会对数据集中的所有记录应用某些函数，并以某种方式更改记录。 操作通常运行一些计算或聚合操作，并将结果返回到运行`SparkContext`的驱动程序。

火花操作在风格上是功能性的。 对于熟悉 Java8 中 Scala、Python 或 Lambda 表达式的函数式编程的程序员来说，这些操作应该很自然。 对于那些没有函数式编程经验的人来说，不用担心；Spark API 相对容易学习。

您将在 Spark 程序中使用的最常见的转换之一是映射操作符。 这将一个函数应用于 RDD 的每个记录，因此*将输入*映射到一些新输出。 例如，下面的代码片段采用我们从本地文本文件创建的 RDD，并将`size`函数应用于 RDD 中的每条记录。 请记住，我们创建了字符串的 RDD。 使用`map`，我们可以将每个字符串转换为整数，从而返回`Ints`的 RDD：

```
val intsFromStringsRDD = rddFromTextFile.map(line => line.size)

```

您应该在 shell 中看到类似以下行的输出；这表示 RDD 的类型：

```
intsFromStringsRDD: org.apache.spark.rdd.RDD[Int] = 
MapPartitionsRDD[2] at map at <console>:26

```

在前面的代码中，我们看到了`=>`语法的使用。 这是匿名函数的 Scala 语法，该函数不是命名方法(例如，在 Scala 或 Python 中使用`def`关键字定义的函数)。

While a detailed treatment of anonymous functions is beyond the scope of this book, they are used extensively in Spark code in Scala and Python, as well as in Java 8 (both in examples and real-world applications), so it is useful to cover a few practicalities.
The line `=> line.size` syntax means that we are applying a function where `=>` is the operator, and the output is the result of the code to the right of the `=>` operator. In this case, the input is line, and the output is the result of calling `line.size`. In Scala, this function that maps a string to an integer is expressed as `String => Int`.
This syntax saves us from having to separately define functions every time we use methods such as map; this is useful when the function is simple and will only be used once, as in this example.

现在，我们可以应用一个通用操作 COUNT 来返回 RDD 中的记录数：

```
intsFromStringsRDD.count

```

结果应该类似于以下控制台输出：

```
res0: Long = 299

```

也许我们想要找出该文本文件中每行的平均长度。 我们可以首先使用`sum`函数将所有记录的长度相加，然后将总和除以记录数：

```
val sumOfRecords = intsFromStringsRDD.sum 
val numRecords = intsFromStringsRDD.count 
val aveLengthOfRecord = sumOfRecords / numRecords

```

结果如下：

```
scala> intsFromStringsRDD.count
res0: Long = 299

scala> val sumOfRecords = intsFromStringsRDD.sum
sumOfRecords: Double = 17512.0

scala> val numRecords = intsFromStringsRDD.count
numRecords: Long = 299

scala> val aveLengthOfRecord = sumOfRecords / numRecords
aveLengthOfRecord: Double = 58.5685618729097

```

大多数情况下，Spark 操作都会返回一个新的 RDD，但大多数操作除外，它们返回计算结果(例如，在前面的示例中，`Long`表示 Count，`Double`表示 SUM)。 这意味着我们可以自然地将操作链接在一起，使我们的程序流程更简洁、更具表现力。 例如，使用以下代码可以获得与前一行代码相同的结果：

```
val aveLengthOfRecordChained = rddFromTextFile.map(line => line.size).sum / rddFromTextFile.count

```

需要注意的重要一点是，Spark 转换是懒惰的。 也就是说，在 RDD 上调用转换不会立即触发计算。 相反，转换被链接在一起，并且只有在调用操作时才会有效地进行计算。 这使得 Spark 可以更高效，只在必要时将结果返回给驱动程序，以便大多数操作在群集上并行执行。

这意味着如果您的 Spark 程序从不使用操作操作，它将永远不会触发实际的计算，并且您将不会得到任何结果。 例如，下面的代码将简单地返回一个表示转换链的新 RDD：

```
val transformedRDD = rddFromTextFile.map(line => line.size).filter(size => size > 10).map(size => size * 2)

```

这将在控制台中返回以下结果：

```
transformedRDD: org.apache.spark.rdd.RDD[Int] = 
MapPartitionsRDD[6] at map at <console>:26

```

请注意，不会进行实际计算，也不会返回结果。 如果我们现在对生成的 RDD 调用一个操作(如 SUM)，则计算将被触发：

```
val computation = transformedRDD.sum

```

现在，您将看到 Spark 作业正在运行，并产生以下控制台输出：

```
computation: Double = 35006.0

```

The complete list of transformations and actions possible on RDDs, as well as a set of more detailed examples, are available in the Spark programming guide (located at [http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations](http://spark.apache.org/docs/latest/programming-guide.html)), and the API documentation (the Scala API documentation) is located at ([http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD)).

# 缓存 RDDS

Spark 最强大的功能之一是跨集群在内存中缓存数据的能力。 这是通过在 RDD 上使用缓存方法实现的：

```
rddFromTextFile.cache
res0: rddFromTextFile.type = MapPartitionsRDD[1] at textFile at 
<console>:27

```

在 RDD 上调用`cache`会告诉 Spark 应该将 RDD 保存在内存中。 第一次在启动计算的 RDD 上调用操作时，将从其源读取数据并将其放入内存。 因此，第一次调用此类操作时，运行任务所需的时间部分取决于从输入源读取数据所需的时间。 但是，当下次访问数据时(例如，在分析中的后续查询或机器学习模型中的迭代)，可以直接从内存中读取数据，从而避免昂贵的 I/O 操作，并在许多情况下显著加快计算速度。

如果我们现在对缓存的 RDD 调用`count`或`sum`函数，则 RDD 将加载到内存中：

```
val aveLengthOfRecordChained = rddFromTextFile.map(line => 
line.size).sum / rddFromTextFile.count

```

Spark also allows more fine-grained control over caching behavior. You can use the persist method to specify what approach Spark uses to cache data. More information on RDD caching can be found here:
[http://spark.apache.org/docs/latest/programmingguide.html#rdd-persistence](http://spark.apache.org/docs/latest/programmingguide.html#rdd-persistence)

# 广播变量和累加器

Spark 的另一个核心特性是能够创建两种特殊类型的变量--广播变量和累加器。

**广播变量**是从驱动程序对象创建的*只读*变量，可用于将执行计算的节点。 这在需要以高效方式向工作节点提供相同数据的应用程序(如分布式系统)中非常有用。 Spark 使创建广播变量与调用`SparkContext`上的方法一样简单，如下所示：

```
val broadcastAList = sc.broadcast(List("a", "b", "c", "d", "e"))

```

通过对变量调用`value`，可以从创建广播变量的驱动程序以外的节点(即工作者节点)访问该变量：

```
sc.parallelize(List("1", "2", "3")).map(x => broadcastAList.value ++  
  x).collect

```

此代码创建一个新的 RDD，其中包含来自`("1", "2", "3")`的集合(在本例中为 Scala`List`)的三条记录。 在 map 函数中，它返回一个新集合，并将新 RDD 的相关 rom 附加到我们的广播变量`broadcastAList`：

```
...
res1: Array[List[Any]] = Array(List(a, b, c, d, e, 1), List(a, b, 
c, d, e, 2), List(a, b, c, d, e, 3))

```

请注意前面代码中的`collect`方法。 这是一个 Spark*操作*，它将整个 RDD 作为 Scala(或 Python 或 Java)集合返回给驱动程序。

当我们希望在驱动程序中对本地结果进行进一步处理时，我们通常会使用。

Note that `collect` should generally only be used in cases where we really want to return the full result set to the driver and perform further processing. If we try to call `collect` on a very large dataset, we might run out of memory on the driver and crash our program.
It is preferable to perform as much heavy-duty processing on our Spark cluster as possible, preventing the driver from becoming a bottleneck. In many cases, however, such as during iterations in many machine learning models, collecting results to the driver is necessary.

在检查结果时，我们将看到，对于我们的新 RDD 中的三个记录，我们现在都有一个记录，即我们最初广播的记录`List`，并附加了新元素(即，现在末尾有`"1"`、`"2"`或`"3"`)：

**累加器**也是广播给工作节点的变量。 广播变量和累加器之间的关键区别在于，虽然`broadcast`变量是只读的，但累加器可以添加到。 这是有限制的，即，特别是，加法必须是关联运算，以便能够正确地并行计算全局累加值并将其返回给驱动程序。 每个 Worker 节点只能访问自己的本地累加器值并将其相加，并且只有驱动程序才能访问全局值。 也可以使用 Value 方法在 Spark 代码中访问累加器。

For more details on broadcast variables and accumulators, refer to the *Shared Variables* section of the *Spark Programming Guide* at [http://spark.apache.org/docs/latest/programming-guide.html#shared-variables](http://spark.apache.org/docs/latest/programming-guide.html#shared-variables).

# SchemaRDD

**SchemaRDD**是 RDD 和模式信息的组合。 它还提供了许多丰富且易于使用的 API(即`DataSet`API)。 SchemaRDD 不用于 2.0，由`DataFrame`和`Dataset`API 在内部使用。

架构用于描述结构化数据的逻辑组织方式。 在获取模式信息后，SQL 引擎能够为相应的数据提供结构化查询能力。 `DataSet`API 替代了 Spark SQL 解析器的功能。 它是一个实现原始程序逻辑树的 API。 后续处理步骤重用 Spark SQL 的核心逻辑。 我们可以放心地认为`DataSet`API 的处理功能完全等同于 SQL 查询。

SchemaRDD 是一个 RDD 子类。 当程序调用`DataSet`API 时，会创建一个新的 SchemaRDD 对象，并通过在原逻辑规划树上添加新的逻辑运算节点来创建`new`对象的逻辑规划属性。 `DataSet`API(如 RDD)的操作有两种类型--**转换**和**操作**。

与关系操作相关的 API 被归因于转换类型。

与数据输出源关联的操作属于 Action 类型。 与 RDD 类似，只有在调用 Action 类型的操作时，才会触发并交付 Spark 作业以供集群执行。

# 火花数据帧

在 Apache Spark 中，`Dataset`是分布式数据集合。 `Dataset`是从 Spark 1.6 开始添加的新接口。 它提供了 RDDS 的好处和 Spark SQL 执行引擎的好处。 可以从 JVM 对象构造`Dataset`，然后使用函数转换(`map`、`flatMap`、`filter`等)对其进行操作。 `Dataset`API 仅适用于 Scala 和 Java。 它不适用于 Python 或 R。

`DataFrame`是具有命名列的数据集。 它相当于关系数据库中的表或 R/Python 中的数据框，具有更丰富的优化。 `DataFrame`由结构化数据文件、配置单元中的表、外部数据库或现有 RDDS 构成。 `DataFrame`API 在 Scala、Python、Java 和 R 中可用。

Spark`DataFrame`需要首先实例化 Spark 会话：

```
import org.apache.spark.sql.SparkSession 
val spark = SparkSession.builder().appName("Spark SQL").config("spark.some.config.option", "").getOrCreate() 
import spark.implicits._

```

接下来，我们使用`spark.read.json`函数从 Json 文件创建`DataFrame`：

```
scala> val df = spark.read.json("/home/ubuntu/work/ml-resources
  /spark-ml/Chapter_01/data/example_one.json")

```

请注意，Spark`Implicits`用于将 RDD 隐式转换为数据帧类型：

```
org.apache.spark.sql
Class SparkSession.implicits$
Object org.apache.spark.sql.SQLImplicits
Enclosing class: [SparkSession](https://spark.apache.org/docs/2.0.0/api/java/org/apache/spark/sql/SparkSession.html)

```

Scala 中可用于将常见 Scala 对象转换为`DataFrames`的隐式方法。

输出将类似于以下清单：

```
df: org.apache.spark.sql.DataFrame = [address: struct<city: 
string, state: string>, name: string]

```

现在我们想看看它是如何实际加载到`DataFrame`中的：

```
scala> df.show
+-----------------+-------+
|          address|   name|
+-----------------+-------+
|  [Columbus,Ohio]|    Yin|
|[null,California]|Michael|
+-----------------+-------+

```

# Scala 中 Spark 程序的第一步

现在，我们将使用上一节中介绍的思想编写一个基本的 Spark 程序来操作数据集。 我们将从 Scala 开始，然后用 Java 和 Python 编写相同的程序。 我们的计划将基于探索在线商店的一些数据，关于哪些用户购买了哪些产品。 数据包含在名为`UserPurchaseHistory.csv`的**逗号分隔值**(**CSV**)文件中。 该文件预计位于`data`目录中。

内容显示在以下代码片段中。 CSV 的第一列是用户名，第二列是产品名称，最后一列是价格：

```
John,iPhone Cover,9.99
John,Headphones,5.49
Jack,iPhone Cover,9.99
Jill,Samsung Galaxy Cover,8.95
Bob,iPad Cover,5.49

```

对于我们的 Scala 程序，我们需要使用构建工具**Scala 构建工具**(**SBT**)创建两个文件-Scala 代码和项目构建配置文件。 为了便于使用，我们建议您在本章中使用-spark-app。 此代码还包含数据目录下的 CSV 文件。 您需要在系统上安装 SBT 才能运行此示例程序(撰写本书时我们使用的是 0.13.8 版)。

Setting up SBT is beyond the scope of this book; however, you can find more information at [http://www.scala-sbt.org/release/docs/Getting-Started/Setup.html](http://www.scala-sbt.org/release/docs/Getting-Started/Setup.html).

我们的 SBT 配置文件`build.sbt`如下所示(请注意，每行代码之间需要空行)：

```
name := "scala-spark-app" 
version := "1.0" 
scalaVersion := "2.11.7" 
libraryDependencies += "org.apache.spark" %% "spark-core" % "2.0.0"

```

最后一行将对 Spark 的依赖添加到我们的项目中。

我们的 Scala 程序包含在`ScalaApp.scala`文件中。 我们将一块一块地介绍这个节目。 首先，我们需要导入所需的 Spark 类：

```
import org.apache.spark.SparkContext 
import org.apache.spark.SparkContext._ 

/** 
 * A simple Spark app in Scala 
 */ 
object ScalaApp {

```

在我们的 main 方法中，我们需要初始化`SparkContext`对象，并使用它通过`textFile`方法访问 CSV 数据文件。 然后，我们将通过在分隔符字符(本例中为逗号)上拆分字符串并提取用户名、产品和价格的相关记录来映射原始文本：

```
def main(args: Array[String]) { 
  val sc = new SparkContext("local[2]", "First Spark App") 
  // we take the raw data in CSV format and convert it into a 
   set of records of the form (user, product, price) 
  val data = sc.textFile("data/UserPurchaseHistory.csv") 
    .map(line => line.split(",")) 
    .map(purchaseRecord => (purchaseRecord(0), 
     purchaseRecord(1), purchaseRecord(2)))

```

现在我们有了 RDD，其中每条记录都由(`user`，`product`，`price`)组成，我们可以为我们的商店计算各种有趣的指标，如下所示：

*   购买的总数量
*   购买的唯一用户数
*   我们的总收入
*   我们最受欢迎的产品

让我们计算一下前面的指标：

```
// let's count the number of purchases 
val numPurchases = data.count() 
// let's count how many unique users made purchases 
val uniqueUsers = data.map{ case (user, product, price) => user 
}.distinct().count() 
// let's sum up our total revenue 
val totalRevenue = data.map{ case (user, product, price) => 
price.toDouble }.sum() 
// let's find our most popular product 
val productsByPopularity = data 
  .map{ case (user, product, price) => (product, 1) } 
  .reduceByKey(_ + _) 
  .collect() 
  .sortBy(-_._2)     
val mostPopular = productsByPopularity(0)

```

计算最流行产品的最后一段代码是 Hadoop 流行的*Map/Reduce*模式的一个示例。 首先，我们将(`user`，`product`，`price`)的记录映射到`(product, 1)`的记录。 然后，我们执行`reduceByKey`运算，将每个唯一产品的 1 相加。

一旦我们有了这个转换后的 RDD(它包含每个产品的购买数量)，我们将调用`collect`，它将计算结果作为本地 Scala 集合返回给驱动程序。 然后我们将在本地对这些计数进行排序(请注意，在实践中，如果数据量很大，我们将并行执行排序，通常使用 Spark 操作，如`sortByKey`)。

最后，我们将计算结果打印到控制台：

```
    println("Total purchases: " + numPurchases) 
    println("Unique users: " + uniqueUsers) 
    println("Total revenue: " + totalRevenue) 
    println("Most popular product: %s with %d 
    purchases".format(mostPopular._1, mostPopular._2)) 
  } 
}

```

我们可以通过在项目的基目录中运行`sbt run`或者在您的 Scala IDE 中运行该程序(如果您正在使用该程序)来运行该程序。 输出应与以下内容类似：

```
...
[info] Compiling 1 Scala source to ...
[info] Running ScalaApp
...
Total purchases: 5
Unique users: 4
Total revenue: 39.91
Most popular product: iPhone Cover with 2 purchases

```

我们可以看到，我们有来自四个不同用户的`5`购买，总收入为`39.91`。 我们最受欢迎的产品是`iPhone cover with 2 purchases`。

# 在 Java 中使用 Spark 程序的第一步

Java API 在原则上与 Scala API 非常相似。 然而，尽管 Scala 可以非常容易地调用 Java 代码，但在某些情况下，从 Java 调用 Scala 代码是不可能的。 当 Scala 代码利用 Scala 特性(如隐式转换、默认参数和 Scala 反射 API)时尤其如此。

Spark 通常会大量使用这些特性，因此有必要专门为 Java 提供一个单独的 API，其中包括公共类的 Java 版本。 因此，`SparkContext`变成`JavaSparkContext`，RDD 变成 JavaRDD。

版本 8 之前的 Java 版本不支持匿名函数，也没有用于函数式编程的简洁语法，因此 Spark Java API 中的函数必须实现带有`call`方法签名的`WrappedFunction`接口。 虽然它明显更冗长，但我们通常会创建一次性匿名类来传递给我们的 Spark 操作，这些操作实现该接口和`call`方法，以实现与 Scala 中的匿名函数大致相同的效果。

Spark 支持 Java 8 的匿名函数(或*lambda*)语法。 使用这种语法可以使用 Java8 编写的 Spark 程序看起来非常接近等价的 Scala 程序。

在 Scala 中，键/值对的 RDD 提供了可通过隐式转换自动访问的特殊运算符(例如`reduceByKey`和`saveAsSequenceFile`)。 在 Java 中，需要特殊类型的`JavaRDD`类才能访问类似的函数。 其中包括使用键/值对的`JavaPairRDD`和使用数字记录的`JavaDoubleRDD`。

In this section, we covered the standard Java API syntax. For more details and examples related to working RDDs in Java, as well as the Java 8 lambda syntax, refer to the Java sections of the *Spark Programming Guide* found at [http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations](http://spark.apache.org/docs/latest/programming-guide.html).

我们将在下面的 Java 程序中看到这些差异的大部分示例，该程序包含在本章的示例代码中的名为`java-spark-app`的目录中。 `code`目录还包含`data`子目录下的 CSV 数据文件。

我们将使用**Maven**构建工具构建并运行该项目，我们假设您已经在系统上安装了该工具。

Installing and setting up Maven is beyond the scope of this book. Usually, Maven can easily be installed using the package manager on your Linux system or HomeBrew or MacPorts on Mac OS X.
Detailed installation instructions can be found at [http://maven.apache.org/download.cgi](http://maven.apache.org/download.cgi).

该项目包含一个名为`JavaApp.java`的 Java 文件，其中包含我们的程序代码：

```
import org.apache.spark.api.java.JavaRDD; 
import org.apache.spark.api.java.JavaSparkContext; 
import scala.Tuple2; 
import java.util.*; 
import java.util.stream.Collectors; 

/** 
 * A simple Spark app in Java 
 */ 
public class JavaApp { 
  public static void main(String[] args) {

```

与我们的 Scala 示例一样，我们首先需要初始化上下文。 请注意，这里我们将使用`JavaSparkContext`类，而不是前面使用的`SparkContext`类。 我们将以同样的方式使用`JavaSparkContext`类来访问我们的数据，使用`textFile`，然后将每行拆分为必需的字段。 请注意，我们如何使用匿名类定义一个拆分函数，该函数在突出显示的代码中执行字符串处理：

```
JavaSparkContext sc = new JavaSparkContext("local[2]", 
     "First Spark App"); 
// we take the raw data in CSV format and convert it into a 
// set of records of the form (user, product, price) 
JavaRDD<String[]> data =   sc.textFile("data/UserPurchaseHistory.csv").map(s ->         s.split(","));

```

现在，我们可以计算与在 Scala 示例中相同的度量。 注意 Java 和 Scala API 的一些方法是如何相同的(例如，`distinct`和`count`)。 还要注意我们传递给 map 函数的匿名类的使用。 此处突出显示此代码：

```
// let's count the number of purchases 
long numPurchases = data.count(); 
// let's count how many unique users made purchases 
long uniqueUsers = data.map(strings ->  
      strings[0]).distinct().count(); 
// let's sum up our total revenue 
Double totalRevenue = data.map(strings ->  
      Double.parseDouble(strings[2])).reduce((Double v1,  
Double v2) -> new Double(v1.doubleValue() + v2.doubleValue()));

```

在以下代码行中，我们可以看到计算最受欢迎产品的方法与 Scala 示例中的方法相同。 额外的代码可能看起来很复杂，但它主要与创建匿名函数所需的 Java 代码有关(我们在这里突出显示了这一点)。 实际功能是相同的：

```
// let's find our most popular product 
List<Tuple2<String, Integer>> pairs = data.mapToPair(strings -> new Tuple2<String, Integer>(strings[1], 1)).reduceByKey((Integer i1, Integer i2) -> i1 + i2).collect(); 

Map<String, Integer> sortedData = new HashMap<>(); 
Iterator it = pairs.iterator(); 
while (it.hasNext()) { 
    Tuple2<String, Integer> o = (Tuple2<String, Integer>) it.next(); 
    sortedData.put(o._1, o._2); 
} 
List<String> sorted = sortedData.entrySet() 
        .stream() 
        .sorted(Comparator.comparing((Map.Entry<String, Integer> 
          entry) -> entry.getValue()).reversed())
         .map(Map.Entry::getKey) 
        .collect(Collectors.toList()); 
String mostPopular = sorted.get(0); 
            int purchases = sortedData.get(mostPopular); 
    System.out.println("Total purchases: " + numPurchases); 
    System.out.println("Unique users: " + uniqueUsers); 
    System.out.println("Total revenue: " + totalRevenue); 
    System.out.println(String.format("Most popular product:
     %s with %d purchases", mostPopular, purchases)); 
  } 
}

```

可以看到，除了用于通过匿名内部类声明变量和函数的额外样板代码之外，通用结构类似于 Scala 版本。 研究这两个示例并将 Scala 代码行与 Java 中的行进行比较，以了解在每种语言中如何实现相同的结果，这是一个很好的练习。

此程序可以通过从项目的基目录执行以下命令来运行：

```
  $ mvn exec:java -Dexec.mainClass="JavaApp"

```

您将看到与 Scala 版本非常相似的输出，具有相同的计算结果：

```
...
14/01/30 17:02:43 INFO spark.SparkContext: Job finished: collect 
at JavaApp.java:46, took 0.039167 s
Total purchases: 5
Unique users: 4
Total revenue: 39.91
Most popular product: iPhone Cover with 2 purchases

```

# 使用 Python 编写 Spark 程序的第一步

Spark 的 Python API 几乎用 Python 语言公开了 Spark 的 Scala API 的所有功能。 有些功能还不受支持(例如，使用 GraphX 和一些 API 方法进行图形处理)。 有关详细信息，请参阅*火花编程指南*([http://spark.apache.org/docs/latest/programming-guide.html](http://spark.apache.org/docs/latest/programming-guide.html))的 Python 节。

**PySpark**是使用 Spark 的 Java API 构建的。 数据在原生 Python 中处理、缓存，并在 JVM 中混洗。 Python 驱动程序的`SparkContext`使用 Py4J 启动 JVM 并创建`JavaSparkContext`。 驱动程序使用 Py4J 在 Python 和 Java`SparkContext`对象之间进行本地通信。 Python 中的 RDD 转换映射到 Java 中的`PythonRDD`对象上的转换。 `PythonRDD`Object 在远程工作者机器上启动 Python 子进程，并使用管道与它们通信。 这些子流程用于发送用户代码和处理数据。

根据前面的示例，我们现在将编写一个 Python 版本。 我们假设您的系统上安装了 Python 版本 2.6 和更高版本(例如，大多数 Linux 和 MacOSX 系统都预装了 Python)。

示例程序包含在本章的示例代码中，位于名为`python-spark-app`的目录中，该目录还包含`data`子目录下的 CSV 数据文件。 该项目包含此处提供的脚本`pythonapp.py`。

一个简单的 Python Spark 应用程序：

```
from pyspark import SparkContext

sc = SparkContext("local[2]", "First Spark App")
# we take the raw data in CSV format and convert it into a set of 
    records of the form (user, product, price)
data = sc.textFile("data/UserPurchaseHistory.csv").map(lambda 
    line: line.split(",")).map(lambda record: (record[0], record[1], 
    record[2]))
# let's count the number of purchases
numPurchases = data.count()
# let's count how many unique users made purchases
uniqueUsers = data.map(lambda record: record[0]).distinct().count()
# let's sum up our total revenue
totalRevenue = data.map(lambda record: float(record[2])).sum()
# let's find our most popular product
products = data.map(lambda record: (record[1], 
    1.0)).reduceByKey(lambda a, b: a + b).collect()
mostPopular = sorted(products, key=lambda x: x[1], reverse=True)[0]

print "Total purchases: %d" % numPurchases
print "Unique users: %d" % uniqueUsers
print "Total revenue: %2.2f" % totalRevenue
print "Most popular product: %s with %d purchases" % 
    (mostPopular[0], mostPopular[1])

```

如果您比较我们程序的 Scala 和 Python 版本，您会发现一般来说，语法看起来非常相似。 一个关键区别是我们如何表示匿名函数(也称为`lambda`函数；因此，Python 语法使用此关键字)。 在 Scala 中，我们已经看到将输入`x`映射到输出`y`的匿名函数表示为`x => y`，而在 Python 中，它表示为`lambda x: y`。 在前面代码中突出显示的行中，我们应用了一个匿名函数，该函数将通常相同类型的两个输入`a`和`b`映射到一个输出。 在本例中，我们应用的函数是加号函数；因此，`lambda a, b: a + b`。

运行该脚本的最佳方式是从示例项目的基目录运行以下命令：

```
 $SPARK_HOME/bin/spark-submit pythonapp.py

```

在这里，`SPARK_HOME`变量应该替换为您在本章开始时最初解压 Spark 预构建二进制包的目录的路径。

运行该脚本后，您应该看到类似于 Scala 和 Java 示例的输出，我们的计算结果是相同的：

```
...
14/01/30 11:43:47 INFO SparkContext: Job finished: collect at 
pythonapp.py:14, took 0.050251 s
Total purchases: 5
Unique users: 4
Total revenue: 39.91
Most popular product: iPhone Cover with 2 purchases

```

# 在 R 中实现 Spark 程序的第一步

**SparkR**是一个 R 包，它提供了从 R 使用 Apache Spark 的前端。在 Spark 1.6.0 中，SparkR 提供了大型数据集上的分布式数据框架。 SparkR 还支持使用 MLlib 的分布式机器学习。 这是你在阅读机器学习章节时应该尝试的东西。

# SparkR 数据帧

`DataFrame`是组织到分发的名称列中的数据集合。 这个概念非常类似于关系数据库或 R 的数据框架，但有更好的优化。 这些数据帧的源可以是 CSV、TSV、蜂窝表、本地 R 数据帧等。

可以使用`./bin/sparkR shell`运行火花分配。

根据前面的示例，我们现在将编写一个 R 版本。 我们假设您的系统上安装了 R(R 版本 3.0.2(2013-09-25)-*飞盘航行*)、R Studio 和更高版本(例如，大多数 Linux 和 Mac OS X 系统都预装了 Python)。

示例程序包含在本章的示例代码中，位于名为`r-spark-app`的目录中，该目录还包含`data`子目录下的 CSV 数据文件。 该项目包含一个脚本`r-script-01.R`，下面提供了该脚本。 确保将`PATH`更改为适合您环境的值。

```
Sys.setenv(SPARK_HOME = "/PATH/spark-2.0.0-bin-hadoop2.7") 
.libPaths(c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"), 
 .libPaths())) 
#load the Sparkr library 
library(SparkR) 
sc <- sparkR.init(master = "local", sparkPackages="com.databricks:spark-csv_2.10:1.3.0") 
sqlContext <- sparkRSQL.init(sc) 

user.purchase.history <- "/PATH/ml-resources/spark-ml/Chapter_01/r-spark-app/data/UserPurchaseHistory.csv" 
data <- read.df(sqlContext, user.purchase.history, "com.databricks.spark.csv", header="false") 
head(data) 
count(data) 

parseFields <- function(record) { 
  Sys.setlocale("LC_ALL", "C") # necessary for strsplit() to work correctly 
  parts <- strsplit(as.character(record), ",") 
  list(name=parts[1], product=parts[2], price=parts[3]) 
} 

parsedRDD <- SparkR:::lapply(data, parseFields) 
cache(parsedRDD) 
numPurchases <- count(parsedRDD) 

sprintf("Number of Purchases : %d", numPurchases) 
getName <- function(record){ 
  record[1] 
} 

getPrice <- function(record){ 
  record[3] 
} 

nameRDD <- SparkR:::lapply(parsedRDD, getName) 
nameRDD = collect(nameRDD) 
head(nameRDD) 

uniqueUsers <- unique(nameRDD) 
head(uniqueUsers) 

priceRDD <- SparkR:::lapply(parsedRDD, function(x) { as.numeric(x$price[1])}) 
take(priceRDD,3) 

totalRevenue <- SparkR:::reduce(priceRDD, "+") 

sprintf("Total Revenue : %.2f", s) 

products <- SparkR:::lapply(parsedRDD, function(x) { list( toString(x$product[1]), 1) }) 
take(products, 5) 
productCount <- SparkR:::reduceByKey(products, "+", 2L) 
productsCountAsKey <- SparkR:::lapply(productCount, function(x) { list( as.integer(x[2][1]), x[1][1])}) 

productCount <- count(productsCountAsKey) 
mostPopular <- toString(collect(productsCountAsKey)[[productCount]][[2]]) 
sprintf("Most Popular Product : %s", mostPopular)

```

在 bash 终端上使用以下命令运行该脚本：

```
  $ Rscript r-script-01.R 

```

您的输出将类似于以下清单：

```
> sprintf("Number of Purchases : %d", numPurchases)
[1] "Number of Purchases : 5"

> uniqueUsers <- unique(nameRDD)
> head(uniqueUsers)
[[1]]
[[1]]$name
[[1]]$name[[1]]
[1] "John"
[[2]]
[[2]]$name
[[2]]$name[[1]]
[1] "Jack"
[[3]]
[[3]]$name
[[3]]$name[[1]]
[1] "Jill"
[[4]]
[[4]]$name
[[4]]$name[[1]]
[1] "Bob"

> sprintf("Total Revenue : %.2f", totalRevenueNum)
[1] "Total Revenue : 39.91"

> sprintf("Most Popular Product : %s", mostPopular)
[1] "Most Popular Product : iPad Cover"

```

# 让 Spark 在 Amazon EC2 上运行

Spark 项目提供了在 Amazon 的 EC2 服务的云中运行 Spark 集群的脚本。 这些脚本位于`ec2`目录中。 您可以使用以下命令运行此目录中包含的`spark-ec2`脚本：

```
>./ec2/spark-ec2 

```

以不带参数的方式运行它将显示帮助输出：

```
Usage: spark-ec2 [options] <action> <cluster_name>
<action> can be: launch, destroy, login, stop, start, get-master

Options:
...

```

在创建 Spark EC2 集群之前，您需要确保您拥有
Amazon 帐户。

If you don't have an Amazon Web Services account, you can sign up at [http://aws.amazon.com/](http://aws.amazon.com/).
The AWS console is available at [http://aws.amazon.com/console/](http://aws.amazon.com/console/).

您还需要创建 Amazon EC2 密钥对并检索相关的安全凭据。 EC2 的 Spark 文档(可在[http://spark.apache.org/docs/latest/ec2-scripts.html](http://spark.apache.org/docs/latest/ec2-scripts.html)上找到)解释了要求：

Create an Amazon EC2 key pair for yourself. This can be done by logging into your Amazon Web Services account through the AWS console, clicking on Key Pairs on the left sidebar, and creating and downloading a key. Make sure that you set the permissions for the private key file to 600 (that is, only you can read and write it) so that ssh will work. Whenever you want to use the spark-ec2 script, set the environment variables `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` to your Amazon EC2 access key `ID` and secret access key, respectively. These can be obtained from the AWS homepage by clicking Account | Security Credentials | Access Credentials.

创建密钥对时，请选择易于记忆的名称。 我们将简单地使用密钥对的名称*spark*。 密钥对文件本身将称为`spark.pem`。 如前所述，请确保正确设置了密钥对文件权限，并使用以下命令导出了 AWS 凭据的环境变量：

```
  $ chmod 600 spark.pem
 $ export AWS_ACCESS_KEY_ID="..."
 $ export AWS_SECRET_ACCESS_KEY="..."

```

您还应该小心保护您下载的密钥对文件的安全，不要丢失它，因为它在创建时只能下载一次！

请注意，在下一节中启动 Amazon EC2 群集将为您的 AWS 账户带来*费用*。

# 启动 EC2 Spark 集群

我们现在已经准备好通过切换到`ec2`目录，然后运行集群启动命令来启动一个小的 Spark 集群：

```
 $  cd ec2
 $ ./spark-ec2 --key-pair=rd_spark-user1 --identity-file=spark.pem  
    --region=us-east-1 --zone=us-east-1a launch my-spark-cluster

```

这将启动一个名为 test-cluster 的新 Spark 集群，其中有一个实例类型为`m3.medium`的主节点和一个从节点。 此群集将使用为 Hadoop 2 构建的 Spark 版本启动。我们使用的密钥对名称是 Spark，密钥对文件是`spark.pem`(如果您为文件指定了不同的名称或具有现有的 AWS 密钥对，请改用该名称)。

群集可能需要相当长的时间才能完全启动和初始化。 运行启动命令后，您应该立即看到类似以下内容：

```
Setting up security groups...
Creating security group my-spark-cluster-master
Creating security group my-spark-cluster-slaves
Searching for existing cluster my-spark-cluster in region 
    us-east-1...
Spark AMI: ami-5bb18832
Launching instances...
Launched 1 slave in us-east-1a, regid = r-5a893af2
Launched master in us-east-1a, regid = r-39883b91
Waiting for AWS to propagate instance metadata...
Waiting for cluster to enter 'ssh-ready' state...........
Warning: SSH connection error. (This could be temporary.)
Host: ec2-52-90-110-128.compute-1.amazonaws.com
SSH return code: 255
SSH output: ssh: connect to host ec2-52-90-110-128.compute- 
    1.amazonaws.com port 22: Connection refused
Warning: SSH connection error. (This could be temporary.)
Host: ec2-52-90-110-128.compute-1.amazonaws.com
SSH return code: 255
SSH output: ssh: connect to host ec2-52-90-110-128.compute-
    1.amazonaws.com port 22: Connection refused
Warnig: SSH connection error. (This could be temporary.)
Host: ec2-52-90-110-128.compute-1.amazonaws.com
SSH return code: 255
SSH output: ssh: connect to host ec2-52-90-110-128.compute-
    1.amazonaws.com port 22: Connection refused
Cluster is now in 'ssh-ready' state. Waited 510 seconds.

```

如果群集已成功启动，您最终应该会看到类似于以下清单的控制台输出：

```
./tachyon/setup.sh: line 5: /root/tachyon/bin/tachyon: 
    No such file or directory
./tachyon/setup.sh: line 9: /root/tachyon/bin/tachyon-start.sh: 
    No such file or directory
[timing] tachyon setup:  00h 00m 01s
Setting up rstudio
spark-ec2/setup.sh: line 110: ./rstudio/setup.sh: 
    No such file or directory
[timing] rstudio setup:  00h 00m 00s
Setting up ganglia
RSYNC'ing /etc/ganglia to slaves...
ec2-52-91-214-206.compute-1.amazonaws.com
Shutting down GANGLIA gmond:                               [FAILED]
Starting GANGLIA gmond:                                    [  OK  ]
Shutting down GANGLIA gmond:                               [FAILED]
Starting GANGLIA gmond:                                    [  OK  ]
Connection to ec2-52-91-214-206.compute-1.amazonaws.com closed.
Shutting down GANGLIA gmetad:                              [FAILED]
Starting GANGLIA gmetad:                                   [  OK  ]
Stopping httpd:                                            [FAILED]
Starting httpd: httpd: Syntax error on line 154 of /etc/httpd
    /conf/httpd.conf: Cannot load /etc/httpd/modules/mod_authz_core.so 
    into server: /etc/httpd/modules/mod_authz_core.so: cannot open 
    shared object file: No such file or directory              [FAILED]
[timing] ganglia setup:  00h 00m 03s
Connection to ec2-52-90-110-128.compute-1.amazonaws.com closed.
Spark standalone cluster started at 
    http://ec2-52-90-110-128.compute-1.amazonaws.com:8080
Ganglia started at http://ec2-52-90-110-128.compute-
    1.amazonaws.com:5080/ganglia
Done!
ubuntu@ubuntu:~/work/spark-1.6.0-bin-hadoop2.6/ec2$

```

这将创建两个 VM-类型为 m1.Large 的 Spark Master 和 Spark Slave，如以下屏幕截图所示：

![](images/image_01_003.png)

要测试我们是否可以连接到新群集，我们可以运行以下命令：

```
  $ ssh -i spark.pem root@ ec2-52-90-110-128.compute-1.amazonaws.com

```

请记住将主节点的公共域名(前面命令中`root@`之后的地址)替换为正确的 Amazon EC2 公共域名，该名称将在启动群集后显示在您的控制台输出中。

您还可以通过运行以下代码行来检索群集的主公共域名：

```
  $ ./spark-ec2 -i spark.pem get-master test-cluster

```

成功运行`ssh`命令后，您将连接到 EC2 中的 Spark 主节点，并且您的终端输出应与以下屏幕截图匹配：

![](images/image_01_004.png)

我们可以通过切换到`Spark`目录并在本地模式下运行示例来测试我们的集群是否正确设置了 Spark：

```
  $ cd spark
 $ MASTER=local[2] ./bin/run-example SparkPi

```

您应该会看到与在本地计算机上运行相同命令所得到的输出类似的输出：

```
...
14/01/30 20:20:21 INFO SparkContext: Job finished: reduce at 
SparkPi.scala:35, took 0.864044012 s
Pi is roughly 3.14032
...

```

现在我们有了一个具有多个节点的实际集群，我们可以在集群模式下测试 Spark。 我们可以在集群上运行相同的示例，通过传入主 URL 而不是本地版本来使用我们的一个从节点：

```
    `$ MASTER=spark://` ec2-52-90-110-128.compute-
      1.amazonaws.com:`7077 ./bin/run-example SparkPi` 

```

Note that you will need to substitute the preceding master domain name with the correct domain name for your specific cluster.

同样，输出应该类似于在本地运行示例；但是，日志消息将显示您的驱动程序已连接到 Spark Master：

```
...
14/01/30 20:26:17 INFO client.Client$ClientActor: Connecting to 
    master spark://ec2-54-220-189-136.eu-
    west-1.compute.amazonaws.com:7077
14/01/30 20:26:17 INFO cluster.SparkDeploySchedulerBackend: 
    Connected to Spark cluster with app ID app-20140130202617-0001
14/01/30 20:26:17 INFO client.Client$ClientActor: Executor added: 
    app-20140130202617-0001/0 on worker-20140130201049-
    ip-10-34-137-45.eu-west-1.compute.internal-57119 
    (ip-10-34-137-45.eu-west-1.compute.internal:57119) with 1 cores
14/01/30 20:26:17 INFO cluster.SparkDeploySchedulerBackend:
    Granted executor ID app-20140130202617-0001/0 on hostPort 
    ip-10-34-137-45.eu-west-1.compute.internal:57119 with 1 cores, 
    2.4 GB RAM
14/01/30 20:26:17 INFO client.Client$ClientActor: 
    Executor updated: app-20140130202617-0001/0 is now RUNNING
14/01/30 20:26:18 INFO spark.SparkContext: Starting job: reduce at 
    SparkPi.scala:39
...

```

您可以随意体验您的集群。 在 Scala 中试用交互式控制台，例如：

```
  **$ ./bin/spark-shell --master spark://** ec2-52-90-110-128.compute-
    1.amazonaws.com**:7077**

```

完成后，键入`exit`离开控制台。 您也可以通过运行以下命令来尝试 PySpark 控制台：

```
  **$ ./bin/pyspark --master spark://** ec2-52-90-110-128.compute-
    1.amazonaws.com**:7077**

```

您可以使用 Spark Master Web 界面查看注册到 Spark Master 的应用程序。 要加载 Master Web UI，请导航到`ec2-52-90-110-128.compute-1.amazonaws.com:8080`(同样，请记住将此域名替换为您自己的主域名)。

请记住，*您将被 Amazon*收取集群使用费。 完成测试集群后，不要忘记停止或终止它。 为此，您可以首先通过键入`exit`返回到您自己的本地系统来退出`ssh`会话，然后运行以下命令：

```
  $ ./ec2/spark-ec2 -k spark -i spark.pem destroy test-cluster

```

您应该会看到以下输出：

```
Are you sure you want to destroy the cluster test-cluster?
The following instances will be terminated:
Searching for existing cluster test-cluster...
Found 1 master(s), 1 slaves
> ec2-54-227-127-14.compute-1.amazonaws.com
> ec2-54-91-61-225.compute-1.amazonaws.com
ALL DATA ON ALL NODES WILL BE LOST!!
Destroy cluster test-cluster (y/N): y
Terminating master...
Terminating slaves...

```

点击*Y*，然后*输入*以销毁群集。

祝贺你!。 您刚刚在云中设置了一个 Spark 集群，在该集群上运行了一个完全并行的示例程序，然后终止了它。 如果您想在集群上试用后续章节(或您自己的 Spark 程序)中的任何示例代码，请随意尝试 Spark EC2 脚本并启动您选择的大小和实例配置文件的集群。 (只需注意成本，并记住在完成后将其关闭！)

# 在 Amazon Elastic Map Reduce 上配置和运行 Spark

使用 Amazon Elastic Map Reduce 启动安装了 Spark 的 Hadoop 群集。 执行以下步骤以创建安装了 Spark 的 EMR 群集：

1.  启动 Amazon EMR 群集。
2.  在[https://console.aws.amazon.com/elasticmapreduce/](https://console.aws.amazon.com/elasticmapreduce/)打开 Amazon EMR UI 控制台。
3.  选择创建群集：

![](images/image_01_005.png)

4.  选择相应的 Amazon AMI 版本 3.9.0 或更高版本，如以下屏幕截图所示：

![](images/image_01_006.png)

5.  对于要安装的应用程序字段，从用户界面上显示的列表中选择 Spark 1.5.2 或更高版本，然后单击 Add。
6.  根据需要选择其他硬件选项：
    *   实例类型
    *   要与 SSH 一起使用的密钥对
    *   权限
    *   IAM 角色(默认或自定义)

请参阅以下屏幕截图：

![](images/image_01_007.png)

7.  单击创建群集。 群集将开始实例化，如以下屏幕截图所示：

![](images/image_01_008.png)

8.  登录到主服务器。 一旦 EMR 集群准备就绪，您就可以通过 SSH 连接到主服务器：

```
 **$ ssh -i rd_spark-user1.pem**
   hadoop@ec2-52-3-242-138.compute-1.amazonaws.com 

```

The output will be similar to following listing:

```
     Last login: Wed Jan 13 10:46:26 2016

 __|  __|_  )
 _|  (     /   Amazon Linux AMI
 ___|___|___|

 https://aws.amazon.com/amazon-linux-ami/2015.09-release-notes/
 23 package(s) needed for security, out of 49 available
 Run "sudo yum update" to apply all updates.
 [hadoop@ip-172-31-2-31 ~]$ 

```

9.  启动 Spark Shell：

```
      [hadoop@ip-172-31-2-31 ~]$ spark-shell
 16/01/13 10:49:36 INFO SecurityManager: Changing view acls to: 
          hadoop
 16/01/13 10:49:36 INFO SecurityManager: Changing modify acls to: 
          hadoop
 16/01/13 10:49:36 INFO SecurityManager: SecurityManager: 
          authentication disabled; ui acls disabled; users with view 
          permissions: Set(hadoop); users with modify permissions: 
          Set(hadoop)
 16/01/13 10:49:36 INFO HttpServer: Starting HTTP Server
 16/01/13 10:49:36 INFO Utils: Successfully started service 'HTTP 
          class server' on port 60523.
 Welcome to
 ____              __
 / __/__  ___ _____/ /__
 _ / _ / _ &grave;/ __/  '_/
 /___/ .__/_,_/_/ /_/_   version 1.5.2
 /_/
 scala> sc

```

10.  从 EMR 运行 Basic Spark Sample：

```
    scala> val textFile = sc.textFile("s3://elasticmapreduce/samples
      /hive-ads/tables/impressions/dt=2009-04-13-08-05
      /ec2-0-51-75-39.amazon.com-2009-04-13-08-05.log")
 scala> val linesWithCartoonNetwork = textFile.filter(line =>  
      line.contains("cartoonnetwork.com")).count()

```

Your output will be as follows:

```
     linesWithCartoonNetwork: Long = 9

```

# Spark 中的 UI

Spark 提供了一个 Web 界面，可用于监视作业、查看环境和运行 SQL 命令。

`SparkContext`在端口`4040`上启动 Web 用户界面，显示有关应用程序的有用信息。 这包括以下内容：

*   计划程序阶段和任务列表
*   RDD 大小和内存使用情况摘要
*   环境信息
*   有关正在运行的执行器的信息

在 Web 浏览器中转到`http://<driver-node>:4040`即可访问此界面。 如果多个`SparkContexts`在同一台主机上运行，则它们将绑定到以端口`4040`(`4041`、`4042`等)开始的端口。

以下屏幕截图显示了 Web UI 提供的部分信息：

![](images/image_01_009.png)

UI showing the Environment of the Spark Content

![](images/image_01_010.png)

UI table showing Executors available

# Spark 支持的机器学习算法

Spark ML 支持以下算法：

*   **协同过滤**
    *   **交替最小二乘(ALS)：**协同过滤通常用于推荐系统。 这些技术旨在填充用户-项目关联矩阵中缺失的条目。 `spark.mllib`目前支持基于模型的协同过滤。 在此实现中，用户和产品由一组可用于预测丢失条目的潜在因素来描述。 `spark.mllib`使用 ALS 算法来学习这些潜在因素。

*   **聚类**：这是一个无监督的学习问题，其目标是基于相似性的概念将实体的子集彼此分组。 聚类用于探索性分析，并作为分层监督学习管道的一个组成部分。 当在学习管道中使用时，针对每个群集训练不同的分类器或回归模型。 Spark 中实施了以下群集技术：
    *   **k-Means**：这是常用的群集算法之一，可将数据点群集到预定义数量的群集中。 群集的数量由用户自行选择。 `spark.mllib`实现包括 k-Means++方法的并行变体([http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf](http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf))。
    *   **高斯混合**：**高斯混合模型**(**GMM**)表示复合分布，其中点取自 k 个高斯子分布之一。 这些分布中的每一个都有自己的概率。 `spark.mllib`实现使用期望最大化算法来归纳给定一组样本的最大似然模型。
    *   **幂迭代聚类(Power Iteration Clusters，PIC)**：这是一个可伸缩的算法，用于在给定成对相似性作为边属性的情况下对图的顶点进行聚类。 它使用幂迭代计算图的(归一化的亲和度矩阵)的伪特征向量。

幂迭代是一种特征值算法。 给定一个矩阵*X*，该算法将产生一个数*λ*(特征值)和一个非零向量*v*(特征向量)，使得*XV=λv*。

矩阵的伪特征向量可以看作是相邻矩阵的特征向量。 更具体地说，伪本征向量定义为：

设*A*是*n*乘*n*矩阵。 设*E*是满足*||E||=EURO*的任意矩阵。 然后定义*A+E*的特征向量为*A*的伪本征向量。 该特征向量使用它来聚集图形顶点。

`spark.mllib`包括使用*GraphX*的 PIC 实现。 它获取元组的 RDD，并输出具有群集分配的模型。 相似之处必须是非负面的。 PIC 假设相似性度量是对称的。

(在统计学中，相似性度量或相似性函数是量化两个对象之间的相似性的实值函数。 这样的度量与距离度量相反；余弦相似度就是这样的一个例子)

无论排序如何，一对(`srcId`，`dstId`)应在输入数据中最多出现一次。

主题是聚类中心，文档对应于数据集中的示例主题和文档都存在于特征空间中，其中特征向量是字数的向量(也称为词袋)
而不是使用传统的距离方法估计聚类，LDA 使用基于文本文档如何生成的模型的函数

层次聚类是寻求构建聚类层次的聚类分析的常用方法之一。

*   **分类**
    *   **决策树：**决策树及其集成是分类和回归的方法之一。 决策树很受欢迎，因为它们易于解释、处理分类特征并扩展到多类分类设置。 它们不需要功能缩放，还能够捕获非线性和功能交互。 树系综算法、随机森林和 Boosting 是分类和回归场景中表现最好的算法。

`spark.mllib`实现二进制和多类分类和回归的决策树。 它既支持连续功能，也支持分类功能。 该实现按行对数据进行分区，从而允许对数百万个实例进行分布式训练。

朴素贝叶斯(Naive Bayes)是一种多类分类算法，它假设每对特征之间是独立的。 在单遍训练数据中，该算法计算给定标签的每个特征的条件概率分布，然后应用贝叶斯定理计算给定观测的标签的条件概率分布，然后用于预测。 `spark.mllib`支持多项式朴素贝叶斯和伯努利朴素贝叶斯。 这些模型通常用于文档分类。

它是预测结果概率的**广义线性模型**(**GLM**)的特例。 有关实现的更多背景和更多细节，请参考`spark.mllib`中关于 Logistic 回归的文档。

GLM 被认为是线性回归的推广，它允许响应变量具有不同于正态分布的误差分布。

Spark ML 支持用于二进制和多类分类以及回归的随机森林。 它可以用于连续值或分类值。

*   **降维**：这是减少要进行机器学习的变量数量的过程。 它可以用来从原始特征中提取潜在特征，或者在保持整体结构的同时压缩数据。 MLlib 在`RowMatrix`类之上提供支持降维。

MLlib 支持使用`RowMatrix`以面向行的格式存储的瘦高矩阵的 PCA。
Spark 支持使用 TF-IDF、ChiSquare、选择器、规格化器和 Word2Vector 进行特征提取和转换。

*   **频繁模式挖掘**：
    *   **FP-Growth**：FP 代表频繁模式。 算法首先计算数据集中出现的项(属性和值对)，并将它们存储在标题表中。

在第二遍中，算法通过插入实例(由项目组成)来构建 FP-tree 结构。 每个实例中的项都按其在数据集中的频率降序排序；这确保可以快速处理树。 每个实例中不符合最小覆盖阈值的项目将被丢弃。 对于多个实例共享最频繁项的用例，FP-tree 在树根附近提供高压缩。

它实现了一种并行规则生成算法，用于构造以单个项为后件的规则。

*   **前缀范围**：这是一个序列模式挖掘算法。
*   **评估指标**：`spark.mllib`附带了一套用于评估算法的指标。
*   ****PMML 模型导出****：**预测模型标记语言**(**PMML**)是一种基于 XML 的预测模型交换格式。 PMML 为分析应用程序提供了一种描述和交换由机器学习算法生成的预测模型的机制。

`spark.mllib`允许将其机器学习模型导出到 PMML 及其等效的 PMML 模型。

*   **优化(开发人员)**
    *   **随机梯度下降**：用于优化梯度下降以最小化目标函数；该函数是可微函数的和。

梯度下降方法和**随机次梯度下降**(**SGD**)作为 MLlib 的底层原语被包括在内，并在此基础上开发了各种 ML 算法。

*   **有限内存 BFGS(L-BFGS)**：这是一个优化算法，属于近似**Broyden-Fletcher-Goldfarb-Shanno**(**BFGS**)算法的拟牛顿方法家族。 它使用有限的计算机内存。 它用于机器学习中的参数估计。

BFGS 方法近似于牛顿方法，牛顿方法是一类寻找函数驻点的爬山优化技术。 对于这类问题，一个必要的最优条件是梯度应为零**。**

# 与现有库相比，使用 Spark ML 的优势

伯克利的 AMQ 实验室对 Spark 进行了评估，RDDS 通过 Amazon EC2 上的一系列实验以及用户应用程序的基准测试进行了评估。

*   **使用的算法**：逻辑回归和 k-均值
*   **用例**：第一次迭代，多次迭代。

所有测试都使用`m1.xlarge`个 EC2 节点，具有 4 个核心和 15 GB 的 RAM。 HDFS 用于存储 256MB 的数据块。 请参阅下图：

![](images/image_01_011.png)

上图显示了 Hadoop 和 Spark 在**逻辑回归**的第一次迭代和后续迭代中的性能比较：

![](images/image_01_012.png)

上图显示了 Hadoop 和 Spark 在 K 均值聚类算法的第一次迭代和后续迭代中的性能比较。

总体结果显示如下：

*   在迭代机器学习和图形应用方面，Spark 的表现比 Hadoop 高出 20 倍。 加速来自于通过将数据作为 Java 对象存储在内存中来避免 I/O 和反序列化成本。
*   编写的应用程序性能和伸缩性都很好。 Spark 可以将在 Hadoop 上运行的分析报告速度提高 40 倍。
*   当节点发生故障时，Spark 可以通过只重建丢失的 RDD 分区来快速恢复。
*   使用 Spark 以 5-7 秒的延迟交互地查询 1 TB 的数据集。

For more information, go to [http://people.csail.mit.edu/matei/papers/2012/nsdi_spark.pdf](http://people.csail.mit.edu/matei/papers/2012/nsdi_spark.pdf).

Spark 与 Hadoop 的 Sort 基准测试--2014 年，Databricks 团队参与了 Sort 基准测试([http://sortbenchmark.org/](http://sortbenchmark.org/))。 这是在一个 100 TB 的数据集上完成的。 Hadoop 在一个专用数据中心运行，在 EC2 上运行了一个包含 200 多个节点的 Spark 集群。 Spark 在 HDFS 分布式存储上运行。

Spark 比 Hadoop 快 3 倍，使用的机器少 10 倍。 请参阅下图：

![](images/image_01_013.png)

# Google 计算引擎上的 Spark 集群--DataProc

**Cloud DataProc**是在 Google Compute Engine 上运行的 Spark 和 Hadoop 服务。 它是一项托管服务。 Cloud DataProc 自动化有助于快速创建集群、轻松管理集群，并通过在不需要集群时关闭集群来节省资金。

在本节中，我们将学习如何使用 DataProc 创建 Spark 集群并在其上运行示例应用程序。

确保您已经创建了 Google Compute Engine 帐户并安装了 Google Cloud SDK([https://cloud.google.com/sdk/gcloud/](https://cloud.google.com/sdk/gcloud/))。

# Hadoop 和 Spark 版本

DataProc 支持以下 Hadoop 和 Spark 版本。 请注意，随着新版本的推出，这将随着时间的推移而改变：

*   火花 1.5.2。
*   Hadoop 2.7.1
*   猪 0.15.0
*   蜂巢 1。1。1。
*   GCS 连接器 1.4.3-hadoop2
*   BigQuery 连接器 0.7.3-hadoop2([https://github.com/GoogleCloudPlatform/bigdata-interop](https://github.com/GoogleCloudPlatform/bigdata-interop))

For more information, go to [http://cloud.google.com/dataproc-versions](http://cloud.google.com/dataproc-versions).

在接下来的步骤中，我们将使用 Google Cloud Console(用于创建星火集群和提交作业的用户界面)。

# 创建群集

您可以通过云平台控制台创建星火集群。 选择项目，然后单击继续以打开群集页面。 您将看到属于您的项目的 Cloud DataProc 集群(如果您已经创建了任何集群)。

单击 Create a cluster 按钮以打开 create a Cloud data pros cluster 页面。 请参阅以下屏幕截图：

![](images/image_01_014.png)

单击创建群集后，将显示如以下屏幕截图所示的详细表单：

![](images/image_01_015.png)

前面的屏幕截图显示了 create a Cloud Dataproc cluster 页面，其中自动填充了一个新 cluster-1 集群的默认字段。 请看下面的屏幕截图：

![](images/image_01_016.png)

您可以展开 Worker、Bucket、Network、Version、Initialization 和 Access Options 面板，以指定一个或多个 Worker 节点、登台存储桶、网络、初始化、Cloud DataProc 映像版本、操作和集群的项目级访问权限。 提供这些值是可选的。

默认集群创建时没有辅助节点、自动创建的临时存储桶和默认网络，它还具有最新发布的 Cloud DataProc 映像版本。 您可以更改这些默认设置：

![](images/image_01_017.png)

配置完页面上的所有字段后，单击 Create(创建)按钮创建群集。 创建的集群名称将显示在集群页面上。 一旦创建火花群集，状态就会更新为 Running。

单击前面创建的集群名称以打开集群详细信息页面。 它还具有一个概述选项卡和选定的 CPU 利用率图表。

您可以从其他选项卡中检查集群的作业、实例等。

# 提交作业

要将作业从云平台控制台提交到集群，请进入云平台 UI。 选择适当的项目，然后单击继续。 首次提交作业时，将显示以下对话框：

![](images/image_01_018.png)

单击提交作业：

![](images/image_01_019.png)

要提交 Spark 示例作业，请填写提交作业页上的字段，如下所示：

1.  从屏幕上的群集列表中选择群集名称。
2.  将作业类型设置为 Spark。
3.  将`file:///usr/lib/spark/lib/spark-examples.jar`添加到 Jar 文件。 这里，`file:///`表示 Hadoop`LocalFileSystem`方案；Cloud Dataproc 在创建集群时在集群的主节点上安装`/usr/lib/spark/lib/spark-examples.jar`。 或者，您可以指定一个自定义 JAR 的云存储路径(`gs://my-bucket/my-jarfile.jar`)或`HDFS`路径(`hdfs://examples/myexample.jar`)。
4.  将`Main`类或 JAR 设置为`org.apache.spark.examples.SparkPi`。
5.  将参数设置为单个参数`1000`。

单击提交以启动作业。

作业启动后，它将添加到作业列表中。 请参阅以下屏幕截图：

![](images/image_01_020.png)

作业完成后，其状态将更改：

![](images/image_01_021.png)

请查看此处列出的`job`输出。

使用适当的作业 ID 从终端执行命令。

在我们的示例中，作业 ID 是`1ed4d07f-55fc-45fe-a565-290dcd1978f7`，项目 ID 是`rd-spark-1`；因此，该命令如下所示：

```
  $ gcloud beta dataproc --project=rd-spark-1 jobs wait 1ed4d07f-
    55fc-45fe-a565-290dcd1978f7

```

(删节的)输出如下所示：

```
Waiting for job output...
16/01/28 10:04:29 INFO akka.event.slf4j.Slf4jLogger: Slf4jLogger 
    started
16/01/28 10:04:29 INFO Remoting: Starting remoting
...
Submitted application application_1453975062220_0001
Pi is roughly 3.14157732 

```

您还可以通过 SSH 登录 Spark 实例，并在交互模式下运行 spark-shell。

# 简略的 / 概括的 / 简易判罪的 / 简易的

在本章中，我们介绍了如何在我们自己的计算机上本地设置 Spark，以及如何在云中将 Spark 设置为运行在 Amazon EC2 上的集群。 您了解了如何在 Amazon 的**Elastic Map Reduce**(**EMR**)上运行 Spark。 您还学习了如何使用 Google Compute Engine 的 Spark Service 创建集群并运行简单的作业。 我们使用交互式 Scala 控制台讨论了 Spark 的编程模型和 API 的基础知识，并用 Scala、Java、R 和 Python 编写了相同的基本 Spark 程序。 我们还比较了 Hadoop 和 Spark 在不同机器学习算法下的性能指标，以及排序基准测试。

在下一章中，我们将考虑如何使用 Spark 创建一个机器学习系统。