# 第 6 章：使用 MLlib 的机器学习入门

本章分为以下食谱：

*   创建向量
*   创建带标签的点
*   创建矩阵
*   计算汇总统计信息
*   计算相关性
*   做假设检验
*   使用 ML 创建机器学习管道

# 简介

以下是维基百科对机器学习的定义：

> **“机器学习是一门探索构建和研究能够从数据中学习的算法的科学学科。”**

从本质上讲，机器学习是利用过去的数据来预测未来。 机器学习在很大程度上依赖于统计分析和方法论。

在统计中，测量尺度有四种类型：

<colgroup><col style="text-align: left"> <col style="text-align: left"></colgroup> 
| 

比例尺类型

 | 

描述 / 描写 / 形容 / 类别

 |
| --- | --- |
| 标称比例尺 | =，≠标识类别不能是数字吗示例：男性、女性 |
| 序数比例尺 | =，≠，公称比例尺+从最不重要到最重要排列示例：企业层次结构 |
| 区间标尺 | =，≠，，+，-序数比例+观测之间的距离分配给观测值的数字表示顺序任何连续值之间的差异与其他值相同60°温度不是 30°的两倍 |
| 比率标尺 | =，≠，，+，×，…区间比例尺+观测值比率20 美元是 10 美元的两倍 |

数据之间可以进行的另一个区别是连续数据和离散数据之间的区别。 连续数据可以取任何值。 属于区间和比率尺度的大多数数据都是连续的。

离散变量只能采用特定值，并且这些值之间有明确的边界。 例如，一栋房子可以有两个或三个房间，但不能有 2.75 个房间。 属于名义和顺序尺度的数据总是离散的。

MLlib 是 Spark 的机器学习库。 在本章中，我们将重点介绍机器学习的基本原理。

# 创建向量

在理解向量之前，让我们先来关注一下什么是点。 一个点就是一组数字。 这组数字或坐标定义点在空间中的位置。 坐标的数量决定了空间的尺寸。

我们可以将空间视觉化到三维。 三维以上的空间称为，称为**超空间**。 让我们使用这个空间隐喻。

让我们从一个人开始。 人有以下几个维度：

*   体重 / 重量，重力 / 砝码 / 重要性
*   高度 / 高地 / 身高 / 顶点
*   年龄 / 时代 / 寿命，使用年限 / 阶段

我们在这里工作的是三维空间。 因此，点(160，69，24)的解释是体重 160 磅，身高 69 英寸，年龄 24 岁。

### 备注

点和矢量是一回事。 矢量中的尺寸称为**特征**。 以另一种方式，我们可以将特征定义为被观察到的现象的单个可测量属性。

Spark 有局部向量和矩阵，也有分布矩阵。 分布式矩阵由一个或多个 RDDS 支持。 本地向量具有数字索引和双精度值，并存储在一台机器上。

MLlib 中有两类局部向量：密集向量和稀疏向量。 密集向量由其值组成的数组支持，而稀疏向量由两个并行数组支持，一个用于索引，另一个用于值。

因此，Person 数据(160，69，24)将使用密集向量表示为[160.0，69.0，24.0]，使用稀疏向量格式表示为 AS(3，[0，1，2]，[160.0，69.0，24.0])。

使向量变得稀疏还是密集取决于它有多少空值或 0。 让我们来看一个有 10,000 个值的向量，其中 9,000 个值为 0。 如果我们使用密集的矢量格式，这将是一个简单的结构，但 90%的空间将被浪费。 稀疏向量格式在这里会工作得更好，因为它只保留非零的索引。

稀疏数据非常常见，Spark 支持它的`libsvm`格式，即每行存储一个特征向量。

## How to Do It…

1.  启动 Spark Shell：

    ```scala
    $ spark-shell

    ```

2.  显式导入 MLlib 向量(不要与其他向量类混淆)：

    ```scala
    Scala> import org.apache.spark.mllib.linalg.{Vectors,Vector}

    ```

3.  创建密集向量：

    ```scala
    scala> val dvPerson = Vectors.dense(160.0,69.0,24.0)

    ```

4.  创建稀疏向量：

    ```scala
    scala> val svPerson = Vectors.sparse(3,Array(0,1,2),Array(160.0,69.0,24.0))

    ```

## 它是如何工作的.

下面的是`vectors.dense`的方法签名：

```scala
def dense(values: Array[Double]): Vector
```

在这里，值表示向量中的双精度元素数组。

下面是`Vectors.sparse`的方法签名：

```scala
def sparse(size: Int, indices: Array[Int], values: Array[Double]): Vector
```

这里，`size`表示向量的大小，`indices`是索引数组，`values`是双精度值数组。 请确保将`double`指定为数据类型或在至少一个值中使用 DECIMAL；否则它将为数据集抛出异常，因为数据集只有整数。

# 创建标签点

标记点是一个局部向量(稀疏/密集)，它有一个关联的标签。 标签数据用于监督学习，以帮助训练算法。 您将在下一章中对此有更多了解。

标签在`LabeledPoint`中存储为双精度值。 这意味着当您有分类标签时，需要将它们映射到双精度值。 您为一个类别分配了什么值是无关紧要的，只是一个方便的问题。

<colgroup><col style="text-align: left"> <col style="text-align: left"></colgroup> 
| 

类型 / 品种 / 象征 / 印刷文字

 | 

标签值

 |
| --- | --- |
| 二分分类 | 0 或 1 |
| 多类分类 | 0，1，2..。 |
| 回归 / 后退 / 逆行 / 复原 | 十进制值 |

## How to Do It…

1.  启动 Spark Shell：

    ```scala
    $spark-shell

    ```

2.  显式导入 MLlib 向量：

    ```scala
    scala> import org.apache.spark.mllib.linalg.{Vectors,Vector}

    ```

3.  导入`LabeledPoint`：

    ```scala
    scala> import org.apache.spark.mllib.regression.LabeledPoint

    ```

4.  创建具有正标签和密集向量的标签点：

    ```scala
    scala> val willBuySUV = LabeledPoint(1.0,Vectors.dense(300.0,80,40))

    ```

5.  使用负标签和密集向量创建标签点：

    ```scala
    scala> val willNotBuySUV = LabeledPoint(0.0,Vectors.dense(150.0,60,25))

    ```

6.  创建具有正标签和稀疏向量的标签点：

    ```scala
    scala> val willBuySUV = LabeledPoint(1.0,Vectors.sparse(3,Array(0,1,2),Array(300.0,80,40)))

    ```

7.  使用负标签和稀疏向量创建标签点：

    ```scala
    scala> val willNotBuySUV = LabeledPoint(0.0,Vectors.sparse(3,Array(0,1,2),Array(150.0,60,25)))

    ```

8.  创建具有相同数据的`libsvm`文件：

    ```scala
    $vi person_libsvm.txt (libsvm indices start with 1)
    0  1:150 2:60 3:25
    1  1:300 2:80 3:40

    ```

9.  上传`person_libsvm.txt`到`hdfs`：

    ```scala
    $ hdfs dfs -put person_libsvm.txt person_libsvm.txt

    ```

10.  再做几个导入：

    ```scala
    scala> import org.apache.spark.mllib.util.MLUtils
    scala> import org.apache.spark.rdd.RDD

    ```

11.  从`libsvm`文件加载数据：

    ```scala
    scala> val persons = MLUtils.loadLibSVMFile(sc,"person_libsvm.txt")

    ```

# 创建矩阵

矩阵只是一个表示多个特征向量的表。 可以在一台机器上存储的矩阵称为**局部矩阵**，可以跨分布的局部矩阵称为**分布式矩阵**。

局部矩阵具有基于整数的索引，而分布式矩阵具有基于长的索引。 两者的价值都是双倍的。

有三种类型的分布式矩阵：

*   `RowMatrix`：它将行作为特征向量。
*   `IndexedRowMatrix`：此也有行索引。
*   `CoordinateMatrix`：这个仅仅是`MatrixEntry`的一个矩阵。 `MatrixEntry`表示由其行和列索引表示的矩阵中的条目。

## How to Do It…

1.  启动Spark外壳：

    ```scala
    $spark-shell

    ```

2.  导入矩阵相关类：

    ```scala
    scala> import org.apache.spark.mllib.linalg.{Vectors,Matrix, Matrices}

    ```

3.  创建密集本地矩阵：

    ```scala
    scala> val people = Matrices.dense(3,2,Array(150d,60d,25d, 300d,80d,40d))

    ```

4.  创建`personRDD`作为矢量的 RDD：

    ```scala
    scala> val personRDD = sc.parallelize(List(Vectors.dense(150,60,25), Vectors.dense(300,80,40)))

    ```

5.  导入`RowMatrix`及相关类：

    ```scala
    scala> import org.apache.spark.mllib.linalg.distributed.{IndexedRow, IndexedRowMatrix,RowMatrix, CoordinateMatrix, MatrixEntry}

    ```

6.  创建`personRDD`：

    ```scala
    scala> val personMat = new RowMatrix(personRDD)

    ```

    的行矩阵
7.  打印行数：

    ```scala
    scala> print(personMat.numRows)

    ```

8.  打印列数：

    ```scala
    scala> print(personMat.numCols)

    ```

9.  创建索引行的 RDD：

    ```scala
    scala> val personRDD = sc.parallelize(List(IndexedRow(0L, Vectors.dense(150,60,25)), IndexedRow(1L, Vectors.dense(300,80,40))))

    ```

10.  创建索引行矩阵：

    ```scala
    scala> val pirmat = new IndexedRowMatrix(personRDD)

    ```

11.  打印行数：

    ```scala
    scala> print(pirmat.numRows)

    ```

12.  打印列数：

    ```scala
    scala> print(pirmat.numCols)

    ```

13.  将索引的行矩阵转换回行矩阵：

    ```scala
    scala> val personMat = pirmat.toRowMatrix

    ```

14.  创建矩阵条目的 RDD：

    ```scala
    scala> val meRDD = sc.parallelize(List(
     MatrixEntry(0,0,150),
     MatrixEntry(1,0,60),
    MatrixEntry(2,0,25),
    MatrixEntry(0,1,300),
    MatrixEntry(1,1,80),
    MatrixEntry(2,1,40)
    ))

    ```

15.  创建 __T1__ 矩阵坐标：__T0Собработельныйдля
16.  打印行数：

    ```scala
    scala> print(pcmat.numRows)

    ```

17.  打印列数：

    ```scala
    scala> print(pcmat.numCols)

    ```

# 计算汇总统计信息

汇总统计数据用于汇总观察结果，以获得数据的总体意义。 摘要包括以下内容：

*   数据的中心趋势-平均值、模式、中位数
*   数据分布-方差、标准差
*   边界条件-最小、最大

本食谱介绍了如何生成汇总统计数据。

## How to Do It…

1.  启动 Spark Shell：

    ```scala
    $ spark-shell

    ```

2.  导入矩阵相关类：

    ```scala
    scala> import org.apache.spark.mllib.linalg.{Vectors,Vector}
    scala> import org.apache.spark.mllib.stat.Statistics

    ```

3.  创建`personRDD`作为矢量的 RDD：

    ```scala
    scala> val personRDD = sc.parallelize(List(Vectors.dense(150,60,25), Vectors.dense(300,80,40)))

    ```

4.  计算列汇总统计信息：

    ```scala
    scala> val summary = Statistics.colStats(personRDD)

    ```

5.  打印此摘要的平均值：

    ```scala
    scala> print(summary.mean)

    ```

6.  打印差异：

    ```scala
    scala> print(summary.variance)

    ```

7.  打印每列中的非零值：

    ```scala
    scala> print(summary.numNonzeros)

    ```

8.  打印样本大小：

    ```scala
    scala> print(summary.count)

    ```

9.  打印每列的最大值：

    ```scala
    scala> print(summary.max)

    ```

# 计算相关性

相关性是两个变量之间的统计关系，因此当一个变量发生变化时，另一个变量也会发生变化。 相关分析衡量这两个变量的相关程度。

如果一个变量的增加导致另一个变量的增加，则称为，称为**正相关**。 如果一个变量的增加导致另一个变量的减少，则它是**负相关**。

Spark 支持两种关联算法：Pearson 和 Spearman。 皮尔逊算法适用于两个连续变量，比如一个人的身高和体重，或者房子的大小和房价。 斯皮尔曼处理一个连续变量和一个分类变量，例如，邮政编码和房价。

## 做好准备

让我们使用一些真实的数据，这样我们可以更有意义地计算相关性。 以下是 2014 年初加利福尼亚州萨拉托加市的房屋面积和价格：

<colgroup><col style="text-align: left"> <col style="text-align: left"></colgroup> 
| 

房屋面积(平方英尺)

 | 

普赖斯 （人名）

 |
| --- | --- |
| 2100 | $1，620，000 |
| 二三零零 | $1，690，000 |
| 2046 年 | $1，400，000 |
| 4314 | $2，000，000 |
| 1244 | $1，060，000 |
| 4608 | $3，830，000 |
| 2173 | $1，230，000 |
| 2750 | $2，400，000 |
| 4010 | $3，380，000 |
| 一九五九年 | $1，480，000 |

## How to Do It…

1.  启动 Spark Shell：

    ```scala
    $ spark-shell

    ```

2.  导入统计信息及相关类：

    ```scala
    scala> import org.apache.spark.mllib.linalg._
    scala> import org.apache.spark.mllib.stat.Statistics

    ```

3.  创建房屋大小的 RDD：

    ```scala
    scala> val sizes = sc.parallelize(List(2100, 2300, 2046, 4314, 1244, 4608, 2173, 2750, 4010, 1959.0))

    ```

4.  创建房价 RDD：

    ```scala
    scala> val prices = sc.parallelize(List(1620000 , 1690000, 1400000, 2000000, 1060000, 3830000, 1230000, 2400000, 3380000, 1480000.00))

    ```

5.  Compute the correlation:

    ```scala
    scala> val correlation = Statistics.corr(sizes,prices)
    correlation: Double = 0.8577177736252577 

    ```

    `0.85`表示非常强的正相关性。

    因为我们这里没有特定的算法，所以默认情况下，它是 Pearson 算法。 重载`corr`方法以将算法名称作为第三个参数。

6.  计算与皮尔逊的相关性：

    ```scala
    scala> val correlation = Statistics.corr(sizes,prices)

    ```

7.  计算与 Spearman 的相关性：

    ```scala
    scala> val correlation = Statistics.corr(sizes,prices,"spearman")

    ```

上例中的两个变量都是连续的，因此 Spearman 假定大小是离散的。 使用 Spearman 的一个更好的例子是邮政编码与价格的对比。

# 做假设检验

假设检验是确定给定假设为真的概率的一种方法。 让我们假设一个样本数据表明，女性倾向于更多地投票给民主党。 对于更多的人口来说，这可能是真的，也可能不是。 如果样本数据中恰好存在此模式，该怎么办？

看待假设检验目标的另一种方式是回答这个问题：如果样本中有模式，那么模式偶然出现的可能性有多大？

我们该怎么做？ 有一句话说，证明一件事的最好方法就是试着反驳它。

证明错误的假设称为**零假设**。 假设检验适用于分类数据。 让我们来看一个关于党派关系的快速民调的例子。

<colgroup><col style="text-align: left"> <col style="text-align: left"> <col style="text-align: left"></colgroup> 
| 

群 / 政党 / 聚会

 | 

马累 （马尔代夫首都）

 | 

雌性的 / 母性的 / 女性特有的 / 女性的

 |
| --- | --- | --- |
| （美国的）民主党 | 32 位 | 41 |
| 共和党 （美国两大政党之一） | 28 | 25 个 |
| 自主的 / 独立的 / 分离的 / 单独的 | 34 | 26 |

## How to Do It…

1.  启动 Spark Shell：

    ```scala
    $ spark-shell

    ```

2.  导入相关类：

    ```scala
    scala> import org.apache.spark.mllib.stat.Statistics
    scala> import org.apache.spark.mllib.linalg.{Vector,Vectors}
    scala> import org.apache.spark.mllib.linalg.{Matrix, Matrices}

    ```

3.  为民主党创建一个载体：

    ```scala
    scala> val dems = Vectors.dense(32.0,41.0)

    ```

4.  为共和党创建载体：

    ```scala
    scala> val reps= Vectors.dense(28.0,25.0)

    ```

5.  为独立对象创建向量：

    ```scala
    scala> val indies = Vectors.dense(34.0,26.0)

    ```

6.  对均匀分布的观测数据进行卡方拟合优度检验：

    ```scala
    scala> val dfit = Statistics.chiSqTest(dems)
    scala> val rfit = Statistics.chiSqTest(reps)
    scala> val ifit = Statistics.chiSqTest(indies)

    ```

7.  打印拟合优度结果：

    ```scala
    scala> print(dfit)
    scala> print(rfit)
    scala> print(ifit)

    ```

8.  创建输入矩阵：

    ```scala
    scala> val mat = Matrices.dense(2,3,Array(32.0,41.0, 28.0,25.0, 34.0,26.0))

    ```

9.  做卡方独立性检验：

    ```scala
    scala> val in = Statistics.chiSqTest(mat)

    ```

10.  打印独立性测试结果：

    ```scala
    scala> print(in)

    ```

# 使用 ML 创建机器学习管道

Spark ML 是 Spark 中的一个新库，用于构建机器学习管道。 这个库是与 MLlib 一起开发的。 它有助于将多个机器学习算法组合到单个管道中，并使用 DataFrame 作为数据集。

## 做好准备

让我们首先了解一下 Spark ML 中的一些基本概念。 它使用转换器将一个 DataFrame 转换为另一个 DataFrame。 简单转换的一个示例可以是追加一列。 您可以将其等同于关系世界中的“ALTER TABLE”。

另一方面，估计器代表一种机器学习算法，它从数据中学习。 估计器的输入是 DataFrame，输出是转换器。 每个 Estimator 都有一个`fit()`方法，用于训练算法。

机器学习管道定义为一系列阶段；每个阶段既可以是估计器，也可以是转换器。

我们将在这个食谱中使用的例子是某人是不是篮球运动员。 为此，我们将有一个由一个估计器和一个转换器组成的管道。

估计器获取训练数据来训练算法，然后变压器进行预测。

现在，假设`LogisticRegression`是我们正在使用的机器学习算法。 我们将在后续章节中解释有关`LogisticRegression`以及其他算法的详细信息。

## How to Do It…

1.  启动 Spark Shell：

    ```scala
    $ spark-shell

    ```

2.  执行导入：

    ```scala
    scala> import org.apache.spark.mllib.linalg.{Vector,Vectors}
    scala> import org.apache.spark.mllib.regression.LabeledPoint
    scala> import org.apache.spark.ml.classification.LogisticRegression

    ```

3.  为身高 80 英寸、体重 250 磅的篮球运动员勒布朗创建一个标签点：

    ```scala
    scala> val lebron = LabeledPoint(1.0,Vectors.dense(80.0,250.0))

    ```

4.  为不是篮球运动员、身高 70 英寸、体重 150 磅的 Tim 创建一个标签点：

    ```scala
    scala> val tim = LabeledPoint(0.0,Vectors.dense(70.0,150.0))

    ```

5.  为身高 80 英寸、体重 207 磅的篮球运动员布列塔尼创建一个标签点：

    ```scala
    scala> val brittany = LabeledPoint(1.0,Vectors.dense(80.0,207.0))

    ```

6.  为不是篮球运动员、身高 65 英寸、体重 120 磅的 Stacey 创建一个标签点：

    ```scala
    scala> val stacey = LabeledPoint(0.0,Vectors.dense(65.0,120.0))

    ```

7.  创建培训 RDD：

    ```scala
    scala> val trainingRDD = sc.parallelize(List(lebron,tim,brittany,stacey))

    ```

8.  创建培训数据帧：

    ```scala
    scala> val trainingDF = trainingRDD.toDF

    ```

9.  创建 __T0__ 估计器：Колибриобработается
10.  通过用训练数据帧

    ```scala
    scala> val transformer = estimator.fit(trainingDF)

    ```

    拟合估计器来创建转换器
11.  现在，让我们创建一个测试数据-John 身高 90 英寸，体重 270 磅，是一名篮球运动员：

    ```scala
    scala> val john = Vectors.dense(90.0,270.0)

    ```

12.  创建另一个测试数据-Tom 身高 62 英寸，体重 150 磅，不是篮球运动员：

    ```scala
    scala> val tom = Vectors.dense(62.0,120.0)

    ```

13.  创建培训 RDD：

    ```scala
    scala> val testRDD = sc.parallelize(List(john,tom))

    ```

14.  创建`Features`案例类：

    ```scala
    scala> case class Feature(v:Vector)

    ```

15.  将`testRDD`映射到`Features`的 RDD：

    ```scala
    scala> val featuresRDD = testRDD.map( v => Feature(v))

    ```

16.  将`featuresRDD`转换为列名为`"features"`的 DataFrame：

    ```scala
    scala> val featuresDF = featuresRDD.toDF("features")

    ```

17.  通过将`predictions`列添加到`featuresDF`来转换`featuresDF`：

    ```scala
    scala> val predictionsDF = transformer.transform(featuresDF)

    ```

18.  打印`predictionsDF`：

    ```scala
    scala> predictionsDF.foreach(println)

    ```

19.  `PredictionDF`，如您所见，除了保留特性之外，还创建了三个列-`rawPrediction`、`probability`和`prediction`。 让我们只选择`features`和`prediction`：

    ```scala
    scala> val shorterPredictionsDF = predictionsDF.select("features","prediction")

    ```

20.  将预测重命名为`isBasketBallPlayer`：

    ```scala
    scala> val playerDF = shorterPredictionsDF.toDF("features","isBasketBallPlayer")

    ```

21.  打印`playerDF`的架构：

    ```scala
    scala> playerDF.printSchema

    ```