# 第八章。火花图处理

图是计算机科学中的一个数学概念和数据结构。它在许多现实世界的用例中有巨大的应用。它用于模拟实体之间的成对关系。这里的实体称为顶点，两个顶点由一条边连接。一个图由一组顶点和连接它们的边组成。

从概念上来说，这是一个看似简单的抽象，但是当涉及到处理大量的顶点和边时，它是计算密集型的，并且会消耗大量的处理时间和计算资源。下面是一个具有四个顶点和三条边的图的表示:

![Spark Graph Processing](graphics/B05289_08_01_new.jpg)

图 1

我们将在本章中讨论以下主题:

*   图形及其用途
*   GraphX 库
*   PageRank 算法
*   连通分量算法
*   图形框架
*   图形查询

# 理解图形及其用法

有许多应用程序结构可以建模为图形。在社交网络应用中，用户之间的关系可以被建模为图，其中用户形成图的顶点，用户之间的关系形成图的边。在多阶段作业调度应用程序中，单个任务构成图的顶点，任务的顺序构成图的边。在道路交通建模系统中，城镇形成图的顶点，连接城镇的道路形成边。

给定图的边有一个非常重要的性质，即*连接的方向*。在许多用例中，连接的方向并不重要。通过道路连接城市就是这样一个例子。但是，如果用例是在一个城市内产生行驶方向，那么交通路口之间的连接是有方向的。走任意两个交通路口都会有道路连通，但也有可能是单行道。所以这完全取决于交通的流向。如果道路从交通路口 J1 到 J2 开放，但从 J2 到 J1 关闭，则行驶方向图将具有从 J1 到 J2 的连通性，而不是从 J2 到 J1 的连通性。在这种情况下，连接 J1 和 J2 的边有一个方向。如果 J2 和 both 乐队之间的道路是双向开放的，那么连接 J2 和 both 乐队的边缘就没有方向。所有边都有方向的图叫做**有向图**。

### 类型

当用图形表示一个图形时，必须在有向图的边上给出方向。如果它不是有向图，那么边可以表示成没有任何方向，或者两边都有方向。这取决于个人的选择。*图 1* 不是有向图，而是用边所连接的两个顶点的方向来表示。

在*图 2* 中，社交网络应用用例中两个用户之间的关系表示为一个图。用户形成顶点，用户之间的关系形成边。用户 A 跟随用户 b，同时用户 A 是用户 b 的儿子，在这个图中，有两条平行的边共享同一个源和目的顶点。包含平行边的图称为多重图。*图 2* 所示的图也是有向图。这是一个很好的例子**指导多重图形**。

![Understanding graphs and their usage](graphics/image_08_002.jpg)

图 2

在现实世界的用例中，图的顶点和边代表现实世界的实体。这些实体具有属性。例如，在来自社交网络应用程序的用户的社交连接图中，用户形成顶点，并且用户具有许多属性，例如姓名、电子邮件、电话号码等。类似地，用户之间的关系形成了图的边，连接用户顶点的边可以具有诸如关系的属性。任何图形处理应用程序库都应该足够灵活，能够将任何类型的属性附加到图形的顶点和边上。

# 火花图形库

对于图形处理，开源世界中有许多库。Giraph、Pregel、GraphLab 和 Spark GraphX 就是其中的一些。Spark GraphX 是最近进入这个领域的公司之一。

Spark GraphX 有什么特别之处？Spark GraphX 是一个建立在 Spark 数据处理框架之上的图形处理库。与其他图形处理库相比，Spark GraphX 具有真正的优势。它可以利用 Spark 的所有数据处理能力。然而，在现实中，图形处理算法的性能并不是唯一需要考虑的方面。

在许多应用程序中，需要建模为图形的数据自然不会以这种形式存在。在许多用例中，除了图形处理之外，还花费了大量的处理器时间和其他计算资源来获得正确格式的数据，以便可以应用图形处理算法。这是 Spark 数据处理框架和 Spark GraphX 库的结合实现其价值的最佳点。使用 Spark 工具包中的大量工具可以轻松完成数据处理工作，使数据准备好被 Spark GraphX 使用。总之，Spark GraphX 库是 Spark 系列的一部分，它结合了 Spark 的核心数据处理能力和非常易于使用的图形处理库。

再次回顾更大的图景，如*图 3* 所示，在进入用例之前，设置上下文并看看这里讨论了什么。与其他章节不同，在本章中，代码示例将只在 Scala 中完成，因为 Spark GraphX 库目前只有 Scala 应用编程接口可用。

![The Spark GraphX library](graphics/image_08_003.jpg)

图 3

## GraphX 概述

在任何真实的用例中，理解包含顶点和边的图的概念都是很容易的。但是当涉及到实现时，即使是好的设计者和程序员也不会很好地理解这种数据结构。原因很简单:与列表、集合、映射、队列等其他无处不在的数据结构不同，图形在大多数应用程序中并不常用。考虑到这一点，在使用一些真实的用例之前，这些概念被缓慢而稳定地引入，一步一步，用简单而琐碎的例子。

Spark GraphX 库最重要的方面是数据类型 Graph，它扩展了 Spark **弹性分布式数据集** ( **RDD** )并引入了新的图抽象。Spark GraphX 中的图抽象是一个有向多重图，它的属性附着在所有的顶点和边上。这些顶点和边的属性可以是 Scala 类型系统支持的用户定义类型。这些类型在图形类型中被参数化。给定的图可能需要顶点或边具有不同的数据类型。这可以通过使用继承层次结构相关的类型系统来实现。除了所有这些基本的基本规则之外，该库还包括一组图形构建器和算法。

图中的顶点由唯一的 64 位长标识符`org.apache.spark.graphx.VertexId`标识。也可以使用简单的 Scala 类型 Long 来代替 VertexId 类型。除此之外，顶点可以采用任何类型作为属性。图中的边应该有一个源顶点标识符、一个目标顶点标识符和任何类型的属性。

*图 4* 显示了顶点属性为字符串类型、边属性为字符串类型的图形。除了属性之外，每个顶点都有一个唯一的标识符，每个边都有一个源顶点编号和目标顶点编号。

![GraphX overview](graphics/image_08_004.jpg)

图 4

在处理图形时，有一些方法可以得到顶点和边。但是在进行处理时，图中这些独立的对象可能是不够的。

如前所述，顶点有其唯一的标识符和属性。边由其源顶点和目标顶点唯一标识。为了在图形处理应用程序中轻松处理每条边，Spark GraphX 库的三元组抽象提供了一种从单个对象访问源顶点、目标顶点和边的属性的简单方法。

下面的 Scala 代码片段用于使用 Spark GraphX 库创建图 4*所示的图形。创建图形后，会在图形上调用许多方法来展示图形的各种属性。在 Scala REPL 提示符下，尝试以下语句:*

 *```
scala> import org.apache.spark._
  import org.apache.spark._
    scala> import org.apache.spark.graphx._

	import org.apache.spark.graphx._
	scala> import org.apache.spark.rdd.RDD
	import org.apache.spark.rdd.RDD
  scala> //Create an RDD of users containing tuple values with a mandatory
  Long and another String type as the property of the vertex
  scala> val users: RDD[(Long, String)] = sc.parallelize(Array((1L,
  "Thomas"), (2L, "Krish"),(3L, "Mathew")))
  users: org.apache.spark.rdd.RDD[(Long, String)] = ParallelCollectionRDD[0]
  at parallelize at <console>:31
  scala> //Created an RDD of Edge type with String type as the property of the edge
  scala> val userRelationships: RDD[Edge[String]] = sc.parallelize(Array(Edge(1L, 2L, "Follows"),    Edge(1L, 2L, "Son"),Edge(2L, 3L, "Follows")))
userRelationships: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[String]] = ParallelCollectionRDD[1] at parallelize at <console>:31
    scala> //Create a graph containing the vertex and edge RDDs as created beforescala> val userGraph = Graph(users, userRelationships)
	userGraph: org.apache.spark.graphx.Graph[String,String] = org.apache.spark.graphx.impl.GraphImpl@ed5cf29

	scala> //Number of edges in the graph
	scala> userGraph.numEdges
      res3: Long = 3
    scala> //Number of vertices in the graph
	scala> userGraph.numVertices
      res4: Long = 3
	  scala> //Number of edges coming to each of the vertex. 
	  scala> userGraph.inDegrees
res7: org.apache.spark.graphx.VertexRDD[Int] = VertexRDDImpl[19] at RDD at
 VertexRDD.scala:57
scala> //The first element in the tuple is the vertex id and the second
 element in the tuple is the number of edges coming to that vertex
 scala> userGraph.inDegrees.foreach(println)
      (3,1)

      (2,2)
    scala> //Number of edges going out of each of the vertex. scala> userGraph.outDegrees
	res9: org.apache.spark.graphx.VertexRDD[Int] = VertexRDDImpl[23] at RDD at VertexRDD.scala:57
    scala> //The first element in the tuple is the vertex id and the second
	element in the tuple is the number of edges going out of that vertex
	scala> userGraph.outDegrees.foreach(println)
      (1,2)

      (2,1)
    scala> //Total number of edges coming in and going out of each vertex. 
	scala> userGraph.degrees
res12: org.apache.spark.graphx.VertexRDD[Int] = VertexRDDImpl[27] at RDD at
 VertexRDD.scala:57
    scala> //The first element in the tuple is the vertex id and the second 
	element in the tuple is the total number of edges coming in and going out of that vertex.
	scala> userGraph.degrees.foreach(println)
      (1,2)

      (2,3)

      (3,1)
    scala> //Get the vertices of the graph
	scala> userGraph.vertices
res11: org.apache.spark.graphx.VertexRDD[String] = VertexRDDImpl[11] at RDD at VertexRDD.scala:57
    scala> //Get all the vertices with the vertex number and the property as a tuplescala> userGraph.vertices.foreach(println)
      (1,Thomas)

      (3,Mathew)

      (2,Krish)
    scala> //Get the edges of the graph
	scala> userGraph.edges
res15: org.apache.spark.graphx.EdgeRDD[String] = EdgeRDDImpl[13] at RDD at
 EdgeRDD.scala:41
    scala> //Get all the edges properties with source and destination vertex numbers
	scala> userGraph.edges.foreach(println)
      Edge(1,2,Follows)

      Edge(1,2,Son)

      Edge(2,3,Follows)
    scala> //Get the triplets of the graph
	scala> userGraph.triplets
res18: org.apache.spark.rdd.RDD[org.apache.spark.graphx.EdgeTriplet[String,String]]
 = MapPartitionsRDD[32] at mapPartitions at GraphImpl.scala:48
    scala> userGraph.triplets.foreach(println)
	((1,Thomas),(2,Krish),Follows)
	((1,Thomas),(2,Krish),Son)
	((2,Krish),(3,Mathew),Follows)

```

读者将熟悉使用 RDDs 的 Spark 编程。前面的代码片段阐明了使用关系数据库构建图的顶点和边的过程。rdd 可以使用保存在各种数据存储中的数据来构建。在真实的用例中，大部分时间数据将来自外部来源，例如 NoSQL 数据存储，并且有方法使用这样的数据构建关系数据库。一旦构建了关系数据库，就可以用它来构建图。

前面的代码片段还解释了图中可用的各种方法，以获得给定图的所有必要细节。这里介绍的示例用例就大小而言是一个非常小的图。在现实世界的用例中，一个图的顶点和边的数量可以是数百万。由于所有这些抽象都是作为 RDD 实现的，所有固有的不变性、分区、分布和并行处理的优点都是开箱即用的，因此使图形处理具有高度的可扩展性。最后，下表显示了顶点和边的表示方式:

**顶点表**:

<colgroup><col> <col></colgroup> 
| **氧化铈** | **顶点属性** |
| one | 托马斯 |
| Two | 克里斯 |
| three | 马太福音 |

**边桌:**

<colgroup><col> <col> <col></colgroup> 
| **氧化铈来源** | **目的地氧化铈** | **边缘属性** |
| one | Two | 遵照 |
| one | Two | 儿子 |
| Two | three | 遵照 |

**三联表**:

<colgroup><col> <col> <col> <col> <col></colgroup> 
| **氧化铈来源** | **目的地氧化铈** | **源顶点属性** | **边缘属性** | **目标顶点属性** |
| one | Two | 托马斯 | 遵照 | 克里斯 |
| one | Two | 托马斯 | 儿子 | 克里斯 |
| Two | three | 克里斯 | 遵照 | 马太福音 |

### 注

需要注意的是，这些表格仅用于解释目的。真正的内部代表遵循 RDD 代表的规则和条例。

如果任何东西被描绘成一个 RDD，它必然会被分割和分配。但是，如果分割和分布是自由进行的，对图形没有任何控制，那么当涉及到图形处理性能时，它将是次优的。正因为如此，Spark GraphX 库的创建者提前考虑了这个问题，并实现了一个图划分策略，以便将图优化表示为 RDDs。

## 图形分割

了解一下图形关系数据库是如何划分的以及如何分布在不同的分区中是很重要的。这对于高级优化非常有用，这些优化决定了作为图形组成部分的各种关系数据库的划分和分布。

一般来说，一个给定的图有三个关系数据库。除了顶点 RDD 和边 RDD，内部还使用了一个 RDD，那就是路由 RDD。为了获得最佳性能，形成给定边所需的所有顶点都保存在存储边的同一分区中。如果给定的顶点参与多个边，并且这些边位于不同的分区中，那么这个特定的顶点可以存储在多个分区中。

为了跟踪给定顶点冗余存储的分区，还会维护一个路由 RDD，包含顶点详细信息和每个顶点可用的分区。

*图 5* 对此进行了解释:

![Graph partitioning](graphics/image_08_005.jpg)

图 5

在*图 5* 中，假设边被划分为分区 1 和分区 2。还假设顶点被分成分区 1 和 2。

在分区 1 中，边所需的所有顶点都在本地可用。但是在分区 2 中，只有一个边的顶点在本地可用。因此，丢失的顶点也存储在分区 2 中，以便所有需要的顶点都在本地可用。

为了跟踪复制，顶点路由 RDD 维护给定顶点可用的分区号。在*图 5* 中，在顶点路由 RDD 中，标注符号用于显示复制这些顶点的分区。这样，在处理边或三元组时，与组成顶点相关的所有信息在本地都是可用的，性能将是高度最优的。因为关系数据库是不可变的，所以即使它们存储在多个分区中，与信息被改变相关的问题也被消除了。

## 图形处理

向用户展示的图形的组成元素是顶点 RDD 和边 RDD。就像任何其他数据结构一样，图形也会因为底层数据的变化而经历很多变化。为了使所需的图形操作支持各种用例，有许多算法可用，使用这些算法可以处理隐藏在图形数据结构中的数据，以产生期望的业务结果。在进入处理图形的算法之前，最好先了解一些使用航空旅行用例进行图形处理的基础知识。

假设一个人正在努力寻找一张从曼彻斯特到班加罗尔的廉价往返机票。在出行偏好中，这个人已经提到，他/她不在乎停靠的次数，但价格应该是最低的。假设机票预订系统为前进和返回旅程选择了相同的停靠点，并以最便宜的价格生成了以下路线或旅程:

曼彻斯特→伦敦→科伦坡→班加罗尔

班加罗尔→科伦坡→伦敦→曼彻斯特

这个路线计划是一个完美的图表例子。如果向前行程被认为是一个图，而返回行程被认为是另一个图，则返回行程图可以通过反转向前行程图来产生。在 Scala REPL 提示符下，尝试以下语句:

```
scala> import org.apache.spark._
import org.apache.spark._
scala> import org.apache.spark.graphx._
import org.apache.spark.graphx._
scala> import org.apache.spark.rdd.RDD
import org.apache.spark.rdd.RDD
scala> //Create the vertices with the stops
scala> val stops: RDD[(Long, String)] = sc.parallelize(Array((1L, "Manchester"), (2L, "London"),(3L, "Colombo"), (4L, "Bangalore")))
stops: org.apache.spark.rdd.RDD[(Long, String)] = ParallelCollectionRDD[33] at parallelize at <console>:38
scala> //Create the edges with travel legs
scala> val legs: RDD[Edge[String]] = sc.parallelize(Array(Edge(1L, 2L, "air"),    Edge(2L, 3L, "air"),Edge(3L, 4L, "air"))) 
legs: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[String]] = ParallelCollectionRDD[34] at parallelize at <console>:38 
scala> //Create the onward journey graph
scala> val onwardJourney = Graph(stops, legs)onwardJourney: org.apache.spark.graphx.Graph[String,String] = org.apache.spark.graphx.impl.GraphImpl@190ec769scala> onwardJourney.triplets.map(triplet => (triplet.srcId, (triplet.srcAttr, triplet.dstAttr))).sortByKey().collect().foreach(println)
(1,(Manchester,London))
(2,(London,Colombo))
(3,(Colombo,Bangalore))
scala> val returnJourney = onwardJourney.reversereturnJourney: org.apache.spark.graphx.Graph[String,String] = org.apache.spark.graphx.impl.GraphImpl@60035f1e
scala> returnJourney.triplets.map(triplet => (triplet.srcId, (triplet.srcAttr,triplet.dstAttr))).sortByKey(ascending=false).collect().foreach(println)
(4,(Bangalore,Colombo))
(3,(Colombo,London))
(2,(London,Manchester))

```

在返程航段中，前进航段的来源和目的地是相反的。当图形反转时，只有边的源顶点和目标顶点反转，顶点的标识保持不变。

换句话说，每个顶点的顶点标识符保持不变。处理图形时，知道三元组属性的名称很重要。它们对于编写程序和处理图形很有用。作为同一个 Scala REPL 会话的继续，尝试以下语句:

```
scala> returnJourney.triplets.map(triplet => (triplet.srcId,triplet.dstId,triplet.attr,triplet.srcAttr,triplet.dstAttr)).foreach(println) 
(2,1,air,London,Manchester) 
(3,2,air,Colombo,London) 
(4,3,air,Bangalore,Colombo) 

```

下表列出了可用于处理图表和从图表中提取所需数据的三元组的属性。可以交叉验证前面的代码片段和下表，以便完全理解:

<colgroup><col> <col></colgroup> 
| **三元组属性** | **描述** |
| `srcId` | 源顶点标识符 |
| `dstId` | 目标顶点标识符 |
| `attr` | 边缘属性 |
| `srcAttr` | 源顶点属性 |
| `dstAttr` | 目标顶点属性 |

在图中，顶点是关系图，边是关系图，正因为如此，变换是可能的。

现在，为了演示图形转换，使用了相同的用例，只做了一点小改动。假设一家旅行社从航空公司获得了所选航线的特别折扣价格。旅行社决定保持折扣，并向他/她的顾客提供市场价格，为此，他/她在航空公司给出的价格上加 10%。该旅行社注意到机场名称显示不一致，希望确保在整个网站上显示时有一致的表示，并决定将所有停靠站名称改为大写。作为同一个 Scala REPL 会话的继续，尝试以下语句:

```
 scala> // Create the vertices 
scala> val stops: RDD[(Long, String)] = sc.parallelize(Array((1L,
 "Manchester"), (2L, "London"),(3L, "Colombo"), (4L, "Bangalore"))) 
stops: org.apache.spark.rdd.RDD[(Long, String)] = ParallelCollectionRDD[66] at parallelize at <console>:38 
scala> //Create the edges 
scala> val legs: RDD[Edge[Long]] = sc.parallelize(Array(Edge(1L, 2L, 50L),    Edge(2L, 3L, 100L),Edge(3L, 4L, 80L))) 
legs: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Long]] = ParallelCollectionRDD[67] at parallelize at <console>:38 
scala> //Create the graph using the vertices and edges 
scala> val journey = Graph(stops, legs) 
journey: org.apache.spark.graphx.Graph[String,Long] = org.apache.spark.graphx.impl.GraphImpl@8746ad5 
scala> //Convert the stop names to upper case 
scala> val newStops = journey.vertices.map {case (id, name) => (id, name.toUpperCase)} 
newStops: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, String)] = MapPartitionsRDD[80] at map at <console>:44 
scala> //Get the edges from the selected journey and add 10% price to the original price 
scala> val newLegs = journey.edges.map { case Edge(src, dst, prop) => Edge(src, dst, (prop + (0.1*prop))) } 
newLegs: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Double]] = MapPartitionsRDD[81] at map at <console>:44 
scala> //Create a new graph with the original vertices and the new edges 
scala> val newJourney = Graph(newStops, newLegs) 
newJourney: org.apache.spark.graphx.Graph[String,Double]
 = org.apache.spark.graphx.impl.GraphImpl@3c929623 
scala> //Print the contents of the original graph 
scala> journey.triplets.foreach(println) 
((1,Manchester),(2,London),50) 
((3,Colombo),(4,Bangalore),80) 
((2,London),(3,Colombo),100) 
scala> //Print the contents of the transformed graph 
scala>  newJourney.triplets.foreach(println) 
((2,LONDON),(3,COLOMBO),110.0) 
((3,COLOMBO),(4,BANGALORE),88.0) 
((1,MANCHESTER),(2,LONDON),55.0) 

```

本质上，这些转变是真正的 RDD 转变。如果对这些不同的关系数据库是如何拼凑成一个图形有一个概念性的理解，任何精通 RDD 编程的程序员都能够很好地处理图形。这是 Spark 统一编程模型的力量的又一个证明。

前面的用例在顶点和边 RDDs 上进行了映射转换。类似地，过滤器转换是另一种常用的有用类型。除此之外，所有的变换和动作都可以用来处理顶点和边的关系数据。

## 图形结构处理

在前一节中，一种类型的图形处理是通过单独处理所需的顶点或边来完成的。这种方法的一个缺点是处理要经历三个不同的阶段，如下所示:

*   从图中提取顶点或边
*   处理顶点或边
*   用处理过的顶点和边重新创建一个新图形

这是繁琐的，容易出现用户编程错误。为了避免这个问题，在 Spark GraphX 库中有一些结构操作符可以让用户将图形作为一个单独的单元来处理，从而生成一个新的图形。

在前一节中已经讨论过一个重要的结构操作，它是图的反转，产生一个所有边的方向都反转的新图。另一个经常使用的结构操作是从给定的图中提取子图。最终的子图可以是整个父图本身，也可以是父图的子集，这取决于对父图所做的操作。

从外部来源的数据创建图形时，边可能有无效的顶点。如果顶点和边是根据来自两个不同来源或不同应用程序的数据创建的，这是很有可能的。有了这些顶点和边，如果创建了一个图，一些边将有无效的顶点，处理将导致意想不到的结果。下面是一个用例，其中一些包含无效顶点的边和修剪是通过使用结构运算符来消除的。在 Scala REPL 提示符下，尝试以下语句:

```
scala> import org.apache.spark._
  import org.apache.spark._    scala> import org.apache.spark.graphx._
  import org.apache.spark.graphx._    scala> import org.apache.spark.rdd.RDD
  import org.apache.spark.rdd.RDD    scala> //Create an RDD of users containing tuple values with a mandatory
  Long and another String type as the property of the vertex
  scala> val users: RDD[(Long, String)] = sc.parallelize(Array((1L,
  "Thomas"), (2L, "Krish"),(3L, "Mathew")))
users: org.apache.spark.rdd.RDD[(Long, String)] = ParallelCollectionRDD[104]
 at parallelize at <console>:45
    scala> //Created an RDD of Edge type with String type as the property of
	the edge
	scala> val userRelationships: RDD[Edge[String]] =
	sc.parallelize(Array(Edge(1L, 2L, "Follows"), Edge(1L, 2L,
	"Son"),Edge(2L, 3L, "Follows"), Edge(1L, 4L, "Follows"), Edge(3L, 4L, "Follows")))
	userRelationships:
	org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[String]] =
	ParallelCollectionRDD[105] at parallelize at <console>:45
    scala> //Create a vertex property object to fill in if an invalid vertex id is given in the edge
	scala> val missingUser = "Missing"
missingUser: String = Missing
    scala> //Create a graph containing the vertex and edge RDDs as created
	before
	scala> val userGraph = Graph(users, userRelationships, missingUser)
userGraph: org.apache.spark.graphx.Graph[String,String] = org.apache.spark.graphx.impl.GraphImpl@43baf0b9
    scala> //List the graph triplets and find some of the invalid vertex ids given and for them the missing vertex property is assigned with the value "Missing"scala> userGraph.triplets.foreach(println)
      ((3,Mathew),(4,Missing),Follows)  
      ((1,Thomas),(2,Krish),Son)    
      ((2,Krish),(3,Mathew),Follows)    
      ((1,Thomas),(2,Krish),Follows)    
      ((1,Thomas),(4,Missing),Follows)
    scala> //Since the edges with the invalid vertices are invalid too, filter out
	those vertices and create a valid graph. The vertex predicate here can be any valid filter condition of a vertex. Similar to vertex predicate, if the filtering is to be done on the edges, instead of the vpred, use epred as the edge predicate.
	scala> val fixedUserGraph = userGraph.subgraph(vpred = (vertexId, attribute) => attribute != "Missing")
fixedUserGraph: org.apache.spark.graphx.Graph[String,String] = org.apache.spark.graphx.impl.GraphImpl@233b5c71 
 scala> fixedUserGraph.triplets.foreach(println)
  ((2,Krish),(3,Mathew),Follows)
  ((1,Thomas),(2,Krish),Follows)
  ((1,Thomas),(2,Krish),Son)

```

在巨大的图形中，有时取决于用例，可能有很多平行的边。在某些使用情况下，可以合并平行边的数据，只保留一条边，而不是保留许多平行边。在前面的用例中，没有任何无效边的最终图形，存在平行边，一个具有属性`Follows`，另一个具有`Son`，它们具有相同的源和目的顶点。

将这些平行边合并成一条边，属性从平行边连接起来，这样可以减少边的数量，而不会丢失信息。这是通过图的组边结构操作来完成的。作为同一个 Scala REPL 会话的继续，尝试以下语句:

```
scala> // Import the partition strategy classes 
scala> import org.apache.spark.graphx.PartitionStrategy._ 
import org.apache.spark.graphx.PartitionStrategy._ 
scala> // Partition the user graph. This is required to group the edges 
scala> val partitionedUserGraph = fixedUserGraph.partitionBy(CanonicalRandomVertexCut) 
partitionedUserGraph: org.apache.spark.graphx.Graph[String,String] = org.apache.spark.graphx.impl.GraphImpl@5749147e 
scala> // Generate the graph without parallel edges and combine the properties of duplicate edges 
scala> val graphWithoutParallelEdges = partitionedUserGraph.groupEdges((e1, e2) => e1 + " and " + e2) 
graphWithoutParallelEdges: org.apache.spark.graphx.Graph[String,String] = org.apache.spark.graphx.impl.GraphImpl@16a4961f 
scala> // Print the details 
scala> graphWithoutParallelEdges.triplets.foreach(println) 
((1,Thomas),(2,Krish),Follows and Son) 
((2,Krish),(3,Mathew),Follows) 

```

图中之前的结构变化通过对边进行分组来减少边的数量。当边属性是数字时，如果通过聚合它们来合并是有意义的，那么也可以通过移除平行边来减少边的数量，这可以大大减少图形处理时间。

### 注

在这段代码中需要注意的一点是，在对边进行分组操作之前，已经对图进行了分区。

默认情况下，给定图的边和组成顶点不需要位于同一分区中。要使分组操作起作用，所有平行边必须位于同一分区上。CanonicalRandomVertexCut 分区策略确保两个顶点之间的所有边都发生同位置，而不管方向如何。

在 Spark GraphX 库中有更多的结构操作符，参考 Spark 文档可以很好地了解它们。它们可以根据用例来使用。

# 网球赛事分析

由于基本的图形处理基础已经到位，现在是时候采用使用图形的真实用例了。这里，网球锦标赛的结果是用一个图表来模拟的。巴克莱 ATP 世界巡回赛 2015 单打比赛结果采用图表建模。顶点包含玩家的详细信息，而边包含所玩的单个匹配。边的形成方式是，源顶点是赢得比赛的玩家，目的顶点是输掉比赛的玩家。edge 属性包含比赛类型、获胜者在比赛中获得的分数以及比赛中选手的人头数。这里使用的积分系统是虚构的，只不过是获胜者在特定比赛中获得的权重。最初的小组赛分量最小，半决赛分量最大，决赛分量最大。使用这种结果建模方式，通过处理图表找出以下细节:

*   列出所有匹配细节。
*   列出所有匹配的球员姓名，匹配类型和结果。
*   列出第一组所有在比赛中得分的获胜者。
*   列出第 2 组所有在比赛中得分的获胜者。
*   列出所有在比赛中得分的半决赛获胜者。
*   用比赛中的分数列出最后的获胜者。
*   列出玩家在整个锦标赛中获得的总积分。
*   找出运动员得分最高的分数，列出比赛的获胜者。
*   在小组赛中，由于抽签的循环方案，同一名选手可能会相遇不止一次。查找是否有任何这样的玩家在本次锦标赛中多次交手。
*   列出至少赢过一场比赛的选手。
*   列出至少输掉一场比赛的球员。
*   列出至少赢过一场比赛和至少输过一场比赛的球员。
*   列出完全没有胜算的玩家。
*   列出完全没有损失的玩家。

不熟悉网球游戏的人不用担心，因为这里不讨论游戏规则，也不要求了解这个用例。实际上，这只是两个人之间的游戏，一方赢，一方输。在 Scala REPL 提示符下，尝试以下语句:

```
scala> import org.apache.spark._
  import org.apache.spark._    
  scala> import org.apache.spark.graphx._
  import org.apache.spark.graphx._    
  scala> import org.apache.spark.rdd.RDD
  import org.apache.spark.rdd.RDD
    scala> //Define a property class that is going to hold all the properties of the vertex which is nothing but player information
	scala> case class Player(name: String, country: String)
      defined class Player
    scala> // Create the player vertices
	scala> val players: RDD[(Long, Player)] = sc.parallelize(Array((1L, Player("Novak Djokovic", "SRB")), (3L, Player("Roger Federer", "SUI")),(5L, Player("Tomas Berdych", "CZE")), (7L, Player("Kei Nishikori", "JPN")), (11L, Player("Andy Murray", "GBR")),(15L, Player("Stan Wawrinka", "SUI")),(17L, Player("Rafael Nadal", "ESP")),(19L, Player("David Ferrer", "ESP"))))
players: org.apache.spark.rdd.RDD[(Long, Player)] = ParallelCollectionRDD[145] at parallelize at <console>:57
    scala> //Define a property class that is going to hold all the properties of the edge which is nothing but match informationscala> case class Match(matchType: String, points: Int, head2HeadCount: Int)
      defined class Match
    scala> // Create the match edgesscala> val matches: RDD[Edge[Match]] = sc.parallelize(Array(Edge(1L, 5L, Match("G1", 1,1)), Edge(1L, 7L, Match("G1", 1,1)), Edge(3L, 1L, Match("G1", 1,1)), Edge(3L, 5L, Match("G1", 1,1)), Edge(3L, 7L, Match("G1", 1,1)), Edge(7L, 5L, Match("G1", 1,1)), Edge(11L, 19L, Match("G2", 1,1)), Edge(15L, 11L, Match("G2", 1, 1)), Edge(15L, 19L, Match("G2", 1, 1)), Edge(17L, 11L, Match("G2", 1, 1)), Edge(17L, 15L, Match("G2", 1, 1)), Edge(17L, 19L, Match("G2", 1, 1)), Edge(3L, 15L, Match("S", 5, 1)), Edge(1L, 17L, Match("S", 5, 1)), Edge(1L, 3L, Match("F", 11, 1))))
matches: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Match]] = ParallelCollectionRDD[146] at parallelize at <console>:57
    scala> //Create a graph with the vertices and edges
	scala> val playGraph = Graph(players, matches)
playGraph: org.apache.spark.graphx.Graph[Player,Match] = org.apache.spark.graphx.impl.GraphImpl@30d4d6fb 

```

包含网球锦标赛的图表已经创建，从现在开始，要做的就是处理这个基础图表并从中提取信息，以满足用例的要求:

```
scala> //Print the match details
	scala> playGraph.triplets.foreach(println)
((15,Player(Stan Wawrinka,SUI)),(11,Player(Andy Murray,GBR)),Match(G2,1,1))    
((15,Player(Stan Wawrinka,SUI)),(19,Player(David Ferrer,ESP)),Match(G2,1,1))    
((7,Player(Kei Nishikori,JPN)),(5,Player(Tomas Berdych,CZE)),Match(G1,1,1))    
((1,Player(Novak Djokovic,SRB)),(7,Player(Kei Nishikori,JPN)),Match(G1,1,1))    
((3,Player(Roger Federer,SUI)),(1,Player(Novak Djokovic,SRB)),Match(G1,1,1))    
((1,Player(Novak Djokovic,SRB)),(3,Player(Roger Federer,SUI)),Match(F,11,1))    
((1,Player(Novak Djokovic,SRB)),(17,Player(Rafael Nadal,ESP)),Match(S,5,1))    
((3,Player(Roger Federer,SUI)),(5,Player(Tomas Berdych,CZE)),Match(G1,1,1))    
((17,Player(Rafael Nadal,ESP)),(11,Player(Andy Murray,GBR)),Match(G2,1,1))    
((3,Player(Roger Federer,SUI)),(7,Player(Kei Nishikori,JPN)),Match(G1,1,1))    
((1,Player(Novak Djokovic,SRB)),(5,Player(Tomas Berdych,CZE)),Match(G1,1,1))    
((17,Player(Rafael Nadal,ESP)),(15,Player(Stan Wawrinka,SUI)),Match(G2,1,1))    
((11,Player(Andy Murray,GBR)),(19,Player(David Ferrer,ESP)),Match(G2,1,1))    
((3,Player(Roger Federer,SUI)),(15,Player(Stan Wawrinka,SUI)),Match(S,5,1))    
((17,Player(Rafael Nadal,ESP)),(19,Player(David Ferrer,ESP)),Match(G2,1,1))
    scala> //Print matches with player names and the match type and the resultscala> playGraph.triplets.map(triplet => triplet.srcAttr.name + " won over " + triplet.dstAttr.name + " in  " + triplet.attr.matchType + " match").foreach(println)
      Roger Federer won over Tomas Berdych in  G1 match    
      Roger Federer won over Kei Nishikori in  G1 match    
      Novak Djokovic won over Roger Federer in  F match    
      Novak Djokovic won over Rafael Nadal in  S match    
      Roger Federer won over Stan Wawrinka in  S match    
      Rafael Nadal won over David Ferrer in  G2 match    
      Kei Nishikori won over Tomas Berdych in  G1 match    
      Andy Murray won over David Ferrer in  G2 match    
      Stan Wawrinka won over Andy Murray in  G2 match    
      Stan Wawrinka won over David Ferrer in  G2 match    
      Novak Djokovic won over Kei Nishikori in  G1 match    
      Roger Federer won over Novak Djokovic in  G1 match    
      Rafael Nadal won over Andy Murray in  G2 match    
      Rafael Nadal won over Stan Wawrinka in  G2 match    
      Novak Djokovic won over Tomas Berdych in  G1 match 

```

这里值得注意的是，在图形中使用三元组有助于从单个对象中提取给定网球比赛所需的所有数据元素，包括谁在比赛、谁赢了以及比赛类型。以下分析用例的实现涉及过滤比赛的网球比赛记录。这里，只使用简单的过滤逻辑，但是在真实的用例中，任何复杂的逻辑都可以在函数中实现，并且可以作为参数传递给过滤器转换:

```
scala> //Group 1 winners with their group total points
scala> playGraph.triplets.filter(triplet => triplet.attr.matchType == "G1").map(triplet => (triplet.srcAttr.name, triplet.attr.points)).foreach(println)
      (Kei Nishikori,1)    
      (Roger Federer,1)    
      (Roger Federer,1)    
      (Novak Djokovic,1)    
      (Novak Djokovic,1)    
      (Roger Federer,1)
    scala> //Find the group total of the players
	scala> playGraph.triplets.filter(triplet => triplet.attr.matchType == "G1").map(triplet => (triplet.srcAttr.name, triplet.attr.points)).reduceByKey(_+_).foreach(println)
      (Roger Federer,3)    
      (Novak Djokovic,2)    
      (Kei Nishikori,1)
    scala> //Group 2 winners with their group total points
	scala> playGraph.triplets.filter(triplet => triplet.attr.matchType == "G2").map(triplet => (triplet.srcAttr.name, triplet.attr.points)).foreach(println)
      (Rafael Nadal,1)    
      (Rafael Nadal,1)    
      (Andy Murray,1)    
      (Stan Wawrinka,1)    
      (Stan Wawrinka,1)    
      (Rafael Nadal,1) 

```

以下分析用例的实现包括按关键字分组和进行汇总计算。它不局限于仅仅找到网球比赛记录点的总和，如下面的用例实现所示；相反，用户定义的函数也可以用来进行计算:

```
scala> //Find the group total of the players
	scala> playGraph.triplets.filter(triplet => triplet.attr.matchType == "G2").map(triplet => (triplet.srcAttr.name, triplet.attr.points)).reduceByKey(_+_).foreach(println)
      (Stan Wawrinka,2)    
      (Andy Murray,1)    
      (Rafael Nadal,3)
    scala> //Semi final winners with their group total points
	scala> playGraph.triplets.filter(triplet => triplet.attr.matchType == "S").map(triplet => (triplet.srcAttr.name, triplet.attr.points)).foreach(println)
      (Novak Djokovic,5)    
      (Roger Federer,5)
    scala> //Find the group total of the players
	scala> playGraph.triplets.filter(triplet => triplet.attr.matchType == "S").map(triplet => (triplet.srcAttr.name, triplet.attr.points)).reduceByKey(_+_).foreach(println)
      (Novak Djokovic,5)    
      (Roger Federer,5)
    scala> //Final winner with the group total points
	scala> playGraph.triplets.filter(triplet => triplet.attr.matchType == "F").map(triplet => (triplet.srcAttr.name, triplet.attr.points)).foreach(println)
      (Novak Djokovic,11)
    scala> //Tournament total point standing
	scala> playGraph.triplets.map(triplet => (triplet.srcAttr.name, triplet.attr.points)).reduceByKey(_+_).foreach(println)
      (Stan Wawrinka,2)

      (Rafael Nadal,3)    
      (Kei Nishikori,1)    
      (Andy Murray,1)    
      (Roger Federer,8)    
      (Novak Djokovic,18)
    scala> //Find the winner of the tournament by finding the top scorer of the tournament
	scala> playGraph.triplets.map(triplet => (triplet.srcAttr.name, triplet.attr.points)).reduceByKey(_+_).map{ case (k,v) => (v,k)}.sortByKey(ascending=false).take(1).map{ case (k,v) => (v,k)}.foreach(println)
      (Novak Djokovic,18)
    scala> //Find how many head to head matches held for a given set of players in the descending order of head2head count
	scala> playGraph.triplets.map(triplet => (Set(triplet.srcAttr.name , triplet.dstAttr.name) , triplet.attr.head2HeadCount)).reduceByKey(_+_).map{case (k,v) => (k.mkString(" and "), v)}.map{ case (k,v) => (v,k)}.sortByKey().map{ case (k,v) => v + " played " + k + " time(s)"}.foreach(println)
      Roger Federer and Novak Djokovic played 2 time(s)    
      Roger Federer and Tomas Berdych played 1 time(s)    
      Kei Nishikori and Tomas Berdych played 1 time(s)    
      Novak Djokovic and Tomas Berdych played 1 time(s)    
      Rafael Nadal and Andy Murray played 1 time(s)    
      Rafael Nadal and Stan Wawrinka played 1 time(s)    
      Andy Murray and David Ferrer played 1 time(s)    
      Rafael Nadal and David Ferrer played 1 time(s)    
      Stan Wawrinka and David Ferrer played 1 time(s)    
      Stan Wawrinka and Andy Murray played 1 time(s)    
      Roger Federer and Stan Wawrinka played 1 time(s)    
      Roger Federer and Kei Nishikori played 1 time(s)    
      Novak Djokovic and Kei Nishikori played 1 time(s)    
      Novak Djokovic and Rafael Nadal played 1 time(s) 

```

以下分析用例的实现包括从查询中找到唯一的记录。火花独特的转变做到了这一点:

```
 scala> //List of players who have won at least one match
	scala> val winners = playGraph.triplets.map(triplet => triplet.srcAttr.name).distinct
winners: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[201] at distinct at <console>:65
    scala> winners.foreach(println)
      Kei Nishikori    
      Stan Wawrinka    
      Andy Murray    
      Roger Federer    
      Rafael Nadal    
      Novak Djokovic
    scala> //List of players who have lost at least one match
	scala> val loosers = playGraph.triplets.map(triplet => triplet.dstAttr.name).distinct
loosers: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[205] at distinct at <console>:65
    scala> loosers.foreach(println)
      Novak Djokovic    
      Kei Nishikori    
      David Ferrer    
      Stan Wawrinka    
      Andy Murray    
      Roger Federer    
      Rafael Nadal    
      Tomas Berdych
    scala> //List of players who have won at least one match and lost at least one match
	scala> val wonAndLost = winners.intersection(loosers)
wonAndLost: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[211] at intersection at <console>:69
    scala> wonAndLost.foreach(println)
      Novak Djokovic    
      Rafael Nadal    
      Andy Murray    
      Roger Federer    
      Kei Nishikori    
      Stan Wawrinka 
    scala> //List of players who have no wins at all
	scala> val lostAndNoWins = loosers.collect().toSet -- wonAndLost.collect().toSet
lostAndNoWins: 
scala.collection.immutable.Set[String] = Set(David Ferrer, Tomas Berdych)
    scala> lostAndNoWins.foreach(println)
      David Ferrer    
      Tomas Berdych
    scala> //List of players who have no loss at all
	scala> val wonAndNoLosses = winners.collect().toSet -- loosers.collect().toSet
 wonAndNoLosses: 
	  scala.collection.immutable.Set[String] = Set() 
scala> //The val wonAndNoLosses returned an empty set which means that there is no single player in this tournament who have only wins
scala> wonAndNoLosses.foreach(println)

```

在这个用例中，没有做太多的努力来使结果变得漂亮，因为它们被简化为简单的基于 RDD 的结构，这些结构可以使用本书最初章节中已经介绍过的 RDD 编程技术进行操作。

Spark 的高度简洁和统一的编程模型，结合 Spark GraphX 库，帮助开发人员用很少的代码行构建真实世界的用例。这也表明，一旦用相关数据构建了正确的图结构，通过支持的图操作，隐藏在底层数据中的许多真相就可以被揭示出来。

# 应用 PageRank 算法

谢尔盖·布林(Sergey Brin)和劳伦斯·佩奇(Lawrence Page)的一篇名为《大规模超文本网络搜索引擎的剖析》的研究论文彻底改变了网络搜索，谷歌将其搜索引擎建立在 PageRank 概念的基础上，并开始主导其他网络搜索引擎。

使用谷歌搜索网页时，会显示根据其算法排名靠前的页面。在图而不是网页的上下文中，如果基于相同的算法对顶点进行排序，可以做出许多新的推断。从外部来看，这听起来像是 PageRank 算法只对网络搜索有用。但是它有巨大的潜力应用到许多其他领域。

用图的说法，如果有一条边，E，连接两个顶点，从 V1 到 V2，根据 PageRank 算法，V2 比 V1 更重要。在一个巨大的顶点和边的图中，可以计算每个顶点的 PageRank。

PageRank 算法可以很好地应用于上一节中介绍的网球锦标赛分析用例。在这里采用的图形表示中，每个匹配都表示为一条边。源顶点有赢家的细节，目的顶点有输家的细节。在网球比赛中，如果这可以被称为某种虚构的重要性排名，那么在给定的比赛中，获胜者比失败者具有更高的重要性排名。

如果上一个用例中的图被用来演示 PageRank 算法，那么该图必须被反转，以便每次匹配的获胜者成为每条边的目的顶点。在 Scala REPL 提示符下，尝试以下语句:

```
scala> import org.apache.spark._
  import org.apache.spark._ 
  scala> import org.apache.spark.graphx._
  import org.apache.spark.graphx._    
  scala> import org.apache.spark.rdd.RDD
  import org.apache.spark.rdd.RDD
    scala> //Define a property class that is going to hold all the properties of the vertex which is nothing but player informationscala> case class Player(name: String, country: String)
      defined class Player
    scala> // Create the player verticesscala> val players: RDD[(Long, Player)] = sc.parallelize(Array((1L, Player("Novak Djokovic", "SRB")), (3L, Player("Roger Federer", "SUI")),(5L, Player("Tomas Berdych", "CZE")), (7L, Player("Kei Nishikori", "JPN")), (11L, Player("Andy Murray", "GBR")),(15L, Player("Stan Wawrinka", "SUI")),(17L, Player("Rafael Nadal", "ESP")),(19L, Player("David Ferrer", "ESP"))))
players: org.apache.spark.rdd.RDD[(Long, Player)] = ParallelCollectionRDD[212] at parallelize at <console>:64
    scala> //Define a property class that is going to hold all the properties of the edge which is nothing but match informationscala> case class Match(matchType: String, points: Int, head2HeadCount: Int)
      defined class Match
    scala> // Create the match edgesscala> val matches: RDD[Edge[Match]] = sc.parallelize(Array(Edge(1L, 5L, Match("G1", 1,1)), Edge(1L, 7L, Match("G1", 1,1)), Edge(3L, 1L, Match("G1", 1,1)), Edge(3L, 5L, Match("G1", 1,1)), Edge(3L, 7L, Match("G1", 1,1)), Edge(7L, 5L, Match("G1", 1,1)), Edge(11L, 19L, Match("G2", 1,1)), Edge(15L, 11L, Match("G2", 1, 1)), Edge(15L, 19L, Match("G2", 1, 1)), Edge(17L, 11L, Match("G2", 1, 1)), Edge(17L, 15L, Match("G2", 1, 1)), Edge(17L, 19L, Match("G2", 1, 1)), Edge(3L, 15L, Match("S", 5, 1)), Edge(1L, 17L, Match("S", 5, 1)), Edge(1L, 3L, Match("F", 11, 1))))
matches: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Match]] = ParallelCollectionRDD[213] at parallelize at <console>:64
    scala> //Create a graph with the vertices and edgesscala> val playGraph = Graph(players, matches)
playGraph: org.apache.spark.graphx.Graph[Player,Match] = org.apache.spark.graphx.impl.GraphImpl@263cd0e2
    scala> //Reverse this graph to have the winning player coming in the destination vertex
	scala> val rankGraph = playGraph.reverse
rankGraph: org.apache.spark.graphx.Graph[Player,Match] = org.apache.spark.graphx.impl.GraphImpl@7bb131fb
    scala> //Run the PageRank algorithm to calculate the rank of each vertex
	scala> val rankedVertices = rankGraph.pageRank(0.0001).vertices
rankedVertices: org.apache.spark.graphx.VertexRDD[Double] = VertexRDDImpl[1184] at RDD at VertexRDD.scala:57
    scala> //Extract the vertices sorted by the rank
	scala> val rankedPlayers = rankedVertices.join(players).map{case 
	(id,(importanceRank,Player(name,country))) => (importanceRank,
	name)}.sortByKey(ascending=false)

	rankedPlayers: org.apache.spark.rdd.RDD[(Double, String)] = ShuffledRDD[1193] at sortByKey at <console>:76

	scala> rankedPlayers.collect().foreach(println)
      (3.382662570589846,Novak Djokovic)    
      (3.266079758089846,Roger Federer)    
      (0.3908953124999999,Rafael Nadal)    
      (0.27431249999999996,Stan Wawrinka)    
      (0.1925,Andy Murray)    
      (0.1925,Kei Nishikori)    
      (0.15,David Ferrer)    
      (0.15,Tomas Berdych) 

```

如果仔细检查前面的代码，可以看到排名最高的玩家赢得了最多的比赛。

# 连通分量算法

在图中，找到一个由连通顶点组成的子图是一个非常普遍的要求，有着巨大的应用。在任何图中，两个顶点通过由一条或多条边组成的路径相互连接，并且不连接到同一图中的任何其他顶点，称为连通分支。例如，在图 G 中，顶点 V1 通过一条边与 V2 相连，V2 通过另一条边与 V3 相连。在同一个图 G 中，顶点 V4 通过另一条边连接到 V5。在这种情况下，V1 和 V3 连接，V4 和 V5 连接，V1 和 V5 不连接。在图 G 中，有两个相连的组件。Spark GraphX 库实现了连接组件算法。

在社交网络应用中，如果用户之间的连接被建模为图，则通过检查是否存在具有这两个顶点的连接组件来找到给定用户是否连接到另一个用户。在计算机游戏中，从 A 点到 B 点的迷宫遍历可以使用连接组件算法来完成，方法是将迷宫连接建模为顶点，将连接连接的路径建模为图中的边。

在计算机网络中，检查数据包是否可以从一个 IP 地址发送到另一个 IP 地址是通过使用连接组件算法来实现的。在物流应用中，例如快递服务，通过使用连接组件算法来检查包是否可以从点 A 发送到点 B。*图 6* 显示了具有三个相连组件的图表:

![Connected component algorithm](graphics/image_08_006.jpg)

图 6

*图 6* 是图形的图示。其中，有三个由边连接的*顶点簇*。换句话说，这个图中有三个相连的部分。

在社交网络应用程序中，用户相互关注的用例在这里再次被提出，用于说明目的。通过提取图的连通分量，可以看出任意两个用户是否连通。*图 7* 显示用户图形:

![Connected component algorithm](graphics/image_08_007.jpg)

图 7

在*图 7* 描绘的图形中，很明显有两个相连的组件。很容易说托马斯和马修有联系，同时托马斯和马丁没有联系。如果提取连通分量图，可以看到 Thomas 和 Martin 将具有相同的连通分量标识符，同时 Thomas 和 Martin 将具有不同的连通分量标识符。在 Scala REPL 提示符下，尝试以下语句:

```
	 scala> import org.apache.spark._

  import org.apache.spark._    
  scala> import org.apache.spark.graphx._

  import org.apache.spark.graphx._    
  scala> import org.apache.spark.rdd.RDD

  import org.apache.spark.rdd.RDD    

  scala> // Create the RDD with users as the vertices
  scala> val users: RDD[(Long, String)] = sc.parallelize(Array((1L, "Thomas"), (2L, "Krish"),(3L, "Mathew"), (4L, "Martin"), (5L, "George"), (6L, "James")))

users: org.apache.spark.rdd.RDD[(Long, String)] = ParallelCollectionRDD[1194] at parallelize at <console>:69

	scala> // Create the edges connecting the users
	scala> val userRelationships: RDD[Edge[String]] = sc.parallelize(Array(Edge(1L, 2L, "Follows"),Edge(2L, 3L, "Follows"), Edge(4L, 5L, "Follows"), Edge(5L, 6L, "Follows")))

userRelationships: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[String]] = ParallelCollectionRDD[1195] at parallelize at <console>:69

	scala> // Create a graph
	scala> val userGraph = Graph(users, userRelationships)

userGraph: org.apache.spark.graphx.Graph[String,String] = org.apache.spark.graphx.impl.GraphImpl@805e363

	scala> // Find the connected components of the graph
	scala> val cc = userGraph.connectedComponents()

cc: org.apache.spark.graphx.Graph[org.apache.spark.graphx.VertexId,String] = org.apache.spark.graphx.impl.GraphImpl@13f4a9a9

	scala> // Extract the triplets of the connected components
	scala> val ccTriplets = cc.triplets

ccTriplets: org.apache.spark.rdd.RDD[org.apache.spark.graphx.EdgeTriplet[org.apache.spark.graphx.VertexId,String]] = MapPartitionsRDD[1263] at mapPartitions at GraphImpl.scala:48

	scala> // Print the structure of the tripletsscala> ccTriplets.foreach(println)
      ((1,1),(2,1),Follows)    

      ((4,4),(5,4),Follows)    

      ((5,4),(6,4),Follows)    

      ((2,1),(3,1),Follows)

	scala> //Print the vertex numbers and the corresponding connected component id. The connected component id is generated by the system and it is to be taken only as a unique identifier for the connected component
	scala> val ccProperties = ccTriplets.map(triplet => "Vertex " + triplet.srcId + " and " + triplet.dstId + " are part of the CC with id " + triplet.srcAttr)

ccProperties: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[1264] at map at <console>:79

	scala> ccProperties.foreach(println)

      Vertex 1 and 2 are part of the CC with id 1    

      Vertex 5 and 6 are part of the CC with id 4    

      Vertex 2 and 3 are part of the CC with id 1    

      Vertex 4 and 5 are part of the CC with id 4

	scala> //Find the users in the source vertex with their CC id
	scala> val srcUsersAndTheirCC = ccTriplets.map(triplet => (triplet.srcId, triplet.srcAttr))

srcUsersAndTheirCC: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId)] = MapPartitionsRDD[1265] at map at <console>:79

	scala> //Find the users in the destination vertex with their CC id
	scala> val dstUsersAndTheirCC = ccTriplets.map(triplet => (triplet.dstId, triplet.dstAttr))

dstUsersAndTheirCC: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId)] = MapPartitionsRDD[1266] at map at <console>:79

	scala> //Find the union
	scala> val usersAndTheirCC = srcUsersAndTheirCC.union(dstUsersAndTheirCC)

usersAndTheirCC: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId)] = UnionRDD[1267] at union at <console>:83

	scala> //Join with the name of the users
	scala> val usersAndTheirCCWithName = usersAndTheirCC.join(users).map{case (userId,(ccId,userName)) => (ccId, userName)}.distinct.sortByKey()

usersAndTheirCCWithName: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, String)] = ShuffledRDD[1277] at sortByKey at <console>:85

	scala> //Print the user names with their CC component id. If two users share the same CC id, then they are connected
	scala> usersAndTheirCCWithName.collect().foreach(println)

      (1,Thomas)    

      (1,Mathew)    

      (1,Krish)    

      (4,Martin)    

      (4,James)    

      (4,George) 

```

Spark GraphX 库中还有一些更多的图形处理算法，对整套算法的详细论述值得单独一本书来讨论。这里的要点是，Spark GraphX 库提供了非常易于使用的图形算法，非常适合 Spark 的统一编程模型。

# 理解图形框架

Spark GraphX 库是对编程语言支持最少的图形处理库。Scala 是 Spark GraphX 库支持的唯一编程语言。GraphFrames 是一个新的图形处理库，作为外部 Spark 包提供，由加州大学伯克利分校和麻省理工学院的 Databricks 开发，构建在 Spark DataFrames 之上。因为它是建立在数据帧之上的，所以所有可以在数据帧上完成的操作都有可能在 GraphFrames 上实现，并且支持编程语言，如 Scala、Java、Python 和 R，具有统一的 API。由于 GraphFrames 是建立在 DataFrames 之上的，因此数据的持久性、对众多数据源的支持以及 Spark SQL 中强大的图形查询都是用户免费获得的额外好处。

就像 Spark GraphX 库一样，在 GraphFrames 中，数据存储在顶点和边中。顶点和边使用数据帧作为数据结构。本章开头介绍的第一个用例再次用于阐述基于 GraphFrames 的图形处理。

### 注

**注意事项** : GraphFrames 是一个外部火花包。它与 Spark 2.0 有些不兼容。因此，下面的代码片段将无法在 Spark 2.0 中使用。他们使用 Spark 1.6。参考他们的网站查看 Spark 2.0 支持。

在 Spark 1.6 的 Scala REPL 提示符下，尝试以下语句。由于 GraphFrames 是一个外部 Spark 包，在启动适当的 REPL 时，必须导入库，并且在终端提示符中使用以下命令来启动 REPL，并确保加载库时没有任何错误消息:

```
	 $ cd $SPARK_1.6__HOME 
	$ ./bin/spark-shell --packages graphframes:graphframes:0.1.0-spark1.6 
	Ivy Default Cache set to: /Users/RajT/.ivy2/cache 
	The jars for the packages stored in: /Users/RajT/.ivy2/jars 
	:: loading settings :: url = jar:file:/Users/RajT/source-code/spark-source/spark-1.6.1
	/assembly/target/scala-2.10/spark-assembly-1.6.2-SNAPSHOT-hadoop2.2.0.jar!
	/org/apache/ivy/core/settings/ivysettings.xml 
	graphframes#graphframes added as a dependency 
	:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0 
	confs: [default] 
	found graphframes#graphframes;0.1.0-spark1.6 in list 
	:: resolution report :: resolve 153ms :: artifacts dl 2ms 
	:: modules in use: 
	graphframes#graphframes;0.1.0-spark1.6 from list in [default] 
   --------------------------------------------------------------------- 
   |                  |            modules            ||   artifacts   | 
   |       conf       | number| search|dwnlded|evicted|| number|dwnlded| 
   --------------------------------------------------------------------- 
   |      default     |   1   |   0   |   0   |   0   ||   1   |   0   | 
   --------------------------------------------------------------------- 
   :: retrieving :: org.apache.spark#spark-submit-parent 
   confs: [default] 
   0 artifacts copied, 1 already retrieved (0kB/5ms) 
   16/07/31 09:22:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 
   Welcome to 
      ____              __ 
     / __/__  ___ _____/ /__ 
    _\ \/ _ \/ _ `/ __/  '_/ 
   /___/ .__/\_,_/_/ /_/\_\   version 1.6.1 
       /_/ 

	  Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_66) 
	  Type in expressions to have them evaluated. 
	  Type :help for more information. 
	  Spark context available as sc. 
	  SQL context available as sqlContext. 
	  scala> import org.graphframes._ 
	  import org.graphframes._ 
	  scala> import org.apache.spark.rdd.RDD 
	  import org.apache.spark.rdd.RDD 
	  scala> import org.apache.spark.sql.Row 
	  import org.apache.spark.sql.Row 
	  scala> import org.apache.spark.graphx._ 
	  import org.apache.spark.graphx._ 
	  scala> //Create a DataFrame of users containing tuple values with a mandatory Long and another String type as the property of the vertex 
	  scala> val users = sqlContext.createDataFrame(List((1L, "Thomas"),(2L, "Krish"),(3L, "Mathew"))).toDF("id", "name") 
	  users: org.apache.spark.sql.DataFrame = [id: bigint, name: string] 
	  scala> //Created a DataFrame for Edge with String type as the property of the edge 
	  scala> val userRelationships = sqlContext.createDataFrame(List((1L, 2L, "Follows"),(1L, 2L, "Son"),(2L, 3L, "Follows"))).toDF("src", "dst", "relationship") 
	  userRelationships: org.apache.spark.sql.DataFrame = [src: bigint, dst: bigint, relationship: string] 
	  scala> val userGraph = GraphFrame(users, userRelationships) 
	  userGraph: org.graphframes.GraphFrame = GraphFrame(v:[id: bigint, name: string], e:[src: bigint, dst: bigint, relationship: string]) 
	  scala> // Vertices in the graph 
	  scala> userGraph.vertices.show() 
	  +---+------+ 
	  | id|  name| 
	  +---+------+ 
	  |  1|Thomas| 
	  |  2| Krish| 
	  |  3|Mathew| 
	  +---+------+ 
	  scala> // Edges in the graph 
	  scala> userGraph.edges.show() 
	  +---+---+------------+ 
	  |src|dst|relationship| 
	  +---+---+------------+ 
	  |  1|  2|     Follows| 
	  |  1|  2|         Son| 
	  |  2|  3|     Follows| 
	  +---+---+------------+ 
	  scala> //Number of edges in the graph 
	  scala> val edgeCount = userGraph.edges.count() 
	  edgeCount: Long = 3 
	  scala> //Number of vertices in the graph 
	  scala> val vertexCount = userGraph.vertices.count() 
	  vertexCount: Long = 3 
	  scala> //Number of edges coming to each of the vertex.  
	  scala> userGraph.inDegrees.show() 
	  +---+--------+ 
	  | id|inDegree| 
	  +---+--------+ 
	  |  2|       2| 
	  |  3|       1| 
	  +---+--------+ 
	  scala> //Number of edges going out of each of the vertex.  
	  scala> userGraph.outDegrees.show() 
	  +---+---------+ 
	  | id|outDegree| 
	  +---+---------+ 
	  |  1|        2| 
	  |  2|        1| 
	  +---+---------+ 
	  scala> //Total number of edges coming in and going out of each vertex.  
	  scala> userGraph.degrees.show() 
	  +---+------+ 
	  | id|degree| 
	  +---+------+ 
	  |  1|     2| 
	  |  2|     3| 
	  |  3|     1| 
	  +---+------+ 
	  scala> //Get the triplets of the graph 
	  scala> userGraph.triplets.show() 
	  +-------------+----------+----------+ 
	  |         edge|       src|       dst| 
	  +-------------+----------+----------+ 
	  |[1,2,Follows]|[1,Thomas]| [2,Krish]| 
	  |    [1,2,Son]|[1,Thomas]| [2,Krish]| 
	  |[2,3,Follows]| [2,Krish]|[3,Mathew]| 
	  +-------------+----------+----------+ 
	  scala> //Using the DataFrame API, apply filter and select only the needed edges 
	  scala> val numFollows = userGraph.edges.filter("relationship = 'Follows'").count() 
	  numFollows: Long = 2 
	  scala> //Create an RDD of users containing tuple values with a mandatory Long and another String type as the property of the vertex 
	  scala> val usersRDD: RDD[(Long, String)] = sc.parallelize(Array((1L, "Thomas"), (2L, "Krish"),(3L, "Mathew"))) 
	  usersRDD: org.apache.spark.rdd.RDD[(Long, String)] = ParallelCollectionRDD[54] at parallelize at <console>:35 
	  scala> //Created an RDD of Edge type with String type as the property of the edge 
	  scala> val userRelationshipsRDD: RDD[Edge[String]] = sc.parallelize(Array(Edge(1L, 2L, "Follows"),    Edge(1L, 2L, "Son"),Edge(2L, 3L, "Follows"))) 
	  userRelationshipsRDD: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[String]] = ParallelCollectionRDD[55] at parallelize at <console>:35 
	  scala> //Create a graph containing the vertex and edge RDDs as created before 
	  scala> val userGraphXFromRDD = Graph(usersRDD, userRelationshipsRDD) 
	  userGraphXFromRDD: org.apache.spark.graphx.Graph[String,String] = 
	  org.apache.spark.graphx.impl.GraphImpl@77a3c614 
	  scala> //Create the GraphFrame based graph from Spark GraphX based graph 
	  scala> val userGraphFrameFromGraphX: GraphFrame = GraphFrame.fromGraphX(userGraphXFromRDD) 
	  userGraphFrameFromGraphX: org.graphframes.GraphFrame = GraphFrame(v:[id: bigint, attr: string], e:[src: bigint, dst: bigint, attr: string]) 
	  scala> userGraphFrameFromGraphX.triplets.show() 
	  +-------------+----------+----------+
	  |         edge|       src|       dst| 
	  +-------------+----------+----------+ 
	  |[1,2,Follows]|[1,Thomas]| [2,Krish]| 
	  |    [1,2,Son]|[1,Thomas]| [2,Krish]| 
	  |[2,3,Follows]| [2,Krish]|[3,Mathew]| 
	  +-------------+----------+----------+ 
	  scala> // Convert the GraphFrame based graph to a Spark GraphX based graph 
	  scala> val userGraphXFromGraphFrame: Graph[Row, Row] = userGraphFrameFromGraphX.toGraphX 
	  userGraphXFromGraphFrame: org.apache.spark.graphx.Graph[org.apache.spark.sql.Row,org.apache.spark.sql.Row] = org.apache.spark.graphx.impl.GraphImpl@238d6aa2 

```

为 GraphFrame 创建数据框时，唯一要记住的是顶点和边有一些强制列。在顶点的数据框中，id 列是必需的。在边的数据框中，src 和 dst 列是必需的。除此之外，任意数量的任意列可以与 GraphFrame 的顶点和边一起存储。在 Spark GraphX 库中，顶点标识符必须是一个长整数，但是 GraphFrame 没有任何这样的限制，并且支持任何类型的顶点标识符。读者应该已经熟悉了数据帧；任何可以在数据框上完成的操作都可以在图形框的顶点和边上完成。

### 类型

Spark GraphX 支持的所有图形处理算法也受到 GraphFrames 的支持。

Python 版本的 GraphFrames 功能较少。由于 Python 不是 Spark GraphX 库支持的编程语言，因此 Python 中不支持 GraphFrame 到 GraphX 和 GraphX 到 GraphFrame 的转换。由于读者熟悉使用 Python 在 Spark 中创建数据帧，这里省略了 Python 示例。此外，针对 Python 的 GraphFrames API 中存在一些悬而未决的缺陷，并且在编写本文时，并非之前使用 Scala 演示的所有功能都能在 Python 中正常运行。

# 理解图形框架查询

Spark GraphX 库是基于 RDD 的图形处理库，但是 GraphFrames 是基于 Spark DataFrame 的图形处理库，可以作为外部包使用。Spark GraphX 支持很多图形处理算法，但是 GraphFrames 不仅支持图形处理算法，还支持图形查询。图处理算法和图查询的主要区别在于，图处理算法用于处理隐藏在图数据结构中的数据，而图查询用于搜索隐藏在图数据结构中的数据中的模式。用 GraphFrame 的话来说，图形查询也称为主题查找。这在遗传学和其他处理序列基序的生物科学中有巨大的应用。

从用例的角度来看，以社交媒体应用程序中用户相互跟踪的用例为例。用户之间有关系。在前几节中，这些关系被建模为图形。在现实世界的用例中，这样的图可能会变得非常巨大，如果需要找到在两个方向上都有关系的用户，可以用图查询中的模式来表示，并且可以使用简单的编程构造来找到这样的关系。下面的演示模拟了 GraphFrame 中用户之间的关系，并使用它进行模式搜索。

在 Spark 1.6 的 Scala REPL 提示符下，尝试以下语句:

```
 $ cd $SPARK_1.6_HOME 
	  $ ./bin/spark-shell --packages graphframes:graphframes:0.1.0-spark1.6 
	  Ivy Default Cache set to: /Users/RajT/.ivy2/cache 
	  The jars for the packages stored in: /Users/RajT/.ivy2/jars 
	  :: loading settings :: url = jar:file:/Users/RajT/source-code/spark-source/spark-1.6.1/assembly/target/scala-2.10/spark-assembly-1.6.2-SNAPSHOT-hadoop2.2.0.jar!/org/apache/ivy/core/settings/ivysettings.xml 
	  graphframes#graphframes added as a dependency 
	  :: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0 
	  confs: [default] 
	  found graphframes#graphframes;0.1.0-spark1.6 in list 
	  :: resolution report :: resolve 145ms :: artifacts dl 2ms 
	  :: modules in use: 
	  graphframes#graphframes;0.1.0-spark1.6 from list in [default] 
	  --------------------------------------------------------------------- 
	  |                  |            modules            ||   artifacts   | 
	  |       conf       | number| search|dwnlded|evicted|| number|dwnlded| 
	  --------------------------------------------------------------------- 
	  |      default     |   1   |   0   |   0   |   0   ||   1   |   0   | 
	  --------------------------------------------------------------------- 
	  :: retrieving :: org.apache.spark#spark-submit-parent 
	  confs: [default] 
	  0 artifacts copied, 1 already retrieved (0kB/5ms) 
	  16/07/29 07:09:08 WARN NativeCodeLoader: 
	  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 
	  Welcome to 
      ____              __ 
     / __/__  ___ _____/ /__ 
    _\ \/ _ \/ _ `/ __/  '_/ 
   /___/ .__/\_,_/_/ /_/\_\   version 1.6.1 
      /_/ 

	  Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_66) 
	  Type in expressions to have them evaluated. 
	  Type :help for more information. 
	  Spark context available as sc. 
	  SQL context available as sqlContext. 
	  scala> import org.graphframes._ 
	  import org.graphframes._ 
	  scala> import org.apache.spark.rdd.RDD 
	  import org.apache.spark.rdd.RDD 
	  scala> import org.apache.spark.sql.Row 
	  import org.apache.spark.sql.Row 
	  scala> import org.apache.spark.graphx._ 
	  import org.apache.spark.graphx._ 
	  scala> //Create a DataFrame of users containing tuple values with a mandatory String field as id and another String type as the property of the vertex. Here it can be seen that the vertex identifier is no longer a long integer. 
	  scala> val users = sqlContext.createDataFrame(List(("1", "Thomas"),("2", "Krish"),("3", "Mathew"))).toDF("id", "name") 
	  users: org.apache.spark.sql.DataFrame = [id: string, name: string] 
	  scala> //Create a DataFrame for Edge with String type as the property of the edge 
	  scala> val userRelationships = sqlContext.createDataFrame(List(("1", "2", "Follows"),("2", "1", "Follows"),("2", "3", "Follows"))).toDF("src", "dst", "relationship") 
	  userRelationships: org.apache.spark.sql.DataFrame = [src: string, dst: string, relationship: string] 
	  scala> //Create the GraphFrame 
	  scala> val userGraph = GraphFrame(users, userRelationships) 
	  userGraph: org.graphframes.GraphFrame = GraphFrame(v:[id: string, name: string], e:[src: string, dst: string, relationship: string]) 
	  scala> // Search for pairs of users who are following each other 
	  scala> // In other words the query can be read like this. Find the list of users having a pattern such that user u1 is related to user u2 using the edge e1 and user u2 is related to the user u1 using the edge e2\. When a query is formed like this, the result will list with columns u1, u2, e1 and e2\. When modelling real-world use cases, more meaningful variables can be used suitable for the use case. 
	  scala> val graphQuery = userGraph.find("(u1)-[e1]->(u2); (u2)-[e2]->(u1)") 
	  graphQuery: org.apache.spark.sql.DataFrame = [e1: struct<src:string,dst:string,relationship:string>, u1: struct<
	  d:string,name:string>, u2: struct<id:string,name:string>, e2: struct<src:string,dst:string,relationship:string>] 
	  scala> graphQuery.show() 
	  +-------------+----------+----------+-------------+

	  |           e1|        u1|        u2|           e2| 
	  +-------------+----------+----------+-------------+ 
	  |[1,2,Follows]|[1,Thomas]| [2,Krish]|[2,1,Follows]| 
	  |[2,1,Follows]| [2,Krish]|[1,Thomas]|[1,2,Follows]| 
	  +-------------+----------+----------+-------------+

```

请注意，图形查询结果中的列是由搜索模式中给出的元素组成的。图案的形成方式没有限制。

### 注

请注意图形查询结果的数据类型。它是一个数据帧对象。这为使用熟悉的 Spark SQL 库处理查询结果带来了极大的灵活性。

Spark GraphX 库的最大限制是其 API 目前不被 Python 和 r 等编程语言支持，由于 GraphFrames 是一个基于 DataFrame 的库，一旦成熟，它将启用 DataFrames 支持的所有编程语言的图形处理。这款 Spark 外部套装绝对是 Spark 的潜在候选产品。

# 参考文献

有关更多信息，请访问以下链接:

*   [https://spark . Apache . org/docs/1 . 5 . 2/graphx-programming-guide . html](https://spark.apache.org/docs/1.5.2/graphx-programming-guide.html)
*   [https://en . Wikipedia . org/wiki/2015 _ ATP _ World _ Tour _ 总决赛 _ % E2 % 80 % 93 _ 单打](https://en.wikipedia.org/wiki/2015_ATP_World_Tour_Finals_%E2%80%93_Singles)
*   [http://www.protennislive.com/posting/2015/605/mds.pdf](http://www.protennislive.com/posting/2015/605/mds.pdf)
*   [http://infolab.stanford.edu/~backrub/google.html](http://infolab.stanford.edu/~backrub/google.html)
*   [http://graph frames . github . io/index . html](http://graphframes.github.io/index.html)
*   [https://github . com/graph frames/graph frames](https://github.com/graphframes/graphframes)
*   [https://spark-packages.org/package/graphframes/graphframes](https://spark-packages.org/package/graphframes/graphframes)

# 总结

图是一种非常有用的数据结构，具有很大的应用潜力。尽管它在大多数应用程序中并不常用，但是在一些独特的应用程序用例中，使用 Graph 作为数据结构是必不可少的。数据结构只有在与经过良好测试和高度优化的算法结合使用时才能有效使用。数学家和计算机科学家已经提出了许多算法来处理作为图形数据结构一部分的数据。Spark GraphX 库在 Spark 核心之上实现了大量这样的算法。本章对 Spark GraphX 库进行了一次旋风式的浏览，并在入门级别通过用例介绍了一些基础知识。

名为 GraphFrames 的基于 DataFrame 的图形抽象包含在独立于 Spark 的外部 Spark 包中，在图形处理和图形查询方面具有巨大的潜力。为了进行图形查询以在图形中找到模式，我们提供了这个外部 Spark 包的简要介绍。

任何教授新技术的书都必须以涵盖其显著特征的应用作为结尾。火花也不例外。到目前为止，本书已经涵盖了 Spark 作为下一代数据处理平台。现在是捆绑所有松散的部分并构建端到端应用程序的时候了。下一章将介绍使用 Spark 的数据处理应用程序的设计和开发，以及在此基础上构建的一系列库。*