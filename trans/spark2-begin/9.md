# 第九章。设计火花应用

功能性思考。想象一下，应用程序的功能被设计成一条管道，每一部分都连接在一起，共同完成整个工作的某一部分。这一切都是为了处理数据，而这正是 Spark 以高度通用的方式所做的。数据处理从进入处理管道的种子数据开始。种子数据可以是摄入系统的新数据，也可以是位于企业数据存储中的某种主数据集，需要对其进行切片和切割以生成不同的视图，从而满足各种目的和业务需求。设计和开发数据处理应用程序时，这种切片和切割将成为常态。

任何应用程序开发练习都是从研究领域、业务需求和技术工具选择开始的。这里不会有什么不同。尽管本章将介绍 Spark 应用程序的设计和开发，但最初的重点将是数据处理应用程序的整体架构、用例、数据以及将数据从一种状态转换为另一种状态的应用程序。Spark 只是一个将数据处理逻辑和数据组装在一起的驱动程序，使用其强大的基础设施来产生所需的结果。

我们将在本章中讨论以下主题:

*   λ架构
*   火花微博
*   数据字典
*   编程风格
*   数据摄取

# λ架构

应用程序架构对于任何类型的软件开发都非常重要。它是决定如何构建软件的蓝图，具有良好的通用性和在需要时进行定制的能力。对于常见的应用需求，一些流行的架构是可用的，并且不需要任何基础架构工作来使用它们。这些公共架构框架是由一些最优秀的人为了大众的利益而设计的。这些流行的架构非常有用，因为没有进入的障碍，而且被这么多人使用。有一些流行的架构可用于 web 应用程序开发、数据处理等。

Lambda 架构是最近流行的架构，非常适合开发数据处理应用程序。市场上有许多工具和技术可以用来开发数据处理应用程序。但是独立于技术，数据处理应用程序组件如何分层和组合在一起是由架构框架驱动的。这就是为什么 Lambda Architecture 是一个技术不可知的架构框架，并且根据需要，可以做出适当的技术选择来开发各个组件。*图 1* 抓住了 Lambda 架构的精髓:

![Lambda Architecture](graphics/image_09_001.jpg)

图 1

Lambda 架构由三层组成:

*   批处理层是主要的数据存储。任何类型的处理都发生在这个数据集上。这是黄金数据集。
*   服务层处理主数据集，并为特定目的准备视图，这里它们被称为目的视图。这个中间处理步骤是为查询服务或者为特定需求生成输出所必需的。查询和特定数据集准备不直接访问主数据集。
*   速度层是关于数据流处理的。数据流以实时方式进行处理，如果业务需要，可以准备可变的实时视图。生成输出的查询或特定过程可能会消耗来自目标数据视图和实时视图的数据。

使用 Lambda 架构的原理来构建一个大数据处理系统，Spark 将在这里被用作一个数据处理工具。Spark 非常适合所有三个不同层的所有数据处理需求。

本章将讨论微博应用程序的一些选定的数据处理用例。应用程序功能、其部署基础架构和可伸缩性因素超出了这项工作的范围。在典型的批处理层中，主数据集可以是普通的可拆分序列化格式或 NoSQL 数据存储，具体取决于数据访问方法。如果应用程序用例都是批处理操作，那么标准的序列化格式就足够了。但是如果用例要求随机访问，NoSQL 数据存储将是理想的选择。这里，为了简单起见，所有的数据文件都存储在本地的纯文本文件中。

典型的应用程序开发最终会形成一个功能齐全的应用程序。但是在这里，用例是在 Spark 数据处理应用程序中实现的。数据处理始终作为主应用程序功能的一部分工作，并计划以批处理模式运行，或者作为等待数据并处理数据的侦听器运行。因此，对应于每个用例，开发了单独的 Spark 应用程序，并且可以根据具体情况安排或使它们在监听器模式下运行。

# Lambda 架构的微博

博客已经以各种形式存在了几十年。在博客作为出版媒介的最初几天，只有专业或有抱负的作家通过博客媒介发表文章。它传播了一种错误的观念，即只有严肃的内容才会通过博客发布。近年来，微博的概念包括了博客文化中的普通大众。微博是人的思维过程以几句话、照片、视频或链接的形式突然爆发。推特和 Tumblr 等网站以尽可能大的规模普及了这种文化，有数亿活跃用户使用该网站。

## 微博概述

**SfbMicroBlog** 是一款微博应用，有数百万用户发布短消息。将要使用此应用程序的新用户需要使用用户名和密码注册。要发布消息，用户必须先登录。用户无需登录就能做的唯一一件事就是阅读用户发布的公共消息。用户可以跟随其他用户。跟随的行为是单向的关系。如果用户 A 跟随用户 B，则用户 A 可以看到用户 B 发布的所有消息；同时，用户 B 无法看到用户 A 发布的消息，因为用户 B 没有跟随用户 A，默认情况下，所有用户发布的消息都是公共消息，每个人都可以看到。但是用户有一些设置，使消息只对关注消息所有者的用户可见。成为追随者后，也允许不跟随。

用户名在所有用户中必须是唯一的。登录需要用户名和密码。每个用户都必须有一个主要的电子邮件地址，没有这个地址，注册过程将无法完成。为了获得额外的安全性和密码恢复，可以在配置文件中保存备用电子邮件地址或手机号码。

消息不能超过 140 个字符的长度。消息可以包含前缀为#符号的单词，以将它们分组到不同的主题下。消息可以包含前缀为@符号的用户名，以通过发布的消息直接寻址用户。换句话说，用户可以在他们的消息中称呼任何其他用户，而无需成为追随者。

一旦发布，消息就不能更改。一旦发布，这些消息就不能删除。

## 熟悉数据

所有进入主数据集的数据都来自一个流。处理数据流，检查每条消息的适当报头，并完成将其存储在数据存储中的正确操作。以下列表包含通过同一数据流进入商店的重要数据项:

*   **用户**:该数据集包含用户登录时或用户数据发生变化时的用户详细信息
*   **关注者**:该数据集包含当一个用户选择关注另一个用户时获取的关系数据
*   **消息**:该数据集包含注册用户发布的消息

这个数据集列表构成了黄金数据集。基于这个主数据集，创建了各种视图来满足应用程序中重要业务功能的需求。以下列表包含主数据集的重要视图:

*   **用户留言**:此视图包含系统中每个用户发布的留言。当给定用户想要查看他/她发布的消息时，将使用该视图生成的数据。这也是给定用户的追随者使用的。这是主数据集用于特定目的的情况。消息数据集给出了该视图所需的所有数据。
*   **给用户的消息**:在消息中，可以通过在@符号前加上收件人的用户名来称呼特定的用户。该数据视图包含用@符号寻址的用户和相应的消息。实现中有一个限制:一封邮件只能有一个收件人。
*   **标记消息**:在消息中，前缀为#符号的单词成为可搜索的消息。例如，消息中的单词#spark 表示该消息可通过单词#spark 进行搜索。对于给定的标签，用户可以在一个列表中看到所有的公共消息和他/她关注的用户的消息。这个视图包含成对的标签和相应的消息。实现中有一个限制:一条消息中只能有一个标签。
*   **关注用户**:该视图包含关注给定用户的用户列表。在*图 2* 中，用户 **U1** 和 **U3** 在 **U4** 之后的用户列表中。
*   **关注用户**:该视图包含给定用户关注的用户列表。在*图 2* 中，用户 **U2** 和 **U4** 在用户列表中，用户**之后是 U1** :

![Getting familiar with data](graphics/image_09_002.jpg)

图 2

简而言之，*图 3* 给出了解决方案的 Lambda Architecture 视图，并给出了数据集和相应视图的细节:

![Getting familiar with data](graphics/image_09_003-1.jpg)

图 3

## 设置数据字典

数据字典描述数据、其含义及其与其他数据项的关系。对于 SfbMicroBlog 应用程序，数据字典将是一个非常小的字典，用于实现所选的用例。以此为基础，读者可以扩展和实现自己的数据项，并包括数据处理用例。给出了所有主数据集以及数据视图的数据字典。

下表显示了用户数据集的数据项:

<colgroup><col> <col> <col></colgroup> 
| **用户数据** | **类型** | **目的** |
| 身份 | 长的 | 用于唯一标识用户，也是用户关系图中的顶点标识符 |
| 用户名 | 线 | 用于唯一标识系统用户 |
| 西方人名的第一个字 | 线 | 用于捕获用户的名字 |
| 姓 | 线 | 用于捕获用户的姓氏 |
| 电子邮件 | 线 | 用于与用户交流 |
| 备用电子邮件 | 线 | 用于密码恢复 |
| 主电话 | 线 | 用于密码恢复 |

下表捕获了跟随者数据集的数据项:

<colgroup><col> <col> <col></colgroup> 
| **跟随者数据** | **类型** | **目的** |
| 追随者用户名 | 线 | 用于识别谁是追随者 |
| 跟随用户名 | 线 | 用于识别谁被跟踪 |

下表捕获消息数据集的数据项:

<colgroup><col> <col> <col></colgroup> 
| **消息数据** | **类型** | **目的** |
| 用户名 | 线 | 用于捕获发布消息的用户 |
| 消息标识 | 长的 | 用于唯一标识消息 |
| 消息 | 线 | 用于捕获正在发布的消息 |
| 时间戳 | 长的 | 用于捕获邮件发布的时间 |

下表捕获了“给用户的消息”视图的数据项:

<colgroup><col> <col> <col></colgroup> 
| **给用户的消息数据** | **类型** | **目的** |
| 来自用户名 | 线 | 用于捕获发布消息的用户 |
| 至用户名 | 线 | 用于捕获消息所发往的用户；它是以@符号为前缀的用户名 |
| 消息标识 | 长的 | 用于唯一标识消息 |
| 消息 | 线 | 用于捕获正在发布的消息 |
| 时间戳 | 长的 | 用于捕获邮件发布的时间 |

下表捕获了标记邮件视图的数据项:

<colgroup><col> <col> <col></colgroup> 
| **标记信息数据** | **类型** | **目的** |
| 标签 | 线 | 前缀为#符号的单词 |
| 用户名 | 线 | 用于捕获发布消息的用户 |
| 消息标识 | 长的 | 用于唯一标识消息 |
| 消息 | 线 | 用于捕获正在发布的消息 |
| 时间戳 | 长的 | 用于捕获邮件发布的时间 |

用户的追随者关系非常简单，由保存在数据存储中的用户标识号对组成。

# 实现 Lambda 架构

Lambda 体系结构的概念是在本章的开头介绍的。因为它是一个与技术无关的架构框架，所以在用它设计应用程序时，捕捉特定实现中使用的技术选择是非常必要的。下面几节正是这么做的。

## 批次层

批处理层的核心是数据存储。对于大数据应用，数据存储有很多选择。通常情况下， **Hadoop 分布式文件系统** ( **HDFS** )结合 Hadoop 纱是当前公认的数据存储平台，主要是因为能够在 Hadoop 集群中划分和分发数据。

任何持久性存储都支持两种类型的数据访问:

*   批量写入/读取
*   随机写入/读取

这两者都需要单独的数据存储解决方案。对于批处理数据操作，通常使用可拆分的序列化格式，如 Avro 和 Parquet。对于随机数据操作，通常使用 NoSQL 数据存储。其中一些 NoSQL 解决方案位于 HDFS 之上，而另一些则没有。不管它们是否在 HDFS 之上，它们都提供数据的分区和分布。因此，根据使用案例和正在使用的分布式平台，可以使用适当的解决方案。

在 HDFS 存储数据时，通常使用的格式如 XML 和 JSON 会失败，因为 HDFS 会对文件进行分区和分发。当这种情况发生时，这些格式有开始标记和结束标记，并且在文件中随机位置的拆分会使数据变脏。正因为如此，像 Avro 或 Parquet 这样的可拆分文件格式在 HDFS 存储非常有效。

说到 NoSQL 数据存储解决方案，市场上有很多选择，尤其是来自开源世界的选择。其中一些 NoSQL 数据存储，如 Hbase，位于 HDFS 之上。一些 NoSQL 数据存储，如 Cassandra 和 Riak，不需要 HDFS，可以部署在常规操作系统上，并且可以以无主机方式部署，因此集群中没有单点故障。NoSQL 商店的选择同样取决于组织内特定技术的使用、现有的生产支持合同以及许多其他参数。

### 类型

这本书不推荐给定的一组数据存储技术与 Spark 结合使用，因为 Spark 驱动程序在大多数流行的序列化格式和 NoSQL 数据存储中都有大量可用。换句话说，大多数数据存储供应商已经开始大力支持 Spark。最近另一个有趣的趋势是，许多著名的 ETL 工具已经开始支持 Spark，正因为如此，那些使用这种 ETL 工具的人可能会在他们的 ETL 处理管道中使用 Spark 应用程序。

在此应用程序中，为了保持简单性并避免为读者运行应用程序所需的复杂基础架构设置，既不使用基于 HDFS 的数据存储，也不使用基于 NoSQL 的数据存储。数据始终以文本文件格式存储在本地系统上。有兴趣在 HDFS 或其他 NoSQL 数据存储上试用这些示例的读者可以继续尝试，并对应用程序的数据写入/读取部分进行一些更改。

## 服务层

服务层可以使用各种方法在 Spark 中实现。如果数据不是结构化的，并且纯粹是基于对象的，那么基于 RDD 的低级方法是合适的。如果数据是结构化的，那么数据框架是理想的。这里讨论的用例是处理结构化数据，因此只要有可能，就会使用 Spark SQL 库。从数据存储中读取数据并创建关系数据库。关系数据库被转换成数据帧，所有的服务需求都是使用火花 SQL 完成的。这样，代码将变得简洁易懂。

## 速度层

速度层将被实现为一个 Spark Streaming 应用程序，使用 Kafka 作为代理，由它自己的生产者产生消息。Spark Streaming 应用程序将充当卡夫卡主题的消费者，并接收正在产生的数据。正如在涵盖火花流的章节中所讨论的，制作人可以是卡夫卡控制台制作人或卡夫卡支持的任何其他制作人。但是在这里作为消费者工作的 Spark Streaming 应用程序不会实现将处理过的消息持久化到文本文件的逻辑，因为它们通常不会在现实世界的用例中使用。使用这个应用程序作为基础，读者可以实现自己的持久性机制。

### 查询

这些查询都是从速度层和服务层生成的。由于数据是以数据帧的形式提供的，如前所述，用例的所有查询都是使用 Spark SQL 实现的。显而易见的原因是，Spark SQL 是一种统一数据源和目的地的整合技术。当读者使用本书中的示例时，当他们准备在真实的用例中实现它时，总体方法可以保持不变，但是数据源和目的地可能会有所不同。以下是可以从服务层生成的一些查询。读者可以根据自己的想象对数据字典进行必要的更改，并能够编写这些视图或查询:

*   查找由给定标签分组的消息
*   查找发送给给定用户的消息
*   查找给定用户的关注者
*   查找给定用户的关注用户

# 使用火花应用

该应用程序的主力是由许多 Spark 应用程序组成的数据处理引擎。一般来说，它们可以分为以下几种类型:

*   接收数据的 Spark Streaming 应用程序:这是主侦听器应用程序，它接收作为流的数据，并将其存储在适当的主数据集中。
*   创建目标视图和查询的 Spark 应用程序:这是用于从主数据集创建各种目标视图的应用程序。除此之外，查询也包含在这个应用程序中。
*   一个执行自定义数据处理的 Spark GraphX 应用程序:这是用于处理用户-追随者关系的应用程序。

所有这些应用程序都是独立开发的，它们是独立提交的，但是流处理应用程序将始终作为侦听器应用程序运行，以处理传入的消息。除了主数据流应用程序之外，所有其他应用程序都像常规作业一样进行调度，例如 UNIX 系统中的 cron 作业。在这个应用程序中，所有这些应用程序都产生各种目的视图。时间安排取决于应用程序的类型以及主数据集和视图之间可以承受的延迟。完全取决于业务功能。因此，本章将重点关注 Spark 应用程序开发，而不是时间安排，以便将重点放在前面几章中吸取的经验教训上。

### 类型

在实现真实用例时，将速度层的数据保存到文本文件中并不理想。为简单起见，所有数据都存储在文本文件中，以最简单的设置支持所有级别的阅读器。使用 Spark Streaming 的速度层实现是一个没有持久性逻辑的框架实现。读者可以增强这一点，将持久性引入到他们想要的数据存储中。

# 编码风格

前面几章已经讨论了编码风格，并完成了大量的 Spark 应用程序编程。到目前为止，这本书已经证明了 Spark 应用程序开发可以用 Scala、Python 和 r 来完成。在前面的大部分章节中，选择的语言是 Scala 和 Python。在本章中，同样的趋势将继续下去。仅针对 Spark GraphX 应用程序，由于没有 Python 支持，应用程序将仅在 Scala 中开发。

编码的风格将变得简单而切题。故意避免应用程序开发的错误处理和其他最佳实践，以专注于 Spark 特性。在本章中，只要有可能，代码都是从相应语言的 Spark REPL 运行的。由于完整应用程序的剖析以及作为应用程序编译、构建和运行它们的脚本已经在讨论 Spark Streaming 的章节中介绍过，源代码下载将使其作为完整的准备运行的应用程序提供。此外，涵盖 Spark Streaming 的章节讨论了完整 Spark 应用程序的剖析，包括构建和运行 Spark 应用程序的脚本。同样的方法也将用于本章将要开发的应用程序中。当运行这种独立的 Spark 应用程序时，正如本书最初几章所讨论的，读者可以启用 Spark 监控并查看应用程序的行为。为简洁起见，这里不再讨论这些问题。

# 设置源代码

*图 4* 显示了本章正在使用的源代码和数据目录的结构。由于读者应该熟悉它们，这里没有提供它们的描述，它们已经在[第 6 章](6.html "Chapter 6.  Spark Stream Processing")、*火花流处理*中介绍过。使用卡夫卡运行程序有外部库文件依赖要求。为此，下载 JAR 文件的指令在`lib`文件夹的`TODO.txt` 文件中。`submitPy.sh` 和`submit.sh` 文件也使用了 Kafta 安装中的一些`Kafka`库。所有这些外部 JAR 文件依赖已经在[第 6 章](6.html "Chapter 6.  Spark Stream Processing")、*火花流处理*中介绍过了。

![Setting up the source code](graphics/image_09_004.jpg)

图 4

# 了解数据摄取

Spark Streaming 应用程序作为侦听器应用程序工作，从它的生产者那里接收数据。由于 Kafka 将被用作消息代理，Spark Streaming 应用程序将成为它的消费者应用程序，监听它的生产者发送的消息的主题。由于批处理图层中的主数据集具有以下数据集，因此每个主题最好都有单独的卡夫卡主题，以及数据集。

*   用户数据集:用户
*   追随者数据集:追随者
*   消息数据集:消息

*图 5* 提供了基于 Kafka 的 Spark Streaming 应用程序结构的全貌:

![Understanding data ingestion](graphics/image_09_005.jpg)

图 5

由于[第 6 章](6.html "Chapter 6.  Spark Stream Processing")、*火花流处理*已经介绍了卡夫卡设置，这里只介绍应用程序代码。

以下脚本是从终端窗口运行的。确保`$KAFKA_HOME`环境变量指向安装 Kafka 的目录。此外，在单独的终端窗口中启动 Zookeeper、Kafka 服务器、Kafka 制作人和 Spark Streaming 日志事件数据处理应用程序也非常重要。一旦如脚本所示创建了必要的卡夫卡主题，适当的制作者就必须开始制作消息。在继续下一步之前，请参考[第 6 章](6.html "Chapter 6.  Spark Stream Processing")、*火花流处理*中已经介绍过的卡夫卡设置细节。

在终端窗口提示中尝试以下命令:

```
 $ # Start the Zookeeper 
$ cd $KAFKA_HOME
$ $KAFKA_HOME/bin/zookeeper-server-start.sh
 $KAFKA_HOME/config/zookeeper.properties
      [2016-07-30 12:50:15,896] INFO binding to port 0.0.0.0/0.0.0.0:2181
	  (org.apache.zookeeper.server.NIOServerCnxnFactory)

	$ # Start the Kafka broker in a separate terminal window
	$ $KAFKA_HOME/bin/kafka-server-start.sh $KAFKA_HOME/config/server.properties
      [2016-07-30 12:51:39,206] INFO [Kafka Server 0], started 
	  (kafka.server.KafkaServer)

	$ # Create the necessary Kafka topics. This is to be done in a separate terminal window
	$ $KAFKA_HOME/bin/kafka-topics.sh --create --zookeeper localhost:2181
	--replication-factor 1 --partitions 1 --topic user
      Created topic "user".
    $ $KAFKA_HOME/bin/kafka-topics.sh --create --zookeeper localhost:2181
	--replication-factor 1 --partitions 1 --topic follower
      Created topic "follower".

	$ $KAFKA_HOME/bin/kafka-topics.sh --create --zookeeper localhost:2181
	--replication-factor 1 --partitions 1 --topic message
      Created topic "message".

	$ # Start producing messages and publish to the topic "message"
	$ $KAFKA_HOME/bin/kafka-console-producer.sh --broker-list localhost:9092 
	--topic message

```

本节提供了卡夫卡主题消费者应用程序的 Scala 代码的细节，该应用程序处理卡夫卡生产者产生的消息。在运行下面的代码片段之前，假设 Kafka 已经启动并运行，所需的生产者正在产生消息，然后，如果应用程序正在运行，它将开始消费消息。用于数据摄取的 Scala 程序通过将其提交给 Spark 集群来运行。从 Scala 目录开始，如图*图 4* 所示，先编译程序，然后运行。更多说明请参考`README.txt` 文件。要编译和运行程序，需要执行以下两个命令:

```
 $ ./compile.sh
	$ ./submit.sh com.packtpub.sfb.DataIngestionApp 1

```

下面的代码是要使用前面的命令编译和运行的程序列表:

```
 /**
	The following program can be compiled and run using SBT
	Wrapper scripts have been provided with thisThe following script can be run to compile the code
	./compile.sh
	The following script can be used to run this application in Spark.
	The second command line argument of value 1 is very important.
	This is to flag the shipping of the kafka jar files to the Spark cluster
	./submit.sh com.packtpub.sfb.DataIngestionApp 1
	**/
	package com.packtpub.sfb
	import java.util.HashMap
	import org.apache.spark.streaming._
	import org.apache.spark.sql.{Row, SparkSession}
	import org.apache.spark.streaming.kafka._
	import org.apache.kafka.clients.producer.{ProducerConfig, KafkaProducer, ProducerRecord}
	import org.apache.spark.storage.StorageLevel
	import org.apache.log4j.{Level, Logger}
	object DataIngestionApp {
	def main(args: Array[String]) {
	// Log level settings
	LogSettings.setLogLevels()
	//Check point directory for the recovery
	val checkPointDir = "/tmp"
    /**
    * The following function has to be used to have checkpointing and driver recovery
    * The way it should be used is to use the StreamingContext.getOrCreate with this function and do a start of that
	* This function example has been discussed but not used in the chapter covering Spark Streaming. But here it is being used    */
    def sscCreateFn(): StreamingContext = {
	// Variables used for creating the Kafka stream
	// Zookeeper host
	val zooKeeperQuorum = "localhost"
	// Kaka message group
	val messageGroup = "sfb-consumer-group"
	// Kafka topic where the programming is listening for the data
	// Reader TODO: Here only one topic is included, it can take a comma separated string containing the list of topics. 
	// Reader TODO: When using multiple topics, use your own logic to extract the right message and persist to its data store
	val topics = "message"
	val numThreads = 1     
	// Create the Spark Session, the spark context and the streaming context      
	val spark = SparkSession
	.builder
	.appName(getClass.getSimpleName)
	.getOrCreate()
	val sc = spark.sparkContext
	val ssc = new StreamingContext(sc, Seconds(10))
	val topicMap = topics.split(",").map((_, numThreads.toInt)).toMap
	val messageLines = KafkaUtils.createStream(ssc, zooKeeperQuorum, messageGroup, topicMap).map(_._2)
	// This is where the messages are printed to the console. 
	// TODO - As an exercise to the reader, instead of printing messages to the console, implement your own persistence logic
	messageLines.print()
	//Do checkpointing for the recovery
	ssc.checkpoint(checkPointDir)
	// return the Spark Streaming Context
	ssc
    }
	// Note the function that is defined above for creating the Spark streaming context is being used here to create the Spark streaming context. 
	val ssc = StreamingContext.getOrCreate(checkPointDir, sscCreateFn)
	// Start the streaming
    ssc.start()
	// Wait till the application is terminated               
    ssc.awaitTermination() 
	}
	}
	object LogSettings {
	/** 
	Necessary log4j logging level settings are done 
	*/
	def setLogLevels() {
    val log4jInitialized = Logger.getRootLogger.getAllAppenders.hasMoreElements
    if (!log4jInitialized) {
	// This is to make sure that the console is clean from other INFO messages printed by Spark
	Logger.getRootLogger.setLevel(Level.INFO)
    }
	}
	}

```

用于数据摄取的 Python 程序通过将其提交给 Spark 集群来运行。从 Python 目录开始，如图*图 4* 所示，运行程序。更多说明请参考`README.txt`文件。所有的 Kafka 安装要求都是有效的，即使是在运行这个 Python 程序的时候。运行程序时应遵循以下命令。由于 Python 是一种解释语言，这里不需要编译:

```
 $ ./submitPy.sh DataIngestionApp.py 1

```

以下代码片段是同一应用程序的 Python 实现:

```
 # The following script can be used to run this application in Spark
# ./submitPy.sh DataIngestionApp.py 1
  from __future__ import print_function
  import sys
  from pyspark import SparkContext
  from pyspark.streaming import StreamingContext
  from pyspark.streaming.kafka import KafkaUtils
  if __name__ == "__main__":
# Create the Spark context
  sc = SparkContext(appName="DataIngestionApp")
  log4j = sc._jvm.org.apache.log4j
  log4j.LogManager.getRootLogger().setLevel(log4j.Level.WARN)
# Create the Spark Streaming Context with 10 seconds batch interval
  ssc = StreamingContext(sc, 10)
# Check point directory setting
  ssc.checkpoint("\tmp")
# Zookeeper host
  zooKeeperQuorum="localhost"
# Kaka message group
  messageGroup="sfb-consumer-group"
# Kafka topic where the programming is listening for the data
# Reader TODO: Here only one topic is included, it can take a comma separated  string containing the list of topics. 
# Reader TODO: When using multiple topics, use your own logic to extract the right message and persist to its data store
topics = "message"
numThreads = 1    
# Create a Kafka DStream
kafkaStream = KafkaUtils.createStream(ssc, zooKeeperQuorum, messageGroup, {topics: numThreads})
messageLines = kafkaStream.map(lambda x: x[1])
# This is where the messages are printed to the console. Instead of this, implement your own persistence logic
messageLines.pprint()
# Start the streaming
ssc.start()
# Wait till the application is terminated   
ssc.awaitTermination() 

```

# 生成目标视图和查询

Scala 和 Python 中的以下实现是创建本章前面部分讨论的目标视图和查询的应用程序。在 Scala REPL 提示符下，尝试以下语句:

```
 //TODO: Change the following directory to point to your data directory
scala> val dataDir = "/Users/RajT/Documents/Writing/SparkForBeginners/To-PACKTPUB/Contents/B05289-09-DesigningSparkApplications/Code/Data/"
      dataDir: String = /Users/RajT/Documents/Writing/SparkForBeginners/To-PACKTPUB/Contents/B05289-09-DesigningSparkApplications/Code/Data/
    scala> //Define the case classes in Scala for the entities
	scala> case class User(Id: Long, UserName: String, FirstName: String, LastName: String, EMail: String, AlternateEmail: String, Phone: String)
      defined class User
    scala> case class Follow(Follower: String, Followed: String)
      defined class Follow
    scala> case class Message(UserName: String, MessageId: Long, ShortMessage: String, Timestamp: Long)
      defined class Message
    scala> case class MessageToUsers(FromUserName: String, ToUserName: String, MessageId: Long, ShortMessage: String, Timestamp: Long)
      defined class MessageToUsers
    scala> case class TaggedMessage(HashTag: String, UserName: String, MessageId: Long, ShortMessage: String, Timestamp: Long)
      defined class TaggedMessage
    scala> //Define the utility functions that are to be passed in the applications
	scala> def toUser =  (line: Seq[String]) => User(line(0).toLong, line(1), line(2),line(3), line(4), line(5), line(6))
      toUser: Seq[String] => User
    scala> def toFollow =  (line: Seq[String]) => Follow(line(0), line(1))
      toFollow: Seq[String] => Follow
    scala> def toMessage =  (line: Seq[String]) => Message(line(0), line(1).toLong, line(2), line(3).toLong)
      toMessage: Seq[String] => Message
    scala> //Load the user data into a Dataset
	scala> val userDataDS = sc.textFile(dataDir + "user.txt").map(_.split("\\|")).map(toUser(_)).toDS()
      userDataDS: org.apache.spark.sql.Dataset[User] = [Id: bigint, UserName: string ... 5 more fields]
    scala> //Convert the Dataset into data frame
	scala> val userDataDF = userDataDS.toDF()
      userDataDF: org.apache.spark.sql.DataFrame = [Id: bigint, UserName: string ... 5 more fields]
    scala> userDataDF.createOrReplaceTempView("user")
	scala> userDataDF.show()
      +---+--------+---------+--------+--------------------+----------------+--------------+

      | Id|UserName|FirstName|LastName|               EMail|  AlternateEmail|         Phone|

      +---+--------+---------+--------+--------------------+----------------+--------------+

      |  1| mthomas|     Mark|  Thomas| mthomas@example.com|mt12@example.com|+4411860297701|

      |  2|mithomas|  Michael|  Thomas|mithomas@example.com| mit@example.com|+4411860297702|

      |  3|  mtwain|     Mark|   Twain|  mtwain@example.com| mtw@example.com|+4411860297703|

      |  4|  thardy|   Thomas|   Hardy|  thardy@example.com|  th@example.com|+4411860297704|

      |  5| wbryson|  William|  Bryson| wbryson@example.com|  bb@example.com|+4411860297705|

      |  6|   wbrad|  William|Bradford|   wbrad@example.com|  wb@example.com|+4411860297706|

      |  7| eharris|       Ed|  Harris| eharris@example.com|  eh@example.com|+4411860297707|

      |  8|   tcook|   Thomas|    Cook|   tcook@example.com|  tk@example.com|+4411860297708|

      |  9| arobert|     Adam|  Robert| arobert@example.com|  ar@example.com|+4411860297709|

      | 10|  jjames|    Jacob|   James|  jjames@example.com|  jj@example.com|+4411860297710|

      +---+--------+---------+--------+--------------------+----------------+--------------+
    scala> //Load the follower data into an Dataset
	scala> val followerDataDS = sc.textFile(dataDir + "follower.txt").map(_.split("\\|")).map(toFollow(_)).toDS()
      followerDataDS: org.apache.spark.sql.Dataset[Follow] = [Follower: string, Followed: string]
    scala> //Convert the Dataset into data frame
	scala> val followerDataDF = followerDataDS.toDF()
      followerDataDF: org.apache.spark.sql.DataFrame = [Follower: string, Followed: string]
    scala> followerDataDF.createOrReplaceTempView("follow")
	scala> followerDataDF.show()
      +--------+--------+

      |Follower|Followed|

      +--------+--------+

      | mthomas|mithomas|

      | mthomas|  mtwain|

      |  thardy| wbryson|

      |   wbrad| wbryson|

      | eharris| mthomas|

      | eharris|   tcook|

      | arobert|  jjames|

      +--------+--------+
    scala> //Load the message data into an Dataset
	scala> val messageDataDS = sc.textFile(dataDir + "message.txt").map(_.split("\\|")).map(toMessage(_)).toDS()
      messageDataDS: org.apache.spark.sql.Dataset[Message] = [UserName: string, MessageId: bigint ... 2 more fields]
    scala> //Convert the Dataset into data frame
	scala> val messageDataDF = messageDataDS.toDF()
      messageDataDF: org.apache.spark.sql.DataFrame = [UserName: string, MessageId: bigint ... 2 more fields]
    scala> messageDataDF.createOrReplaceTempView("message")
	scala> messageDataDF.show()
      +--------+---------+--------------------+----------+

      |UserName|MessageId|        ShortMessage| Timestamp|

      +--------+---------+--------------------+----------+

      | mthomas|        1|@mithomas Your po...|1459009608|

      | mthomas|        2|Feeling awesome t...|1459010608|

      |  mtwain|        3|My namesake in th...|1459010776|

      |  mtwain|        4|Started the day w...|1459011016|

      |  thardy|        5|It is just spring...|1459011199|

      | wbryson|        6|Some days are rea...|1459011256|

      |   wbrad|        7|@wbryson Stuff ha...|1459011333|

      | eharris|        8|Anybody knows goo...|1459011426|

      |   tcook|        9|Stock market is p...|1459011483|

      |   tcook|       10|Dont do day tradi...|1459011539|

      |   tcook|       11|I have never hear...|1459011622|

      |   wbrad|       12|#Barcelona has pl...|1459157132|

      |  mtwain|       13|@wbryson It is go...|1459164906|

      +--------+---------+--------------------+----------+ 

```

这些步骤完成了将所有所需数据从持久存储加载到数据框的过程。这里，数据来自文本文件。在现实世界的用例中，它可能来自流行的 NoSQL 数据存储、传统的关系数据库管理系统表，或者来自从 HDFS 加载的 Avro 或 Parquet 序列化数据存储。

下一节使用这些数据帧并创建各种目的视图和查询:

```
 scala> //Create the purposed view of the message to users
	scala> val messagetoUsersDS = messageDataDS.filter(_.ShortMessage.contains("@")).map(message => (message.ShortMessage.split(" ").filter(_.contains("@")).mkString(" ").substring(1), message)).map(msgTuple => MessageToUsers(msgTuple._2.UserName, msgTuple._1, msgTuple._2.MessageId, msgTuple._2.ShortMessage, msgTuple._2.Timestamp))
      messagetoUsersDS: org.apache.spark.sql.Dataset[MessageToUsers] = [FromUserName: string, ToUserName: string ... 3 more fields]

	scala> //Convert the Dataset into data frame
	scala> val messagetoUsersDF = messagetoUsersDS.toDF()
      messagetoUsersDF: org.apache.spark.sql.DataFrame = [FromUserName: string, ToUserName: string ... 3 more fields]

	scala> messagetoUsersDF.createOrReplaceTempView("messageToUsers")
	scala> messagetoUsersDF.show()
      +------------+----------+---------+--------------------+----------+

      |FromUserName|ToUserName|MessageId|        ShortMessage| Timestamp|

      +------------+----------+---------+--------------------+----------+

      |     mthomas|  mithomas|        1|@mithomas Your po...|1459009608|

      |       wbrad|   wbryson|        7|@wbryson Stuff ha...|1459011333|

      |      mtwain|   wbryson|       13|@wbryson It is go...|1459164906|

      +------------+----------+---------+--------------------+----------+
    scala> //Create the purposed view of tagged messages 
	scala> val taggedMessageDS = messageDataDS.filter(_.ShortMessage.contains("#")).map(message => (message.ShortMessage.split(" ").filter(_.contains("#")).mkString(" "), message)).map(msgTuple => TaggedMessage(msgTuple._1, msgTuple._2.UserName, msgTuple._2.MessageId, msgTuple._2.ShortMessage, msgTuple._2.Timestamp))
      taggedMessageDS: org.apache.spark.sql.Dataset[TaggedMessage] = [HashTag: string, UserName: string ... 3 more fields]

	scala> //Convert the Dataset into data frame
	scala> val taggedMessageDF = taggedMessageDS.toDF()
      taggedMessageDF: org.apache.spark.sql.DataFrame = [HashTag: string, UserName: string ... 3 more fields]

	scala> taggedMessageDF.createOrReplaceTempView("taggedMessages")
	scala> taggedMessageDF.show()
      +----------+--------+---------+--------------------+----------+

      |   HashTag|UserName|MessageId|        ShortMessage| Timestamp|

      +----------+--------+---------+--------------------+----------+

      |#Barcelona| eharris|        8|Anybody knows goo...|1459011426|

      |#Barcelona|   wbrad|       12|#Barcelona has pl...|1459157132|

      +----------+--------+---------+--------------------+----------+

	scala> //The following are the queries given in the use cases
	scala> //Find the messages that are grouped by a given hash tag
	scala> val byHashTag = spark.sql("SELECT a.UserName, b.FirstName, b.LastName, a.MessageId, a.ShortMessage, a.Timestamp FROM taggedMessages a, user b WHERE a.UserName = b.UserName AND HashTag = '#Barcelona' ORDER BY a.Timestamp")
      byHashTag: org.apache.spark.sql.DataFrame = [UserName: string, FirstName: string ... 4 more fields]

	scala> byHashTag.show()
      +--------+---------+--------+---------+--------------------+----------+

      |UserName|FirstName|LastName|MessageId|        ShortMessage| Timestamp|

      +--------+---------+--------+---------+--------------------+----------+

      | eharris|       Ed|  Harris|        8|Anybody knows goo...|1459011426|

      |   wbrad|  William|Bradford|       12|#Barcelona has pl...|1459157132|

      +--------+---------+--------+---------+--------------------+----------+

	scala> //Find the messages that are addressed to a given user
	scala> val byToUser = spark.sql("SELECT FromUserName, ToUserName, MessageId, ShortMessage, Timestamp FROM messageToUsers WHERE ToUserName = 'wbryson' ORDER BY Timestamp")
      byToUser: org.apache.spark.sql.DataFrame = [FromUserName: string, ToUserName: string ... 3 more fields]

	scala> byToUser.show()
      +------------+----------+---------+--------------------+----------+

      |FromUserName|ToUserName|MessageId|        ShortMessage| Timestamp|

      +------------+----------+---------+--------------------+----------+

      |       wbrad|   wbryson|        7|@wbryson Stuff ha...|1459011333|

      |      mtwain|   wbryson|       13|@wbryson It is go...|1459164906|

      +------------+----------+---------+--------------------+----------+
    scala> //Find the followers of a given user
	scala> val followers = spark.sql("SELECT b.FirstName as FollowerFirstName, b.LastName as FollowerLastName, a.Followed FROM follow a, user b WHERE a.Follower = b.UserName AND a.Followed = 'wbryson'")
      followers: org.apache.spark.sql.DataFrame = [FollowerFirstName: string, FollowerLastName: string ... 1 more field]
    scala> followers.show()
      +-----------------+----------------+--------+

      |FollowerFirstName|FollowerLastName|Followed|

      +-----------------+----------------+--------+

      |          William|        Bradford| wbryson|

      |           Thomas|           Hardy| wbryson|

      +-----------------+----------------+--------+

	scala> //Find the followedUsers of a given user
	scala> val followedUsers = spark.sql("SELECT b.FirstName as FollowedFirstName, b.LastName as FollowedLastName, a.Follower FROM follow a, user b WHERE a.Followed = b.UserName AND a.Follower = 'eharris'")
      followedUsers: org.apache.spark.sql.DataFrame = [FollowedFirstName: string, FollowedLastName: string ... 1 more field]
    scala> followedUsers.show()
      +-----------------+----------------+--------+

      |FollowedFirstName|FollowedLastName|Follower|

      +-----------------+----------------+--------+

      |           Thomas|            Cook| eharris|

      |             Mark|          Thomas| eharris|

      +-----------------+----------------+--------+ 

```

在前面的 Scala 代码片段中，使用了基于数据集和数据框架的编程模型，因为选择的编程语言是 Scala。现在，由于 Python 不是强类型语言，因此 Python 中不支持数据集应用编程接口，因此下面的 Python 代码将传统的基于 RDD 的 Spark 编程模型与基于数据框架的编程模型结合使用。在 Python REPL 提示符下，尝试以下语句:

```
 >>> from pyspark.sql import Row
	>>> #TODO: Change the following directory to point to your data directory
	>>> dataDir = "/Users/RajT/Documents/Writing/SparkForBeginners/To-PACKTPUB/Contents/B05289-09-DesigningSparkApplications/Code/Data/"
	>>> #Load the user data into an RDD
	>>> userDataRDD = sc.textFile(dataDir + "user.txt").map(lambda line: line.split("|")).map(lambda p: Row(Id=int(p[0]), UserName=p[1], FirstName=p[2], LastName=p[3], EMail=p[4], AlternateEmail=p[5], Phone=p[6]))
	>>> #Convert the RDD into data frame
	>>> userDataDF = userDataRDD.toDF()
	>>> userDataDF.createOrReplaceTempView("user")
	>>> userDataDF.show()
      +----------------+--------------------+---------+---+--------+--------------+--------+

      |  AlternateEmail|               EMail|FirstName| Id|LastName|         Phone|UserName|

      +----------------+--------------------+---------+---+--------+--------------+--------+

      |mt12@example.com| mthomas@example.com|     Mark|  1|  Thomas|+4411860297701| mthomas|

      | mit@example.com|mithomas@example.com|  Michael|  2|  Thomas|+4411860297702|mithomas|

      | mtw@example.com|  mtwain@example.com|     Mark|  3|   Twain|+4411860297703|  mtwain|

      |  th@example.com|  thardy@example.com|   Thomas|  4|   Hardy|+4411860297704|  thardy|

      |  bb@example.com| wbryson@example.com|  William|  5|  Bryson|+4411860297705| wbryson|

      |  wb@example.com|   wbrad@example.com|  William|  6|Bradford|+4411860297706|   wbrad|

      |  eh@example.com| eharris@example.com|       Ed|  7|  Harris|+4411860297707| eharris|

      |  tk@example.com|   tcook@example.com|   Thomas|  8|    Cook|+4411860297708|   tcook|

      |  ar@example.com| arobert@example.com|     Adam|  9|  Robert|+4411860297709| arobert|

      |  jj@example.com|  jjames@example.com|    Jacob| 10|   James|+4411860297710|  jjames|

      +----------------+--------------------+---------+---+--------+--------------+--------+

	>>> #Load the follower data into an RDD
	>>> followerDataRDD = sc.textFile(dataDir + "follower.txt").map(lambda line: line.split("|")).map(lambda p: Row(Follower=p[0], Followed=p[1]))
	>>> #Convert the RDD into data frame
	>>> followerDataDF = followerDataRDD.toDF()
	>>> followerDataDF.createOrReplaceTempView("follow")
	>>> followerDataDF.show()
      +--------+--------+

      |Followed|Follower|

      +--------+--------+

      |mithomas| mthomas|

      |  mtwain| mthomas|

      | wbryson|  thardy|

      | wbryson|   wbrad|

      | mthomas| eharris|

      |   tcook| eharris|

      |  jjames| arobert|

      +--------+--------+

	>>> #Load the message data into an RDD
	>>> messageDataRDD = sc.textFile(dataDir + "message.txt").map(lambda line: line.split("|")).map(lambda p: Row(UserName=p[0], MessageId=int(p[1]), ShortMessage=p[2], Timestamp=int(p[3])))
	>>> #Convert the RDD into data frame
	>>> messageDataDF = messageDataRDD.toDF()
	>>> messageDataDF.createOrReplaceTempView("message")
	>>> messageDataDF.show()
      +---------+--------------------+----------+--------+

      |MessageId|        ShortMessage| Timestamp|UserName|

      +---------+--------------------+----------+--------+

      |        1|@mithomas Your po...|1459009608| mthomas|

      |        2|Feeling awesome t...|1459010608| mthomas|

      |        3|My namesake in th...|1459010776|  mtwain|

      |        4|Started the day w...|1459011016|  mtwain|

      |        5|It is just spring...|1459011199|  thardy|

      |        6|Some days are rea...|1459011256| wbryson|

      |        7|@wbryson Stuff ha...|1459011333|   wbrad|

      |        8|Anybody knows goo...|1459011426| eharris|

      |        9|Stock market is p...|1459011483|   tcook|

      |       10|Dont do day tradi...|1459011539|   tcook|

      |       11|I have never hear...|1459011622|   tcook|

      |       12|#Barcelona has pl...|1459157132|   wbrad|

      |       13|@wbryson It is go...|1459164906|  mtwain|

      +---------+--------------------+----------+--------+ 

```

这些步骤完成了将所有所需数据从持久存储加载到数据框的过程。这里，数据来自文本文件。在现实世界的用例中，它可能来自流行的 NoSQL 数据存储、传统的关系数据库管理系统表，或者来自从 HDFS 加载的 Avro 或 Parquet 序列化数据存储。下一节使用这些数据帧并创建各种目的视图和查询:

```
 >>> #Create the purposed view of the message to users
	>>> messagetoUsersRDD = messageDataRDD.filter(lambda message: "@" in message.ShortMessage).map(lambda message : (message, " ".join(filter(lambda s: s[0] == '@', message.ShortMessage.split(" "))))).map(lambda msgTuple: Row(FromUserName=msgTuple[0].UserName, ToUserName=msgTuple[1][1:], MessageId=msgTuple[0].MessageId, ShortMessage=msgTuple[0].ShortMessage, Timestamp=msgTuple[0].Timestamp))
	>>> #Convert the RDD into data frame
	>>> messagetoUsersDF = messagetoUsersRDD.toDF()
	>>> messagetoUsersDF.createOrReplaceTempView("messageToUsers")
	>>> messagetoUsersDF.show()
      +------------+---------+--------------------+----------+----------+

      |FromUserName|MessageId|        ShortMessage| Timestamp|ToUserName|

      +------------+---------+--------------------+----------+----------+

      |     mthomas|        1|@mithomas Your po...|1459009608|  mithomas|

      |       wbrad|        7|@wbryson Stuff ha...|1459011333|   wbryson|

      |      mtwain|       13|@wbryson It is go...|1459164906|   wbryson|

      +------------+---------+--------------------+----------+----------+

	>>> #Create the purposed view of tagged messages 
	>>> taggedMessageRDD = messageDataRDD.filter(lambda message: "#" in message.ShortMessage).map(lambda message : (message, " ".join(filter(lambda s: s[0] == '#', message.ShortMessage.split(" "))))).map(lambda msgTuple: Row(HashTag=msgTuple[1], UserName=msgTuple[0].UserName, MessageId=msgTuple[0].MessageId, ShortMessage=msgTuple[0].ShortMessage, Timestamp=msgTuple[0].Timestamp))
	>>> #Convert the RDD into data frame
	>>> taggedMessageDF = taggedMessageRDD.toDF()
	>>> taggedMessageDF.createOrReplaceTempView("taggedMessages")
	>>> taggedMessageDF.show()
      +----------+---------+--------------------+----------+--------+

      |   HashTag|MessageId|        ShortMessage| Timestamp|UserName|

      +----------+---------+--------------------+----------+--------+

      |#Barcelona|        8|Anybody knows goo...|1459011426| eharris|

      |#Barcelona|       12|#Barcelona has pl...|1459157132|   wbrad|

      +----------+---------+--------------------+----------+--------+

	>>> #The following are the queries given in the use cases
	>>> #Find the messages that are grouped by a given hash tag
	>>> byHashTag = spark.sql("SELECT a.UserName, b.FirstName, b.LastName, a.MessageId, a.ShortMessage, a.Timestamp FROM taggedMessages a, user b WHERE a.UserName = b.UserName AND HashTag = '#Barcelona' ORDER BY a.Timestamp")
	>>> byHashTag.show()
      +--------+---------+--------+---------+--------------------+----------+

      |UserName|FirstName|LastName|MessageId|        ShortMessage| Timestamp|

      +--------+---------+--------+---------+--------------------+----------+

      | eharris|       Ed|  Harris|        8|Anybody knows goo...|1459011426|

      |   wbrad|  William|Bradford|       12|#Barcelona has pl...|1459157132|

      +--------+---------+--------+---------+--------------------+----------+

	>>> #Find the messages that are addressed to a given user
	>>> byToUser = spark.sql("SELECT FromUserName, ToUserName, MessageId, ShortMessage, Timestamp FROM messageToUsers WHERE ToUserName = 'wbryson' ORDER BY Timestamp")
	>>> byToUser.show()
      +------------+----------+---------+--------------------+----------+

      |FromUserName|ToUserName|MessageId|        ShortMessage| Timestamp|

      +------------+----------+---------+--------------------+----------+

      |       wbrad|   wbryson|        7|@wbryson Stuff ha...|1459011333|

      |      mtwain|   wbryson|       13|@wbryson It is go...|1459164906|

      +------------+----------+---------+--------------------+----------+

	>>> #Find the followers of a given user
	>>> followers = spark.sql("SELECT b.FirstName as FollowerFirstName, b.LastName as FollowerLastName, a.Followed FROM follow a, user b WHERE a.Follower = b.UserName AND a.Followed = 'wbryson'")>>> followers.show()
      +-----------------+----------------+--------+

      |FollowerFirstName|FollowerLastName|Followed|

      +-----------------+----------------+--------+

      |          William|        Bradford| wbryson|

      |           Thomas|           Hardy| wbryson|

      +-----------------+----------------+--------+

	>>> #Find the followed users of a given user
	>>> followedUsers = spark.sql("SELECT b.FirstName as FollowedFirstName, b.LastName as FollowedLastName, a.Follower FROM follow a, user b WHERE a.Followed = b.UserName AND a.Follower = 'eharris'")
	>>> followedUsers.show()
      +-----------------+----------------+--------+

      |FollowedFirstName|FollowedLastName|Follower|

      +-----------------+----------------+--------+

      |           Thomas|            Cook| eharris|

      |             Mark|          Thomas| eharris| 
 +-----------------+----------------+--------+ 

```

实现用例所需的目标视图和查询是作为一个应用程序开发的。但是实际上，在一个应用程序中拥有所有的视图和查询并不是一个好的设计实践。最好通过保持视图并定期刷新视图来将它们分开。如果只使用一个应用程序，可以使用缓存和向 Spark 集群广播的定制上下文对象来访问视图。

# 了解定制数据流程

这里创建的视图是为了服务于各种查询并产生所需的输出。还有一些其他类型的数据处理应用程序，它们通常是为了实现真实的用例而开发的。从 Lambda 架构的角度来看，这也属于服务层。这些自定义数据流程之所以落入服务层，主要是因为这些流程大多使用或处理主数据集中的数据，并创建视图或输出。定制处理的数据也很有可能保留为视图，下面的用例就是其中之一。

在 SfbMicroBlog 微博应用中，一个非常常见的需求是查看给定用户 A 是否以某种方式以直接追随者关系或传递方式连接到用户 B。这个用例可以使用一个图形数据结构来实现，以查看这两个用户是否在同一个连接的组件中，他们是否以可传递的方式连接，或者他们是否根本没有连接。为此，使用基于 Spark GraphX 库的 Spark 应用程序，构建了一个以所有用户为顶点、以下关系为边的图。在 Scala REPL 提示符下，尝试以下语句:

```
 scala> import org.apache.spark.rdd.RDD
    import org.apache.spark.rdd.RDD    
	scala> import org.apache.spark.graphx._
    import org.apache.spark.graphx._    
	scala> //TODO: Change the following directory to point to your data directory
	scala> val dataDir = "/Users/RajT/Documents/Writing/SparkForBeginners/To-PACKTPUB/Contents/B05289-09-DesigningSparkApplications/Code/Data/"
dataDir: String = /Users/RajT/Documents/Writing/SparkForBeginners/To-PACKTPUB/Contents/B05289-09-DesigningSparkApplications/Code/Data/

	scala> //Define the case classes in Scala for the entities
	scala> case class User(Id: Long, UserName: String, FirstName: String, LastName: String, EMail: String, AlternateEmail: String, Phone: String)
      defined class User

	scala> case class Follow(Follower: String, Followed: String)
      defined class Follow

	scala> case class ConnectedUser(CCId: Long, UserName: String)
      defined class ConnectedUser

	scala> //Define the utility functions that are to be passed in the applications
	scala> def toUser =  (line: Seq[String]) => User(line(0).toLong, line(1), line(2),line(3), line(4), line(5), line(6))
      toUser: Seq[String] => User

	scala> def toFollow =  (line: Seq[String]) => Follow(line(0), line(1))
      toFollow: Seq[String] => Follow

	scala> //Load the user data into an RDD
	scala> val userDataRDD = sc.textFile(dataDir + "user.txt").map(_.split("\\|")).map(toUser(_))
userDataRDD: org.apache.spark.rdd.RDD[User] = MapPartitionsRDD[160] at map at <console>:34

	scala> //Convert the RDD into data frame
	scala> val userDataDF = userDataRDD.toDF()
userDataDF: org.apache.spark.sql.DataFrame = [Id: bigint, UserName: string ... 5 more fields]

	scala> userDataDF.createOrReplaceTempView("user")
	scala> userDataDF.show()
      +---+--------+---------+--------+-----------+----------------+--------------+

Id|UserName|FirstName|LastName| EMail|  AlternateEmail|   Phone|

      +---+--------+---------+--------+----------+-------------+--------------+

|  1| mthomas|     Mark|  Thomas| mthomas@example.com|mt12@example.com|
+4411860297701|

|  2|mithomas|  Michael|  Thomas|mithomas@example.com| mit@example.com|
+4411860297702|

|  3|  mtwain|     Mark|   Twain|  mtwain@example.com| mtw@example.com|
+4411860297703|

|  4|  thardy|   Thomas|   Hardy|  thardy@example.com|  th@example.com|
+4411860297704|

|  5| wbryson|  William|  Bryson| wbryson@example.com|  bb@example.com|
+4411860297705|

|  6|   wbrad|  William|Bradford|   wbrad@example.com|  wb@example.com|
+4411860297706|

|  7| eharris|       Ed|  Harris| eharris@example.com|  eh@example.com|
+4411860297707|

|  8|   tcook|   Thomas|    Cook|   tcook@example.com|  tk@example.com|
+4411860297708|

|  9| arobert|     Adam|  Robert| arobert@example.com|  ar@example.com|
+4411860297709|

| 10|  jjames|    Jacob|   James|  jjames@example.com|  jj@example.com|
+4411860297710|    
      +---+--------+---------+--------+-------------+--------------+--------------+

	scala> //Load the follower data into an RDD
	scala> val followerDataRDD = sc.textFile(dataDir + "follower.txt").map(_.split("\\|")).map(toFollow(_))
followerDataRDD: org.apache.spark.rdd.RDD[Follow] = MapPartitionsRDD[168] at map at <console>:34

	scala> //Convert the RDD into data frame
	scala> val followerDataDF = followerDataRDD.toDF()
followerDataDF: org.apache.spark.sql.DataFrame = [Follower: string, Followed: string]

	scala> followerDataDF.createOrReplaceTempView("follow")
	scala> followerDataDF.show()
      +--------+--------+

      |Follower|Followed|

      +--------+--------+

      | mthomas|mithomas|

      | mthomas|  mtwain|

      |  thardy| wbryson|

      |   wbrad| wbryson|

      | eharris| mthomas|

      | eharris|   tcook|

      | arobert|  jjames|

      +--------+--------+

	scala> //By joining with the follower and followee users with the master user data frame for extracting the unique ids
	scala> val fullFollowerDetails = spark.sql("SELECT b.Id as FollowerId, c.Id as FollowedId, a.Follower, a.Followed FROM follow a, user b, user c WHERE a.Follower = b.UserName AND a.Followed = c.UserName")
fullFollowerDetails: org.apache.spark.sql.DataFrame = [FollowerId: bigint, FollowedId: bigint ... 2 more fields]

	scala> fullFollowerDetails.show()
      +----------+----------+--------+--------+

      |FollowerId|FollowedId|Follower|Followed|

      +----------+----------+--------+--------+

      |         9|        10| arobert|  jjames|

      |         1|         2| mthomas|mithomas|

      |         7|         8| eharris|   tcook|

      |         7|         1| eharris| mthomas|

      |         1|         3| mthomas|  mtwain|

      |         6|         5|   wbrad| wbryson|

      |         4|         5|  thardy| wbryson|

      +----------+----------+--------+--------+

	scala> //Create the vertices of the connections graph
	scala> val userVertices: RDD[(Long, String)] = userDataRDD.map(user => (user.Id, user.UserName))
userVertices: org.apache.spark.rdd.RDD[(Long, String)] = MapPartitionsRDD[194] at map at <console>:36

	scala> userVertices.foreach(println)
      (6,wbrad)

      (7,eharris)

      (8,tcook)

      (9,arobert)

      (10,jjames)

      (1,mthomas)

      (2,mithomas)

      (3,mtwain)

      (4,thardy)

      (5,wbryson)

	scala> //Create the edges of the connections graph 
	scala> val connections: RDD[Edge[String]] = fullFollowerDetails.rdd.map(conn => Edge(conn.getAs[Long]("FollowerId"), conn.getAs[Long]("FollowedId"), "Follows"))
      connections: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[String]] = MapPartitionsRDD[217] at map at <console>:29

	scala> connections.foreach(println)
	Edge(9,10,Follows)
	Edge(7,8,Follows)
	Edge(1,2,Follows)
	Edge(7,1,Follows)
	Edge(1,3,Follows)
	Edge(6,5,Follows)
	Edge(4,5,Follows)
	scala> //Create the graph using the vertices and the edges
	scala> val connectionGraph = Graph(userVertices, connections)
      connectionGraph: org.apache.spark.graphx.Graph[String,String] = org.apache.spark.graphx.impl.GraphImpl@3c207acd 

```

用户在顶点的用户图和形成边的连接关系已经完成。在这个图数据结构上，运行图处理算法，连接组件算法。下面的代码片段实现了这一点:

```
 scala> //Calculate the connected users
	scala> val cc = connectionGraph.connectedComponents()
      cc: org.apache.spark.graphx.Graph[org.apache.spark.graphx.VertexId,String] = org.apache.spark.graphx.impl.GraphImpl@73f0bd11

	scala> // Extract the triplets of the connected users
	scala> val ccTriplets = cc.triplets
      ccTriplets: org.apache.spark.rdd.RDD[org.apache.spark.graphx.EdgeTriplet[org.apache.spark.graphx.VertexId,String]] = MapPartitionsRDD[285] at mapPartitions at GraphImpl.scala:48

	scala> // Print the structure of the triplets
	scala> ccTriplets.foreach(println)
      ((9,9),(10,9),Follows)

      ((1,1),(2,1),Follows)

      ((7,1),(8,1),Follows)

      ((7,1),(1,1),Follows)

      ((1,1),(3,1),Follows)

      ((4,4),(5,4),Follows) 
 ((6,4),(5,4),Follows) 

```

创建了连接组件图`cc`及其三元组`ccTriplets`，现在可以用它来运行各种查询。由于图表是基于 RDD 的数据结构，如果有必要进行查询，将图表 RDD 转换为数据框架是一种常见的做法。下面的代码演示了这一点:

```
 scala> //Print the vertex numbers and the corresponding connected component id. The connected component id is generated by the system and it is to be taken only as a unique identifier for the connected component
   scala> val ccProperties = ccTriplets.map(triplet => "Vertex " + triplet.srcId + " and " + triplet.dstId + " are part of the CC with id " + triplet.srcAttr)
      ccProperties: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[288] at map at <console>:48

	scala> ccProperties.foreach(println)
      Vertex 9 and 10 are part of the CC with id 9

      Vertex 1 and 2 are part of the CC with id 1

      Vertex 7 and 8 are part of the CC with id 1

      Vertex 7 and 1 are part of the CC with id 1

      Vertex 1 and 3 are part of the CC with id 1

      Vertex 4 and 5 are part of the CC with id 4

      Vertex 6 and 5 are part of the CC with id 4

	scala> //Find the users in the source vertex with their CC id
	scala> val srcUsersAndTheirCC = ccTriplets.map(triplet => (triplet.srcId, triplet.srcAttr))
      srcUsersAndTheirCC: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId)] = MapPartitionsRDD[289] at map at <console>:48

	scala> //Find the users in the destination vertex with their CC id
	scala> val dstUsersAndTheirCC = ccTriplets.map(triplet => (triplet.dstId, triplet.dstAttr))
      dstUsersAndTheirCC: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId)] = MapPartitionsRDD[290] at map at <console>:48

	scala> //Find the union
	scala> val usersAndTheirCC = srcUsersAndTheirCC.union(dstUsersAndTheirCC)
      usersAndTheirCC: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId)] = UnionRDD[291] at union at <console>:52

	scala> //Join with the name of the users
	scala> //Convert the RDD to DataFrame
	scala> val usersAndTheirCCWithName = usersAndTheirCC.join(userVertices).map{case (userId,(ccId,userName)) => (ccId, userName)}.distinct.sortByKey().map{case (ccId,userName) => ConnectedUser(ccId, userName)}.toDF()
      usersAndTheirCCWithName: org.apache.spark.sql.DataFrame = [CCId: bigint, UserName: string]

	scala> usersAndTheirCCWithName.createOrReplaceTempView("connecteduser")
	scala> val usersAndTheirCCWithDetails = spark.sql("SELECT a.CCId, a.UserName, b.FirstName, b.LastName FROM connecteduser a, user b WHERE a.UserName = b.UserName ORDER BY CCId")
      usersAndTheirCCWithDetails: org.apache.spark.sql.DataFrame = [CCId: bigint, UserName: string ... 2 more fields]

	scala> //Print the usernames with their CC component id. If two users share the same CC id, then they are connected
	scala> usersAndTheirCCWithDetails.show()
      +----+--------+---------+--------+

      |CCId|UserName|FirstName|LastName|

      +----+--------+---------+--------+

      |   1|mithomas|  Michael|  Thomas|

      |   1|  mtwain|     Mark|   Twain|

      |   1|   tcook|   Thomas|    Cook|

      |   1| eharris|       Ed|  Harris|

      |   1| mthomas|     Mark|  Thomas|

      |   4|   wbrad|  William|Bradford|

      |   4| wbryson|  William|  Bryson|

      |   4|  thardy|   Thomas|   Hardy|

      |   9|  jjames|    Jacob|   James|

      |   9| arobert|     Adam|  Robert| 
 +----+--------+---------+--------+ 

```

使用前面的一个 purposed 视图的实现来获取用户及其连接的组件标识号的列表，如果需要找出两个用户是否连接，只需读取这两个用户的记录，看看他们是否有相同的连接组件标识号。

# 参考文献

有关更多信息，请访问以下链接:

*   [http://lambda-architecture.net/](http://lambda-architecture.net/)
*   [https://www . dre . Vanderbilt . edu/~ Schmidt/PDF/Context-Object-pattern . PDF](https://www.dre.vanderbilt.edu/~schmidt/PDF/Context-Object-Pattern.pdf)

# 总结

本章以单个应用程序的用例来结束本书，这些用例是使用本书前几章中学习的 Spark 概念实现的。从数据处理应用架构的角度来看，本章讲述了 Lambda Architecture 作为数据处理应用的技术无关架构框架，在大数据应用开发领域具有巨大的适用性。

从数据处理应用程序开发的角度来看，已经介绍了基于 RDD 的 Spark 编程、基于数据集的 Spark 编程、基于 Spark SQL 的用于处理结构化数据的数据帧、基于 Spark Streaming 的用于持续监听传入消息并对其进行处理的监听器程序，以及基于 Spark GraphX 的用于处理追随者关系的应用程序。到目前为止，所涉及的用例为读者添加自己的功能和增强本章中讨论的应用用例提供了巨大的空间。