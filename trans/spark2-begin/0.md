# 零、前言

名为 Spark 的数据处理框架最初是为了证明，通过在多次迭代中重用数据集，它为 Hadoop MapReduce 作业表现不佳的地方提供了价值。研究论文 *Mesos:数据中心细粒度资源共享平台*讲述了 Spark 设计背后的理念。加州大学伯克利分校研究人员为测试 Mesos 而构建的一个非常简单的参考实现已经发展得越来越远，成为一个成熟的数据处理框架，后来成为最活跃的 Apache 项目之一。它从头开始设计，在 Hadoop、Mesos 等集群上以独立模式进行分布式数据处理。Spark 是一个基于 JVM 的数据处理框架，因此它适用于大多数支持基于 JVM 的应用的操作系统。Spark 广泛安装在 UNIX 和 Mac OS X 平台上，Windows 的采用也在不断增加。

Spark 使用编程语言 Scala、Java、Python 和 r 提供了一个统一的编程模型。换句话说，无论使用哪种语言来编程 Spark 应用，API 在所有语言中都几乎保持不变。通过这种方式，组织可以采用 Spark 并使用他们选择的编程语言开发应用。这也使得 Spark 应用能够从一种语言快速移植到另一种语言，如果需要的话，不需要太多的努力。Spark 的大部分是使用 Scala 开发的，因此 Spark 编程模型本质上支持函数式编程原则。最基本的 Spark 数据抽象是弹性分布式数据集(RDD)，所有其他库都是基于这个数据集构建的。基于 RDD 的 Spark 编程模型是开发人员可以构建数据处理应用的最低级别。

Spark 发展迅速，迎合了更多数据处理用例的需求。当对产品路线图采取这种前瞻性步骤时，就出现了让业务用户的编程水平更高的需求。Spark Core 之上的 Spark SQL 库及其数据框架抽象是为了满足大量非常熟悉无处不在的 SQL 的开发人员的需求而构建的。

数据科学家使用 R 来满足他们的计算需求。R 最大的限制是所有需要处理的数据都要*适合*进入运行 R 程序的计算机的主内存。Spark 的 R API 将数据科学家引入了他们熟悉的数据框架抽象中的分布式数据处理世界。换句话说，使用 R API 进行 Spark，数据的处理可以在 Hadoop 或 Mesos 上并行完成，增长远远超出了主机常驻内存的限制。

在当前大规模应用收集数据的时代，数据被吸收的速度非常快。许多应用用例要求对流式传输的数据进行实时处理。建立在 Spark Core 之上的 Spark Streaming 库也是如此。

静态数据或流式数据被馈送到机器学习算法，以训练数据模型，并使用它们来提供业务问题的答案。在 Spark 之前创建的所有机器学习框架在处理计算机的内存、无法进行并行处理、重复读写周期等方面都有很多限制。Spark 没有任何这些限制，因此基于 Spark Core 和 Spark DataFrames 构建的 Spark MLlib 机器学习库被证明是将数据处理管道和机器学习活动结合在一起的同类最佳机器学习库。

图形是一种非常有用的数据结构，在一些特殊的用例中大量使用。用于处理图形数据结构中的数据的算法是计算密集型的。在 Spark 之前，出现了许多图形处理框架，其中一些处理速度非常快，但是对生成图形数据结构所需的数据进行预处理却成为了大多数图形处理应用的一大瓶颈。建立在 Spark 之上的 Spark GraphX 库填补了这一空白，使数据处理和图形处理成为链式活动。

过去，存在许多数据处理框架，其中许多都是专有的，迫使组织陷入供应商锁定的陷阱。Spark 为各种数据处理需求提供了一个非常可行的替代方案，而且没有许可成本；同时，它得到了许多领先公司的支持，提供专业的生产支持。

# 这本书涵盖了什么

[第 1 章](1.html "Chapter 1. Spark Fundamentals")、 *Spark 基本面*讨论了 Spark 作为一个框架的基本面，以及它所附带的 API 和库，以及 Spark 与之交互的整个数据处理生态系统。

[第 2 章](2.html "Chapter 2. Spark Programming Model")、 *Spark 编程模型*讨论了 Spark 中使用的基于函数式编程方法论原则的统一编程模型，涵盖了弹性分布式数据集(RDD)、Spark 转换和 Spark 动作的基础。

[第 3 章](3.html "Chapter 3. Spark SQL")、 *Spark SQL* 讨论了 Spark SQL，它是最强大的 Spark 库之一，结合 Spark DataFrame API 使用无处不在的 SQL 构造来操作数据，以及它如何与 Spark 程序一起工作。本章还讨论了如何使用 Spark SQL 来访问来自各种数据源的数据，从而统一不同的数据源进行数据处理。

[第 4 章](4.html "Chapter 4. Spark Programming with R")、*带 R 的 Spark 编程*，讨论 Spark 上的 SparkR 或 R，是 Spark 的 R API 这使得 R 用户能够使用他们熟悉的数据框架抽象来利用 Spark 的数据处理能力。它为 R 用户熟悉 Spark 数据处理生态系统打下了非常好的基础。

[第 5 章](5.html "Chapter 5. Spark Data Analysis with Python")、*使用 Python 进行 Spark 数据分析*讨论了使用 Spark 进行数据处理和 Python 进行数据分析，使用了 Python 可用的各种图表和绘图库。本章讨论将这两个相关的活动组合在一起，作为 Spark 应用，并选择 Python 作为编程语言。

[第 6 章](6.html "Chapter 6.  Spark Stream Processing")、*Spark流处理*讨论Spark流，这是最强大的Spark库之一，用于捕获和处理作为流摄入的数据。还讨论了作为分布式消息代理的卡夫卡和作为消费者的 Spark Streaming 应用。

[第 7 章](7.html "Chapter 7.  Spark Machine Learning")*Spark 机器学习*讨论了 Spark MLlib，这是最强大的 Spark 库之一，用于开发入门级别的机器学习应用。

[第 8 章](8.html "Chapter 8. Spark Graph Processing")、 *Spark Graph Processing* 讨论了 Spark GraphX，这是处理图数据结构最强大的 Spark 库之一，并且附带了很多处理图中数据的算法。本章涵盖了 GraphX 的基础知识以及使用 GraphX 提供的算法实现的一些用例。

[第 9 章](9.html "Chapter 9.  Designing Spark Applications")、*设计 Spark 应用*，讨论了 Spark 数据处理应用的设计和开发，涵盖了本书前几章中涉及到的 Spark 的各种特性。

# 这本书你需要什么

Spark 2.0.0 或更高版本将安装在至少一台独立的机器上，以运行代码示例并进行进一步的活动来了解更多关于该主题的信息。对于[第 6 章](6.html "Chapter 6.  Spark Stream Processing")，Spark Stream Processing，Kafka 需要被安装和配置为一个消息代理，它的命令行生产者产生消息，而使用 Spark 开发的应用是这些消息的消费者。

# 这本书是给谁的

如果您是一名应用开发人员、数据科学家或大数据解决方案架构师，有兴趣将 Spark 的数据处理能力与 R 结合起来，并使用 Scala 或 Python 将数据处理、流处理、机器学习和图形处理整合到一个具有统一 API 的统一且高度互操作的框架中，这本书就是为您准备的。

# 惯例

在这本书里，你会发现许多区分不同种类信息的文本样式。以下是这些风格的一些例子和对它们的意义的解释。

文本中的码字、数据库表名、文件夹名、文件名、文件扩展名、路径名、伪 URL、用户输入和 Twitter 句柄如下所示:“最好自定义这个属性`spark.driver.memory`使其具有更高的值。”

代码块设置如下:

```scala
Python 3.5.0 (v3.5.0:374f501f4567, Sep 12 2015, 11:00:19)
[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin
```

任何命令行输入或输出都编写如下:

```scala
$ python 
Python 3.5.0 (v3.5.0:374f501f4567, Sep 12 2015, 11:00:19)  
[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin 
Type "help", "copyright", "credits" or "license" for more information. 
>>> 

```

**新名词**和**重要词汇**以粗体显示。你在屏幕上看到的单词，例如菜单或对话框中的单词，会出现在这样的文本中:“本书中的快捷键基于`Mac OS X 10.5+` 方案。”

### 注

警告或重要提示会出现在这样的框中。

### 类型

提示和技巧是这样出现的。

# 读者反馈

我们随时欢迎读者的反馈。让我们知道你对这本书的看法——你喜欢或不喜欢什么。读者反馈对我们来说很重要，因为它有助于我们开发出你真正能从中获益的标题。要向我们发送一般反馈，只需给 feedback@packtpub.com 发电子邮件，并在邮件主题中提及书名。如果你对某个主题有专业知识，并且对写作或投稿感兴趣，请参见我们位于[www.packtpub.com/authors](http://www.packtpub.com/authors)的作者指南。

# 客户支持

现在，您已经自豪地拥有了一本书，我们有许多东西可以帮助您从购买中获得最大收益。

## 下载示例代码

你可以从你在[http://www.packtpub.com](http://www.packtpub.com)的账户下载这本书的示例代码文件。如果您在其他地方购买了这本书，您可以访问[http://www.packtpub.com/support](http://www.packtpub.com/support)并注册，以便将文件直接通过电子邮件发送给您。

您可以按照以下步骤下载代码文件:

1.  使用您的电子邮件地址和密码登录或注册我们的网站。
2.  将鼠标指针悬停在顶部的 **SUPPORT** 选项卡上。
3.  点击**代码下载&勘误表**。
4.  在**搜索**框中输入图书名称。
5.  选择要下载代码文件的书籍。
6.  从您购买这本书的下拉菜单中选择。
7.  点击**代码下载**。

下载文件后，请确保使用最新版本的解压缩文件夹:

*   视窗系统的 WinRAR / 7-Zip
*   zipeg/izp/un ARX for MAC
*   适用于 Linux 的 7-Zip / PeaZip

这本书的代码包也托管在 GitHub 上，网址为[。我们还有来自丰富的图书和视频目录的其他代码包，可在](https://github.com/PacktPublishing/Apache-Spark-2-for-Beginners)[https://github.com/PacktPublishing/](https://github.com/PacktPublishing/)获得。看看他们！

## 下载本书的彩色图片

我们还为您提供了一个 PDF 文件，其中包含本书中使用的截图/图表的彩色图像。彩色图像将帮助您更好地理解输出中的变化。您可以从[http://www . packtpub . com/sites/default/files/downloads/Apache phark2 for beginers _ color images . pdf](http://www.packtpub.com/sites/default/files/downloads/Bookname_ColorImages.pdf)下载此文件。

## 勘误表

尽管我们尽了最大努力来确保我们内容的准确性，但错误还是会发生。如果你在我们的某本书里发现一个错误，也许是文本或代码中的错误，如果你能向我们报告，我们将不胜感激。通过这样做，你可以让其他读者免受挫折，并帮助我们改进这本书的后续版本。如果您发现任何勘误表，请访问[http://www.packtpub.com/submit-errata](http://www.packtpub.com/submit-errata)，选择您的书籍，点击**勘误表提交表**链接，并输入您的勘误表的详细信息。一旦您的勘误表得到验证，您的提交将被接受，勘误表将上传到我们的网站或添加到该标题勘误表部分下的任何现有勘误表列表中。

要查看之前提交的勘误表，请前往[https://www.packtpub.com/books/content/support](https://www.packtpub.com/books/content/support)并在搜索栏中输入图书名称。所需信息将出现在**勘误表**部分。

## 盗版

互联网上版权材料的盗版是所有媒体的一个持续问题。在 Packt，我们非常重视版权和许可证的保护。如果您在互联网上遇到任何形式的我们作品的非法拷贝，请立即向我们提供位置地址或网站名称，以便我们寻求补救。

请联系我们在 copyright@packtpub.com 的链接到可疑的盗版材料。

我们感谢您在保护我们的作者方面的帮助，以及我们为您带来有价值内容的能力。

## 问题

如果你对这本书的任何方面有问题，你可以联系我们在 questions@packtpub.com，我们将尽最大努力解决这个问题。