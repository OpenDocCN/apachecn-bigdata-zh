# 第一章。火花基础

数据是任何组织最重要的资产之一。组织中收集和使用数据的规模正在超乎想象地增长。获取数据的速度、使用的数据类型的多样性以及处理和存储的数据量每时每刻都在打破历史记录。如今，即使在小规模组织中，数据从千兆字节增长到万亿字节再到千兆字节也是很常见的。出于同样的原因，处理需求也在增长，这就要求能够处理静态数据和移动数据。

拿任何组织来说；它的成功取决于它的领导者所做的决定，为了做出合理的决定，你需要好的数据和处理数据所产生的信息的支持。这对于如何及时且经济高效地处理数据以便做出正确的决策提出了巨大的挑战。自从计算机出现的早期，数据处理技术就有了发展。这些年来，无数的数据处理产品和框架进入市场并消失了。大多数这些数据处理产品和框架本质上不是通用的。大多数组织依靠他们自己定制的应用程序来满足他们的数据处理需求，以筒仓的方式，或者结合特定的产品。

大规模互联网应用，俗称**物联网** ( **物联网**)应用，预示着对开放框架的普遍需求，以处理各种类型的数据，高速处理大量摄入的数据。大规模的网站、媒体流应用程序和组织的巨大批处理需求使得这种需求更加相关。随着互联网的发展，开源社区也在大幅增长，提供由知名软件公司支持的生产质量软件。大量公司开始使用开源软件，并开始在生产环境中部署它们。

从技术角度来看，数据处理需求面临巨大挑战。数据量开始从单台机器溢出到大量机器的集群中。单个中央处理器的处理能力趋于稳定，现代计算机开始将它们结合在一起以获得更大的处理能力，称为多核计算机。这些应用程序的设计和开发不是为了利用多核计算机中的所有处理器，并且浪费了典型现代计算机中可用的大量处理能力。

### 注

在本书中，术语*节点*、*主机*和*机器*指的是以独立模式或集群运行的计算机。

在这种情况下，一个理想的数据处理框架应该具备哪些品质？

*   它应该能够处理分布在计算机集群中的数据块
*   它应该能够以并行方式处理数据，以便将一个巨大的数据处理作业分成多个并行处理的任务，从而大大减少处理时间
*   它应该能够使用计算机中所有内核或处理器的处理能力
*   它应该能够使用群集中所有可用的计算机
*   它应该能够在商用硬件上运行

有两个值得一提的开源数据处理框架满足了所有这些需求。第一个是 Apache Hadoop，第二个是 Apache Spark。

我们将在本章中讨论以下主题:

*   Apache Hadoop
*   阿帕奇火花
*   火花 2.0 安装

# Apache Hadoop 概述

Apache Hadoop 是一个开源软件框架，从底层开始设计，用于在计算机集群上进行分布式数据存储，并对分布在计算机集群上的数据进行分布式数据处理。该框架带有用于数据存储的分布式文件系统，即 **Hadoop 分布式文件系统** ( **HDFS** )和数据处理框架，即 MapReduce。HDFS 的创作灵感来自谷歌研究论文*谷歌文件系统*和 MapReduce 基于谷歌研究论文 *MapReduce:大型集群上的简化数据处理*。

Hadoop 通过实现巨大的 Hadoop 集群进行数据处理，在很大程度上被组织所采用。从 Hadoop MapReduce 版本 1 (MRv1)到 Hadoop MapReduce 版本 2 (MRv2)，它看到了巨大的增长。从纯数据处理的角度来看，MRv1 由 HDFS 和 MapReduce 作为核心组件组成。许多应用程序，通常被称为 Hadoop 上的 SQL 应用程序，如 Hive 和 Pig，被堆叠在 MapReduce 框架的顶部。非常常见的是，尽管这些类型的应用程序是独立的 Apache 项目，但作为一个套件，许多这样的项目提供了巨大的价值。

另一个资源协商者项目出现在 Hadoop 生态系统上，除了 MapReduce 类型的计算框架。从组件架构分层的角度来看，在 HDFS 之上和 MapReduce 之下引入了纱，用户可以编写自己的应用程序，这些应用程序可以在纱和 HDFS 上运行，以利用 Hadoop 生态系统的分布式数据存储和数据处理能力。换句话说，最新的 MapReduce 版本 2 (MRv2)成为了位于 HDFS 和纱之上的应用框架之一。

*图 1* 简要介绍了这些组件及其堆叠方式:

![An overview of Apache Hadoop](graphics/image_01_002.jpg)

图 1

MapReduce 是一个通用的数据处理模型。数据处理经过两个步骤，即*映射*步骤和*减少*步骤。在第一步中，输入数据被分成许多较小的部分，这样每一部分都可以独立处理。一旦*映射*步骤完成，其输出被合并，最终结果在*减少*步骤中生成。在典型的字数统计示例中，创建以每个单词为关键字且值为 1 的键值对是*映射*步骤。在键上对这些对进行排序，将具有相同键的对的值相加落入中间的*组合*步骤。产生包含唯一单词及其出现次数的对是*减少*的步骤。

从应用程序编程的角度来看，过度简化的 MapReduce 应用程序的基本要素如下:

*   输入位置
*   输出位置
*   从`MapReduce`库中适当的接口和类中为数据处理需求实现的映射功能
*   从`MapReduce`库中适当的接口和类中减少为数据处理需要而实现的功能

提交 MapReduce 作业以便在 Hadoop 中运行，一旦作业完成，就可以从指定的输出位置获取输出。

这种将`MapReduce`数据处理作业划分到*地图*和*减少*任务的两步过程非常有效，并且非常适合许多批处理数据处理用例。在整个过程中，大量的输入/输出操作都是在幕后进行的。即使在 MapReduce 作业的中间步骤中，如果内部数据结构中充满了数据，或者当任务完成超过一定百分比时，就会写入磁盘。因此，MapReduce 作业中的后续步骤必须从磁盘读取。

然后，另一个最大的挑战来了，当有多个 MapReduce 作业要以链式方式完成时。换句话说，如果一个大数据处理工作要由两个 MapReduce 作业完成，那么第一个 MapReduce 作业的输出就是第二个 MapReduce 作业的输入。在这种情况下，无论第一个 MapReduce 作业的输出大小如何，都必须先将其写入磁盘，然后第二个 MapReduce 才能将其用作输入。所以在这个简单的情况下，有一个确定的*和不必要的*写操作。

在许多批处理数据处理用例中，这些输入/输出操作不是大问题。如果结果非常可靠，对于许多批处理数据处理用例，延迟是可以容忍的。但是最大的挑战来自于实时数据处理。MapReduce 作业中涉及的大量 I/O 操作使其不适合以尽可能低的延迟进行实时数据处理。

# 了解阿帕奇火花

Spark 是一个基于 **Java 虚拟机** ( **JVM** )的分布式数据处理引擎，可以伸缩，与其他很多数据处理框架相比速度很快。Spark 起源于*加州大学伯克利分校*，后来成为阿帕奇的顶级项目之一。研究论文 *Mesos:数据中心细粒度资源共享平台*讲述了 Spark 设计背后的理念。研究报告指出:

> *“为了测试简单的专门框架提供价值的假设，我们确定了一类被我们实验室的机器学习研究人员发现在 Hadoop 上表现不佳的作业:迭代作业，即数据集在多次迭代中被重用。我们构建了一个名为 Spark 的专门框架，针对这些工作负载进行了优化。”*

Spark 在速度方面最大的宣称是它能够*“在内存中运行程序比 Hadoop MapReduce 快 100 倍，或者在磁盘上快 10 倍”*。Spark 之所以提出这一主张，是因为它在工作节点的主内存中进行处理，并防止对磁盘进行不必要的*输入/输出操作。Spark 提供的另一个优势是，即使在应用程序编程级别，也能够在不写入磁盘或最大限度减少磁盘写入次数的情况下将任务连锁。*

 *与 MapReduce 相比，Spark 在数据处理方面是如何变得如此高效的？它配备了非常先进的**有向无环图** ( **DAG** )数据处理引擎。这意味着，对于每个 Spark 作业，都会创建一组任务，由引擎执行。用数学术语来说，DAG 由一组顶点和连接它们的有向边组成。任务按照 DAG 布局执行。在 MapReduce 的情况下，DAG 仅由两个顶点组成，一个顶点用于*地图*任务，另一个顶点用于 *reduce* 任务。边缘从*贴图*顶点指向*减少*顶点。内存中的数据处理与基于 DAG 的数据处理引擎相结合，使 Spark 非常高效。在 Spark 的例子中，任务的 DAG 可以尽可能的复杂。值得庆幸的是，Spark 附带的实用程序可以很好地可视化正在运行的任何 Spark 作业的 DAG。在字数统计的例子中，Spark 的 Scala 代码看起来像下面的代码片段。这个编程方面的细节将在接下来的章节中介绍:

```scala
val textFile = sc.textFile("README.md") 
val wordCounts = textFile.flatMap(line => line.split(" ")).map(word => 
 (word, 1)).reduceByKey((a, b) => a + b) 
wordCounts.collect()

```

Spark 附带的网络应用程序能够监控工作人员和应用程序。动态生成的前一个 Spark 作业的 DAG 将类似于*图 2* ，如下所示:

![Understanding Apache Spark](graphics/image_01_003.jpg)

图 2

Spark 编程范例非常强大，它公开了一个统一的编程模型，支持多种编程语言的应用程序开发。Spark 支持 Scala、Java、Python 和 R 语言的编程，即使所有支持的编程语言之间没有功能对等。除了用这些编程语言编写 Spark 应用程序之外，Spark 还有一个交互外壳，具有面向编程语言 Scala、Python 和 r 的**读取、评估、打印和循环**(**【REPL】**)功能。目前，Spark 中没有 REPL 对 Java 的支持。Spark REPL 是一个非常通用的工具，可以用来以交互方式尝试和测试 Spark 应用程序代码。Spark REPL 能够实现简单的原型制作、调试和更多功能。

除了核心数据处理引擎之外，Spark 还附带了一个强大的特定于领域的库堆栈，这些库使用核心 Spark 库，并提供了对各种大数据处理需求有用的各种功能。下表列出了支持的库:

<colgroup><col> <col> <col></colgroup> 
| **库** | **使用** | **支持的语言** |
| 火花 SQL | 允许在 Spark 应用程序中使用 SQL 语句或数据框应用编程接口 | Scala、Java、Python 和 R |
| 火花流 | 支持实时数据流的处理 | Scala、Java 和 Python |
| Spark MLlib(消歧义) | 支持机器学习应用程序的开发 | Scala、Java、Python 和 R |
| Spark GraphX | 支持图形处理并支持不断增长的图形算法库 | 斯卡拉 |

Spark 可以部署在各种平台上。Spark 运行在**操作系统** ( **操作系统** ) Windows 和 UNIX(如 Linux 和 Mac OS)上。Spark 可以独立模式部署在具有受支持操作系统的单个节点上。Spark 也可以部署在 Hadoop YARN 以及 Apache Mesos 上的集群节点中。Spark 也可以部署在亚马逊 EC2 云中。Spark 可以从各种各样的数据存储中访问数据，其中最受欢迎的包括 HDFS、Apache Cassandra、Hbase、Hive 等等。除了前面列出的数据存储，如果有可用的驱动程序或连接器程序，Spark 可以从几乎任何数据源访问数据。

### 类型

本书中使用的所有示例都是在 Mac OS X 10 . 9 . 5 版计算机上开发、测试和运行的。相同的说明适用于除 Windows 之外的所有其他平台。在 Windows 中，对应所有的 UNIX 命令，有一个扩展名为`.cmd`的文件，必须使用。比如 UNIX 中的`spark-shell`，Windows 中有一个`spark-shell.cmd`。所有受支持的操作系统的程序行为和结果应该是相同的。

在任何分布式应用程序中，通常都有一个控制执行的驱动程序，并且会有一个或多个工作节点。驱动程序将任务分配给合适的工人。即使 Spark 以独立模式运行，情况也是如此。在 Spark 应用程序的情况下，其 **SparkContext** 对象是驱动程序，它与适当的集群管理器通信以运行任务。Spark 主服务器是 Spark 核心库的一部分，Mesos 主服务器和 Hadoop 纱资源管理器是 Spark 支持的一些集群管理器。在 Hadoop SHART 部署 Spark 的情况下，Spark 驱动程序在 Hadoop SHART 应用程序主进程内部运行，或者 Spark 驱动程序作为 Hadoop SHART 的客户端运行。*图 3* 描述了 Spark 的独立部署:

![Understanding Apache Spark](graphics/image_01_006.jpg)

图 3

在 Spark 的 Mesos 部署模式中，集群管理器将是 **Mesos 主**。*图 4* 描述了火花的介子展开:

![Understanding Apache Spark](graphics/image_01_008.jpg)

图 4

在 Spark 的 Hadoop YARN 部署模式中，集群管理器将是 Hadoop 资源管理器，其地址将从 Hadoop 配置中获取。换句话说，在提交 Spark 作业时，不需要给出显式的主 URL，它将从 Hadoop 配置中获取集群管理器的详细信息。*图 5* 描述了 Spark 的 Hadoop 纱部署:

![Understanding Apache Spark](graphics/image_01_010.jpg)

图 5

火花也在云中运行。在亚马逊 EC2 上部署 Spark 的情况下，除了从常规支持的数据源访问数据之外，Spark 还可以从亚马逊 S3 访问数据，这是亚马逊提供的在线数据存储服务。

# 在机器上安装火花

Spark 支持 Scala、Java、Python 和 R 中的应用程序开发，在本书中，使用了 Scala、Python 和 R。这就是为本书中的例子选择语言的原因。Spark interactive shell，或称 REPL，允许用户像在终端提示符下输入操作系统命令一样动态执行程序，它仅适用于 Scala、Python 和 R. REPL 语言。在将 Spark 代码放在一个文件中并作为应用程序运行之前，这是尝试 Spark 代码的最佳方式。REPL 甚至帮助有经验的程序员尝试和测试代码，从而促进快速原型制作。因此，特别是对于初学者来说，使用 REPL 是开始使用 Spark 的最佳方式。

作为安装 Spark 和用 Python 和 R 进行 Spark 编程的先决条件，Python 和 R 都要在安装 Spark 之前安装。

## Python 安装

访问[https://www.python.org](https://www.python.org/)为你的电脑下载安装 Python。安装完成后，请确保所需的二进制文件位于操作系统搜索路径中，并且 Python 交互式外壳正常运行。外壳应该显示类似于以下内容的内容:

```scala
$ python 
Python 3.5.0 (v3.5.0:374f501f4567, Sep 12 2015, 11:00:19)  
[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin 
Type "help", "copyright", "credits" or "license" for more information. 
>>> 

```

为了绘图和绘图，正在使用`matplotlib`库。

### 注

Python 3 . 5 . 0 版本被用作 Python 的首选版本。尽管 Spark 支持 Python 2.7 版本的编程，但作为一种前瞻性的实践，使用了可用的最新、最稳定的 Python 版本。此外，大多数重要的库也将移植到 Python 版本。

访问[http://matplotlib.org](http://matplotlib.org/)下载安装库。要确保库安装正确，图表和绘图显示正确，请访问[http://matplotlib.org/examples/index.html](http://matplotlib.org/examples/index.html)页面，获取一些示例代码，并查看您的计算机是否拥有图表和绘图所需的所有资源和组件。在尝试运行一些图表和绘图示例时，在 Python 代码中导入库的上下文中，它可能会抱怨缺少区域设置。在这种情况下，请在适当的用户配置文件中设置以下环境变量，以消除错误消息:

```scala
export LC_ALL=en_US.UTF-8 
export LANG=en_US.UTF-8

```

## R 安装

访问[https://www.r-project.org](https://www.r-project.org/)为你的电脑下载安装 R。安装完成后，请确保所需的二进制文件位于操作系统搜索路径中，并且 R 交互式外壳正常运行。外壳应该显示类似于以下内容的内容:

```scala
$ r 
R version 3.2.2 (2015-08-14) -- "Fire Safety" 
Copyright (C) 2015 The R Foundation for Statistical Computing 
Platform: x86_64-apple-darwin13.4.0 (64-bit) 
R is free software and comes with ABSOLUTELY NO WARRANTY. 
You are welcome to redistribute it under certain conditions. 
Type 'license()' or 'licence()' for distribution details. 
  Natural language support but running in an English locale 
R is a collaborative project with many contributors. 
Type 'contributors()' for more information and 
'citation()' on how to cite R or R packages in publications. 
Type 'demo()' for some demos, 'help()' for on-line help, or 
'help.start()' for an HTML browser interface to help. 
Type 'q()' to quit R. 
[Previously saved workspace restored] 
>

```

### 注

r 版本 3.2.2 是 r 的选择。

## 火花安装

火花安装可以用许多不同的方法来完成。安装 Spark 最重要的先决条件是系统中安装了 Java 1.8 JDK，并且`JAVA_HOME`环境变量被设置为指向 Java 1.8 JDK 安装目录。访问[http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html)了解、选择和下载适合您电脑的安装类型。spark 2 . 0 . 0 版本是遵循本书给出的示例的首选版本。任何对从源代码中构建和使用 Spark 感兴趣的人都应该访问:[http://spark.apache.org/docs/latest/building-spark.html](http://spark.apache.org/docs/latest/building-spark.html)获取说明。默认情况下，当您从源代码构建 Spark 时，它不会为 Spark 构建 R 库。为此，必须构建 SparkR 库，并且在从源代码构建 Spark 时必须包含适当的概要文件。以下命令显示了如何包含构建 SparkR 库所需的配置文件:

```scala
$ mvn -DskipTests -Psparkr clean package

```

完成 Spark 安装后，在适当的用户配置文件中定义以下环境变量:

```scala
export SPARK_HOME=<the Spark installation directory> 
export PATH=$SPARK_HOME/bin:$PATH

```

如果系统中有多个版本的 Python 可执行文件，那么最好在以下环境变量设置中明确指定要由 Spark 使用的 Python 可执行文件:

```scala
export PYSPARK_PYTHON=/usr/bin/python

```

在`$SPARK_HOME/bin/pyspark`脚本中，有一段代码决定了 Spark 要使用的 Python 可执行文件:

```scala
# Determine the Python executable to use if PYSPARK_PYTHON or PYSPARK_DRIVER_PYTHON isn't set: 
if hash python2.7 2>/dev/null; then 
  # Attempt to use Python 2.7, if installed: 
  DEFAULT_PYTHON="python2.7" 
else 
  DEFAULT_PYTHON="python" 
fi

```

所以，为 Spark 显式设置 Python 可执行文件总是更好，即使系统中只有一个 Python 版本。这是一种保护措施，用于防止将来安装 Python 的附加版本时出现意外行为。

一旦所有前面的步骤成功完成，请确保 Scala、Python 和 R 语言的所有火花外壳都正常工作。在操作系统终端提示符下运行以下命令，确保没有错误，并且显示类似以下内容。以下一组命令用于调出 Spark 的 Scala REPL:

```scala
$ cd $SPARK_HOME 
$ ./bin/spark-shellUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties 
Setting default log level to "WARN". 
To adjust logging level use sc.setLogLevel(newLevel). 
16/06/28 20:53:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 
16/06/28 20:53:49 WARN SparkContext: Use an existing SparkContext, some configuration may not take effect. 
Spark context Web UI available at http://192.168.1.6:4040 
Spark context available as 'sc' (master = local[*], app id = local-1467143629623). 
Spark session available as 'spark'. 
Welcome to 
      ____              __ 
     / __/__  ___ _____/ /__ 
    _\ \/ _ \/ _ `/ __/  '_/ 
   /___/ .__/\_,_/_/ /_/\_\   version 2.0.1 
      /_/ 

Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_66) 
Type in expressions to have them evaluated. 
Type :help for more information. 
scala> 
scala>exit 

```

在前面的显示中，根据安装了 Spark 的计算机中的设置，验证 JDK 版本、Scala 版本和 Spark 版本是否正确。验证最重要的一点是不显示错误消息。

以下一组命令用于调出 Spark 的 Python REPL:

```scala
$ cd $SPARK_HOME 
$ ./bin/pyspark 
Python 3.5.0 (v3.5.0:374f501f4567, Sep 12 2015, 11:00:19)  
[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin 
Type "help", "copyright", "credits" or "license" for more information. 
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties 
Setting default log level to "WARN". 
To adjust logging level use sc.setLogLevel(newLevel). 
16/06/28 20:58:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 
Welcome to 
      ____              __ 
     / __/__  ___ _____/ /__ 
    _\ \/ _ \/ _ `/ __/  '_/ 
   /__ / .__/\_,_/_/ /_/\_\   version 2.0.1 
      /_/ 

Using Python version 3.5.0 (v3.5.0:374f501f4567, Sep 12 2015 11:00:19) 
SparkSession available as 'spark'. 
>>>exit() 

```

在前面的显示中，根据安装了 Spark 的计算机中的设置，验证 Python 版本和 Spark 版本是否正确。验证最重要的一点是不显示错误消息。

以下命令用于调出火花的 REPL:

```scala
$ cd $SPARK_HOME 
$ ./bin/sparkR 
R version 3.2.2 (2015-08-14) -- "Fire Safety" 
Copyright (C) 2015 The R Foundation for Statistical Computing 
Platform: x86_64-apple-darwin13.4.0 (64-bit) 

R is free software and comes with ABSOLUTELY NO WARRANTY. 
You are welcome to redistribute it under certain conditions. 
Type 'license()' or 'licence()' for distribution details. 

  Natural language support but running in an English locale 

R is a collaborative project with many contributors. 
Type 'contributors()' for more information and 
'citation()' on how to cite R or R packages in publications. 

Type 'demo()' for some demos, 'help()' for on-line help, or 
'help.start()' for an HTML browser interface to help. 
Type 'q()' to quit R. 

[Previously saved workspace restored] 

Launching java with spark-submit command /Users/RajT/source-code/spark-source/spark-2.0/bin/spark-submit   "sparkr-shell" /var/folders/nf/trtmyt9534z03kq8p8zgbnxh0000gn/T//RtmphPJkkF/backend_port59418b49bb6  
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties 
Setting default log level to "WARN". 
To adjust logging level use sc.setLogLevel(newLevel). 
16/06/28 21:00:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 

 Welcome to 
    ____              __  
   / __/__  ___ _____/ /__  
  _\ \/ _ \/ _ `/ __/  '_/  
 /___/ .__/\_,_/_/ /_/\_\   version  2.0.1 
    /_/  

 Spark context is available as sc, SQL context is available as sqlContext 
During startup - Warning messages: 
1: 'SparkR::sparkR.init' is deprecated. 
Use 'sparkR.session' instead. 
See help("Deprecated")  
2: 'SparkR::sparkRSQL.init' is deprecated. 
Use 'sparkR.session' instead. 
See help("Deprecated")  
>q() 

```

在前面的显示中，根据安装了 Spark 的计算机中的设置，验证 R 版本和 Spark 版本是否正确。验证最重要的一点是不显示错误消息。

如果 Scala、Python 和 R 的所有 REPL 都运行良好，那么几乎可以肯定 Spark 安装是好的。作为最后的测试，运行 Spark 附带的一些示例程序，并确保它们给出的正确结果接近命令下面显示的结果，并且没有在控制台中抛出任何错误消息。运行这些示例程序时，除了命令下面显示的输出之外，控制台中还会显示许多其他消息。它们被省略以集中于结果:

```scala
$ cd $SPARK_HOME 
$ ./bin/run-example SparkPi 
Pi is roughly 3.1484 
$ ./bin/spark-submit examples/src/main/python/pi.py 
Pi is roughly 3.138680 
$ ./bin/spark-submit examples/src/main/r/dataframe.R 
root 
 |-- name: string (nullable = true) 
 |-- age: double (nullable = true) 
root 
 |-- age: long (nullable = true) 
 |-- name: string (nullable = true) 
    name 
1 Justin 

```

## 开发工具安装

本书将要讨论的大部分代码都可以在合适的 REPL 进行尝试和测试。但是没有一些基本的构建工具，正确的 Spark 应用程序开发是不可能的。作为最低要求，为了在 Scala 中开发和构建 Spark 应用程序， **Scala 构建工具** ( **sbt** )是必须的。访问[http://www.scala-sbt.org](http://www.scala-sbt.org/)下载安装 sbt。

Maven 是构建 Java 应用程序的首选构建工具。这本书不是在讲 Java 中的 Spark 应用程序开发，但是系统中也安装了 Maven 就不错了。如果 Spark 是从源代码构建的，Maven 将派上用场。访问[https://maven.apache.org](https://maven.apache.org/)下载安装 Maven。

除了 Java，Scala 还有很多**集成开发环境** ( **IDEs** )。这是个人的选择，开发人员可以为他/她开发 Spark 应用程序所使用的语言选择他/她选择的工具。

## 可选软件安装

Scala 的 Spark REPL 是进入一些小代码片段的原型开发和测试的良好开端。但是当需要在 Scala 中开发、构建和打包 Spark 应用程序时，最好有基于 sbt 的 Scala 项目，并使用受支持的 IDE 开发它们，包括但不限于 Eclipse 或 IntelliJ IDEA。请访问适当的网站下载和安装 Scala 的首选集成开发环境。

如今，笔记本风格的应用程序开发工具在数据分析师和研究人员中非常普遍。这类似于实验室笔记本。在一个典型的实验笔记本中，会有指导、详细描述和进行实验的步骤。然后进行实验。一旦实验完成，就会有结果记录在笔记本上。如果所有这些结构组合在一起，并适合软件程序的上下文，并以实验笔记本的格式建模，将会有文档、代码、输入和通过运行代码生成的输出。这将产生非常好的效果，尤其是如果程序生成大量的图表和情节。

### 类型

对于不熟悉笔记本风格应用开发 IDEs 的人来说，有一篇很不错的文章叫做*交互笔记本:分享代码*，可以从[http://www . nature . com/news/Interactive-Notebooks-Sharing-the-Code-1.16261](http://www.nature.com/news/interactive-notebooks-sharing-the-code-1.16261)上阅读。作为 Python 的可选软件开发 IDE，IPython 笔记本将在下一节中介绍。安装后，在开始认真开发之前，先熟悉一下工具。

### IPython

以 Python 中的 Spark 应用程序开发为例，IPython 提供了一个优秀的笔记本式开发工具，它是 Jupyter 的 Python 语言内核。Spark 可以与 IPython 集成，这样当 Python 的 Spark REPL 被调用时，它将启动 IPython 笔记本。然后，创建一个笔记本，并开始在笔记本中编写代码，就像 Python 的 Spark REPL 中给出命令的方式一样。访问[http://ipython.org](http://ipython.org/)下载安装 IPython 笔记本。安装完成后，调用 IPython 笔记本界面，并确保一些示例 Python 代码运行正常。从存储笔记本的目录或要存储笔记本的目录中调用命令。这里，IPython 笔记本是从一个临时目录启动的。当调用以下命令时，它将打开 web 界面，并通过单击“新建”下拉框并选择适当的 Python 版本来创建一个新笔记本。

下面的屏幕截图显示了如何在 IPython 笔记本中将降价风格的文档、Python 程序和生成的输出组合在一起:

```scala
$ cd /Users/RajT/temp 
$ ipython notebook 

```

![IPython](graphics/image_01_011.jpg)

图 6

*图 6* 展示了如何使用 IPython 笔记本编写简单的 Python 程序。IPython 笔记本可以配置为 Spark 的首选外壳，当调用 Python 的 Spark REPL 时，它将启动 IPython 笔记本，Spark 应用程序开发可以使用 IPython 笔记本完成。为此，请在适当的用户配置文件中定义以下环境变量:

```scala
export PYSPARK_DRIVER_PYTHON=ipython 
export PYSPARK_DRIVER_PYTHON_OPTS='notebook' 

```

现在，不要从命令提示符调用 IPython 笔记本，而是调用 Python 的 Spark REPL。就像之前所做的一样，创建一个新的 IPython 笔记本，并开始用 Python 编写 Spark 代码:

```scala
$ cd /Users/RajT/temp 
$ pyspark 

```

请看下面的截图:

![IPython](graphics/image_01_012.jpg)

图 7

### 类型

在任何语言的标准 Spark REPL 中，都可以引用位于本地文件系统中的文件及其相对路径。当使用 IPython 笔记本时，本地文件将以其完整路径引用。

### 工作室

在 R 用户社区中，R 的首选 IDE 是 RStudio。RStudio 也可以用来开发 R 中的 Spark 应用。访问[https://www.rstudio.com](https://www.rstudio.com/)下载并安装 RStudio。一旦安装完成，在运行任何 Spark R 代码之前，必须包含`SparkR`库并设置一些变量，以确保 Spark R 程序从 rstu 迪奥顺利运行。下面的代码片段做到了这一点:

```scala
SPARK_HOME_DIR <- "/Users/RajT/source-code/spark-source/spark-2.0" 
Sys.setenv(SPARK_HOME=SPARK_HOME_DIR) 
.libPaths(c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"), .libPaths())) 
library(SparkR) 
spark <- sparkR.session(master="local[*]")

```

在前面的 R 代码中，将`SPARK_HOME_DIR`变量定义改为指向安装 Spark 的目录。*图 8* 显示了来自 RStudio 的火花代码的示例运行:

![RStudio](graphics/image_01_013.jpg)

图 8

一旦所有需要的软件都按照前面给出的细节进行了安装、配置和工作，就为 Scala、Python 和 r 中的 Spark 应用程序开发奠定了基础

### 类型

Jupyter 笔记本通过针对各种语言的定制内核实现策略，支持多种语言。Jupyter 有一个原生的 R 内核，即 IRkernel，可以作为 R 包安装。

### Apache Zeppelin

阿帕奇齐柏林飞船是另一个有希望的项目，目前正在孵化中。它是一个基于网络的笔记本，类似于 Jupyter，但通过其解释器策略支持多种语言、外壳和技术，从而从本质上支持 Spark 应用程序开发。目前它还处于初级阶段，但它有很大的潜力成为最好的基于笔记本电脑的应用程序开发平台之一。齐柏林飞艇拥有非常强大的内置图表和绘图功能，可以使用笔记本中编写的程序生成的数据。

齐柏林飞艇具有高扩展性，能够使用其解释器框架插入多种类型的解释器。终端用户，就像任何其他基于笔记本的系统一样，在笔记本界面输入各种命令。这些命令将由某个解释器处理以生成输出。与许多其他笔记本风格的系统不同，齐柏林飞艇支持大量开箱即用的解释器或后端，如 Spark、Spark SQL、Shell、Markdown 等。就前端而言，同样是可插拔架构，即**氦框架**。后端生成的数据由前端组件显示，如 Angular JS。有各种选项以表格格式、解释器生成的原始格式、图表和绘图显示数据。由于后端、前端等关注点的架构分离，以及插入各种组件的能力，这是为正确的工作选择异构组件的好方法。同时，它集成得非常好，提供了一个和谐的最终用户友好的数据处理生态系统。尽管齐柏林飞艇中的各种组件都有可插拔的架构能力，但是可视化效果是有限的。换句话说，在齐柏林飞艇中，开箱即用的图表和绘图选项很少。一旦笔记本运行良好并产生了预期的结果，通常情况下，笔记本会与其他人共享，为此，笔记本将被持久保存。齐柏林飞艇在这里又有所不同，它有一个高度多功能的笔记本存储系统。笔记本可以保存到文件系统、亚马逊 S3 或 Git 中，如果需要，还可以添加其他存储目标。

**平台即服务** ( **PaaS** )自围绕云作为应用程序开发和部署平台进行大规模创新以来，在过去几年中一直在发展。对于软件开发人员来说，有许多通过云交付的平台即服务平台，这消除了他们拥有自己的应用程序开发堆栈的需要。Databricks 推出了一个基于云的大数据平台，在该平台上，用户可以访问基于笔记本电脑的 Spark 应用程序开发界面，以及可以向其提交 Spark 应用程序的微集群基础架构。还有一个社区版，迎合更广泛的发展社区的需求。这个 PaaS 平台最大的优势是它是一个基于浏览器的界面，用户可以针对多个版本的 Spark 和不同类型的集群运行他们的代码。

# 参考文献

有关更多信息，请参考以下链接:

*   [http://static . googleuser content . com/media/research . Google . com/en//archive/GFS-sosp 2003 . pdf](http://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf)
*   [http://static . googleuser content . com/media/research . Google . com/en//archive/MapReduce-osdi 04 . pdf](http://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf)
*   [https://www.cs.berkeley.edu/~alig/papers/mesos.pdf](https://www.cs.berkeley.edu/~alig/papers/mesos.pdf)
*   [http://spark.apache.org/](http://spark.apache.org/)
*   [https://jupyter.org/](https://jupyter.org/)
*   [https://github . com/irkir/irkir](https://github.com/IRkernel/IRkernel)
*   [https://zeppelin.incubator.apache.org/](https://zeppelin.incubator.apache.org/)
*   [https://community.cloud.databricks.com/](https://community.cloud.databricks.com/)

# 总结

Spark 是一个非常强大的数据处理平台，支持统一的编程模型。它支持 Scala、Java、Python 和 R 中的应用程序开发，提供了一个用于各种类型的数据处理需求的高互操作库堆栈，以及大量第三方库，这些库利用了涵盖各种其他数据处理用例的 Spark 生态系统。本章简要介绍了 Spark，并为 Spark 应用程序开发设置了开发环境，这将在本书的后续章节中介绍。

下一章将讨论 Spark 编程模型、基本抽象和术语、Spark 转换和 Spark 动作，以及实际的用例。*