# 第 6 章：可视化洞察力和趋势

到目前为止，我们的重点是收集、分析和处理来自 Twitter 的数据。 我们已经做好了准备，可以使用我们的数据进行可视化渲染，并提取洞察力和趋势。 我们将简要介绍一下 Python 生态系统中的可视化工具。 我们将重点介绍 Bokeh 作为呈现和查看大型数据集的强大工具。 Bokeh 是 Python Anaconda 分发生态系统的一部分。

在本章中，我们将介绍以下几点：

*   使用图表和单词云评估社交网络社区中的关键词和模因
*   绘制社区围绕特定主题或主题发展最活跃的位置

# 重温数据密集型应用程序架构

我们已经到达了数据密集型应用架构的最后一层：参与层。 这一层的重点是如何为数据使用者合成、强调和可视化关键上下文相关信息。 控制台中的一堆号码不足以与最终用户互动。 以一种快速、易于理解和有吸引力的方式呈现大量信息是至关重要的。

下图设置了突出显示参与层的章节焦点的上下文。

![Revisiting the data-intensive apps architecture](img/B03968_06_01.jpg)

对于 Python绘图和可视化，我们有相当多的工具和库。 对于我们的目的来说，最有趣和最相关的是以下几个问题：

*   **Matplotlib**是Python 绘图库的始祖。 Matplotlib 最初是开源软件倡导者*John Hunter*的创意，他将 Matplotlib 确立为学术界和数据科学界最流行的绘图库之一。 Matplotlib 允许生成曲线图、直方图、功率谱、条形图、误差图、散点图等。 例子可以在Matplotlib 专用网站[http://matplotlib.org/examples/index.html](http://matplotlib.org/examples/index.html)上找到。
*   ****由*Michael Waskom*开发的 Seborn**是一个快速可视化统计信息的强大库。 它构建在 Matplotlib 之上，并与 Pandas 和 Python 数据堆栈(包括 Numpy)无缝集成。 位于[http://stanford.edu/~mwaskom/software/seaborn/examples/index.html](http://stanford.edu/~mwaskom/software/seaborn/examples/index.html)的 Seborn 图库显示了库的潜力。**
*   **gglot**是相对较新的，其目标是为 Python 数据牧羊人提供与 R 生态系统中著名的 ggplot2 相当的功能。 它具有与 ggplot2 相同的外观和感觉，并使用与 Hadley Wickham 所阐述的相同的图形语法。 Python 端口的 gglot 是由团队在`yhat`开发的。 欲了解更多信息，请访问[http://ggplot.yhathq.com](http://ggplot.yhathq.com)。
*   **D3.js**是非常流行的 JavaScript 库，由*Mike Bostock*开发。 **D3**代表**数据驱动文档**，在任何利用 HTML、SVG 和 CSS 的现代浏览器上使数据栩栩如生。 它通过操作 DOM(文档对象模型)提供动态、强大、交互的可视化效果。 Python 社区迫不及待地将 D3 与 Matplotlib 集成在一起。 在杰克·范德普拉斯(Jack Vanderplas)的推动下，mpld3 被创造出来，目的是将`matplotlib`带到浏览器上。 示例图形位于以下地址：[http://mpld3.github.io/index.html](http://mpld3.github.io/index.html)。
*   **Bokeh**的目标是在非常大或流的数据集上提供高性能的交互性，同时利用`D3.js`的大量概念，而不需要编写一些吓人的`javascript`和`css`代码。 无论有没有服务器，Bokeh 都能在浏览器上提供动态可视化效果。 它与 matplotlib、seborn 和 gglot 无缝集成，在 IPython 笔记本或 Jupyter 笔记本上呈现得非常漂亮。 Bokeh 由 Continuum.io 团队积极开发，是蟒蛇 Python 数据堆栈不可或缺的一部分。

Bokeh 服务器提供了一个功能齐全的动态绘图引擎，它实现了 JSON 中的反应式场景图。 它使用 Web 套接字来保持状态，并使用 Backbone.js 和 Coffee-script 在幕后更新 HTML5 画布。 Bokeh 以 JSON 中的数据为燃料，为其他语言(如 R、Scala 和 Julia)创建了轻松的绑定。

这给出了主要绘图和可视化库的高级概述。 它并不是详尽的。 让我们转到可视化的具体示例。

# 对可视化数据进行预处理

在进入可视化之前，我们将对收集到的数据做一些准备工作：

```py
In [16]:
# Read harvested data stored in csv in a Panda DF
import pandas as pd
csv_in = '/home/an/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark/data/unq_tweetstxt.csv'
pddf_in = pd.read_csv(csv_in, index_col=None, header=0, sep=';', encoding='utf-8')
In [20]:
print('tweets pandas dataframe - count:', pddf_in.count())
print('tweets pandas dataframe - shape:', pddf_in.shape)
print('tweets pandas dataframe - colns:', pddf_in.columns)
('tweets pandas dataframe - count:', Unnamed: 0    7540
id            7540
created_at    7540
user_id       7540
user_name     7538
tweet_text    7540
dtype: int64)
('tweets pandas dataframe - shape:', (7540, 6))
('tweets pandas dataframe - colns:', Index([u'Unnamed: 0', u'id', u'created_at', u'user_id', u'user_name', u'tweet_text'], dtype='object'))
```

为了可视化活动的目的，我们将使用 7540 条 tweet 的数据集。 密钥信息存储在`tweet_text`列中。 我们预览存储在 DataFrame 中的数据，调用 DataFrame 上的`head()`函数：

```py
In [21]:
pddf_in.head()
Out[21]:
  Unnamed: 0   id   created_at   user_id   user_name   tweet_text
0   0   638830426971181057   Tue Sep 01 21:46:57 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: 9_A_6: dreamint...
1   1   638830426727911424   Tue Sep 01 21:46:57 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: PhuketDailyNews...
2   2   638830425402556417   Tue Sep 01 21:46:56 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: 9_A_6: ernestsg...
3   3   638830424563716097   Tue Sep 01 21:46:56 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: PhuketDailyNews...
4   4   638830422256816132   Tue Sep 01 21:46:56 +0000 2015   3276255125   True Equality   ernestsgantt: elsahel12: 9_A_6: dreamintention...
```

我们现在将创建一些实用函数来清理 tweet 文本并解析 Twitter 日期。 首先，我们导入 Python 正则表达式 regex 库`re`和时间库来解析日期和时间：

```py
In [72]:
import re
import time
```

我们创建一个 regex 字典，它将被编译，然后作为函数传递：

*   **RT**：第一个键为`RT`的正则表达式在 tweet 文本的开头查找关键字`RT`：

    ```py
    re.compile(r'^RT'),
    ```

*   **ALNUM**：带有键`ALNUM`的第二个正则表达式在 tweet 文本中查找包含字母数字字符和前接`@`符号的下划线符号的单词：

    ```py
    re.compile(r'(@[a-zA-Z0-9_]+)'),
    ```

*   **hashtag**：第三个键为`HASHTAG`的正则表达式在 tweet 文本中查找包含字母数字字符和前面带有`#`符号的单词：

    ```py
    re.compile(r'(#[\w\d]+)'),
    ```

*   **空格**：第四个带键`SPACES`的正则表达式在 tweet 文本中查找空格或行间字符：

    ```py
    re.compile(r'\s+'), 
    ```

*   **URL**：第五个键为`URL`的正则表达式在 tweet 文本中查找`url`地址，包括前面带有`https://`或`http://`标记的字母数字字符：

    ```py
    re.compile(r'([https://|http://]?[a-zA-Z\d\/]+[\.]+[a-zA-Z\d\/\.]+)')
    In [24]:
    regexp = {"RT": "^RT", "ALNUM": r"(@[a-zA-Z0-9_]+)",
              "HASHTAG": r"(#[\w\d]+)", "URL": r"([https://|http://]?[a-zA-Z\d\/]+[\.]+[a-zA-Z\d\/\.]+)",
              "SPACES":r"\s+"}
    regexp = dict((key, re.compile(value)) for key, value in regexp.items())
    In [25]:
    regexp
    Out[25]:
    {'ALNUM': re.compile(r'(@[a-zA-Z0-9_]+)'),
     'HASHTAG': re.compile(r'(#[\w\d]+)'),
     'RT': re.compile(r'^RT'),
     'SPACES': re.compile(r'\s+'),
     'URL': re.compile(r'([https://|http://]?[a-zA-Z\d\/]+[\.]+[a-zA-Z\d\/\.]+)')}
    ```

我们创建一个实用函数来识别一条推文是转发推文还是原始推文：

```py
In [77]:
def getAttributeRT(tweet):
    """ see if tweet is a RT """
    return re.search(regexp["RT"], tweet.strip()) != None
```

然后，我们提取一条推文中的所有用户句柄：

```py
def getUserHandles(tweet):
    """ given a tweet we try and extract all user handles"""
    return re.findall(regexp["ALNUM"], tweet)
```

我们还提取推文中的所有标签：

```py
def getHashtags(tweet):
    """ return all hashtags"""
    return re.findall(regexp["HASHTAG"], tweet)
```

提取推文中的所有 URL 链接，如下所示：

```py
def getURLs(tweet):
    """ URL : [http://]?[\w\.?/]+"""
    return re.findall(regexp["URL"], tweet)
```

我们在 tweet 文本中去掉前面带有`@`符号的所有 URL 链接和用户句柄。 此函数将成为我们即将构建的 wordcloud 的基础：

```py
def getTextNoURLsUsers(tweet):
    """ return parsed text terms stripped of URLs and User Names in tweet text
        ' '.join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)"," ",x).split()) """
    return ' '.join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|(RT)"," ", tweet).lower().split())
```

我们给数据贴上标签，这样我们就可以为单词 Cloud 创建数据集的组：

```py
def setTag(tweet):
    """ set tags to tweet_text based on search terms from tags_list"""
    tags_list = ['spark', 'python', 'clinton', 'trump', 'gaga', 'bieber']
    lower_text = tweet.lower()
    return filter(lambda x:x.lower() in lower_text,tags_list)
```

我们将 Twitter 日期解析为`yyyy-mm-dd hh:mm:ss`格式：

```py
def decode_date(s):
    """ parse Twitter date into format yyyy-mm-dd hh:mm:ss"""
    return time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(s,'%a %b %d %H:%M:%S +0000 %Y'))
```

我们在处理之前预览数据：

```py
In [43]:
pddf_in.columns
Out[43]:
Index([u'Unnamed: 0', u'id', u'created_at', u'user_id', u'user_name', u'tweet_text'], dtype='object')
In [45]:
# df.drop([Column Name or list],inplace=True,axis=1)
pddf_in.drop(['Unnamed: 0'], inplace=True, axis=1)
In [46]:
pddf_in.head()
Out[46]:
  id   created_at   user_id   user_name   tweet_text
0   638830426971181057   Tue Sep 01 21:46:57 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: 9_A_6: dreamint...
1   638830426727911424   Tue Sep 01 21:46:57 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: PhuketDailyNews...
2   638830425402556417   Tue Sep 01 21:46:56 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: 9_A_6: ernestsg...
3   638830424563716097   Tue Sep 01 21:46:56 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: PhuketDailyNews...
4   638830422256816132   Tue Sep 01 21:46:56 +0000 2015   3276255125   True Equality   ernestsgantt: elsahel12: 9_A_6: dreamintention...
```

我们通过应用所述的实用函数创建新的数据框列。 我们为`htag`、用户句柄、URL、从 URL 中剥离的文本术语以及不需要的字符和标签创建了一个新列。 我们终于解析了日期：

```py
In [82]:
pddf_in['htag'] = pddf_in.tweet_text.apply(getHashtags)
pddf_in['user_handles'] = pddf_in.tweet_text.apply(getUserHandles)
pddf_in['urls'] = pddf_in.tweet_text.apply(getURLs)
pddf_in['txt_terms'] = pddf_in.tweet_text.apply(getTextNoURLsUsers)
pddf_in['search_grp'] = pddf_in.tweet_text.apply(setTag)
pddf_in['date'] = pddf_in.created_at.apply(decode_date)
```

下面的代码提供了新生成的数据帧的快速快照：

```py
In [83]:
pddf_in[2200:2210]
Out[83]:
  id   created_at   user_id   user_name   tweet_text   htag   urls   ptxt   tgrp   date   user_handles   txt_terms   search_grp
2200   638242693374681088   Mon Aug 31 06:51:30 +0000 2015   19525954   CENATIC   El impacto de @ApacheSpark en el procesamiento...   [#sparkSpecial]   [://t.co/4PQmJNuEJB]   el impacto de en el procesamiento de datos y e...   [spark]   2015-08-31 06:51:30   [@ApacheSpark]   el impacto de en el procesamiento de datos y e...   [spark]
2201   638238014695575552   Mon Aug 31 06:32:55 +0000 2015   51115854   Nawfal   Real Time Streaming with Apache Spark\nhttp://...   [#IoT, #SmartMelboune, #BigData, #Apachespark]   [://t.co/GW5PaqwVab]   real time streaming with apache spark iot smar...   [spark]   2015-08-31 06:32:55   []   real time streaming with apache spark iot smar...   [spark]
2202   638236084124516352   Mon Aug 31 06:25:14 +0000 2015   62885987   Mithun Katti   RT @differentsachin: Spark the flame of digita...   [#IBMHackathon, #SparkHackathon, #ISLconnectIN...   []   spark the flame of digital india ibmhackathon ...   [spark]   2015-08-31 06:25:14   [@differentsachin, @ApacheSpark]   spark the flame of digital india ibmhackathon ...   [spark]
2203   638234734649176064   Mon Aug 31 06:19:53 +0000 2015   140462395   solaimurugan v   Installing @ApacheMahout with @ApacheSpark 1.4...   []   [1.4.1, ://t.co/3c5dGbfaZe.]   installing with 1 4 1 got many more issue whil...   [spark]   2015-08-31 06:19:53   [@ApacheMahout, @ApacheSpark]   installing with 1 4 1 got many more issue whil...   [spark]
2204   638233517307072512   Mon Aug 31 06:15:02 +0000 2015   2428473836   Ralf Heineke   RT @RomeoKienzler: Join me @velocityconf on #m...   [#machinelearning, #devOps, #Bl]   [://t.co/U5xL7pYEmF]   join me on machinelearning based devops operat...   [spark]   2015-08-31 06:15:02   [@RomeoKienzler, @velocityconf, @ApacheSpark]   join me on machinelearning based devops operat...   [spark]
2205   638230184848687106   Mon Aug 31 06:01:48 +0000 2015   289355748   Akim Boyko   RT @databricks: Watch live today at 10am PT is...   []   [1.5, ://t.co/16cix6ASti]   watch live today at 10am pt is 1 5 presented b...   [spark]   2015-08-31 06:01:48   [@databricks, @ApacheSpark, @databricks, @pwen...   watch live today at 10am pt is 1 5 presented b...   [spark]
2206   638227830443110400   Mon Aug 31 05:52:27 +0000 2015   145001241   sachin aggarwal   Spark the flame of digital India @ #IBMHackath...   [#IBMHackathon, #SparkHackathon, #ISLconnectIN...   [://t.co/C1AO3uNexe]   spark the flame of digital india ibmhackathon ...   [spark]   2015-08-31 05:52:27   [@ApacheSpark]   spark the flame of digital india ibmhackathon ...   [spark]
2207   638227031268810752   Mon Aug 31 05:49:16 +0000 2015   145001241   sachin aggarwal   RT @pravin_gadakh: Imagine, innovate and Igni...   [#IBMHackathon, #ISLconnectIN2015]   []   gadakh imagine innovate and ignite digital ind...   [spark]   2015-08-31 05:49:16   [@pravin_gadakh, @ApacheSpark]   gadakh imagine innovate and ignite digital ind...   [spark]
2208   638224591920336896   Mon Aug 31 05:39:35 +0000 2015   494725634   IBM Asia Pacific   RT @sachinparmar: Passionate about Spark?? Hav...   [#IBMHackathon, #ISLconnectIN]   [India..]   passionate about spark have dreams of clean sa...   [spark]   2015-08-31 05:39:35   [@sachinparmar]   passionate about spark have dreams of clean sa...   [spark]
2209   638223327467692032   Mon Aug 31 05:34:33 +0000 2015   3158070968   Open Source India   "Game Changer" #ApacheSpark speeds up #bigdata...   [#ApacheSpark, #bigdata]   [://t.co/ieTQ9ocMim]   game changer apachespark speeds up bigdata pro...   [spark]   2015-08-31 05:34:33   []   game changer apachespark speeds up bigdata pro...   [spark]
```

我们将处理后的信息保存为 CSV 格式。 我们有 7540 条记录和 13 个栏目。 在您的情况下，输出将根据您选择的数据集而有所不同：

```py
In [84]:
f_name = '/home/an/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark/data/unq_tweets_processed.csv'
pddf_in.to_csv(f_name, sep=';', encoding='utf-8', index=False)
In [85]:
pddf_in.shape
Out[85]:
(7540, 13)
```

# 一目了然地衡量词语、情绪和模因

我们现在已经准备好开始构建词云，这将让我们对这些推文中包含的重要词条有一种感觉。 我们将为收集到的数据集创建单词云。 单词云提取单词列表中排名靠前的单词，并创建单词的散点图，其中单词的大小与其频率相关。 单词在数据集中出现的频率越高，单词云渲染中的字体大小就越大。 它们包括三个非常不同的主题和两个相互竞争或相似的实体。 我们的第一个主题显然是数据处理和分析，Apache Spark 和 Python 是我们的实体。 我们的第二个主题是 2016 年总统竞选，有两位竞争者：希拉里·克林顿(Hilary Clinton)和唐纳德·特朗普(Donald Trump)。 我们的最后一个主题是流行音乐的世界，贾斯汀·比伯和 Lady Gaga 是两位倡导者。

## 设置 wordcloud

我们将通过分析与 Spark 相关的推文来说明编程步骤。 我们加载数据并预览数据帧：

```py
In [21]:
import pandas as pd
csv_in = '/home/an/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark/data/spark_tweets.csv'
tspark_df = pd.read_csv(csv_in, index_col=None, header=0, sep=',', encoding='utf-8')
In [3]:
tspark_df.head(3)
Out[3]:
  id   created_at   user_id   user_name   tweet_text   htag   urls   ptxt   tgrp   date   user_handles   txt_terms   search_grp
0   638818911773856000   Tue Sep 01 21:01:11 +0000 2015   2511247075   Noor Din   RT @kdnuggets: R leads RapidMiner, Python catc...   [#KDN]   [://t.co/3bsaTT7eUs]   r leads rapidminer python catches up big data ...   [spark, python]   2015-09-01 21:01:11   [@kdnuggets]   r leads rapidminer python catches up big data ...   [spark, python]
1   622142176768737000   Fri Jul 17 20:33:48 +0000 2015   24537879   IBM Cloudant   Be one of the first to sign-up for IBM Analyti...   [#ApacheSpark, #SparkInsight]   [://t.co/C5TZpetVA6, ://t.co/R1L29DePaQ]   be one of the first to sign up for ibm analyti...   [spark]   2015-07-17 20:33:48   []   be one of the first to sign up for ibm analyti...   [spark]
2   622140453069169000   Fri Jul 17 20:26:57 +0000 2015   515145898   Arno Candel   Nice article on #apachespark, #hadoop and #dat...   [#apachespark, #hadoop, #datascience]   [://t.co/IyF44pV0f3]   nice article on apachespark hadoop and datasci...   [spark]   2015-07-17 20:26:57   [@h2oai]   nice article on apachespark hadoop and datasci...   [spark]
```

### 备注

我们将使用的wordcloud 库是由 Andreas Mueller 开发的，托管在他的 gihub 账户[https://github.com/amueller/word_cloud](https://github.com/amueller/word_cloud)上。

库需要**PIL**(**Python Image Library**的缩写)。 通过调用`conda install pil`，可以轻松安装 PIL。 PIL 是一个安装起来很复杂的库，而且还没有移植到 Python3.4 上，所以我们需要运行 Python2.7+环境才能看到我们的单词云：

```py
#
# Install PIL (does not work with Python 3.4)
#
an@an-VB:~$ conda install pil

Fetching package metadata: ....
Solving package specifications: ..................
Package plan for installation in environment /home/an/anaconda:
```

将下载以下软件包：

```py
    package                    |            build
    ---------------------------|-----------------
    libpng-1.6.17              |                0         214 KB
    freetype-2.5.5             |                0         2.2 MB
    conda-env-2.4.4            |           py27_0          24 KB
    pil-1.1.7                  |           py27_2         650 KB
    ------------------------------------------------------------
                                           Total:         3.0 MB
```

将更新以下软件包：

```py
    conda-env: 2.4.2-py27_0 --> 2.4.4-py27_0
    freetype:  2.5.2-0      --> 2.5.5-0     
    libpng:    1.5.13-1     --> 1.6.17-0    
    pil:       1.1.7-py27_1 --> 1.1.7-py27_2

Proceed ([y]/n)? y
```

接下来，我们安装 wordcloud 库：

```py
#
# Install wordcloud
# Andreas Mueller
# https://github.com/amueller/word_cloud/blob/master/wordcloud/wordcloud.py
#

an@an-VB:~$ pip install wordcloud
Collecting wordcloud
  Downloading wordcloud-1.1.3.tar.gz (163kB)
    100% |████████████████████████████████| 163kB 548kB/s 
Building wheels for collected packages: wordcloud
  Running setup.py bdist_wheel for wordcloud
  Stored in directory: /home/an/.cache/pip/wheels/32/a9/74/58e379e5dc614bfd9dd9832d67608faac9b2bc6c194d6f6df5
Successfully built wordcloud
Installing collected packages: wordcloud
Successfully installed wordcloud-1.1.3
```

## 创建文字云

在这个阶段，我们已经准备好使用从 tweet 文本生成的术语列表调用 wordcloud 程序。

让我们开始使用 wordcloud 程序，首先调用`%matplotlib`内联以在笔记本中显示 wordcloud：

```py
In [4]:
%matplotlib inline
In [11]:
```

我们将 dataframe`txt_terms`列转换为单词列表。 我们确保将其全部转换为`str`类型，以避免任何糟糕的意外，并检查列表的前四条记录：

```py
len(tspark_df['txt_terms'].tolist())
Out[11]:
2024
In [22]:
tspark_ls_str = [str(t) for t in tspark_df['txt_terms'].tolist()]
In [14]:
len(tspark_ls_str)
Out[14]:
2024
In [15]:
tspark_ls_str[:4]
Out[15]:
['r leads rapidminer python catches up big data tools grow spark ignites kdn',
 'be one of the first to sign up for ibm analytics for apachespark today sparkinsight',
 'nice article on apachespark hadoop and datascience',
 'spark 101 running spark and mapreduce together in production hadoopsummit2015 apachespark altiscale']
```

我们首先调用 Matplotlib 和 WordCloud 库：

```py
import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS
```

从输入的术语列表中，我们创建一个由空格分隔的统一术语字符串，作为 wordcloud 程序的输入。 Wordcloud 程序删除了停用词：

```py
# join tweets to a single string
words = ' '.join(tspark_ls_str)

# create wordcloud 
wordcloud = WordCloud(
                      # remove stopwords
                      stopwords=STOPWORDS,
                      background_color='black',
                      width=1800,
                      height=1400
                     ).generate(words)

# render wordcloud image
plt.imshow(wordcloud)
plt.axis('off')

# save wordcloud image on disk
plt.savefig('./spark_tweets_wordcloud_1.png', dpi=300)

# display image in Jupyter notebook
plt.show()
```

在这里，我们可以可视化 Apache Spark 和 Python 的单词云。 显然，在 Spark 的例子中，*Hadoop*、*Big Data*和*analytics*是模因，而 Python 回忆起其名称 Monty Python 的根源，重点放在*Developer*、*Apache Spark*以及对 Java 和 Ruby 有一些提示的编程上。

![Creating wordclouds](img/B03968_06_02.jpg)

我们还可以从以下几个词中窥探一下，这些词集中在 2016 年北美总统大选候选人身上：希拉里·克林顿(Hilary Clinton)和唐纳德·特朗普(Donald Trump)。 希拉里·克林顿(Hilary Clinton)似乎被她的对手唐纳德·特朗普(Donald Trump)和伯尼·桑德斯(Bernie Sanders)的存在黯然失色，而特朗普在很大程度上只关注自己：

![Creating wordclouds](img/B03968_06_03.jpg)

有趣的是，在贾斯汀·比伯和 Lady Gaga 的例子中，出现了*Love*这个词。 在比伯的例子中，*Follow*和*Belieber*是关键词，而*节食*、*减肥*和*时尚*是 Lady Gaga 人群关注的焦点。

![Creating wordclouds](img/B03968_06_04.jpg)

# 地理定位推文和地图会议

现在，我们将深入研究如何使用 Bokeh 创建交互式地图。 首先，我们创建一个世界地图，在其中我们对示例 tweet 进行地理定位，当将鼠标移到这些位置上时，我们可以在悬停框中看到用户及其各自的 tweet。

第二张地图的重点是绘制即将在伦敦举行的会议地图。 它可以是一张交互式地图，可以提醒特定城市即将举行的会议的日期、时间和地点。

## 地理定位推文

其目标是创建一个世界地图散点图，显示地图上重要推文的位置，并在悬停这些点时显示推文和作者。 我们将经历三个步骤来构建此交互式可视化：

1.  通过首先加载由各自经度和纬度定义的所有世界国家边界的字典来创建背景世界地图。
2.  加载我们希望通过各自的坐标和作者进行地理定位的重要推文。
3.  最后，在世界地图上散布地图上的推文坐标，并激活悬停工具以交互地可视化推文，并在地图上高亮显示的点上创作推文。

在步骤1 中，我们创建一个名为 Data 的 Python 列表，该列表将包含所有世界各国的边界以及它们各自的纬度和经度：

```py
In [4]:
#
# This module exposes geometry data for World Country Boundaries.
#
import csv
import codecs
import gzip
import xml.etree.cElementTree as et
import os
from os.path import dirname, join

nan = float('NaN')
__file__ = os.getcwd()

data = {}
with gzip.open(join(dirname(__file__), 'AN_Spark/data/World_Country_Boundaries.csv.gz')) as f:
    decoded = codecs.iterdecode(f, "utf-8")
    next(decoded)
    reader = csv.reader(decoded, delimiter=',', quotechar='"')
    for row in reader:
        geometry, code, name = row
        xml = et.fromstring(geometry)
        lats = []
        lons = []
        for i, poly in enumerate(xml.findall('.//outerBoundaryIs/LinearRing/coordinates')):
            if i > 0:
                lats.append(nan)
                lons.append(nan)
            coords = (c.split(',')[:2] for c in poly.text.split())
            lat, lon = list(zip(*[(float(lat), float(lon)) for lon, lat in
                coords]))
            lats.extend(lat)
            lons.extend(lon)
        data[code] = {
            'name'   : name,
            'lats'   : lats,
            'lons'   : lons,
        }
In [5]:
len(data)
Out[5]:
235
```

在第二步中，我们加载一组重要的推文示例，我们希望用它们各自的地理位置信息来可视化这些推文：

```py
In [69]:
# data
#
#
In [8]:
import pandas as pd
csv_in = '/home/an/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark/data/spark_tweets_20.csv'
t20_df = pd.read_csv(csv_in, index_col=None, header=0, sep=',', encoding='utf-8')
In [9]:
t20_df.head(3)
Out[9]:
    id  created_at  user_id     user_name   tweet_text  htag    urls    ptxt    tgrp    date    user_handles    txt_terms   search_grp  lat     lon
0   638818911773856000  Tue Sep 01 21:01:11 +0000 2015  2511247075  Noor Din    RT @kdnuggets: R leads RapidMiner, Python catc...   [#KDN]  [://t.co/3bsaTT7eUs]    r leads rapidminer python catches up big data ...   [spark, python]     2015-09-01 21:01:11     [@kdnuggets]    r leads rapidminer python catches up big data ...   [spark, python]     37.279518   -121.867905
1   622142176768737000  Fri Jul 17 20:33:48 +0000 2015  24537879    IBM Cloudant    Be one of the first to sign-up for IBM Analyti...   [#ApacheSpark, #SparkInsight]   [://t.co/C5TZpetVA6, ://t.co/R1L29DePaQ]    be one of the first to sign up for ibm analyti...   [spark]     2015-07-17 20:33:48     []  be one of the first to sign up for ibm analyti...   [spark]     37.774930   -122.419420
2   622140453069169000  Fri Jul 17 20:26:57 +0000 2015  515145898   Arno Candel     Nice article on #apachespark, #hadoop and #dat...   [#apachespark, #hadoop, #datascience]   [://t.co/IyF44pV0f3]    nice article on apachespark hadoop and datasci...   [spark]     2015-07-17 20:26:57     [@h2oai]    nice article on apachespark hadoop and datasci...   [spark]     51.500130   -0.126305
In [98]:
len(t20_df.user_id.unique())
Out[98]:
19
In [17]:
t20_geo = t20_df[['date', 'lat', 'lon', 'user_name', 'tweet_text']]
In [24]:
# 
t20_geo.rename(columns={'user_name':'user', 'tweet_text':'text' }, inplace=True)
In [25]:
t20_geo.head(4)
Out[25]:
    date    lat     lon     user    text
0   2015-09-01 21:01:11     37.279518   -121.867905     Noor Din    RT @kdnuggets: R leads RapidMiner, Python catc...
1   2015-07-17 20:33:48     37.774930   -122.419420     IBM Cloudant    Be one of the first to sign-up for IBM Analyti...
2   2015-07-17 20:26:57     51.500130   -0.126305   Arno Candel     Nice article on #apachespark, #hadoop and #dat...
3   2015-07-17 19:35:31     51.500130   -0.126305   Ira Michael Blonder     Spark 101: Running Spark and #MapReduce togeth...
In [22]:
df = t20_geo
#
```

在步骤3 中，我们首先导入了所有必要的 Bokeh 库。 我们将实例化 Jupyter 笔记本中的输出。 我们加载了世界各国的边界信息。 我们得到了地理位置的推特数据。 我们实例化 Bokeh 交互工具，如滚轮和框缩放以及悬停工具。

```py
In [29]:
#
# Bokeh Visualization of tweets on world map
#
from bokeh.plotting import *
from bokeh.models import HoverTool, ColumnDataSource
from collections import OrderedDict

# Output in Jupiter Notebook
output_notebook()

# Get the world map
world_countries = data.copy()

# Get the tweet data
tweets_source = ColumnDataSource(df)

# Create world map 
countries_source = ColumnDataSource(data= dict(
    countries_xs=[world_countries[code]['lons'] for code in world_countries],
    countries_ys=[world_countries[code]['lats'] for code in world_countries],
    country = [world_countries[code]['name'] for code in world_countries],
))

# Instantiate the bokeh interactive tools 
TOOLS="pan,wheel_zoom,box_zoom,reset,resize,hover,save"
```

现在我们已经准备好将收集到的各种元素分层成一个称为**p**的对象图形。 定义**p**的标题、宽度和高度。 安装工具。 通过带有浅色背景和边框的补丁创建世界地图背景。 根据推文各自的地理坐标散布推文。 然后，使用用户及其各自的推文激活悬停工具。 最后，在浏览器上呈现图片。 代码如下：

```py
# Instantiante the figure object
p = figure(
    title="%s tweets " %(str(len(df.index))),
    title_text_font_size="20pt",
    plot_width=1000,
    plot_height=600,
    tools=TOOLS)

# Create world patches background
p.patches(xs="countries_xs", ys="countries_ys", source = countries_source, fill_color="#F1EEF6", fill_alpha=0.3,
        line_color="#999999", line_width=0.5)

# Scatter plots by longitude and latitude
p.scatter(x="lon", y="lat", source=tweets_source, fill_color="#FF0000", line_color="#FF0000")
# 

# Activate hover tool with user and corresponding tweet information
hover = p.select(dict(type=HoverTool))
hover.point_policy = "follow_mouse"
hover.tooltips = OrderedDict([
    ("user", "@user"),
   ("tweet", "@text"),
])

# Render the figure on the browser
show(p)
BokehJS successfully loaded.

inspect

#
#
```

下面的代码概述了世界地图，其中的红点表示推文来源的位置：

![Geo-locating tweets](img/B03968_06_05.jpg)

我们可以将鼠标悬停在特定的点上，以显示该位置的推文：

![Geo-locating tweets](img/B03968_06_06.jpg)

我们可以放大到特定位置：

![Geo-locating tweets](img/B03968_06_07.jpg)

最后，我们可以显示给定放大位置的 tweet：

![Geo-locating tweets](img/B03968_06_08.jpg)

## 在 Google 地图上显示即将召开的会议

现在，我们的目标是专注于即将在伦敦举行的次会议。 我们正在映射三个Meetup**Data Science London**、**Apache Spark**和**Machine Learning**。 我们在 Bokeh 可视化中嵌入 Google Map，根据三个 Meetup 的坐标对其进行地理定位，并使用悬停工具获取每个 Meetup 即将举行的活动的名称等信息。

首先，导入所有必要的 Bokeh 库：

```py
In [ ]:
#
# Bokeh Google Map Visualization of London with hover on specific points
#
#
from __future__ import print_function

from bokeh.browserlib import view
from bokeh.document import Document
from bokeh.embed import file_html
from bokeh.models.glyphs import Circle
from bokeh.models import (
    GMapPlot, Range1d, ColumnDataSource,
    PanTool, WheelZoomTool, BoxSelectTool,
    HoverTool, ResetTool,
    BoxSelectionOverlay, GMapOptions)
from bokeh.resources import INLINE

x_range = Range1d()
y_range = Range1d()
```

我们将实例化 Google Map，它将作为 Bokeh 可视化的基础：

```py
# JSON style string taken from: https://snazzymaps.com/style/1/pale-dawn
map_options = GMapOptions(lat=51.50013, lng=-0.126305, map_type="roadmap", zoom=13, styles="""
[{"featureType":"administrative","elementType":"all","stylers":[{"visibility":"on"},{"lightness":33}]},
 {"featureType":"landscape","elementType":"all","stylers":[{"color":"#f2e5d4"}]},
 {"featureType":"poi.park","elementType":"geometry","stylers":[{"color":"#c5dac6"}]},
 {"featureType":"poi.park","elementType":"labels","stylers":[{"visibility":"on"},{"lightness":20}]},
 {"featureType":"road","elementType":"all","stylers":[{"lightness":20}]},
 {"featureType":"road.highway","elementType":"geometry","stylers":[{"color":"#c5c6c6"}]},
 {"featureType":"road.arterial","elementType":"geometry","stylers":[{"color":"#e4d7c6"}]},
 {"featureType":"road.local","elementType":"geometry","stylers":[{"color":"#fbfaf7"}]},
 {"featureType":"water","elementType":"all","stylers":[{"visibility":"on"},{"color":"#acbcc9"}]}]
""")
```

使用上一步中的尺寸和地图选项实例化类`GMapPlot`中的 Bokeh 对象绘图：

```py
# Instantiate Google Map Plot
plot = GMapPlot(
    x_range=x_range, y_range=y_range,
    map_options=map_options,
    title="London Meetups"
)
```

从我们希望绘制的三个会议中引入信息，并通过将鼠标悬停在各自坐标上方来获取信息：

```py
source = ColumnDataSource(
    data=dict(
        lat=[51.49013, 51.50013, 51.51013],
        lon=[-0.130305, -0.126305, -0.120305],
        fill=['orange', 'blue', 'green'],
        name=['LondonDataScience', 'Spark', 'MachineLearning'],
        text=['Graph Data & Algorithms','Spark Internals','Deep Learning on Spark']
    )
)
```

定义要在 Google 地图上绘制的点：

```py
circle = Circle(x="lon", y="lat", size=15, fill_color="fill", line_color=None)
plot.add_glyph(source, circle)
```

定义要在此可视化中使用的 Bokeh 工具的钻柱：

```py
# TOOLS="pan,wheel_zoom,box_zoom,reset,hover,save"
pan = PanTool()
wheel_zoom = WheelZoomTool()
box_select = BoxSelectTool()
reset = ResetTool()
hover = HoverTool()
# save = SaveTool()

plot.add_tools(pan, wheel_zoom, box_select, reset, hover)
overlay = BoxSelectionOverlay(tool=box_select)
plot.add_layout(overlay)
```

使用将携带的信息激活`hover`工具：

```py
hover = plot.select(dict(type=HoverTool))
hover.point_policy = "follow_mouse"
hover.tooltips = OrderedDict([
    ("Name", "@name"),
    ("Text", "@text"),
    ("(Long, Lat)", "(@lon, @lat)"),
])

show(plot)
```

绘制把伦敦的景色看得很清楚的地块：

![Displaying upcoming meetups on Google Maps](img/B03968_06_09.jpg)

一旦我们悬停在突出显示的点上，我们就可以获得给定 Meetup 的信息：

![Displaying upcoming meetups on Google Maps](img/B03968_06_10.jpg)

保留了完全平滑的缩放功能，如以下屏幕截图所示：

![Displaying upcoming meetups on Google Maps](img/B03968_06_11.jpg)

# 摘要

在本章中，我们重点介绍了几种可视化技术。 我们看到了如何构建词云，以及它们的直观能力，可以一目了然地揭示数千条推文中携带的大量关键词、情绪和模因。

然后我们讨论了使用 Bokeh 的交互式地图可视化。 我们从头开始制作了一张世界地图，并创建了一个批判性推文的散点图。 一旦地图呈现在浏览器上，我们就可以交互地从一个点到另一个点悬停，并显示来自世界不同地区的推文。

我们最后的可视化重点是绘制即将在伦敦举行的关于 Spark、数据科学和机器学习的会议及其各自的主题，用实际的 Google Map 制作一个漂亮的交互式可视化。