# 第一章。介绍卡夫卡

在当今世界，实时信息不断由应用程序(商业、社交或任何其他类型)生成，这些信息需要简单的方法来可靠、快速地路由到多种类型的接收器。大多数情况下，产生信息的应用程序和使用这些信息的应用程序相距甚远，彼此无法访问。这些异构应用程序导致重新开发，以便在它们之间提供一个集成点。因此，需要一种机制来无缝集成来自生产者和消费者的信息，以避免任何一端的应用程序重写。

# 欢迎来到阿帕奇卡夫卡的世界

在当前的大数据时代，第一个挑战是收集大量数据，第二个挑战是分析数据。该分析通常包括以下类型的数据以及更多:

*   用户行为数据
*   应用程序性能跟踪
*   日志形式的活动数据
*   事件消息

消息发布是一种在消息之间路由的帮助下连接各种应用程序的机制，例如，通过像 Kafka 这样的消息代理。卡夫卡是任何软件解决方案的实时问题的解决方案；也就是说，处理实时量的信息并快速将其路由到多个消费者。Kafka 提供了来自生产者和消费者的信息之间的无缝集成，而不会阻塞信息的生产者，也不会让生产者知道谁是最终消费者。

Apache Kafka 是一个开源、分布式、分区和复制的基于提交日志的发布-订阅消息传递系统，主要设计有以下特点:

*   **持久消息传递**:要从大数据中获得真正的价值，任何形式的信息丢失都是无法承受的。Apache Kafka 采用 O(1)磁盘结构设计，即使存储了大量的 TBs 级别的消息，也能提供恒定时间性能。使用 Kafka，消息被保存在磁盘上并在集群内复制，以防止数据丢失。
*   **高吞吐量**:牢记大数据，Kafka 被设计为在商品硬件上工作，并处理来自大量客户端的每秒数百兆字节的读写。
*   **分布式** : Apache Kafka 以集群为中心的设计明确支持在 Kafka 服务器上进行消息分区，并在消费者机器集群上分配消费，同时保持每个分区的排序语义。Kafka 集群可以在不停机的情况下弹性透明地增长。
*   **多客户端支持**:Apache Kafka 系统支持来自不同平台的客户端轻松集成，如 Java、。NET、PHP、Ruby 和 Python。
*   **实时**:生产者线程产生的消息应该对消费者线程立即可见；该功能对于基于事件的系统至关重要，例如 **复杂事件处理** ( **CEP** )系统。

Kafka 提供了一个实时发布-订阅解决方案，该解决方案克服了消耗实时和批量数据量的挑战，这些数据量可能会以数量级增长到大于真实数据。Kafka 还支持 Hadoop 系统中的并行数据加载。

下图显示了 Apache Kafka 消息传递系统支持的典型大数据聚合和分析场景:

![Welcome to the world of Apache Kafka](graphics/3090OS_01_01.jpg)

在生产方面，有种不同的生产者，例如:

*   生成应用程序日志的前端 web 应用程序
*   生产者代理生成网络分析日志
*   生成转换日志的生产者适配器
*   生成调用跟踪日志的生产者服务

在消费端，有种不同的消费者，比如:

*   消费消息并将消息存储在 Hadoop 或传统数据仓库中进行离线分析的离线消费者
*   消费消息并将消息存储在任何 NoSQL 数据存储区(如 HBase 或 Cassandra)中的近乎实时的消费者，用于近乎实时的分析
*   实时使用者，如 Spark 或 Storm，用于过滤内存中的消息并触发相关组的警报事件

# 我们为什么需要卡夫卡？

大量数据是由具有任何形式的基于网络或设备的存在和活动的公司生成的。数据是这些基于互联网的系统中较新的成分之一，通常包括用户活动；对应于登录的事件；页面访问量；点击次数；社交网络活动，如喜欢、分享和评论；以及运营和系统指标。由于高吞吐量(每秒数百万条消息)，这些数据通常由日志记录和传统日志聚合解决方案处理。这些传统解决方案是为离线分析系统(如 Hadoop)提供日志数据的可行解决方案。然而，这些解决方案对于构建实时处理系统非常有限。

根据互联网应用的新趋势，活动数据已成为生产数据的一部分，并用于实时运行分析。这些分析可以是:

*   基于相关性的搜索
*   基于受欢迎程度、共现或情感分析的建议
*   向大众投放广告
*   防止垃圾邮件或未经授权的数据抓取的互联网应用安全
*   发送高温警报的设备传感器
*   任何异常的用户行为或应用程序黑客攻击

由于收集和处理的数据量很大，实时使用从生产系统收集的多组数据已成为一项挑战。

Apache Kafka 旨在通过在 Hadoop 系统中提供并行加载机制以及在机器集群上划分实时消耗的能力来统一离线和在线处理。Kafka 可以与 Scribe 或 Flume 进行比较，因为它对处理活动流数据很有用；但从架构的角度来看，它更接近于传统的消息传递系统，如 ActiveMQ 或 RabitMQ。

# 卡夫卡用例

卡夫卡可以用在任何建筑中。本节讨论了阿帕奇卡夫卡和采用卡夫卡的知名公司的一些流行使用案例。以下是流行的卡夫卡用例:

*   **日志聚合**:这是从服务器收集物理日志文件，放在中心位置(文件服务器或 HDFS)进行处理的过程。使用卡夫卡提供了日志或事件数据作为消息流的清晰抽象，从而消除了对文件细节的任何依赖。这也为多数据源和分布式数据消费提供了更低延迟的处理和支持。
*   **流处理** : Kafka 可以用于收集的数据在多个阶段进行处理的用例——一个例子是从主题中消费并丰富的原始数据或者转换成新的 Kafka 主题以供进一步消费。因此，这种处理也称为流处理。
*   **提交日志** : Kafka 可以用来表示任何大规模分布式系统的外部提交日志。卡夫卡集群上的复制日志有助于故障节点恢复其状态。
*   **点击流跟踪**:Kafka 另一个非常重要的使用案例是捕获用户点击流数据，如页面浏览量、搜索量等，作为实时发布-订阅 feeds。由于数据量很大，该数据以每个活动一个主题的形式发布到中心主题。这些主题可供许多消费者订阅，用于包括实时处理和监控在内的广泛应用。
*   **消息传递**:消息代理用于将数据处理与数据生产者脱钩。Kafka 可以取代许多流行的消息代理，因为它提供了更好的吞吐量、内置分区、复制和容错。

在各自的用例中使用 Apache Kafka 的一些公司如下:

*   **LinkedIn**([www.linkedin.com](http://www.linkedin.com)):Apache Kafka 在 LinkedIn 用于活动数据和运营指标的流式传输。除了 Hadoop 等离线分析系统之外，这些数据还为领英新闻订阅源和今日领英等各种产品提供动力。
*   **DataSift**([www.datasift.com](http://www.datasift.com)):在 DataSift，卡夫卡作为采集器监控事件，作为用户实时消费数据流的跟踪器。
*   **推特**([www.twitter.com](http://www.twitter.com)):推特将卡夫卡作为其风暴的一部分——一个流处理基础设施。
*   **four square**([www.foursquare.com](http://www.foursquare.com)):卡夫卡在 Foursquare 提供线上到线上和线上到线下的消息传递。它用于将 Foursquare 监控和生产系统与基于 Foursquare 和 Hadoop 的离线基础架构集成在一起。
*   **Square** ([www.squareup.com](http://www.squareup.com)): Square uses Kafka as a *bus* to move all system events through Square's various datacenters. This includes metrics, logs, custom events, and so on. On the consumer side, it outputs into Splunk, Graphite, or Esper-like real-time alerting.

    ### 注

    前述信息来源为[。](https://cwiki.apache.org/confluence/display/KAFKA/Powered+By)

# 安装卡夫卡

Kafka 是一个 Apache 项目，其当前版本 0.8.1.1 是一个稳定的版本。与旧版本(0.8.x 之前)相比，这款卡夫卡 0.8.x 提供了许多高级功能。它的一些进步如下:

*   在 0.8.x 之前，如果代理失败，主题中任何未消耗的数据分区都可能丢失。现在为分区提供了一个复制因子。这确保了任何提交的消息都不会丢失，因为至少有一个副本可用。
*   前面的特性还确保所有的生产者和消费者都知道复制(复制因子是一个可配置的属性)。默认情况下，生产者的消息发送请求被阻止，直到消息被提交给所有活动副本；但是，生产者也可以配置为将消息提交给单个代理。
*   像卡夫卡的制作人一样，卡夫卡的消费者轮询模型变成了长拉模型，并被阻塞，直到制作人提供了提交的消息，这避免了频繁的拉取。
*   此外，Kafka 0.8.x 还附带了一套管理工具，例如受控集群关闭和 Lead 副本选举工具，用于管理 Kafka 集群。

Kafka 0.8 . x 版本的主要限制是它不能取代 0.8 之前的版本，因为它不是向后兼容的。

回到安装 Kafka，作为第一步，我们需要下载可用的稳定版本(所有进程都在 64 位 CentOS 6.4 OS 上测试过，在其他基于内核的 OS 上可能会有所不同)。现在让我们看看安装卡夫卡需要遵循哪些步骤。

## 安装先决条件

卡夫卡在 Scala 中实现，并使用构建工具 **Gradle** 构建卡夫卡二进制文件。Gradle 是一个构建自动化工具，适用于需要 Java 1.7 或更高版本的 Scala、Groovy 和 Java 项目。

## 安装 Java 1.7 或更高版本

执行以下步骤安装 Java 1.7 或更高版本:

1.  从甲骨文网站下载`jdk-7u67-linux-x64.rpm`版本:[http://www . Oracle . com/technetwork/Java/javase/downloads/index . html](http://www.oracle.com/technetwork/java/javase/downloads/index.html)。
2.  更改文件模式如下:

    ```
    [root@localhost opt]#chmod +x jdk-7u67-linux-x64.rpm 

    ```

3.  Change to the directory in which you want to perform the installation. To do so, type the following command:

    ```
    [root@localhost opt]# cd <directory path name>

    ```

    例如，要在`/usr/java/`目录中安装软件，请键入以下命令:

    ```
    [root@localhost opt]# cd /usr/java

    ```

4.  使用以下命令运行安装程序:

    ```
    [root@localhost java]# rpm -ivh jdk-7u67-linux-x64.rpm 

    ```

5.  最后，添加环境变量`JAVA_HOME`。以下命令将把`JAVA_HOME`环境变量写入包含系统范围环境配置的文件`/etc/profile`:

    ```
    [root@localhost opt]# echo "export JAVA_HOME=/usr/java/jdk1.7.0_67 " >> /etc/profile

    ```

## 下载卡夫卡

执行以下步骤下载卡夫卡 0.8.1.1 版:

1.  Download the current beta release of Kafka (0.8) into a folder on your filesystem (for example, `/opt`) using the following command:

    ```
    [root@localhost opt]#wget http://apache.tradebit.com/pub/kafka/0.8.1.1/kafka_2.9.2-0.8.1.1.tgz

    ```

    ### 注

    前面的网址可能会改变。在[http://kafka.apache.org/downloads.html](http://kafka.apache.org/downloads.html)查看正确的下载版本和位置。

2.  使用以下命令提取下载的`kafka_2.9.2-0.8.1.1.tgz`文件:

    ```
    [root@localhost opt]# tar xzf kafka_2.9.2-0.8.1.1.tgz

    ```

3.  After extraction of the `kafka_2.9.2-0.8.1.1.tgz` file, the directory structure for Kafka 0.8.1.1 looks as follows:

    ![Downloading Kafka](graphics/3090OS_01_02.jpg)

4.  最后，将卡夫卡箱文件夹添加到`PATH`中，如下所示:

    ```
    [root@localhost opt]# export KAFKA_HOME=/opt/kafka_2.9.2-0.8.1.1
    [root@localhost opt]# export PATH=$PATH:$KAFKA_HOME/bin

    ```

## 建造卡夫卡

用于构建 Kafka 版本 0.8.1.1 的默认 Scala 版本是 Scala 2.9.2，但是 Kafka 源代码也可以从其他 Scala 版本编译，例如 2.8.0、2.8.2、2.9.1 或 2.10.1。使用以下命令构建卡夫卡源:

```
[root@localhost opt]# ./gradlew -PscalaVersion=2.9.1 jar

```

在 Kafka 8.x 以后，Gradle 工具用于编译 Kafka 源代码(在`kafka-0.8.1.1-src.tgz`中提供)和构建 Kafka 二进制文件(JAR 文件)。与卡夫卡 JAR 类似，单元测试或源 JAR 也可以使用 Gradle 构建工具来构建。有关构建相关说明的更多信息，请参考[https://github.com/apache/kafka/blob/0.8.1/README.md](https://github.com/apache/kafka/blob/0.8.1/README.md)。

# 总结

在这一章中，我们看到了公司是如何发展收集和处理应用程序生成的数据的机制的，并且正在学习通过对这些数据运行分析来利用这些数据的真正力量。

您还学习了如何安装 0.8.1.x。下一章讨论了设置单代理或多代理卡夫卡集群所需的步骤。