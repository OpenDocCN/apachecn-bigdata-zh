# 第 8 章：星火数据库

创建大数据分析群集、导入数据以及创建 ETL 流来清理和处理数据非常困难，而且成本也很高。 Databricks 的目的是降低复杂性，使集群创建过程和数据处理变得更容易。 他们基于 Apache Spark 创建了一个基于云的平台，可自动创建集群，并简化数据导入、处理和可视化。 目前，存储是基于 AWS 的，但在未来，他们计划扩展到其他云提供商。

设计 Apache Spark 的人员也参与了 Databricks系统。 在写这本书的时候，这项服务只能通过注册才能访问。 我被提供了 30 天的试用期。 在接下来的两章中，我将研究该服务及其组件，并提供一些示例代码来说明它是如何工作的。 本章将介绍以下主题：

*   安装 Databricks
*   AWS 配置
*   帐户管理
*   菜单系统
*   笔记本和文件夹
*   通过库导入作业
*   开发环境
*   Databricks 表
*   Databricks DbUtils 包

鉴于这本书是以静态格式提供的，因此很难全面检查流等功能。

# 概述

在[https://databricks.com/](https://databricks.com/)网站上提供的 Databricks 服务基于集群的概念。 这类似于星火簇，在前面的章节中已经研究和使用过。 它包含主人、工人和执行者。 但是，根据您指定的内存量，集群的配置和大小是自动的。 安全、隔离、进程监控和资源管理等功能都是自动为您管理的。 如果您在短时间内迫切需要使用 200 GB 内存的基于 Spark 的集群，则可以使用此服务动态创建该集群，并处理您的数据。 处理完成后，您可以终止群集以降低成本。

在集群中，我们介绍了笔记本的概念，以及用于创建脚本和运行程序的位置。 可以在笔记本中创建文件夹，笔记本可以基于 Scala、Python 或 SQL。 可以创建作业来执行该功能，并且可以从 Notebook 代码或导入的库中调用作业。 笔记本可以调用笔记本功能。 此外，还提供了基于时间或事件安排作业的功能。

这让您对 Databricks 服务提供了什么感觉。 以下各节将解释已介绍的每个主要项目。 请记住，这里展示的内容是新的和不断发展的。 此外，我使用 AWS US East(北弗吉尼亚)区域进行演示，因为亚洲悉尼区域当前存在导致 Databricks 安装失败的限制。

# 安装 Databricks

为了创建这个演示，我使用了亚马逊提供的一年免费访问服务，在[http://aws.amazon.com/free/](http://aws.amazon.com/free/)上提供了。 这有一些限制，比如 5 GB 的 S3 存储和 750 小时的**Amazon Elastic Compute Cloud**(**EC2**)，但它允许我以低成本访问，并降低了总体 EC2 成本。 AWS 帐户提供以下内容：

*   帐户 ID
*   访问密钥 ID
*   秘密访问密钥

Databricks 使用这些信息来访问您的 AWS 存储、安装 Databricks 系统以及创建您指定的群集组件。 从安装的那一刻起，您就开始招致 AWS EC2 成本，因为 Databricks 系统使用至少两个没有任何集群的运行实例。 成功输入 AWS 和账单信息后，系统将提示您启动 Databricks 云。

![Installing Databricks](graphics/B01989_08_01.jpg)

完成此操作后，您将获得访问您的云的 URL、管理员帐户和密码。 这将允许您访问 Databricks 基于 Web 的用户界面，如以下屏幕截图所示：

![Installing Databricks](graphics/B01989_08_02.jpg)

这是欢迎屏幕。 它在图像顶部显示菜单栏，从左到右包含菜单、搜索、帮助和帐户图标。 在使用系统时，可能还会出现一个钟面图标，显示最近的活动。 在这个单一的界面中，您可以在创建自己的集群和代码之前搜索帮助屏幕和使用示例。

# AWS 计费

请注意，安装 Databricks 系统后，您将开始产生 AWS EC2 存储成本。 Databricks试图通过在整个充电期内保持 EC2 资源处于活动状态来最大限度地降低您的成本。 例如，如果您终止 Databricks 集群，则基于集群的 EC2 实例将在 AWS 为其收费的一小时内仍然存在。 这样，如果您创建一个新的集群，Databricks 就可以重用它们。 以下屏幕截图显示，尽管我使用的是免费 AWS 帐户，并且我谨慎地减少了资源使用量，但我在短时间内产生了 AWS EC2 成本：

![AWS billing](graphics/B01989_08_03.jpg)

您需要了解您创建的 Databricks 群集，并了解当它们存在并被使用时，会产生 AWS 成本。 只保留您真正需要的集群，并终止任何其他集群。

为了检查Databricks 数据导入功能，我还创建了一个 AWS S3 存储桶，并将数据文件上传到其中。 这一点将在本章后面部分解释。

# 数据库菜单

通过选择Databricks Web 界面上的左上角菜单图标，可以扩展菜单系统。 下面的屏幕截图显示了展开到文件夹层次结构`/folder1/folder2/`的顶级菜单选项以及**工作区**选项。 最后，它显示了可以在`folder2`上执行的操作，即创建笔记本、创建仪表板等。

![Databricks menus](graphics/B01989_08_04.jpg)

所有这些操作都将在以后的小节中展开。 下一节将研究帐户管理，然后再讨论群集。

# 账户管理

在 Databricks 中，帐户管理非常简化。 有一个默认的管理员帐户，可以创建后续帐户，但您需要知道管理员密码才能执行此操作。 密码长度需要超过 8 个字符；至少应包含一个数字、一个大写字符和一个非字母数字字符。 **帐户**选项可从右上角菜单选项访问，如以下屏幕截图所示：

![Account management](graphics/B01989_08_05.jpg)

这还允许用户注销。 通过选择帐户设置，您可以更改密码。 通过选择**Accounts**菜单选项，将生成**Accounts**列表。 在那里，您可以找到**添加帐户**的选项，并且可以通过每个帐户行上的**X**选项删除每个帐户，如以下屏幕截图所示：

![Account management](graphics/B01989_08_06.jpg)

也可以从帐户列表中重置帐户密码。 选择**Add Account**选项将创建一个需要电子邮件地址、全名、管理员密码和用户密码的新帐户窗口。 因此，如果您想要创建一个新用户，您需要知道 Databricks 实例管理员密码。 您还必须遵守新密码的规则，如下所示：

*   至少八个字符
*   必须至少包含范围为 0-9 的一个数字
*   必须在以下范围内至少包含一个大写字符：A-Z
*   Must contain at least one non-alphanumeric character: !@#$%

    ![Account management](graphics/B01989_08_07.jpg)

下一节将检查**Clusters**菜单选项，并使您能够管理自己的 Databricks Spark Clusters。

# 集群管理

选择**群集**菜单选项可提供当前 Databricks 群集及其状态的列表。 当然，目前你没有。 选择**Add Cluster**选项允许您创建一个。 请注意，您指定的内存量决定了集群的大小。 创建具有单个主服务器和辅助服务器的群集至少需要 54 GB。 每指定一个额外的 54 GB，就会添加另一个工作进程。

![Cluster management](graphics/B01989_08_08.jpg)

下面的屏幕截图是一个拼接的图像，显示了一个名为`semclust1`的新集群正在创建，并且处于**挂起**状态。 当**挂起**时，群集没有仪表板，并且群集节点不可访问。

![Cluster management](graphics/B01989_08_09.jpg)

创建后，将列出集群内存，其状态从**Pending**更改为**Running**。 默认仪表板已自动连接，并且可以访问 Spark Master 和 Worker 用户界面。 这里需要注意的是，Databricks 会自动启动，并由管理集群进程。 此显示屏右侧还有**选项**列，可用于**配置**、**重新启动**或**终止**群集，如以下屏幕截图所示：

![Cluster management](graphics/B01989_08_10.jpg)

通过重新配置群集，您可以更改其大小。 通过添加更多内存，您可以添加更多工作进程。 下面的屏幕截图显示了一个默认大小为 54 GB 的集群，其内存扩展到`108`GB。

![Cluster management](graphics/B01989_08_11.jpg)

终止群集会将其删除，并且无法恢复。 因此，您需要确保删除是正确的操作过程。 Databricks 会在实际终止之前提示您确认操作。

![Cluster management](graphics/B01989_08_12.jpg)

群集的创建和终止都需要时间。 在终止期间，群集将标记为，并带有橙色横幅，状态为**终止**，如以下屏幕截图所示：

![Cluster management](graphics/B01989_08_13.jpg)

请注意，前面屏幕截图中的集群类型显示为**On-Demand**。 创建簇时，可以选中名为**的复选框 Use Spot Instance to Create a Spot Clusters**。 这些集群比按需集群更便宜，因为它们竞标更便宜的 AWS 现货价格。 但是，它们的启动速度可能比按需群集慢。

Spark 用户界面与您在非 Databricks Spark 集群上预期的用户界面相同。 您可以检查工作器、执行器、配置和日志文件。 当您创建群集时，它们将被添加到您的群集列表中。 其中一个群集将用作运行仪表板的群集。 这可以通过使用**创建仪表板群集**选项进行更改。 将库和笔记本添加到群集时，群集详细信息条目将根据添加的数量进行更新。

我现在唯一想说的是 Databricks Spark 用户界面选项，因为它很熟悉，那就是它显示了使用的 Spark 版本。 下面的屏幕截图摘自主用户界面，显示正在使用的 Spark 版本(1.3.0)非常新。 在撰写本文时，最新的 Apache Spark 版本是 1.3.1，日期为 2015 年 4 月 17 日。

![Cluster management](graphics/B01989_08_14.jpg)

下一节将研究 Databricks 笔记本和文件夹-如何创建它们，以及如何使用它们。

# 笔记本和文件夹

笔记本是一种特殊类型的 Databricks 文件夹，可用于创建 Spark 脚本。 Notebook可以调用 Notebook脚本来创建功能层次结构。 创建时，必须指定 Notebook 的类型(Python、Scala 或 SQL)，然后群集可以指定可以对其运行 Notebook 功能。 下面的屏幕截图显示了笔记本的创建。

![Notebooks and folders](graphics/B01989_08_15.jpg)

请注意，笔记本会话右侧的菜单选项允许更改要的笔记本类型。 下面的示例显示，Python 笔记本可以更改为**Scala**、**SQL**或**Markdown**：

![Notebooks and folders](graphics/B01989_08_16.jpg)

请注意，Scala Notebook 不能更改为 Python，Python Notebook 也不能更改为 Scala。 作为开发语言，术语 Python、Scala 和 SQL 很容易理解，然而，**Markdown**是新的。 Markdown允许从文本中的格式化命令创建格式化文档。 简单的参考可以在[https://forums.databricks.com/static/markdown/help.html](https://forums.databricks.com/static/markdown/help.html)找到。

这意味着可以在创建脚本时将格式化注释添加到笔记本会话中。 笔记本被进一步细分为包含要执行的命令的单元格。 通过将鼠标悬停在左上角并将单元格拖到适当位置，可以在笔记本中移动单元格。 可以将新单元格插入到笔记本内的单元格列表中。

此外，在 Scala 或 Python Notebook 单元格中使用`%sql`命令，可以使用 SQL 语法。 通常，*Shift*+*Enter*组合键会导致执行笔记本或文件夹中的文本块。 使用`%md`命令可以在单元格中添加个 Markdown 注释。 此外，还可以将注释添加到笔记本单元格。 笔记本单元格右上角的菜单选项(如以下屏幕截图所示)显示备注以及最小化和最大化选项：

![Notebooks and folders](graphics/B01989_08_17.jpg)

多个基于 Web 的会话可以共享一个笔记本。 笔记本中发生的操作将填充到查看它的每个 Web 界面。 此外，还可以使用 Markdown 和 Comment 选项来实现用户之间的通信，以帮助分布式组之间的交互式数据调查。

![Notebooks and folders](graphics/B01989_08_18.jpg)

前面的屏幕截图显示了**Notebook1**的 Notebook 会话的标题。 它显示笔记本名称和类型(**Scala**)。 它还显示了将笔记本锁定为只读的选项，以及将其与群集分离的选项。 以下屏幕截图显示了在笔记本工作区中创建文件夹的过程：

![Notebooks and folders](graphics/B01989_08_19.jpg)

**Workspace**主菜单选项中的下拉菜单允许创建文件夹-在本例中名为`folder1`。 后面几节将介绍此菜单中的其他选项。 创建并选择后，名为`folder1`的新文件夹中的下拉菜单将在以下屏幕截图中显示与其关联的操作：

![Notebooks and folders](graphics/B01989_08_20.jpg)

因此，可以将文件夹导出到 DBC 存档。 它可以被锁定或克隆以创建副本。 也可以重命名或删除它。 可以将项目导入其中；例如，文件，稍后将通过示例对其进行说明。 此外，还可以在其中创建新的笔记本、仪表板、库和文件夹。

与可以对文件夹执行操作的方式相同，笔记本也有一组可能的操作。 下面的屏幕截图显示了可通过名为`notebook1`的笔记本的下拉菜单执行的操作，该笔记本当前连接到名为`semclust1`的正在运行的群集。 可以重命名、删除、锁定或克隆笔记本。 也可以将其从其当前群集分离，或者如果它已分离，则将其附加。 也可以将笔记本导出为文件或 DBC 存档。

![Notebooks and folders](graphics/B01989_08_21.jpg)

从文件夹**导入**选项，可以将文件导入到文件夹。 下面的屏幕截图显示了选择此选项时调用的文件下拉选项窗口。 可以将文件从本地服务器拖放到上载窗格，或者单击此窗格打开导航浏览器，在本地服务器上搜索要上载的文件。

![Notebooks and folders](graphics/B01989_08_22.jpg)

请注意，上传的文件需要是特定类型的。 下面的屏幕截图显示了支持的文件类型。 这是浏览要上传的文件时从文件浏览器拍摄的屏幕截图。 这也是有道理的。 支持的文件类型有 Scala、SQL 和 Python；以及 DBC 归档和 JAR 文件库。

![Notebooks and folders](graphics/B01989_08_23.jpg)

在离开此部分之前，还应注意，可以拖放笔记本和文件夹以更改其位置。 下一节将通过简单的工作示例来研究 Databricks 作业和库。

# 作业和库

在 Databricks 中，可以导入 JAR 库并在集群上运行其中的类。 我将创建一段非常简单的 Scala 代码，在我的 CentOS Linux 服务器上本地打印斐波纳契数列的前 100 个元素作为`BigInt`值。 我将使用 SBT 将我的类编译成一个 JAR 文件，在本地运行它来检查结果，然后在我的 Databricks 集群上运行它来比较结果。 代码如下所示：

```
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf

object db_ex1  extends App
{
  val appName = "Databricks example 1"
  val conf = new SparkConf()

  conf.setAppName(appName)

  val sparkCxt = new SparkContext(conf)

  var seed1:BigInt = 1
  var seed2:BigInt = 1
  val limit = 100
  var resultStr = seed1 + " " + seed2 + " "

  for( i <- 1 to limit ){

    val fib:BigInt = seed1 + seed2
    resultStr += fib.toString + " "

    seed1 = seed2
    seed2 = fib
  }

  println()
  println( "Result : " + resultStr )
  println()

  sparkCxt.stop()

} // end application
```

这不是最优雅的段代码，也不是创建斐波那契的最佳方式，但我只想要一个样例 JAR 和类来与 Databricks 一起使用。 在本地运行时，我会得到前 100 个术语，如下所示(为了节省空间，我对此数据进行了裁剪)：

```
Result : 1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987 1597 2584 4181 6765 10946 17711 28657 46368 75025 121393 196418 317811 514229 832040 1346269 2178309 3524578 5702887 9227465 14930352 24157817 39088169 63245986 102334155 165580141 267914296 433494437 701408733 1134903170 1836311903 2971215073 4807526976 7778742049 12586269025 20365011074 32951280099 53316291173

4660046610375530309 7540113804746346429 12200160415121876738 19740274219868223167 31940434634990099905 51680708854858323072 83621143489848422977 135301852344706746049 218922995834555169026 354224848179261915075 573147844013817084101 927372692193078999176

```

已创建的库称为`data-bricks_2.10-1.0.jar`。 从我的文件夹菜单中，我可以使用菜单下拉选项创建新的库。 这允许我将库源指定为 JAR 文件，命名新库，并从本地服务器加载库 JAR 文件。 以下屏幕截图显示了此过程的示例：

![Jobs and libraries](graphics/B01989_08_24.jpg)

创建库后，可以使用**ATTACH**选项将其附加到名为`semclust1`的集群 MyDatabricks 集群。 下面的屏幕截图显示了连接过程中的新库：

![Jobs and libraries](graphics/B01989_08_25.jpg)

在下面的示例中，通过选择**TASK**项上的**JAR**选项创建了名为**job2**的作业。 对于该作业，加载了相同的 JAR 文件，并指定类`db_ex1`在库中运行。 该群集已指定为按需，这意味着将自动创建一个群集来运行该作业。 **活动运行**部分在以下屏幕截图中显示正在运行的作业：

![Jobs and libraries](graphics/B01989_08_26.jpg)

运行后，作业将移至显示屏的**已完成运行**部分。 对于同一作业，下面的屏幕截图显示运行时间为`47`秒，它是手动启动的，并且成功了。

![Jobs and libraries](graphics/B01989_08_27.jpg)

通过选择前面屏幕截图中名为**Run 1**的 Run，可以看到 Run 输出。 下面的屏幕截图显示了与本地运行相同的结果，显示在我的本地服务器执行中。 我已经对输出文本进行了裁剪，以使其在此页面上具有可读性和可读性，但是您可以看到输出是相同的。

![Jobs and libraries](graphics/B01989_08_28.jpg)

因此，即使从这个非常简单的示例来看，显然也可以远程开发应用程序，并将它们作为 JAR 文件加载到 Databricks 集群中以便执行。 但是，每次在 AWS EC2 存储上创建 Databricks 群集时，Spark URL 都会更改，因此应用程序不能对 Spark 主 URL 等详细信息进行硬编码。 Databricks 将自动设置 Spark URL。

以这种方式运行 JAR 文件类时，还可以定义类参数。 作业可以安排在给定时间运行，也可以定期运行。 还可以指定作业超时和警报电子邮件地址。

# 开发环境

已经证明，脚本可以用 Scala、Python 或 SQL 在笔记本中创建，但也可以使用诸如 IntelliJ 或 Eclipse 之类的 IDE 来开发代码。 通过在此开发环境中安装 SBT 插件，可以为您的 Databricks 环境开发代码。 在我写这本书时，Databricks 的当前版本是 1.3.2d。 开始页上**新功能**下的**发行说明**链接包含指向 IDE 集成的链接，即`https://dbc-xxxxxxx-xxxx.cloud.databricks.com/#shell/1547`。

URL 将采用这种形式，以`dbc`开头的部分将更改为与您将创建的 Databricks 云的 URL 相匹配。 我不会在这里详述这一点，但让你来调查吧。 在下一节中，我将研究 Databricks 表数据处理功能。

# 数据库表

Databricks**Tables**菜单选项允许您将数据存储在具有关联模式的表格形式中。 **Tables**菜单选项允许您创建表格和刷新表格列表，如以下屏幕截图所示：

![Databricks tables](graphics/B01989_08_29.jpg)

## 数据导入

您可以通过数据导入创建表格，同时根据列名和类型指定表格结构。 如果要导入的数据具有标题，则可以从中获取列名，尽管所有列类型都假定为字符串。 下面的屏幕截图显示了数据导入选项和表单的串联视图，这些选项和表单在创建表时可用。 导入文件位置选项为**S3**、**dBFS**、**JDBC**和**文件**。

![Data import](graphics/B01989_08_30.jpg)

上一个屏幕截图显示选择了**S3**。 为了浏览我的**S3**存储桶以查找要导入到表中的文件，我需要输入**AWS 密钥 ID**、**秘密访问密钥**和**AWS S3 存储桶名称**。 然后，我可以浏览、选择文件，并通过预览创建表。 在下面的屏幕截图中，我选择了**文件**选项：

![Data import](graphics/B01989_08_31.jpg)

我可以将我的文件拖放到下面屏幕截图中的上载框中，或者单击该框以浏览本地服务器以选择要上载的文件。 一旦选择了文件，就可以定义数据列分隔符，以及数据是否包含标题行。 可以预览数据，并更改列名和数据类型。 还可以指定新的表名和文件类型。 以下屏幕截图显示了用于创建名为`shuttle`的表的示例文件数据加载：

![Data import](graphics/B01989_08_32.jpg)

创建后，可以刷新菜单表列表并查看表架构以确认列名和类型。 通过这种方式，还可以预览表数据的示例。 现在可以从 SQL 会话查看和访问表。 下面的屏幕截图显示了使用`show tables`命令可以看到**穿梭**表格：

![Data import](graphics/B01989_08_33.jpg)

导入后，还可以通过 SQL 会话访问该表中的数据。 下面的屏幕截图显示了一个简单的 SQL 会话语句，以显示从新的**SPORTH**表中提取的数据：

![Data import](graphics/B01989_08_34.jpg)

因此，这提供了从各种数据源导入多个表并创建复杂模式的方法，以便按列和行过滤和联接数据，就像您在传统的关系数据库中所做的那样。 它提供了一种熟悉的大数据处理方法。

本节已经描述了通过数据导入创建表的过程，但是如何通过编程创建表，或者将表创建为外部对象呢？ 以下各节将提供这种表管理方法的示例。

## 外部表格

Databricks 允许您根据外部资源(如 AWS S3 文件或本地文件系统文件)创建表。 在本节中，我将针对基于 S3 的存储桶、路径和一组文件创建一个外部表。 我还将检查 AWS 所需的权限和使用的访问策略。 以下屏幕截图显示正在创建名为**dbawss3test2**的 AWS S3 存储桶。 已授予每个人访问该列表的权限。 我不是建议您这样做，但要确保您的组可以访问您的存储桶。

![External tables](graphics/B01989_08_35.jpg)

此外，还增加了一项政策，以帮助访问。 在这种情况下，匿名用户被授予对存储桶和子内容的只读访问权限。 您可以创建更复杂的策略来限制对您的组和分类文件的访问。 以下屏幕截图显示了新策略：

![External tables](graphics/B01989_08_36.jpg)

有了访问策略和用正确的访问策略创建的存储桶，我现在就可以创建文件夹并上传文件，以便与 Databricks 外部表一起使用。 如下面的屏幕截图所示，我就是这样做的。 上传的文件有 10 列 CSV 文件格式：

![External tables](graphics/B01989_08_37.jpg)

现在已经设置了 AWS S3 资源，需要将它们装载到 Databricks，如下面的基于 Scala 的示例所示。 出于安全目的，我已经从脚本中删除了我的 AWS 和密钥。 您的挂载目录需要以`/mnt`和任何`/`字符开头，并且您的秘密密钥值需要替换为`%2F`。 `dbutils.fs`类用于创建挂载，代码在一秒钟内执行，如下所示：

![External tables](graphics/B01989_08_38.jpg)

现在，可以使用基于 Notebook 的 SQL 会话针对该挂载路径及其包含的文件创建外部表，如下面的屏幕截图所示。 已经针对挂载目录包含的文件创建了名为`s3test1`的表，并将分隔符指定为逗号，以便解析基于 CSV 的内容。

![External tables](graphics/B01989_08_39.jpg)

现在，**Tables**菜单选项显示存在**s3test1**表格，如下面的屏幕截图所示。 因此，应该可以对该表运行一些 SQL：

![External tables](graphics/B01989_08_40.jpg)

我在一个基于 SQL 的 Notebook 会话中运行了一条`SELECT`语句，使用`COUNT(*)`函数从外部表中获取行数，如下面的屏幕截图所示。 可以看到，该表包含**14500**行。

![External tables](graphics/B01989_08_41.jpg)

现在，我将向基于 S3 的文件夹添加另一个文件。 在本例中，它只是 CSV 格式的第一个文件的副本，因此外部表中的行数应该加倍。 以下屏幕截图显示了添加的文件：

![External tables](graphics/B01989_08_42.jpg)

对外部表运行相同的`SELECT`语句确实会提供双倍的行数**[29000**行。 以下屏幕截图显示了 SQL 语句和输出：

![External tables](graphics/B01989_08_43.jpg)

因此，很容易在 Databricks 中创建外部表，并针对动态更改的内容运行 SQL。 文件结构需要统一，如果使用 AWS，则必须定义 S3 存储桶访问。 下一节将研究 Databricks 提供的 DbUtils 包。

# DbUtils 包

前面的基于 Scala 的脚本使用 DbUtils 包，并在最后一节创建挂载，它只使用了该包的一小部分功能。 在本节中，我想介绍 DbUtils 包的更多特性，以及**Databricks 文件系统**(**dBFS**)。 DbUtils 包中的 Help选项可以在连接到 Databricks 集群的 Notebook 中调用，以了解更多关于其结构和功能的信息。 如以下屏幕截图所示，在 Scala 笔记本中执行`dbutils.fs.help()`可提供有关 fsutil、缓存和基于挂载的功能的帮助：

![The DbUtils package](graphics/B01989_08_44.jpg)

也可以获得有关个别功能的帮助，如前面屏幕截图中的文本所示。 下面屏幕截图中的示例解释了**cacheTable**函数，提供了描述性文本以及带有参数和返回类型的函数调用示例：

![The DbUtils package](graphics/B01989_08_45.jpg)

下一节将先简要介绍 dBFS，然后再介绍更多的`dbutils`功能。

## Databricks 文件系统

可以使用`dbfs:/*`表单的 URL 和`dbutils.fs`中提供的函数访问 dBFS。

![Databricks file system](graphics/B01989_08_46.jpg)

前面的屏幕截图显示了使用`ls`函数检查的`/mnt`文件系统，然后显示了挂载目录-`s3data`和`s3data1`。 这些目录是在前面的 Scala S3 挂载示例中创建的。

## Dbutils fsutils

`dbutils`包中的`fsutils`函数组涵盖了`cp`、`head`、`mkdirs`、`mv`、`put`和`rm`等函数。 前面显示的帮助呼叫可以提供有关它们的更多信息。 您可以使用`mkdirs`调用在 dBFS 上创建一个目录，如下所示。 请注意，我在`dbfs:/`下创建了许多目录，在本会话中命名为`data*`。 下面的示例创建了名为`data2`的目录：

![Dbutils fsutils](graphics/B01989_08_47.jpg)

前面的屏幕截图通过执行`ls`显示了 dBFS 上已经存在许多默认目录。 例如，请参阅以下内容：

*   `/tmp`是临时区域
*   `/mnt`是远程目录的装载点，即 s3
*   `/user`是当前包含配置单元的用户存储区域
*   `/mount`为空目录
*   `/FileStore`是桌子、JAR 和 JOB JAR 的存储区
*   `/databricks-datasets`数据集是否由 Databricks 提供

下面显示的`dbutils`copy 命令允许将文件复制到 dBFS 位置。 在本例中，`external1.txt`文件已复制到`/data2`目录，如以下截图所示：

![Dbutils fsutils](graphics/B01989_08_48.jpg)

函数`head`可用于返回 dBFS 上文件头的第一个 maxBytes 字符。 以下示例显示了`external1.txt`文件的格式。 这很有用，因为它告诉我这是一个 CSV 文件，并向我展示了如何处理它。

![Dbutils fsutils](graphics/B01989_08_49.jpg)

还可以在 DBFS 内移动文件。 下面的屏幕截图显示了使用`mv`命令将`external1.txt`文件从目录`data2`移动到名为`data1`的目录。 然后使用`ls`命令确认移动。

![Dbutils fsutils](graphics/B01989_08_50.jpg)

最后，使用 Remove 函数(`rm`)删除刚刚移动的名为`external1.txt`的文件。 `ls`函数调用后的显示该文件不再存在于`data1`目录中，因为函数输出中没有`FileInfo`记录：

![Dbutils fsutils](graphics/B01989_08_51.jpg)

## DbUtils 缓存

DbUtils 中的缓存功能提供了将表和文件缓存(和取消缓存)到 DBFS 的方法。 实际上，这些表也作为文件保存到名为`/FileStore`的 dBFS 目录中。 以下屏幕截图显示缓存功能可用：

![The DbUtils cache](graphics/B01989_08_52.jpg)

## DbUtils 装载

挂载功能允许您挂载远程文件系统、刷新挂载、显示挂载详细信息以及卸载特定挂载的目录。 前面几节中已经给出了一个 S3 挂载的示例，所以我在这里不再重复。 下面的屏幕截图显示了`mounts`函数的输出。 `s3data`和`s3data1`挂载是由我创建的。 根和数据集的另外两个装载已经存在。 挂载按`MountInfo`对象的顺序列出。 我重新编排了文本，使其更有意义，并更好地呈现在页面上。

![The DbUtils mount](graphics/B01989_08_53.jpg)

# 摘要

本章介绍了 Databricks。 它显示了如何访问该服务，还显示了它如何使用 AWS 资源。 请记住，在未来，发明 Databricks 的人计划支持其他基于云的平台，比如 Microsoft Azure。 我认为引入 Databricks 很重要，因为参与 Apache Spark 开发的人也参与了这个系统。 自然的进程似乎是 Hadoop、Spark，然后是 Databricks。

我将在下一章继续研究 Databricks，因为还没有研究可视化等重要特性。 此外，主要的 Spark 功能模块(称为 GraphX、Streaming、MLlib 和 SQL)还没有在 Databricks 术语中引入。 在 Databricks 中使用这些模块处理实际数据有多容易？ 请继续往下读，找出答案。