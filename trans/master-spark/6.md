# ρ6.11-13 章基于图形的存储

使用 Apache Spark(特别是 GraphX)进行处理提供了在基于内存集群的实时图形处理中使用的能力。 但是，Apache Spark 不提供存储；基于图形的数据必须来自某个地方，在处理之后，可能需要存储。 在本章中，我将以 Titan 图形数据库为例来研究基于图形的存储。 本章将介绍以下主题：

*   泰坦一览
*   TinkerPop 概述
*   安装泰坦
*   将 Titan 与 HBase 配合使用
*   将泰坦与卡桑德拉结合使用
*   将泰坦与火花结合使用

这一处理领域的年轻意味着 Apache Spark 和基于图形的存储系统 Titan 之间的存储集成还不成熟。

在上一章中，我们研究了 Neo4j Mazerunner 架构，其中展示了如何将基于 Spark 的事务复制到 Neo4j。 本章讨论 Titan 并不是因为它现在所展示的功能，而是因为当它与 Apache Spark 一起使用时，它为基于图形的存储领域提供了未来的前景。

# ___ 泰坦

TITAN 是由 Aurelius([http://thinkaurelius.com/](http://thinkaurelius.com/))开发的图形数据库。 应用程序源代码和二进制文件可以从 giHub([http://thinkaurelius.github.io/titan/](http://thinkaurelius.github.io/titan/))下载，该位置还包含 TITAN 文档。 Titan 已经在 Apache2 许可下作为开源应用程序发布。 在写这本书的时候，Aurelius 已经被 DataStax 收购了，尽管 Titan 应该会继续发布。

Titan 提供了许多存储选项，但我只会集中讨论两个，HBase-Hadoop NoSQL 数据库，以及 Cassandra-非 Hadoop NoSQL 数据库。 使用这些底层存储机制，Titan 能够在大数据范围内提供基于图形的存储。

基于 TinkerPop3 的 Titan 版本 0.9.0-M2 于 2015 年 6 月发布，它将实现与 Apache Spark 的更好集成(TinkerPop 将在下一节解释)。 我将在本章中使用此版本。 现在，Titan 数据库使用 TinkerPop 进行图形操作。 这个 Titan 版本是试验性的开发版本，但希望将来的版本能够巩固 Titan 的功能。

本章重点介绍 Titan 数据库，而不是替代的图形数据库，如 Neo4j，因为 Titan 可以使用基于 Hadoop 的存储。 此外，Titan 还承诺未来将与 Apache Spark 集成，以实现基于内存的图形处理中的大数据规模。 下图显示了本章中讨论的体系结构。 虚线表示对 Spark 数据库的直接访问，而实线表示通过 Titan 类对数据的 Spark 访问。

![Titan](graphics/B01989_06_01.jpg)

Spark接口还没有正式存在(它只在 M2 开发版本中可用)，但它只是添加以供参考。 尽管 Titan 提供了使用 Oracle 进行存储的选项，但本章将不对此进行介绍。 我将首先研究从 Titan 到 HBase 和 Cassandra 架构，稍后再考虑 Apache Spark 集成。 在考虑(分布式)HBase 时，集成时也需要 ZooKeeper。 由于我使用的是现有的 CDH5 集群，所以已经安装了 HBase 和 ZooKeeper。

# TinkerPop

截至 2015 年 7 月，TinkerPop 目前的版本是 3，它是一个 Apache 孵化器项目，可以在[http://tinkerpop.incubator.apache.org/](http://tinkerpop.incubator.apache.org/)上找到。 它使图形数据库(如 Titan)和图形分析系统(如 Giraph)都可以将其用作图形处理的子系统，而不是创建自己的图形处理模块。

![TinkerPop](graphics/B01989_06_02.jpg)

上图(借用自 TinkerPop 网站)展示了 TinkerPop 架构。 蓝色层显示 Core TinkerPop API，它为图形、顶点和边处理提供图形处理 API。 **Vendor API**框显示供应商将实现以集成其系统的 API。 该图显示有两种可能的 API：一种用于**OLTP**数据库系统，另一种用于**OLAP**分析系统。

该图还显示，**Gremlin**语言用于为 TinkerPop 创建和管理图形，Titan 也是如此。 最后，Gremlin 服务器位于体系结构的顶端，允许集成到 Ganglia 等监控系统。

# 安装 Titan

由于本章需要 Titan，我现在将安装它，并展示如何获取、安装和配置它。 我已经在[s3.thinkaurelius.com/downloads/titan/titan-0.9.0-M2-hadoop1.zip](http://s3.thinkaurelius.com/downloads/titan/titan-0.9.0-M2-hadoop1.zip)下载了 TITAN 的最新预构建版本(0.9.0-M2)。

我已经将压缩版本下载到一个临时目录，如下所示。 执行以下步骤以确保群集中的每个节点上都安装了 Titan：

```
[[hadoop@hc2nn tmp]$ ls -lh titan-0.9.0-M2-hadoop1.zip
-rw-r--r-- 1 hadoop hadoop 153M Jul 22 15:13 titan-0.9.0-M2-hadoop1.zip

```

使用 Linux unzip 命令解压压缩的 Titan 版本文件：

```
[hadoop@hc2nn tmp]$ unzip titan-0.9.0-M2-hadoop1.zip

[hadoop@hc2nn tmp]$ ls -l
total 155752
drwxr-xr-x 10 hadoop hadoop      4096 Jun  9 00:56 titan-0.9.0-M2-hadoop1
-rw-r--r--  1 hadoop hadoop 159482381 Jul 22 15:13 titan-0.9.0-M2-hadoop1.zip

```

现在，使用 Linux`su`(切换用户)命令切换到`root`帐户，并将安装移到`/usr/local/`位置。 将安装的文件和组成员身份更改为`hadoop`用户，并创建名为`titan`的符号链接，以便当前的 Titan 版本可以称为名为`/usr/local/titan`的简化路径：

```
[hadoop@hc2nn ~]$ su –
[root@hc2nn ~]# cd /home/hadoop/tmp
[root@hc2nn titan]# mv titan-0.9.0-M2-hadoop1 /usr/local
[root@hc2nn titan]# cd /usr/local
[root@hc2nn local]# chown -R hadoop:hadoop titan-0.9.0-M2-hadoop1
[root@hc2nn local]# ln -s titan-0.9.0-M2-hadoop1 titan
[root@hc2nn local]# ls -ld *titan*
lrwxrwxrwx  1 root   root     19 Mar 13 14:10 titan -> titan-0.9.0-M2-hadoop1
drwxr-xr-x 10 hadoop hadoop 4096 Feb 14 13:30 titan-0.9.0-M2-hadoop1

```

使用稍后将演示的 Titan Gremlin 外壳，Titan 现在可以使用了。 此版本的 Titan需要 Java 8；请确保您已经安装了它。

# 带 HBase 的 Titan

如前面的图所示，HBase 依赖于动物园管理员。 假设我的 CDH5 集群(在`hc2r1m2`、`hc2r1m3`和`hc2r1m4`节点上运行)上有一个有效的 ZooKeeper 仲裁，那么我只需要确保 HBase 已经安装并在我的 Hadoop 集群上运行。

## HBase 集群

我将使用 Cloudera CDH 集群管理器安装HBase 的分布式版本。 使用管理器控制台，安装 HBase 是一项简单的任务。 唯一需要做的决定是将 HBase 服务器放在集群的什么位置。 下图显示了 CDH HBase 安装中的**按主机查看**表单。 HBase 组件在右侧显示为**添加的角色**。

我选择将 HBase 区域服务器(RS)添加到`hc2r1m2`、`hc2r1m3`和`hc2r1m4`节点。 我已经在`hc2r1m1`主机上安装了 HBase master(M)、HBase rest server(HBREST)和 HBase Thrift server(HBTS)。

![The HBase cluster](graphics/B01989_06_03.jpg)

我过去手动安装和配置了许多基于 Hadoop 的组件，我发现这种简单的基于管理器的组件安装和配置既快速又可靠。 它节省了我的时间，这样我就可以专注于其他系统，比如泰坦(Titan)。

安装 HBase 并从 CDH 管理器控制台启动后，需要对其进行检查以确保其正常工作。 我将使用如下所示的 HBase shell 命令完成此操作：

```
[hadoop@hc2r1m2 ~]$ hbase shell
Version 0.98.6-cdh5.3.2, rUnknown, Tue Feb 24 12:56:59 PST 2015
hbase(main):001:0>

```

正如您从前面的命令中看到的，我以 Linux 用户`hadoop`身份运行 HBase shell。 已安装 HBase 版本 0.98.6；稍后当我们开始使用 Titan 时，此版本号将变得非常重要：

```
hbase(main):001:0> create 'table2', 'cf1'
hbase(main):002:0> put 'table2', 'row1', 'cf1:1', 'value1'
hbase(main):003:0> put 'table2', 'row2', 'cf1:1', 'value2'

```

我已经创建了一个名为`table2`的简单表，列族为`cf1`。 然后，我用两个不同的值添加了两行。 此表已从`hc2r1m2`节点创建，现在将从 HBase 群集中名为`hc2r1m4`的备用节点进行检查：

```
[hadoop@hc2r1m4 ~]$ hbase shell

hbase(main):001:0> scan 'table2'

ROW                     COLUMN+CELL
 row1                   column=cf1:1, timestamp=1437968514021, value=value1
 row2                   column=cf1:1, timestamp=1437968520664, value=value2
2 row(s) in 0.3870 seconds

```

如您所见，在`table2`中可以看到来自不同主机的两个数据行，因此 HBase 已安装并正常工作。 现在是尝试使用 HBase 和 Titan Gremlin shell 在 Titan 中创建图表的时候了。

## Gremlin HBase 脚本

我已经检查了我的 Java版本，以确保我使用的是版本 8，否则 Titan 0.9.0-M2 将无法工作：

```
[hadoop@hc2r1m2 ~]$ java -version
openjdk version "1.8.0_51"

```

如果您没有正确设置 Java 版本，您将收到这样的错误，这些错误似乎没有任何意义，除非您在 Google 上搜索它们：

```
Exception in thread "main" java.lang.UnsupportedClassVersionError: org/apache/tinkerpop/gremlin/groovy/plugin/RemoteAcceptor :
Unsupported major.minor version 52.0

```

交互式 Titan Gremlin shell 可以在 Titan 安装的 bin 目录中找到，如下所示。 启动后，它会提供 Gremlin 提示：

```
[hadoop@hc2r1m2 bin]$ pwd
/usr/local/titan/

[hadoop@hc2r1m2 titan]$ bin/gremlin.sh
gremlin>

```

下面的脚本将使用 Gremlin shell 输入。 脚本的第一部分根据存储(HBase)、使用的 ZooKeeper 服务器、ZooKeeper 端口号和要使用的 HBase 表名定义配置：

```
hBaseConf = new BaseConfiguration();
hBaseConf.setProperty("storage.backend","hbase");
hBaseConf.setProperty("storage.hostname","hc2r1m2,hc2r1m3,hc2r1m4");
hBaseConf.setProperty("storage.hbase.ext.hbase.zookeeper.property.clientPort","2181")
hBaseConf.setProperty("storage.hbase.table","titan")

titanGraph = TitanFactory.open(hBaseConf);

```

下一节将为使用管理系统创建的图形定义个泛型顶点属性的名称和年龄。 然后提交管理系统更改：

```
manageSys = titanGraph.openManagement();
nameProp = manageSys.makePropertyKey('name').dataType(String.class).make();
ageProp  = manageSys.makePropertyKey('age').dataType(String.class).make();
manageSys.buildIndex('nameIdx',Vertex.class).addKey(nameProp).buildCompositeIndex();
manageSys.buildIndex('ageIdx',Vertex.class).addKey(ageProp).buildCompositeIndex();

manageSys.commit();

```

现在，图形中添加了六个顶点。 每一个都有一个数字标签来表示它的身份。 为每个顶点指定年龄和名称值：

```
v1=titanGraph.addVertex(label, '1');
v1.property('name', 'Mike');
v1.property('age', '48');

v2=titanGraph.addVertex(label, '2');
v2.property('name', 'Sarah');
v2.property('age', '45');

v3=titanGraph.addVertex(label, '3');
v3.property('name', 'John');
v3.property('age', '25');

v4=titanGraph.addVertex(label, '4');
v4.property('name', 'Jim');
v4.property('age', '53');

v5=titanGraph.addVertex(label, '5');
v5.property('name', 'Kate');
v5.property('age', '22');

v6=titanGraph.addVertex(label, '6');
v6.property('name', 'Flo');
v6.property('age', '52');

```

最后，将图的边相加以将顶点连接在一起。 每条边都有一个关系值。 创建后，将提交更改以将其存储到 Titan，从而存储到 HBase：

```
v6.addEdge("Sister", v1)
v1.addEdge("Husband", v2)
v2.addEdge("Wife", v1)
v5.addEdge("Daughter", v1)
v5.addEdge("Daughter", v2)
v3.addEdge("Son", v1)
v3.addEdge("Son", v2)
v4.addEdge("Friend", v1)
v1.addEdge("Father", v5)
v1.addEdge("Father", v3)
v2.addEdge("Mother", v5)
v2.addEdge("Mother", v3)

titanGraph.tx().commit();

```

这将生成一个简单的基于人的图表，如下图所示，该图表也在上一章中使用过：

![The Gremlin HBase script](graphics/B01989_06_04.jpg)

然后可以使用与前一个类似的脚本通过 Gremlin shell 在 Titan 中测试此图。 如前面所示，只需在`gremlin>`提示符下输入下面的脚本即可。 它使用相同的初始六行来创建`titanGraph`配置，但随后会创建一个图形遍历变量`g`：

```
hBaseConf = new BaseConfiguration();
hBaseConf.setProperty("storage.backend","hbase");
hBaseConf.setProperty("storage.hostname","hc2r1m2,hc2r1m3,hc2r1m4");
hBaseConf.setProperty("storage.hbase.ext.hbase.zookeeper.property.clientPort","2181")
hBaseConf.setProperty("storage.hbase.table","titan")

titanGraph = TitanFactory.open(hBaseConf);

gremlin> g = titanGraph.traversal()

```

现在，可以使用图形遍历变量来检查图形内容。 使用`ValueMap`选项，可以搜索称为`Mike`和`Flo`的图形节点。 它们已在以下位置成功找到：

```
gremlin> g.V().has('name','Mike').valueMap();
==>[name:[Mike], age:[48]]

gremlin> g.V().has('name','Flo').valueMap();
==>[name:[Flo], age:[52]]

```

因此，已经使用 Gremlin shell 在 Titan 中创建并检查了该图，但是我们还可以使用 HBase shell 检查 HBase 中的存储，并检查 Titan 表的内容。 下面的扫描显示该表存在，并包含此小图形的`72`行数据：

```
[hadoop@hc2r1m2 ~]$ hbase shell
hbase(main):002:0> scan 'titan'
72 row(s) in 0.8310 seconds

```

现在已经创建了图形，并且我确信它已经存储在 HBase 中，我将尝试使用 Apache Spark 访问数据。 如上一章所示，我已经在所有节点上启动了 Apache Spark。 这将是 Apache Spark 1.3 对 HBase 存储的直接访问。 在这个阶段，我不会尝试使用 Titan 来解释 HBase 存储的图表。

## HBase 上的火花

为了从 Spark 访问HBase，我将使用 Cloudera 的`SparkOnHBase`模块进行，该模块可以从[https://github.com/cloudera-labs/SparkOnHBase](https://github.com/cloudera-labs/SparkOnHBase)下载。

下载的文件是压缩格式，需要解压缩。 我已经使用临时目录中的 Linux unzip 命令完成了此操作：

```
[hadoop@hc2r1m2 tmp]$ ls -l SparkOnHBase-cdh5-0.0.2.zip
-rw-r--r-- 1 hadoop hadoop 370439 Jul 27 13:39 SparkOnHBase-cdh5-0.0.2.zip

[hadoop@hc2r1m2 tmp]$ unzip SparkOnHBase-cdh5-0.0.2.zip

[hadoop@hc2r1m2 tmp]$ ls
SparkOnHBase-cdh5-0.0.2  SparkOnHBase-cdh5-0.0.2.zip

```

然后，我进入解包的模块，并使用 Maven 命令`mvn`构建 JAR 文件：

```
[hadoop@hc2r1m2 tmp]$ cd SparkOnHBase-cdh5-0.0.2
[hadoop@hc2r1m2 SparkOnHBase-cdh5-0.0.2]$ mvn clean package

[INFO] -----------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] -----------------------------------------------------------
[INFO] Total time: 13:17 min
[INFO] Finished at: 2015-07-27T14:05:55+12:00
[INFO] Final Memory: 50M/191M
[INFO] -----------------------------------------------------------

```

最后，我将构建的组件移到我的开发区以保持整洁，这样我就可以在我的 Spark HBase 代码中使用这个模块：

```
[hadoop@hc2r1m2 SparkOnHBase-cdh5-0.0.2]$ cd ..
[hadoop@hc2r1m2 tmp]$ mv SparkOnHBase-cdh5-0.0.2 /home/hadoop/spark

```

## 使用 Spark 访问 HBase

与前几章一样，我将使用 SBT 和 Scala 将我的基于 Spark 的脚本编译成应用程序。 然后，我将使用 Spark-Submit 在 Spark 集群上运行这些应用程序。 我的 SBT 配置文件如下所示。 它包含 Hadoop、Spark 和 HBase 库：

```
[hadoop@hc2r1m2 titan_hbase]$ pwd
/home/hadoop/spark/titan_hbase

[hadoop@hc2r1m2 titan_hbase]$ more titan.sbt
name := "T i t a n"
version := "1.0"
scalaVersion := "2.10.4"

libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "2.3.0"
libraryDependencies += "org.apache.spark" %% "spark-core"  % "1.3.1"
libraryDependencies += "com.cloudera.spark" % "hbase"   % "5-0.0.2" from "file:///home/hadoop/spark/SparkOnHBase-cdh5-0.0.2/target/SparkHBase.jar"
libraryDependencies += "org.apache.hadoop.hbase" % "client"   % "5-0.0.2" from "file:///home/hadoop/spark/SparkOnHBase-cdh5-0.0.2/target/SparkHBase.jar"
resolvers += "Cloudera Repository" at "https://repository.cloudera.com/artifactory/clouder
a-repos/"

```

注意，我使用目录`/home/hadoop/spark/titan_hbase`下的Linux`hadoop`帐户在`hc2r1m2`服务器上运行这个应用程序。 我已经创建了一个名为`run_titan.bash.hbase`的 Bash shell 脚本，它允许我运行在`src/main/scala`子目录下创建和编译的任何应用程序：

```
[hadoop@hc2r1m2 titan_hbase]$ pwd ; more run_titan.bash.hbase
/home/hadoop/spark/titan_hbase

#!/bin/bash

SPARK_HOME=/usr/local/spark
SPARK_BIN=$SPARK_HOME/bin
SPARK_SBIN=$SPARK_HOME/sbin

JAR_PATH=/home/hadoop/spark/titan_hbase/target/scala-2.10/t-i-t-a-n_2.10-1.0.jar
CLASS_VAL=$1

CDH_JAR_HOME=/opt/cloudera/parcels/CDH/lib/hbase/
CONN_HOME=/home/hadoop/spark/SparkOnHBase-cdh5-0.0.2/target/

HBASE_JAR1=$CDH_JAR_HOME/hbase-common-0.98.6-cdh5.3.3.jar
HBASE_JAR2=$CONN_HOME/SparkHBase.jar

cd $SPARK_BIN

./spark-submit \
 --jars $HBASE_JAR1 \
 --jars $HBASE_JAR2 \
 --class $CLASS_VAL \
 --master spark://hc2nn.semtech-solutions.co.nz:7077  \
 --executor-memory 100M \
 --total-executor-cores 50 \
 $JAR_PATH

```

Bash 脚本保存在相同的`titan_hbase`目录中，并接受应用程序类名的单个参数。 `spark-submit`调用的参数与前面的示例相同。 在本例中，`src/main/scala`下只有一个脚本，称为`spark3_hbase2.scala`：

```
[hadoop@hc2r1m2 scala]$ pwd
/home/hadoop/spark/titan_hbase/src/main/scala

[hadoop@hc2r1m2 scala]$ ls
spark3_hbase2.scala

```

Scala 脚本首先定义应用程序类所属的包名。 然后，它导入Spark、Hadoop 和 HBase 类：

```
package nz.co.semtechsolutions

import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf

import org.apache.hadoop.hbase._
import org.apache.hadoop.fs.Path
import com.cloudera.spark.hbase.HBaseContext
import org.apache.hadoop.hbase.client.Scan
```

定义了应用程序类名和 Main 方法。 然后根据应用程序名称和 Spark URL 创建一个 Configuration 对象。 最后，根据配置创建 Spark 上下文：

```
object spark3_hbase2
{

  def main(args: Array[String]) {

    val sparkMaster = "spark://hc2nn.semtech-solutions.co.nz:7077"
    val appName = "Spark HBase 2"
    val conf = new SparkConf()

    conf.setMaster(sparkMaster)
    conf.setAppName(appName)

    val sparkCxt = new SparkContext(conf)
```

接下来，创建一个 HBase 配置对象，并添加一个基于 Cloudera CDH`hbase-site.xml`文件的资源：

```
    val jobConf = HBaseConfiguration.create()

    val hbasePath="/opt/cloudera/parcels/CDH/etc/hbase/conf.dist/"

    jobConf.addResource(new Path(hbasePath+"hbase-site.xml"))
```

使用 Spark 上下文和 HBase 配置对象创建 HBase 上下文对象。 还定义了扫描和缓存配置：

```
    val hbaseContext = new HBaseContext(sparkCxt, jobConf)

    var scan = new Scan()
    scan.setCaching(100)
```

最后，使用`hbaseRDD`HBase 上下文方法和扫描对象检索HBase`Titan`表中的数据。 打印 RDD 计数，然后脚本关闭：

```
    var hbaseRdd = hbaseContext.hbaseRDD("titan", scan)

    println( "Rows in Titan hbase table : " + hbaseRdd.count() )

    println( " >>>>> Script Finished <<<<< " )

  } // end main

} // end spark3_hbase2
```

我只打印检索到的数据的计数，因为 Titan 以 GZ 格式压缩数据。 因此，试图直接操纵它没有什么意义。

使用`run_titan.bash.hbase`脚本，运行名为`spark3_hbase2`的 Spark 应用程序。 它输出的 RDD 行数为`72`，与之前找到的 Titan 表行数匹配。 这证明 Apache Spark 已经能够访问原始的 Titan HBase 存储的图形数据，但是 Spark 还没有使用 Titan 库将 Titan 数据作为图形访问。 这将在后面讨论。 下面是代码：

```
[hadoop@hc2r1m2 titan_hbase]$ ./run_titan.bash.hbase nz.co.semtechsolutions.spark3_hbase2

Rows in Titan hbase table : 72
 >>>>> Script Finished <<<<<

```

# 卡桑德拉的泰坦

在本节中，将使用 Cassandra NoSQL 数据库作为 Titan 的存储机制。 虽然它没有使用 Hadoop，但它本身就是一个大规模的基于集群的数据库，并且可以扩展到非常大的集群大小。 本节将遵循与相同的流程。 至于 HBase，将创建一个图表，并使用 Titan Gremlin shell 将其存储在 Cassandra 中。 然后使用 Gremlin 检查，并在 Cassandra 中检查存储的数据。 然后将从 Spark 访问原始的泰坦·卡桑德拉图形数据。 然后，第一步是在集群中的每个节点上安装 Cassandra。

## 安装 Cassandra

创建一个 repo 文件，该文件允许使用 linux`yum`命令安装社区版本的 DataSTax Cassandra。 这需要根用户访问权限，因此使用`su`命令将用户切换到根用户。 在所有节点上安装 Cassandra：

```
[hadoop@hc2nn lib]$ su -
[root@hc2nn ~]# vi /etc/yum.repos.d/datastax.repo

[datastax]
name= DataStax Repo for Apache Cassandra
baseurl=http://rpm.datastax.com/community
enabled=1
gpgcheck=0

```

现在，使用 Linux`yum`命令在集群中的每个节点上安装 Cassandra：

```
[root@hc2nn ~]# yum -y install dsc20-2.0.13-1 cassandra20-2.0.13-1

```

通过更改`cassandra.yaml`文件在`/etc/cassandra/conf`下设置 Cassandra 配置：

```
[root@hc2nn ~]# cd /etc/cassandra/conf   ; vi cassandra.yaml

```

我进行了以下更改，以指定我的群集名、服务器种子 IP 地址、RPC 地址和 snitch 值。 种子节点是其他节点将尝试首先连接到的节点。 在本例中，NameNode(`103`)和 node2(`108`)被用作`seeds`。 告密方法管理网络拓扑和路由：

```
cluster_name: 'Cluster1'
seeds: "192.168.1.103,192.168.1.108"
listen_address:
rpc_address: 0.0.0.0
endpoint_snitch: GossipingPropertyFileSnitch

```

现在可以使用 service 命令在每个节点上以 root 用户身份启动 Cassandra：

```
[root@hc2nn ~]# service cassandra start

```

日志文件可以在`/var/log/cassandra`下找到，数据存储在`/var/lib/cassandra`下。 可以在任何 Cassandra 节点上使用`nodetool`命令检查 Cassandra 群集的状态：

```
[root@hc2nn cassandra]# nodetool status
Datacenter: DC1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address        Load       Tokens  Owns (effective)  Host ID Rack
UN  192.168.1.105  63.96 KB   256     37.2%             f230c5d7-ff6f-43e7-821d-c7ae2b5141d3  RAC1
UN  192.168.1.110  45.86 KB   256     39.9%             fc1d80fe-6c2d-467d-9034-96a1f203c20d  RAC1
UN  192.168.1.109  45.9 KB    256     40.9%             daadf2ee-f8c2-4177-ae72-683e39fd1ea0  RAC1
UN  192.168.1.108  50.44 KB   256     40.5%             b9d796c0-5893-46bc-8e3c-187a524b1f5a  RAC1
UN  192.168.1.103  70.68 KB   256     41.5%             53c2eebd-
a66c-4a65-b026-96e232846243  RAC1

```

名为`cqlsh`的 Cassandra CQL shell 命令可用于访问集群和创建对象。 接下来调用 shell，它显示安装了 Cassandra 版本 2.0.13：

```
[hadoop@hc2nn ~]$ cqlsh
Connected to Cluster1 at localhost:9160.
[cqlsh 4.1.1 | Cassandra 2.0.13 | CQL spec 3.1.1 | Thrift protocol 19.39.0]
Use HELP for help.
cqlsh>

```

接下来，Cassandra 查询语言显示一个名为`keyspace1`的密钥空间，该密钥空间正通过 CQL shell 创建和使用：

```
cqlsh> CREATE KEYSPACE keyspace1 WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };

cqlsh> USE keyspace1;

cqlsh:keyspace1> SELECT * FROM system.schema_keyspaces;

 keyspace_name | durable_writes | strategy_class                              | strategy_options
--------------+------+---------------------------------------------+----------------------------
 keyspace1  | True | org.apache.cassandra.locator.SimpleStrategy | {"replication_factor":"1"}
 system  | True |  org.apache.cassandra.locator.LocalStrategy |                         {}
system_traces | True | org.apache.cassandra.locator.SimpleStrategy | {"replication_factor":"2"}

```

由于 Cassandra 已经安装并正常工作，现在可以使用 Cassandra 创建一个 Titan 图进行存储。 这将在下一节中使用泰坦 Gremlin 外壳来解决。 它将遵循与之前 HBase 部分相同的格式。

## Gremlin Cassandra 脚本

与前面的gremlin 脚本一样，这个 Cassandra 版本创建了相同的简单图形。 此脚本的不同之处在于配置。 后端存储类型定义为 Cassandra，主机名定义为 Cassandra 种子节点。 指定密钥空间和端口号，最后创建图形：

```
cassConf = new BaseConfiguration();
cassConf.setProperty("storage.backend","cassandra");
cassConf.setProperty("storage.hostname","hc2nn,hc2r1m2");
cassConf.setProperty("storage.port","9160")
cassConf.setProperty("storage.keyspace","titan")
titanGraph = TitanFactory.open(cassConf);
```

从这里开始，该脚本与前面的 HBase 示例相同，因此我不再重复。 此脚本将在下载包中以`cassandra_create.bash`的形式提供。 使用前面的配置，可以在 Gremlin shell 中执行相同的检查来检查数据。 这将返回与前面的检查相同的结果，从而证明该图已存储：

```
gremlin> g = titanGraph.traversal()

gremlin> g.V().has('name','Mike').valueMap();
==>[name:[Mike], age:[48]]

gremlin> g.V().has('name','Flo').valueMap();
==>[name:[Flo], age:[52]]

```

使用 Cassandra CQL shell 和 Titan`keyspace`，可以看到在 Cassandra 中创建了许多 Titan 表：

```
[hadoop@hc2nn ~]$ cqlsh
cqlsh> use titan;
cqlsh:titan> describe tables;
edgestore        graphindex        system_properties systemlog  txlog
edgestore_lock_  graphindex_lock_  system_properties_lock_  titan_ids

```

还可以看到，数据存在于 Cassandra 的`edgestore`表中：

```
cqlsh:titan> select * from edgestore;
 key                | column1            | value
--------------------+--------------------+------------------------------------------------
 0x0000000000004815 |               0x02 |                                     0x00011ee0
 0x0000000000004815 |             0x10c0 |                           0xa0727425536fee1ec0
.......
 0x0000000000001005 |             0x10c8 |                       0x00800512644c1b149004a0
 0x0000000000001005 | 0x30c9801009800c20 |   0x000101143c01023b0101696e6465782d706ff30200

```

这让我确信已经在 Gremlin shell 中创建了一个 Titan图，并将其存储在 Cassandra 中。 现在，我将尝试访问 Spark 的数据。

## 火花卡桑德拉连接器

为了从 Spark 访问Cassandra，我将下载 DataSTax Spark Cassandra 连接器和驱动程序库。 与此相匹配的信息和版本可在[http://mvnrepository.com/artifact/com.datastax.spark/](http://mvnrepository.com/artifact/com.datastax.spark/)中找到。

此 URL 的版本兼容性部分显示了应该与每个 Cassandra 和 Spark 版本一起使用的 Cassandra 连接器版本。 版本表显示连接器版本应与正在使用的 Spark 版本匹配。 下一个 url 允许这些库在[http://mvnrepository.com/artifact/com.datastax.spark/spark-cassandra-connector_2.10](http://mvnrepository.com/artifact/com.datastax.spark/spark-cassandra-connector_2.10)处获得源代码。

通过遵循前面的 URL 并选择一个库版本，您将看到一个与库相关联的编译依赖关系表，该表指示您将需要的所有其他依赖库及其版本。 以下库是与 Spark 1.3.1 配合使用所需的库。 如果您使用前面的 URL，您将看到哪个版本的 Cassandra 连接器库与每个版本的 Spark 一起使用。 您还将看到 Cassandra 连接器所依赖的库。 请注意只选择(以及所有)所需的库版本：

```
[hadoop@hc2r1m2 titan_cass]$ pwd ; ls *.jar
/home/hadoop/spark/titan_cass

spark-cassandra-connector_2.10-1.3.0-M1.jar
cassandra-driver-core-2.1.5.jar
cassandra-thrift-2.1.3.jar
libthrift-0.9.2.jar
cassandra-clientutil-2.1.3.jar
guava-14.0.1.jar
joda-time-2.3.jar
joda-convert-1.2.jar

```

## 用 Spark 访问卡桑德拉

现在我已经准备好了 Cassandra 连接器库及其所有依赖项，我可以开始考虑连接到 Cassandra 所需的 Scala 代码。 考虑到我使用 SBT 作为开发工具，首先要做的是设置 SBT 构建配置文件。 我的是这样的：

```
[hadoop@hc2r1m2 titan_cass]$ pwd ; more titan.sbt
/home/hadoop/spark/titan_cass

name := "Spark Cass"
version := "1.0"
scalaVersion := "2.10.4"
libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "2.3.0"
libraryDependencies += "org.apache.spark" %% "spark-core"  % "1.3.1"
libraryDependencies += "com.datastax.spark" % "spark-cassandra-connector"  % "1.3.0-M1" fr
om "file:///home/hadoop/spark/titan_cass/spark-cassandra-connector_2.10-1.3.0-M1.jar"
libraryDependencies += "com.datastax.cassandra" % "cassandra-driver-core"  % "2.1.5" from
"file:///home/hadoop/spark/titan_cass/cassandra-driver-core-2.1.5.jar"
libraryDependencies += "org.joda"  % "time" % "2.3" from "file:///home/hadoop/spark/titan_
cass/joda-time-2.3.jar"
libraryDependencies += "org.apache.cassandra" % "thrift" % "2.1.3" from "file:///home/hado
op/spark/titan_cass/cassandra-thrift-2.1.3.jar"
libraryDependencies += "com.google.common" % "collect" % "14.0.1" from "file:///home/hadoo
p/spark/titan_cass/guava-14.0.1.jar
resolvers += "Cloudera Repository" at "https://repository.cloudera.com/artifactory/clouder
a-repos/"

```

Cassandra 连接器示例的 Scala 脚本名为`spark3_cass.scala`，现在看起来如以下代码所示。 首先，定义包名。 然后，为 Spark 和 Cassandra 连接器导入类。 接下来，定义对象应用程序类`spark3_cass`ID，以及 main 方法：

```
package nz.co.semtechsolutions

import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf

import com.datastax.spark.connector._

object spark3_cass
{

  def main(args: Array[String]) {
```

使用 Spark URL 和应用程序名称创建 Spark Configuration对象。 Cassandra 连接主机将添加到配置中。 然后，使用 Configuration 对象创建 Spark 上下文：

```
    val sparkMaster = "spark://hc2nn.semtech-solutions.co.nz:7077"
    val appName = "Spark Cass 1"
    val conf = new SparkConf()

    conf.setMaster(sparkMaster)
    conf.setAppName(appName)

    conf.set("spark.cassandra.connection.host", "hc2r1m2")

    val sparkCxt = new SparkContext(conf)
```

定义了要检查的 Cassandra`keyspace`和表名。 然后，使用名为`cassandraTable`的 Spark 上下文方法连接到 Cassandra，并获得`edgestore`表的内容作为 RDD。 然后打印该 RDD 的大小，脚本退出。 我们现在不会查看这些数据，因为所需要的只是证明可以与卡桑德拉建立连接：

```
    val keySpace =  "titan"
    val tableName = "edgestore"

    val cassRDD = sparkCxt.cassandraTable( keySpace, tableName )

    println( "Cassandra Table Rows : " + cassRDD.count )

    println( " >>>>> Script Finished <<<<< " )

  } // end main

} // end spark3_cass
```

与前面的示例一样，Spark`submit`命令被放在名为`run_titan.bash.cass`的 Bash 脚本中。 下面显示的这个脚本看起来与已经使用的许多其他脚本相似。 这里要注意的一点是，有一个 JARS 选项，它列出了所有使用的 JAR 文件，以便它们在运行时可用。 已确定此选项中 JAR 文件的顺序，以避免类异常错误：

```
[hadoop@hc2r1m2 titan_cass]$ more run_titan.bash

#!/bin/bash

SPARK_HOME=/usr/local/spark
SPARK_BIN=$SPARK_HOME/bin
SPARK_SBIN=$SPARK_HOME/sbin

JAR_PATH=/home/hadoop/spark/titan_cass/target/scala-2.10/spark-cass_2.10-1.0.jar
CLASS_VAL=$1

CASS_HOME=/home/hadoop/spark/titan_cass/

CASS_JAR1=$CASS_HOME/spark-cassandra-connector_2.10-1.3.0-M1.jar
CASS_JAR2=$CASS_HOME/cassandra-driver-core-2.1.5.jar
CASS_JAR3=$CASS_HOME/cassandra-thrift-2.1.3.jar
CASS_JAR4=$CASS_HOME/libthrift-0.9.2.jar
CASS_JAR5=$CASS_HOME/cassandra-clientutil-2.1.3.jar
CASS_JAR6=$CASS_HOME/guava-14.0.1.jar
CASS_JAR7=$CASS_HOME/joda-time-2.3.jar
CASS_JAR8=$CASS_HOME/joda-convert-1.2.jar

cd $SPARK_BIN

./spark-submit \
 --jars $CASS_JAR8,$CASS_JAR7,$CASS_JAR5,$CASS_JAR4,$CASS_JAR3,$CASS_JAR6,$CASS_JAR2,$CASS_JAR1 \
 --class $CLASS_VAL \
 --master spark://hc2nn.semtech-solutions.co.nz:7077  \
 --executor-memory 100M \
 --total-executor-cores 50 \
 $JAR_PATH

```

此应用程序是使用前面的 Bash 脚本调用的。 它连接到 Cassandra，选择数据，然后返回基于 Cassandra 表数据的`218`行计数。

```
[hadoop@hc2r1m2 titan_cass]$ ./run_titan.bash.cass nz.co.semtechsolutions.spark3_cass

Cassandra Table Rows : 218
 >>>>> Script Finished <<<<<

```

这证明可以从 Apache Spark 访问基于Cassandra 的原始 Titan 表数据。 但是，与 HBase 示例一样，这是基于原始表的 Titan 数据，而不是 Titan图形形式的数据。 下一步将使用 Apache Spark 作为 Titan 数据库的处理引擎。 这将在下一节中进行研究。

# 用火花访问泰坦

到目前为止，本章已经安装了 Titan 0.9.0-M2，并且已经使用 HBase 和 Cassandra 作为后端存储选项成功创建了图表。 这些图表是使用基于Gremlin 的脚本创建的。 在本节中，将通过 Gremlin 脚本使用一个属性文件来使用Apache Spark 处理基于 Titan 的图形。 同样的两个后端存储选项，HBase 和 Cassandra，将用于 Titan。

下面的图基于本章前面的 TinkerPop3 图，显示了本节中使用的体系结构。 我简化了图表，但它与之前的 TinkerPop 版本基本相同。 我刚刚通过 Graph computer API 将链接添加到 Apache Spark。 我还通过 Titan 供应商 API 添加了 HBase 和 Cassandra 存储。 当然，HBase 的分布式安装既使用 ZooKeeper 进行配置，又使用 HDFS 进行存储。

Titan 使用 TinkerPop 的 Hadoop-Gremlin 包进行 OLAP 过程的图形处理。 文档部分的链接位于：[http://s3.thinkaurelius.com/docs/titan/0.9.0-M2/titan-hadoop-tp3.html](http://s3.thinkaurelius.com/docs/titan/0.9.0-M2/titan-hadoop-tp3.html)。

本节将展示如何使用 Bash shell、Groovy 和属性文件来配置和运行基于 Titan Spark 的作业。 它将显示配置作业的不同方法，并且它的还将显示管理日志记录以启用错误跟踪的方法。 此外，还将描述属性文件的不同配置，以提供对 HBase、Cassandra 和 Linux 文件系统的访问。

请记住，本章所基于的 Titan 版本 0.9.0-M2 是一个开发版本。 这只是一个原型版本，还没有准备好投入生产。 我认为随着泰坦未来的发布，泰坦和星火之间的联系将会更加发达和稳定。 目前，考虑到 Titan 版本的性质，本节中的工作仅用于演示目的。

![Accessing Titan with Spark](graphics/B01989_06_05.jpg)

在下一节中，我将解释 Gremlin 和 Groovy 脚本的使用，然后使用 Cassandra 和 HBase 作为存储选项将 Titan 连接到 Spark。

## Gremlin 和 Groovy

Gremlin shell 用于针对 Titan 执行Groovy 命令，可以通过多种方式使用。 第一种使用方法只涉及启动一个 Gremlin shell，将其用作交互式会话。 只需执行以下命令：

```
cd $TITAN_HOME/bin ; ./ gremlin.sh

```

这将启动会话，并自动设置所需的插件，如 TinkerPop 和 Titan(请参见下文)。 显然，前面的`TITAN_HOME`变量用于指示有问题的 bin 目录位于您的 Titan 安装(`TITAN_HOME`)目录中：

```
plugin activated: tinkerpop.server
plugin activated: tinkerpop.utilities
plugin activated: tinkerpop.hadoop
plugin activated: tinkerpop.tinkergraph
plugin activated: aurelius.titan

```

然后，它为您提供一个 Gremlin shell 提示符，您可以在其中交互地对您的 Titan 数据库执行 shell 命令。 该 shell 对于测试脚本和针对 Titan 数据库运行即席命令非常有用。

```
gremlin>

```

第二种方法是在调用`gremlin.sh`命令时将 Groovy 命令内联嵌入到脚本中。 在本例中，EOF 标记之间的 Groovy 命令通过管道传输到Gremlin shell。 执行完最后一个 Groovy 命令后，Gremlin shell 将终止。 当您仍然希望使用 Gremlin shell 的自动环境设置，但仍然希望能够快速重新执行脚本时，这很有用。 此代码片段是从 Bash shell 脚本执行的，如下例所示。 以下脚本使用`titan.sh`脚本管理 Gremlin 服务器：

```
#!/bin/bash

TITAN_HOME=/usr/local/titan/

cd $TITAN_HOME

bin/titan.sh start

bin/gremlin.sh   <<  EOF

 t = TitanFactory.open('cassandra.properties')
 GraphOfTheGodsFactory.load(t)
 t.close()
EOF

bin/titan.sh stop

```

第三种方法涉及将 Groovy 命令移到一个单独的 Groovy 文件中，并在 Gremlin shell 中使用`–e`选项来执行该文件。 此方法为错误跟踪提供了额外的日志记录选项，但这意味着在为 Groovy 脚本设置 Gremlin 环境时需要采取额外的步骤：

```
#!/bin/bash

TITAN_HOME=/usr/local/titan/
SCRIPTS_HOME=/home/hadoop/spark/gremlin
GREMLIN_LOG_FILE=$TITAN_HOME/log/gremlin_console.log

GROOVY_SCRIPT=$1

export GREMLIN_LOG_LEVEL="DEBUG"

cd $TITAN_HOME

bin/titan.sh start

bin/gremlin.sh -e  $SCRIPTS_HOME/$GROOVY_SCRIPT  > $GREMLIN_LOG_FILE 2>&1

bin/titan.sh stop

```

因此，该脚本定义了 Gremlin 日志级别，可以将其设置为不同的日志级别，以获取有关问题的额外信息，即 INFO、WARN 和 DEBUG。 它还将脚本输出重定向到日志文件(`GREMLIN_LOG_FILE`)，并将错误重定向到同一日志文件(`2>&1`)。 这样做的好处是允许持续监视日志文件，并提供会话的个永久记录。 然后，将执行的 Groovy 脚本名称作为参数传递给封装 Bash shell 脚本(`$1`)。

正如我已经提到的，与前面的 Gremlin 会话选项相比，以这种方式调用的 Groovy 脚本需要额外的环境配置来设置 Gremlin 会话。 例如，需要导入将使用的必要的 TinkerPop 和 Aurelius 类：

```
import com.thinkaurelius.titan.core.*
import com.thinkaurelius.titan.core.titan.*
import org.apache.tinkerpop.gremlin.*
```

在描述了启动 Gremlin shell 会话和运行 Groovy 脚本所需的脚本和配置选项之后，从现在开始，我将集中讨论 Groovy 脚本以及配置 Gremlin 会话所需的属性文件。

## TinkerPop 的 Hadoop Gremline

正如本部分前面提到的，Titan 中的 TinkerPop Hadoop Gremlin 包将用于调用 Apache Spark 作为处理引擎(Hadoop Giraph 也可以用于处理)。 [Hadoop Gremlin](http://s3.thinkaurelius.com/docs/titan/0.9.0-M2/titan-hadoop-tp3.html)上提供的链接提供了Hadoop Gremlin 的文档；请记住，此 http://s3.thinkaurelius.com/docs/titan/0.9.0-M2/titan-hadoop-tp3.html 包仍在开发中，可能会有所更改。

此时，我将检查一个属性文件，该文件可用于连接到 Cassandra 作为 Titan 的存储后端。 它包含关于 Cassandra、Apache Spark 和 Hadoop Gremlin 配置的部分。 我的 Cassandra 属性文件名为`cassandra.properties`，如下所示(以散列字符(`#`)开头的行是注释)：

```
####################################
# Storage details
####################################
storage.backend=cassandra
storage.hostname=hc2r1m2
storage.port=9160
storage.cassandra.keyspace=dead
cassandra.input.partitioner.class=org.apache.cassandra.dht.Murmur3Partitioner

```

前面基于 Cassandra 的属性描述了 Cassandra 主机和端口。 这就是存储后端类型是 Cassandra 的原因，要使用的 Cassandra`keyspace`称为`dead`(感恩死亡的缩写-本例中将使用的数据)。 请记住，Cassandra 表是在键空间中分组的。 前面的`partitioner`类定义了将用于对 Cassandra 数据进行分区的 Cassandra 类。 Apache Spark 配置节包含主 URL、执行器内存和要使用的数据`serializer`类：

```
####################################
# Spark
####################################
spark.master=spark://hc2nn.semtech-solutions.co.nz:6077
spark.executor.memory=400M
spark.serializer=org.apache.spark.serializer.KryoSerializer

```

最后，这里显示了属性文件的 Hadoop Gremlin 部分，它定义了用于图形和非图形输入和输出的类。 它还定义了数据输入和输出位置，以及用于缓存 JAR 文件和派生内存的标志：

```
####################################
# Hadoop Gremlin
####################################
gremlin.graph=org.apache.tinkerpop.gremlin.hadoop.structure.HadoopGraph
gremlin.hadoop.graphInputFormat=com.thinkaurelius.titan.hadoop.formats.cassandra.CassandraInputFormat
gremlin.hadoop.graphOutputFormat=org.apache.tinkerpop.gremlin.hadoop.structure.io.gryo.GryoOutputFormat
gremlin.hadoop.memoryOutputFormat=org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat

gremlin.hadoop.deriveMemory=false
gremlin.hadoop.jarsInDistributedCache=true
gremlin.hadoop.inputLocation=none
gremlin.hadoop.outputLocation=output

```

BluePrints 是TinkerPop 属性图模型接口。 Titan 发布了它自己的蓝图实现，所以您看到的不是前面属性中的`blueprints.graph`，而是`gremlin.graph`。 这定义了类，用于定义应该使用的图形。 如果省略此选项，则图形类型将默认为以下类型：

```
com.thinkaurelius.titan.core.TitanFactory

```

`CassandraInputFormat`类定义从 Cassandra 数据库检索数据。 图形输出序列化类被定义为`GryoOutputFormat`。 内存输出格式类被定义为使用 Hadoop Map Reduce 类`SequenceFileOutputFormat`。

已经将`jarsInDistributedCache`值定义为 true，以便将 JAR 文件复制到内存中，从而使 Apache Spark 能够获取它们。 如果有更多的时间，我会研究如何在类路径上使 Titan 类对 Spark 可见，以避免过度使用内存。

鉴于 TinkerPop Hadoop Gremlin 模块仅作为开发原型版本提供，目前的文档很少。 编码示例非常有限，而且似乎没有文档描述前面的每一个属性。

在深入研究 Groovy 脚本的示例之前，我想向您展示一种使用 Configuration 对象配置 Groovy 作业的替代方法。

## 替代 Groovy 配置

可以使用`BaseConfiguration`方法创建 Configuration对象。 在本例中，我创建了一个名为`cassConf`的 Cassandra 配置：

```
cassConf = new BaseConfiguration();

cassConf.setProperty("storage.backend","cassandra");
cassConf.setProperty("storage.hostname","hc2r1m2");
cassConf.setProperty("storage.port","9160")
cassConf.setProperty("storage.cassandra.keyspace","titan")

titanGraph = TitanFactory.open(cassConf);
```

然后使用`setProperty`方法定义 Cassandra 连接属性，如后端类型、主机、端口和`keyspace`。 最后，使用 open 方法创建名为`titanGraph`的 Titan 图。 如稍后所示，可以使用配置对象或属性文件的路径创建 Titan 图。 已经设置的属性与前面描述的 Cassandra 属性文件中定义的属性相匹配。

接下来的几节将展示如何创建和遍历图表。 它们将展示如何将 Cassandra、HBase 和文件系统用于存储。 鉴于我已经花了很长时间来描述 Bash 脚本和属性文件，我将只描述那些在每个实例中需要更改的属性。 我还将在每个实例中提供简单的 Groovy 脚本片段。

## 使用 Cassandra

基于 Cassandra 的属性文件`cassandra.properties`已经描述过了，所以我在这里不再重复细节。 这个示例 Groovy 脚本创建了一个示例图形，并将其存储在 Cassandra 中。 它已使用**文件结尾标记**(**EOF**)执行，以将脚本通过管道传递给 Gremlin shell：

```
t1 = TitanFactory.open('/home/hadoop/spark/gremlin/cassandra.properties')
GraphOfTheGodsFactory.load(t1)

t1.traversal().V().count()

t1.traversal().V().valueMap()

t1.close()

```

已使用`TitanFactory.open`方法和 Cassandra 属性文件创建了 Titan 图。 它被称为`t1`。 使用方法`GraphOfTheGodsFactory.load`将随泰坦提供的示例图形--众神图形加载到图形`t1`中。 然后，已生成顶点计数(`V()`)和`ValueMap`，以显示图形的内容。 输出如下所示：

```
==>12

==>[name:[jupiter], age:[5000]]
==>[name:[hydra]]
==>[name:[nemean]]
==>[name:[tartarus]]
==>[name:[saturn], age:[10000]]
==>[name:[sky]]
==>[name:[pluto], age:[4000]]
==>[name:[alcmene], age:[45]]
==>[name:[hercules], age:[30]]
==>[name:[sea]]
==>[name:[cerberus]]
==>[name:[neptune], age:[4500]]

```

因此，图中有 12 个顶点，每个顶点都有一个名称和年龄元素，如前面的数据所示。 成功创建图形后，现在可以将前面的 graph traversal Gremlin 命令配置为使用 Apache Spark 进行处理。 这只需在遍历命令中指定`SparkGraphComputer`即可。 有关体系结构的详细信息，请参阅本章顶部的完整*TinkerPop*图表。 执行此命令时，您将在看到任务出现在 Spark 群集用户界面上：

```
t1.traversal(computer(SparkGraphComputer)).V().count()

```

## 使用 HBase

使用HBase 时，需要更改属性文件。 以下值取自我的`hbase.properties`文件：

```
gremlin.hadoop.graphInputFormat=com.thinkaurelius.titan.hadoop.formats.hbase.HBaseInputFormat

input.conf.storage.backend=hbase
input.conf.storage.hostname=hc2r1m2
input.conf.storage.port=2181
input.conf.storage.hbase.table=titan
input.conf.storage.hbase.ext.zookeeper.znode.parent=/hbase

```

请记住，HBase 使用 ZooKeeper 进行配置。 因此，用于连接的端口号和服务器现在变成了`zookeeper`服务器和`zookeeper`主端口 2181。 ZooKeeper 中的`znode`父值也被定义为顶级节点`/hbase`。 当然，后端类型现在被定义为`hbase`。

此外，`GraphInputFormat`类已更改为`HBaseInputFormat`，以将 HBase 描述为输入源。 现在可以使用此属性文件创建 Titan 图，如上一节所示。 我不会在这里重复图形创建，因为它将与上一节相同。 接下来，我将继续讨论文件系统存储。

## 使用文件系统

为了运行这个示例，我使用了一个基本的 Gremlin shell(`bin/gremlin.sh`)。 在 Titan 版本的数据目录中，有许多可以加载以创建图形的示例数据文件格式。 在本例中，我将使用名为`grateful-dead.kryo`的文件。 因此，这一次，数据将直接从文件加载到图形，而不指定存储后端，如 Cassandra。 我将使用的属性文件仅包含以下条目：

```
gremlin.graph=org.apache.tinkerpop.gremlin.hadoop.structure.HadoopGraph
gremlin.hadoop.graphInputFormat=org.apache.tinkerpop.gremlin.hadoop.structure.io.gryo.GryoInputFormat
gremlin.hadoop.graphOutputFormat=org.apache.tinkerpop.gremlin.hadoop.structure.io.gryo.GryoOutputFormat
gremlin.hadoop.jarsInDistributedCache=true
gremlin.hadoop.deriveMemory=true

gremlin.hadoop.inputLocation=/usr/local/titan/data/grateful-dead.kryo
gremlin.hadoop.outputLocation=output

```

同样，它使用Hadoop Gremlin 包，但这次将图形输入和输出格式定义为`GryoInputFormat`和`GryoOutputFormat`。 输入位置被指定为实际的基于`kyro`的文件。 因此，输入和输出的源是文件。 现在，Groovy 脚本如下所示。 首先，使用属性文件创建图形。 然后，创建一次图遍历，这样我们就可以计算顶点数并查看结构：

```
graph = GraphFactory.open('/home/hadoop/spark/gremlin/hadoop-gryo.properties')
g1 = graph.traversal()

```

接下来，执行顶点计数，显示有 800 多个顶点；最后，值图显示数据的结构，我明显地对其进行了裁剪以节省空间。 但您可以看到歌曲名称、类型和表演细节：

```
g1.V().count()
==>808
g1.V().valueMap()
==>[name:[MIGHT AS WELL], songType:[original], performances:[111]]
==>[name:[BROWN EYED WOMEN], songType:[original], performances:[347]]

```

这让您对可用的功能有了一个基本的了解。 我敢肯定，如果你在网上搜索，你会找到更复杂的方式来使用 Spark 和泰坦。 以此为例：

```
r = graph.compute(SparkGraphComputer.class).program(PageRankVertexProgram.build().create()).submit().get()

```

上一个示例使用 Compute 方法指定了`SparkGraphComputer`类的使用。 它还展示了如何使用 Program 方法执行 Titan 提供的页面排名顶点程序。 这将通过向每个顶点添加页面排名来修改图表。 我提供这一点作为一个例子，因为我不相信它会在这个时候与 Spark 一起工作。

# 摘要

本章介绍了奥雷利厄斯的泰坦图形数据库。 它展示了如何在 Linux 集群上安装和配置它。 使用 Titan Gremlin shell 示例，已经创建了图表，并将其存储在 HBase 和 Cassandra NoSQL 数据库中。 所需的 Titan 存储选项的选择将取决于您的项目要求：基于 HBase HDFS 的存储还是基于 Cassandra 的非 HDFS 存储。 本章还介绍了您可以交互地使用 Gremlin shell 来开发图形脚本，也可以将它与 Bash shell 脚本配合使用，这样您就可以通过关联的日志记录来运行计划的作业。

这里提供了简单的 Spark Scala 代码，它显示 Apache Spark 可以访问 Titan 在 HBase 和 Cassandra 上创建的底层表。 这是通过使用 Cloudera(用于 HBase)和 DataStAX(用于 Cassandra)提供的数据库连接器模块实现的。 所有示例代码和构建脚本都已随示例输出一起描述。 我包含了这个基于 Scala 的部分，以向您展示基于图形的数据可以在 Scala 中访问。 上一节处理来自 Gremlin shell 的数据，并使用 Spark 作为处理后端。 本节使用 Spark 作为主处理引擎，并从 Spark 访问 Titan 数据。 如果 Gremlin shell 不适合您的需求，您可以考虑使用这种方法。 随着泰坦的成熟，你可以通过 Scala 集成泰坦和 Spark 的方式也会随之成熟。

最后，Titan 的 Gremlin shell 与 Apache Spark 一起用于演示创建和访问基于 Titan 的图形的简单方法。 为此，数据已存储在文件系统、Cassandra 和 HBase 上。

通过[https://groups.google.com/forum/#！forum/aureliusgraphs](https://groups.google.com/forum/#!forum/aureliusgraphs)和[https://groups.google.com/forum/#！forum/gremlin-users](https://groups.google.com/forum/#!forum/gremlin-users)上的 URL，Aurelius 和 Gremlin 用户可以使用谷歌群组。

虽然这个社区看起来比其他 Apache 项目小，但发帖量可能比较少，而且很难得到回复。

卡桑德拉的创造者 DataStax 今年收购了泰坦的创造者奥雷利厄斯(Aurelius)。 Titan 的创建者现在正在参与 DataSTax 的 DSE 图形数据库的开发，这可能会对 Titan 的发展产生连锁反应。 话虽如此，0.9.x 的 Titan 版本已经创建，预计将发布 1.0 版本。

因此，在通过使用 Scala 和 Gremlin 的示例展示了一些 Titan 功能之后，我将在这里结束这一章。 我想展示基于 Spark 的图形处理和图形存储系统的配对。 我喜欢开源系统的开发速度和可访问性。 我不是说泰坦就是你的数据库，但它是一个很好的例子。 如果它的未来能够得到保证，并且它的社区不断发展壮大，那么随着它的成熟，它可以提供一种宝贵的资源。

请注意，本章使用了两个版本的 Spark：1.3 和 1.2.1。 早期版本是必需的，因为它显然是唯一可以与 Titan 的`SparkGraphComputer`一起工作的版本，因此避免了 Kyro 序列化错误。

在下一章中，将根据[http://h2o.ai/](http://h2o.ai/)H2O 产品研究 Apache Spark MLlib 机器学习库的扩展。 将在 Scala 中开发一个基于神经的深度学习示例，以演示其潜在的功能。