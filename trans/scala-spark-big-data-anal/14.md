# 是时候整理一下了——用 Spark MLlib 对你的数据进行聚类

*"If you take a galaxy and try to make it bigger, it becomes a cluster of galaxies, not a galaxy. If you try to make it smaller than that, it seems to blow itself apart"*

-耶利米·奥斯特里克尔

在这一章中，我们将深入研究机器学习，并找出我们如何利用它来为无监督观察的数据集聚类属于某个组或类的记录。简而言之，本章将涵盖以下主题:

*   无监督学习
*   聚类技术
*   层次聚类
*   基于质心的聚类
*   基于分布的聚类(DC)
*   确定集群数量
*   聚类算法的比较分析
*   在计算集群上提交作业

# 无监督学习

在本节中，我们将通过适当的例子简要介绍无监督机器学习技术。让我们从一个实际的例子开始讨论。假设你的硬盘上有一个拥挤的大文件夹，里面有大量非盗版的完全合法的 MP3。现在，如果你能建立一个预测模型，帮助自动将相似的歌曲组合在一起，并将它们组织成你最喜欢的类别，如乡村、说唱、摇滚等，那该怎么办。将一个项目分配给一个组的行为，这样一个 mp3 将会以一种无监督的方式被添加到相应的播放列表中。在前面的章节中，我们假设您得到了一个正确标注数据的训练数据集。不幸的是，当我们在现实世界中收集数据时，我们并不总是那么奢侈。例如，假设我们想要将大量音乐分成有趣的播放列表。如果我们不能直接访问歌曲的元数据，我们怎么可能把它们组合在一起呢？一种可能的方法是混合各种机器学习技术，但聚类通常是解决方案的核心。

简而言之，在无监督机器学习问题中，训练数据集的正确类别不可用或未知。因此，必须从结构化或非结构化数据集推导出类，如图 1 所示。这实质上意味着这种算法的目标是以某种结构化的方式对数据进行预处理。换句话说，无监督学习算法的主要目标是探索输入数据中未标记的未知/隐藏模式*。*然而，无监督学习还包括其他技术，以探索性的方式解释数据的关键特征，从而找到隐藏的模式。为了克服这一挑战，聚类技术被广泛用于以无监督的方式基于某些相似性度量对未标记的数据点进行分组。

关于无监督算法如何工作的深入理论知识，请参考以下三本书: *Bousquet* ，*o；冯·卢克思堡*，*u；Raetsch* ， *G，eds* (2004)。*机器学习高级讲座*。*斯普林格-弗拉格*。ISBN 978-3540231226。或*杜达*、*理查德·欧.*；*哈特*，*彼得·e .*；*鹳**大卫 G* 。(2001).*无监督学习和聚类*。*图案分类*(第 2 版。).*威利*。ISBN 0-471-05669-3 和*乔丹*、*迈克尔一.*；*主教*、*克里斯托弗·M*。(2004) *神经网络*。在*艾伦·塔克* *计算机科学手册，第二版*(第七部分:智能系统)。*博卡拉顿*，佛罗里达州:查普曼与霍尔/华润出版社有限责任公司。ISBN 1-58488-360-X

![](../images/00263.jpeg)

**Figure 1:** Unsupervised learning with Spark

# 无监督学习示例

在聚类任务中，算法通过分析输入示例之间的相似性将相关特征分组到类别中，其中相似特征被聚类并使用圆圈标记。聚类的用途包括但不限于以下内容:搜索结果分组，例如对客户进行分组，用于可疑模式发现的异常检测，用于在测试中发现有用模式的文本分类，用于找到连贯组的社交网络分析，用于找到将相关计算机放在一起的方法的数据中心计算集群，用于星系形成的天文数据分析，以及基于相似特征识别邻域的房地产数据分析。我们将为最后的用例展示一个基于 Spark MLlib 的解决方案。

# 聚类技术

在本节中，我们将讨论集群技术以及挑战和合适的例子。还将简要概述分层聚类、基于质心的聚类和基于分布的聚类。

# 无监督学习和聚类

聚类分析是将数据样本或数据点进行划分，并将其放入相应的同质类或聚类中。因此，聚类的简单定义可以被认为是将对象组织成成员在某些方面相似的组的过程。因此，一个*簇*是一组对象的集合，这些对象之间*相似*，而*不同于*属于其他簇的对象。如图*图 2* 所示，如果给定了一组对象，聚类算法会根据相似性将这些对象归入一个组。然后，诸如 K 均值的聚类算法已经定位了数据点组的质心。然而，为了使聚类准确和有效，该算法评估每个点到聚类质心的距离。最终，聚类的目标是确定一组未标记数据中的内在分组。

![](../images/00059.jpeg)

**Figure 2:** Clustering raw data

Spark 支持 **K-means** 、**高斯混合**、**幂迭代聚类** ( **PIC** )、l **atent dirichlet 分配** ( **LDA** )、**平分 K-means** 、 **Streaming K-means** 等多种聚类算法。LDA 用于文本挖掘中常用的文档分类和聚类。PIC 用于将由成对相似性组成的图的顶点聚类为边属性。然而，为了使本章的目标更加清晰和集中，我们将把我们的讨论限制在 K-means、平分 K-means 和高斯混合算法上。

# 分级聚类

分层聚类技术是基于这样一个基本思想，即对象或特征与附近的对象或特征比与远处的对象或特征更相关。平分 K-means 是这种分层聚类算法的一个例子，该算法基于数据对象的对应距离来连接数据对象以形成聚类。

在层次聚类技术中，一个聚类可以简单地用连接部分聚类所需的最大距离来描述。因此，不同的距离会形成不同的集群。从图形上看，这些集群可以用树形图来表示。有趣的是，通用名层次聚类是从谱系图的概念演变而来的。

# 基于质心的聚类

在基于质心的聚类技术中，聚类由中心向量表示。然而，向量本身不一定是数据点的成员。在这种类型的学习中，在训练模型之前，必须提供一些可能的聚类。K-means 是这种学习类型的一个非常著名的例子，其中，如果您将集群的数量设置为一个固定的整数，比如 K，则 K-means 算法提供了一个作为优化问题的形式定义，这是一个单独的问题，需要解决以找到 K 个集群中心，并将数据对象分配给最近的集群中心。简而言之，这是一个优化问题，其目标是最小化与集群的平方距离。

# 基于分布的聚类

基于分布的聚类算法基于统计分布模型，该模型提供了将相关数据对象聚类到相同分布的更方便的方法。尽管这些算法的理论基础非常稳健，但它们大多存在过度拟合的问题。然而，这个限制可以通过限制模型的复杂性来克服。

# 基于质心的聚类

在这一节中，我们讨论了基于质心的聚类技术及其计算挑战。为了更好地理解基于质心的聚类，将展示一个使用 K 均值和 Spark MLlib 的例子。

# CC 算法面临的挑战

如前所述，在像 K-means 这样的基于质心的聚类算法中，设置聚类数 K 的最优值是一个优化问题。这个问题可以被描述为具有高算法复杂性的 NP-hard(即非确定性多项式时间的 hard)，因此通常的方法是试图仅获得近似解。因此，解决这些优化问题会带来额外的负担，并因此带来重大的缺点。此外，K 均值算法期望每个聚类具有近似相似的大小。换句话说，每个集群中的数据点必须是一致的，以获得更好的集群性能。

该算法的另一个主要缺点是，该算法试图优化聚类中心而不是聚类边界，并且这往往会不恰当地切割聚类之间的边界。然而，有时，我们可以利用视觉检查的优势，这通常不适用于超平面或多维数据上的数据。尽管如此，关于如何找到 K 的最佳值的完整部分将在本章后面讨论。

# K-means 算法是如何工作的？

假设我们有 *n* 个数据点*x<sub class="calibre43">I</sub>T5， *i=1...n 个*需要划分为 *k 个*簇。现在这里的目标是为每个数据点分配一个集群。K-means 然后旨在找到位置 *μ <sub class="calibre43">i</sub> ，i=1...k* 最小化从数据点到聚类的距离的聚类。从数学上讲，K-means 算法试图通过求解以下方程来实现目标，即一个优化问题:*

![](../images/00320.jpeg)

在上式中， *c <sub class="calibre43">i</sub>* 是分配给聚类 *i* 的数据点集合， *d(x，μ<sub class="calibre43">I</sub>)= | | x-μ<sub class="calibre43">I</sub>|<sup class="calibre26">2</sup><sub class="calibre43">2</sub>*是要计算的欧氏距离(我们稍后会解释为什么要使用这个距离度量)。因此，我们可以理解，使用 K-means 的整体聚类操作不是一个微不足道的问题，而是一个 NP-hard 优化问题。这也意味着 K-means 算法不仅试图找到全局最小值，而且经常陷入不同的解。

现在，让我们看看如何在将数据输入 K 均值模型之前制定算法。首先，我们需要决定暂定集群的数量， *k* 隐修会。通常，您需要遵循以下步骤:

![](../images/00367.jpeg)

这里 *|c|* 是 *c* 中的元素个数。

使用 K-means 算法的聚类开始于将所有坐标初始化为质心。在算法的每一遍中，每个点都基于某个距离度量被分配到其最近的质心，通常是*欧几里德距离*。

**Distance calculation:** Note that there are other ways to calculate the distance too, for example:
*Chebyshev distance* can be used to measure the distance by considering only the most notable dimensions.
The *Hamming distance* algorithm can identify the difference between two strings. On the other hand, to make the distance metric scale-undeviating, *Mahalanobis distance* can be used to normalize the covariance matrix. The *Manhattan distance* is used to measure the distance by considering only axis-aligned directions. The *Minkowski distance* algorithm is used to make the Euclidean distance, Manhattan distance, and Chebyshev distance. The *Haversine distance* is used to measure the great-circle distances between two points on a sphere from the location, that is, longitudes and latitudes.

考虑到这些距离测量算法，显然欧几里德距离算法将是最适合解决我们在 K-means 算法中距离计算目的的算法。然后，质心被更新为在该迭代中分配给它的所有点的中心。这种情况一直重复，直到中心的变化最小。简而言之，K-means 算法是一种迭代算法，分两步工作:

*   **聚类分配步骤** : K-means 遍历数据集中的 m 个数据点中的每一个，该数据点被分配给由 K 个质心中最接近的一个表示的聚类。对于每个点，计算到每个质心的距离，并简单地选择距离最小的一个。
*   **更新步骤**:对于每个聚类，计算一个新的质心作为聚类中所有点的平均值。从上一步开始，我们有一组分配给一个集群的点。现在，对于每一个这样的集合，我们计算一个平均值，我们声明一个新的聚类质心。

# 一个利用 Spark MLlib 的 K-means 进行聚类的例子

为了进一步演示聚类示例，我们将使用从[http://course1.winona.edu/bdeppa/Stat%20425/Datasets.html](http://course1.winona.edu/bdeppa/Stat%20425/Datasets.html)下载的*萨拉托加纽约家庭*数据集作为使用 Spark MLlib 的无监督学习技术。数据集包含位于纽约市郊区的房屋的几个特征。例如，价格、地块大小、滨水区、年龄、土地价值、新建筑、中央空调、燃料类型、供暖类型、下水道类型、生活区、大学、卧室、壁炉、浴室和房间数量。但是，下表中只显示了一些功能:

| **价格** | **批量** | **水锋** | **年龄** | **土地价值** | **房间** |
| One hundred and thirty-two thousand five hundred | Zero point zero nine | Zero | forty-two | Five thousand | five |
| One hundred and eighty-one thousand one hundred and fifteen | Zero point nine two | Zero | Zero | Twenty-two thousand three hundred | six |
| One hundred and nine thousand | Zero point one nine | Zero | One hundred and thirty-three | Seven thousand three hundred | eight |
| One hundred and fifty-five thousand | Zero point four one | Zero | Thirteen | Eighteen thousand seven hundred | five |
| Eighty-six thousand and sixty | Zero point one one | Zero | Zero | Fifteen thousand | three |
| One hundred and twenty thousand | Zero point six eight | Zero | Thirty-one | Fourteen thousand | eight |
| One hundred and fifty-three thousand | Zero point four | Zero | Thirty-three | Twenty-three thousand three hundred | eight |
| One hundred and seventy thousand | One point two one | Zero | Twenty-three | One hundred and forty-six thousand | nine |
| Ninety thousand | Zero point eight three | Zero | Thirty-six | Two hundred and twenty-two thousand | eight |
| One hundred and twenty-two thousand nine hundred | One point nine four | Zero | four | Two hundred and twelve thousand | six |
| Three hundred and twenty-five thousand | Two point two nine | Zero | One hundred and twenty-three | One hundred and twenty-six thousand | Twelve |

**Table 1:** Sample data from the Saratoga NY Homes dataset

这里这种聚类技术的目标是显示基于城市中每所房子的特征的探索性分析，以便为位于同一区域的房子找到可能的邻居。在执行特征提取之前，我们需要加载并解析萨拉托加纽约家园数据集。该步骤还包括加载包和相关依赖项、将数据集读取为 RDD、模型训练、预测、收集本地解析数据以及聚类比较。

**第一步**。与进口相关的包装:

```scala
package com.chapter13.Clusteringimport org.apache.spark.{SparkConf, SparkContext}import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}import org.apache.spark.mllib.linalg.Vectorsimport org.apache.spark._import org.apache.spark.rdd.RDDimport org.apache.spark.sql.functions._import org.apache.spark.sql.types._import org.apache.spark.sql._import org.apache.spark.sql.SQLContext

```

**第二步。创建一个火花会话-入口点** -这里我们首先通过设置应用程序名称和主网址来设置火花配置。为简单起见，它是独立的，包含您机器上的所有内核:

```scala
val spark = SparkSession.builder.master("local[*]").config("spark.sql.warehouse.dir", "E:/Exp/").appName("KMeans").getOrCreate()

```

**第三步。加载和解析数据集** -从数据集读取、解析和创建关系数据库，如下所示:

```scala
//Start parsing the datasetval start = System.currentTimeMillis()val dataPath = "data/Saratoga NY Homes.txt"//val dataPath = args(0)val landDF = parseRDD(spark.sparkContext.textFile(dataPath)).map(parseLand).toDF().cache()landDF.show()

```

请注意，要使前面的代码工作，您应该导入以下包:

```scala
import spark.sqlContext.implicits._

```

您将获得以下输出:

![](../images/00339.jpeg)

**Figure 3:** A snapshot of the Saratoga NY Homes dataset

以下是用于从一组`Double`创建一个`Land`类的`parseLand`方法，如下所示:

```scala
// function to create a  Land class from an Array of Doubledef parseLand(line: Array[Double]): Land = {Land(line(0), line(1), line(2), line(3), line(4), line(5),line(6), line(7), line(8), line(9), line(10),line(11), line(12), line(13), line(14), line(15))}

```

将所有特征作为双精度读取的`Land`类如下:

```scala
case class Land(Price: Double, LotSize: Double, Waterfront: Double, Age: Double,LandValue: Double, NewConstruct: Double, CentralAir: Double, FuelType: Double, HeatType: Double, SewerType: Double, LivingArea: Double, PctCollege: Double, Bedrooms: Double,Fireplaces: Double, Bathrooms: Double, rooms: Double)

```

正如你已经知道的，为了训练 K-means 模型，我们需要确保所有的数据点和特征都是数字的。因此，我们还需要将所有数据点转换为双倍数据，如下所示:

```scala
// method to transform an RDD of Strings into an RDD of Doubledef parseRDD(rdd: RDD[String]): RDD[Array[Double]] = {rdd.map(_.split(",")).map(_.map(_.toDouble))}

```

**第四步。准备训练集** -首先，我们需要将数据帧(即`landDF`)转换为双精度的 RDD，并缓存数据以创建一个新的数据帧来链接集群编号，如下所示:

```scala
val rowsRDD = landDF.rdd.map(r => (r.getDouble(0), r.getDouble(1), r.getDouble(2),r.getDouble(3), r.getDouble(4), r.getDouble(5),r.getDouble(6), r.getDouble(7), r.getDouble(8),r.getDouble(9), r.getDouble(10), r.getDouble(11),r.getDouble(12), r.getDouble(13), r.getDouble(14),r.getDouble(15)))rowsRDD.cache()

```

现在，我们需要将前面的双 RDD 转换为密集向量的 RDD，如下所示:

```scala
// Get the prediction from the model with the ID so we canlink them back to other informationval predictions = rowsRDD.map{r => (r._1, model.predict(Vectors.dense(r._2, r._3, r._4, r._5, r._6, r._7, r._8, r._9,r._10, r._11, r._12, r._13, r._14, r._15, r._16)))}

```

**第五步。训练 K-means 模型** -通过指定 10 个集群、20 次迭代和 10 次运行来训练模型，如下所示:

```scala
val numClusters = 5val numIterations = 20val run = 10val model = KMeans.train(numericHome, numClusters,numIterations, run,KMeans.K_MEANS_PARALLEL)

```

The Spark-based implementation of K-means starts working by initializing a set of cluster centers using the K-means algorithm by *Bahmani et al.*, *Scalable K-Means++*, VLDB 2012\. This is a variant of K-means++ that tries to find dissimilar cluster centers by starting with a random center and then doing passes where more centers are chosen with a probability proportional to their squared distance to the current cluster set. It results in a provable approximation to an optimal clustering. The original paper can be found at [http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf](http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf).

**第六步。评估模型错误率** -标准的 K-means 算法旨在最小化每组点之间距离的平方和，即平方欧氏距离，这是 WSSSE 的目标。K-means 算法旨在最小化每组点(即聚类中心)之间距离的平方和。然而，如果你真的想最小化每个集合的点之间的距离的平方和，你会得到一个模型，其中每个聚类是它自己的聚类中心；在这种情况下，度量将是 0。

因此，一旦您通过指定参数训练了您的模型，您就可以使用【设置误差平方和内的 T0】(**WSSE**)来评估结果。从技术上讲，它类似于每个 K 簇中每个观测值的距离之和，可以计算如下:

```scala
// Evaluate clustering by computing Within Set Sum of Squared Errorsval WCSSS = model.computeCost(landRDD)println("Within-Cluster Sum of Squares = " + WCSSS)

```

前面的模型训练集产生了 WCSSS 的值:

```scala
Within-Cluster Sum of Squares = 1.455560123603583E12 

```

**第七步。计算并打印集群中心** -首先，我们从带有 ID 的模型中获得预测，以便我们可以将它们链接回与每个房屋相关的其他信息。请注意，我们将使用我们在步骤 4 *:* 中准备的 RDD 行数

```scala
// Get the prediction from the model with the ID so we can link themback to other informationval predictions = rowsRDD.map{r => (r._1, model.predict(Vectors.dense(r._2, r._3, r._4, r._5, r._6, r._7, r._8, r._9, r._10,r._11, r._12, r._13, r._14, r._15, r._16)))}

```

但是，当要求对价格进行预测时，应提供该信息。这应该按照以下步骤进行:

```scala
val predictions = rowsRDD.map{r => (r._1, model.predict(Vectors.dense(r._1, r._2, r._3, r._4, r._5, r._6, r._7, r._8, r._9, r._10,r._11, r._12, r._13, r._14, r._15, r._16)))}

```

为了获得更好的可视性和探索性分析，请将 RDD 转换为数据帧，如下所示:

```scala
import spark.sqlContext.implicits._val predCluster = predictions.toDF("Price", "CLUSTER")predCluster.show() 
```

 **这将产生下图所示的输出:

![](../images/00044.gif)

**Figure 4:** A snapshot of the clusters predicted

由于数据集中没有可区分的标识，我们表示`Price`字段进行链接。从上图可以了解到有一定价格的房子落在哪里，也就是落在哪个集群。现在，为了更好的可见性，让我们将预测数据框与原始数据框结合起来，以了解每个房屋的单个集群编号:

```scala
val newDF = landDF.join(predCluster, "Price") newDF.show()

```

您应该观察下图中的输出:

![](../images/00138.jpeg)

**Figure 5:** A snapshot of the clusters predicted across each house

为了进行分析，我们将输出转储到 RStudio 中，并生成了如图 6 所示的集群。R 脚本可以在我的 GitHub 存储库[上找到。或者，您可以编写自己的脚本并相应地进行可视化。](https://github.com/rezacsedu/ScalaAndSparkForBigDataAnalytics)

![](../images/00142.jpeg)

**Figure 6:** Clusters of the neighborhoods

现在，为了更广泛的分析和可见性，我们可以观察每个集群的相关统计数据。例如，下面我分别在*图 8* 和*图 9* 中打印了与集群 3 和集群 4 相关的统计数据:

```scala
newDF.filter("CLUSTER = 0").show() newDF.filter("CLUSTER = 1").show()newDF.filter("CLUSTER = 2").show()newDF.filter("CLUSTER = 3").show()newDF.filter("CLUSTER = 4").show()

```

现在获取每个集群的描述性统计信息，如下所示:

```scala
newDF.filter("CLUSTER = 0").describe().show()newDF.filter("CLUSTER = 1").describe().show()newDF.filter("CLUSTER = 2").describe().show()newDF.filter("CLUSTER = 3").describe().show() newDF.filter("CLUSTER = 4").describe().show()

```

首先，让我们观察下图中集群 3 的相关统计数据:

![](../images/00353.jpeg)

**Figure 7:** Statistics on cluster 3

现在让我们观察下图中集群 4 的相关统计数据:

![](../images/00377.jpeg)

**Figure 8:** Statistics on cluster 4

请注意，由于原始截图太大，无法放入此页面，因此修改了原始图像，并删除了包含房屋其他变量的列。

由于该算法的随机性，每次成功迭代，您可能会收到不同的结果。但是，您可以通过如下设置种子来锁定此算法的随机特性:

```scala
val numClusters = 5 val numIterations = 20 val seed = 12345 val model = KMeans.train(landRDD, numClusters, numIterations, seed)

```

**第八步。停止火花会话** -最后，使用如下停止方法停止火花会话:

```scala
spark.stop()

```

在前面的例子中，我们处理了一组非常小的特征；常识和视觉检查也会让我们得出同样的结论。从上面使用 K-means 算法的例子中，我们可以理解这种算法有一些局限性。例如，预测 K 值真的很难，对于全局集群来说，它不能很好地工作。此外，不同的初始分区会导致不同的最终集群，最后，它不适用于不同大小和密度的集群。

To overcome these limitations, we have some more robust algorithms in this book like MCMC (Markov Chain Monte Carlo; see also at [https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo)) presented in the book: *Tribble*, *Seth D.*, **Markov chain Monte Carlo** algorithms using completely uniformly distributed driving sequences, Diss. Stanford University, 2007.

# 层次聚类

在这一节中，我们讨论了层次聚类技术及其计算挑战。为了更好地理解层次聚类，还将展示一个使用带有 Spark MLlib 的层次聚类的二等分 K-means 算法的例子。

# HC 算法概述及挑战

分层聚类技术在计算距离的方式上不同于基于质心的聚类。这是最流行和最广泛使用的聚类分析技术之一，旨在构建聚类层次结构。由于一个聚类通常由多个对象组成，因此也会有其他候选对象来计算距离。因此，除了通常选择的距离函数之外，您还需要决定要使用的链接标准。简而言之，层次聚类中有两种策略:

*   **自下而上的方法**:在这种方法中，每个观察都从自己的集群中开始。之后，成对的集群被合并在一起，一个集群在层次结构中向上移动。
*   **自上而下的方法**:在这种方法中，所有的观察都从一个集群开始，递归地执行拆分，并且一个在层次结构中向下移动。

这些自下而上或自上而下的方法基于考虑最小物距的 s **单链聚类** ( **SLINK** )技术、**考虑物距最大值的完全链聚类**(**CLING**)和带算术平均值的 u **加权对组方法** ( **UPGMA** )。后者也称为**平均连锁聚类**。从技术上讲，这些方法不会在数据集中产生唯一的分区(即不同的集群)。

A comparative analysis on these three approaches can be found at [https://nlp.stanford.edu/IR-book/completelink.html.](https://nlp.stanford.edu/IR-book/completelink.html)

然而，用户仍然需要从层次结构中选择合适的聚类，以便更好地进行聚类预测和分配。虽然像平分 K-means 这样的算法在计算上比 K-means 算法更快，但是这种算法有三个缺点:

*   首先，这些方法对于异常值或包含噪声或缺失值的数据集不是很稳健。这个缺点要么强加额外的集群，要么甚至导致其他集群合并。这个问题通常被称为连锁现象，特别是对于单链聚类。
*   第二，从算法分析来看，对于聚集聚类和分裂聚类来说，其复杂性使得它们对于大数据集来说太慢。
*   第三，SLINK 和 CLINK 以前作为聚类分析的理论基础广泛用于数据挖掘任务，但现在它们被认为是过时的。

# 用火花 MLlib 平分 K 均值

分割 K 均值通常比常规 K 均值快得多，但它通常会产生不同的聚类。一个二分的 K-means 算法是基于论文**斯坦巴赫**卡利皮*和*库马尔*的文档聚类*技术的比较，并进行修改以适应 Spark MLlib。

平分 K-means 是一种从包含所有数据点的单个聚类开始的分割算法。迭代地，它然后在底层找到所有的可分簇，并使用 K-means 将它们一分为二，直到总共有 K 个叶簇或者没有叶簇可分。之后，同一级别的集群被分组在一起以增加并行性。换句话说，平分 K-means 在计算上比常规的 K-means 算法更快。请注意，如果在底层平分所有可分割的簇导致多于 K 个叶簇，则较大的簇将总是获得更高的优先级。

请注意，如果底层所有可分簇的二等分导致多于 K 个叶簇，则较大的簇将总是获得较高的优先级。火花 MLlib 实现中使用了以下参数:

*   **K** :这是想要的叶簇数。然而，如果在计算过程中没有可分割的叶簇，实际的数目可能会更小。默认值为 4。
*   **最大迭代次数**:这是分割聚类的最大 K 均值迭代次数。默认值为 20。
*   **MinDivisibleClusterSize** :这是最小点数。默认值设置为 1。
*   **种子**:这是一个随机种子，它不允许随机聚类，并试图在每次迭代中提供几乎相似的结果。但是，建议使用长种子值，如 12345 等。

# 使用 Spark MLlib 分割邻域的 K-均值聚类

在上一节中，我们看到了如何将相似的房屋聚集在一起以确定邻域。除了采用不同训练参数的模型训练如下之外，二等分 K 均值也类似于常规 K 均值:

```scala
// Cluster the data into two classes using KMeans val bkm = new BisectingKMeans() .setK(5) // Number of clusters of the similar houses.setMaxIterations(20)// Number of max iteration.setSeed(12345) // Setting seed to disallow randomness val model = bkm.run(landRDD)

```

您应该参考前面的示例，并重复前面的步骤来获取经过训练的数据。现在让我们通过计算 WSSSE 来评估集群，如下所示:

```scala
val WCSSS = model.computeCost(landRDD)println("Within-Cluster Sum of Squares = " + WCSSS) // Less is better    

```

您应该观察以下输出:`Within-Cluster Sum of Squares = 2.096980212594632E11`。现在要进行更多分析，请参考上一节中的步骤 5。

# 基于分布的聚类(DC)

在本节中，我们将讨论基于分布的聚类技术及其计算挑战。为了更好地理解基于分布的聚类，将展示一个使用带有 Spark MLlib 的**高斯混合模型** ( **GMMs** )的例子。

# DC 算法面临的挑战

像 GMM 这样的基于分布的聚类算法是一种期望最大化算法。为了避免过拟合问题，GMM 通常使用固定数量的高斯分布对数据集进行建模。分布是随机初始化的，相关参数也是迭代优化的，以使模型更好地适应训练数据集。这是 GMM 最稳健的特征，有助于模型收敛到局部最优。然而，多次运行该算法可能会产生不同的结果。

换句话说，与二分 K-means 算法和软聚类不同，GMM 针对硬聚类进行了优化，并且为了获得这种类型的聚类，对象通常被分配给高斯分布。GMM 的另一个有利特征是，它通过捕获数据点和属性之间所有必需的相关性和依赖性来生成复杂的集群模型。

从负面来看，GMM 对数据的格式和形状有一些假设，这给我们(即用户)带来了额外的负担。更具体地说，如果不满足以下两个标准，性能会急剧下降:

*   非高斯数据集:GMM 算法假设数据集具有潜在的高斯分布，这是生成分布。然而，许多实际数据集并不满足这一假设，即提供低聚类性能。
*   如果集群没有均匀的大小，小集群很有可能被大集群所控制。

# 高斯混合模型是如何工作的？

使用 GMM 是一种流行的软聚类技术。GMM 试图将所有数据点建模为高斯分布的有限混合；每个点属于每个聚类的概率与聚类相关的统计一起被计算，并且代表一个混合分布:其中所有的点都是从具有自己概率的 *K* 高斯子分布之一中导出的。简而言之，GMM 的功能可以用三步伪代码来描述:

1.  **目标函数:**以期望最大化为框架，计算并最大化对数似然
2.  **EM 算法:**
    *   **E 步:**计算成员的后验概率——即更近的数据点
    *   **M 步:**优化参数。
3.  **分配:**在步骤 e 中执行软分配

从技术上讲，当给定一个统计模型时，使用**最大似然估计** ( **最大似然估计**)来估计该模型的参数(即，当应用于数据集时)。另一方面， **EM** 算法是一个求最大似然的迭代过程。

Since the GMM is an unsupervised algorithm, GMM model depends on the inferred variables. Then EM iteration rotates toward performing the expectation (E) and maximization (M) step.

Spark MLlib 实现使用期望最大化算法从给定的一组数据点归纳出最大似然模型。当前实现使用以下参数:

*   **K** 是对数据点进行聚类所需的聚类数
*   **收敛 Tol** 是对数似然性的最大变化，我们认为在该变化下实现了收敛。
*   **最大迭代次数**是在没有达到收敛点的情况下要执行的最大迭代次数。
*   **InitialModel** 是一个可选的起始点，从这里开始 EM 算法。如果省略此参数，将从数据中构造一个随机起点。

# 使用 GMM 和 Spark MLlib 进行聚类的示例

在前面的部分中，我们看到了如何将相似的房屋聚集在一起以确定邻域。使用 GMM，除了采用如下不同训练参数的模型训练之外，还可以对房屋进行聚类以找到邻域:

```scala
val K = 5 val maxIteration = 20 val model = new GaussianMixture().setK(K)// Number of desired clusters.setMaxIterations(maxIteration)//Maximum iterations.setConvergenceTol(0.05) // Convergence tolerance. .setSeed(12345) // setting seed to disallow randomness.run(landRDD) // fit the model using the training set

```

您应该参考前面的例子，重复前面获取训练数据的步骤。现在为了评估模型的性能，GMM 没有提供任何像 WCSS 这样的性能指标作为成本函数。然而，GMM 提供了一些性能指标，如 mu、sigma 和权重。这些参数表示不同聚类之间的最大似然性(在我们的例子中是五个聚类)。这可以证明如下:

```scala
// output parameters of max-likelihood modelfor (i <- 0 until model.K) {println("Cluster " + i)println("Weight=%f\nMU=%s\nSigma=\n%s\n" format(model.weights(i),   model.gaussians(i).mu, model.gaussians(i).sigma))}

```

您应该观察以下输出:

![](../images/00154.jpeg)

**Figure 9:** Cluster 1 **![](../images/00017.jpeg)

**Figure 10:** Cluster 2 **![](../images/00240.jpeg)

**Figure 11:** Cluster 3 **![](../images/00139.jpeg)

**Figure 12:** Cluster 4 **![](../images/00002.jpeg)

**Figure 13:** Cluster 5 **聚类 1 至 4 的权重表示这些聚类是同质的，并且与聚类 5 相比有显著不同。

# 确定集群数量

像 K-means 算法这样的聚类算法的美妙之处在于，它对具有无限个特征的数据进行聚类。当你有原始数据并且想知道数据中的模式时，这是一个很好的工具。然而，在做实验之前决定簇的数量可能不成功，但有时可能会导致过拟合或欠拟合的问题。另一方面，所有三种算法(即 K-means、平分 K-means 和高斯混合)的一个共同点是，必须预先确定聚类的数量，并将其作为参数提供给算法。因此，非正式地说，确定集群的数量是一个要解决的单独的优化问题。

在本节中，我们将使用基于肘关节方法的启发式方法。我们从 K = 2 个聚类开始，然后通过增加 K 并观察成本函数**的值-聚类内平方和** ( **WCSS** )对同一数据集运行 K-means 算法。在某一点上，可以观察到成本函数的大幅下降，但随着 K 值的增加，这种改善变得微不足道。正如聚类分析文献中所建议的，我们可以选择 WCSS 最后一次大幅下降后的 K 作为最佳值。

通过分析以下参数，您可以了解 K 均值的性能:

*   **介数:**这是平方和之间也称为*团内相似性。*
*   **内部:**这是正方形的内部和，也称为*簇间相似性。*
*   **Totwithinss:** 这是所有簇的所有内部的总和，也称为*总簇内相似度。*

需要注意的是，一个健壮和精确的聚类模型将具有较低的内值和较高的介值。然而，这些值取决于聚类的数量，即在构建模型之前选择的 K。

现在让我们讨论如何利用肘关节方法来确定集群的数量。如下所示，我们计算了成本函数 WCSS，作为基于所有特征应用于家庭数据的 K-means 算法的多个聚类的函数。可以观察到，当 K = 5 时，会出现很大的下降。因此，我们选择集群数量为 5，如图*图 10* 所示。基本上，这是上次大跌之后的一次。

![](../images/00303.jpeg)

**Figure 14:** Number of clusters as a function of WCSS

# 聚类算法的比较分析

高斯混合主要用于期望最小化，这是一个优化算法的例子。平分 K-means 比常规 K-means 更快，也产生稍微不同的聚类结果。下面我们尝试比较这三种算法。我们将展示每个算法在模型构建时间和计算成本方面的性能比较。如下面的代码所示，我们可以用 WCSS 来计算成本。以下代码行可用于计算 K 均值和 **b** 运算算法的 WCSS:

```scala
val WCSSS = model.computeCost(landRDD) // land RDD is the training set println("Within-Cluster Sum of Squares = " + WCSSS) // Less is better 

```

对于本章中使用的数据集，我们得到了以下 WCSS 值:

```scala
Within-Cluster Sum of Squares of Bisecting K-means = 2.096980212594632E11 Within-Cluster Sum of Squares of K-means = 1.455560123603583E12

```

这意味着 K-means 在计算成本方面表现出稍好的性能。不幸的是，对于 GMM 算法，我们没有任何像 WCSS 这样的指标。现在让我们观察这三种算法的建模时间。我们可以在开始模型训练之前启动系统时钟，并在训练结束后立即停止，如下所示(对于 K-means):

```scala
val start = System.currentTimeMillis() val numClusters = 5 val numIterations = 20  val seed = 12345 val runs = 50 val model = KMeans.train(landRDD, numClusters, numIterations, seed) val end = System.currentTimeMillis()println("Model building and prediction time: "+ {end - start} + "ms")

```

对于本章中使用的训练集，我们得到了以下模型构建时间值:

```scala
Model building and prediction time for Bisecting K-means: 2680ms Model building and prediction time for Gaussian Mixture: 2193ms Model building and prediction time for K-means: 3741ms

```

在不同的研究文章中，已经发现二等分 K-means 算法已经被证明能够为数据点产生更好的聚类分配。此外，与 K-means 相比，平分 K-means 可以更好地收敛到全局最小值。另一方面，K-means 陷入了局部极小值。换句话说，使用平分 K-means 算法，我们可以避免 K-means 可能遭受的局部极小值。

请注意，根据机器的硬件配置和数据集的随机特性，您可能会观察到上述参数的不同值。

More details analysis is up to the readers from the theoretical views. Interested readers should also refer to Spark MLlib-based clustering techniques at [https://spark.apache.org/docs/latest/mllib-clustering.html](https://spark.apache.org/docs/latest/mllib-clustering.html) to get more insights.

# 提交火花作业进行聚类分析

本章中显示的示例可以针对更大的数据集进行扩展，以满足不同的目的。您可以用所有必需的依赖项打包所有三个集群算法，并在集群中将它们作为 Spark 作业提交。现在使用以下代码行提交您的 K-means 聚类的 Spark 作业，例如(对其他类使用类似的语法)，针对 Saratoga NY Homes 数据集:

```scala
# Run application as standalone mode on 8 cores SPARK_HOME/bin/spark-submit \   --class org.apache.spark.examples.KMeansDemo \   --master local[8] \   KMeansDemo-0.1-SNAPSHOT-jar-with-dependencies.jar \   Saratoga_NY_Homes.txt# Run on a YARN cluster export HADOOP_CONF_DIR=XXX SPARK_HOME/bin/spark-submit \   --class org.apache.spark.examples.KMeansDemo \   --master yarn \   --deploy-mode cluster \  # can be client for client mode   --executor-memory 20G \   --num-executors 50 \   KMeansDemo-0.1-SNAPSHOT-jar-with-dependencies.jar \   Saratoga_NY_Homes.txt# Run on a Mesos cluster in cluster deploy mode with supervising SPARK_HOME/bin/spark-submit \  --class org.apache.spark.examples.KMeansDemo \  --master mesos://207.184.161.138:7077 \ # Use your IP aadress   --deploy-mode cluster \   --supervise \   --executor-memory 20G \   --total-executor-cores 100 \   KMeansDemo-0.1-SNAPSHOT-jar-with-dependencies.jar \   Saratoga_NY_Homes.txt

```

# 摘要

在这一章中，我们更深入地研究了机器学习，并发现了如何利用机器学习来聚类属于无监督观察数据集的记录。因此，在前面章节的理解基础上，通过一些广泛使用的例子，您学习了将可用数据的监督和非监督技术快速有力地应用于新问题所需的实用知识。我们正在讨论的例子将从 Spark 的角度进行演示。对于任何 K-means、平分 K-means 和高斯混合算法，不能保证该算法如果运行多次会产生相同的聚类。例如，我们观察到，用相同的参数多次运行 K-means 算法在每次运行时都会产生略有不同的结果。

关于 K 均值和高斯混合的性能比较，参见 *Jung。et。al 和聚类分析*讲义。除了 K-means、平分 K-means 和高斯混合，MLlib 还提供了其他三种聚类算法的实现，即 PIC、LDA 和流式 K-means。还有一点值得一提的是，为了微调聚类分析，我们经常需要移除被称为异常值或异常的不需要的数据对象。但是使用基于距离的聚类很难识别这样的数据点。因此，可以使用除欧几里德之外的其他距离度量。然而，这些链接将是一个很好的资源开始:

1.  [https://mapr . com/电子书/spark/08-无监督-异常检测-apache-spark.html](https://mapr.com/ebooks/spark/08-unsupervised-anomaly-detection-apache-spark.html)
2.  [https://github.com/keiraqz/anomaly-detection](https://github.com/keiraqz/anomaly-detection)
3.  [http://www.dcc.fc.up.pt/~ltorgo/Papers/ODCM.pdf](https://mapr.com/ebooks/spark/08-unsupervised-anomaly-detection-apache-spark.html)

在下一章中，我们将更深入地研究如何调整 Spark 应用程序以获得更好的性能。我们将看到一些优化 Spark 应用程序性能的最佳实践。************