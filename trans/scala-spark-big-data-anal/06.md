# 开始与 Spark-REPL 和 RDDs 合作

"All this modern technology just makes people try to do everything at once."

比尔·沃特森

在本章中，您将学习 Spark 的工作原理；然后，将向您介绍 RDDs，即 Apache Spark 背后的基本抽象，您将了解到它们只是公开类似 Scala 的 API 的分布式集合。然后，您将看到如何下载 Spark，以及如何通过 Spark shell 使其在本地运行。

简而言之，本章将涵盖以下主题:

*   深入挖掘 Apache Spark
*   阿帕奇火花安装
*   RDDs 简介
*   使用火花外壳
*   动作和转换
*   贮藏
*   加载和保存数据

# 深入挖掘 Apache Spark

Apache Spark 是一个快速的内存数据处理引擎，具有优雅而富有表现力的开发 API，允许数据工作者高效地执行需要快速交互访问数据集的流式机器学习或 SQL 工作负载。Apache Spark 由 Spark 核心和一组库组成。核心是分布式执行引擎，Java、Scala 和 Python APIs 为分布式应用程序开发提供了一个平台。

建立在核心之上的额外库允许流、SQL、图形处理和机器学习的工作负载。例如，SparkML 是为数据科学设计的，它的抽象使数据科学变得更容易。

为了计划和执行分布式计算，Spark 使用了作业的概念，它使用阶段和任务跨工作节点执行。Spark 由一个驱动程序组成，该驱动程序跨一组工作节点协调执行。驱动程序还负责跟踪所有工作节点以及每个工作节点当前正在执行的工作。

让我们更深入地了解各种组件。关键组件是驱动程序和执行器，它们都是 JVM 进程(Java 进程):

*   **驱动程序**:驱动程序包含应用程序、主程序。如果您使用的是 Spark shell，那么它将成为驱动程序，驱动程序将启动集群中的执行器，并控制任务的执行。
*   **执行器**:接下来是执行器，它们是运行在集群中工作节点上的进程。在执行器内部，运行单独的任务或计算。每个工作节点中可能有一个或多个执行器，同样，每个执行器中可能有多个任务。当驱动程序连接到集群管理器时，集群管理器将资源分配给运行执行器。

The cluster manager could be a standalone cluster manager, YARN, or Mesos.

**集群管理器**负责在形成集群的计算节点之间调度和分配资源。通常，这是通过让一个管理器进程知道并管理一个资源集群，并将资源分配给一个请求进程(如 Spark)来实现的。在接下来的部分中，我们将进一步研究三种不同的集群管理器:独立管理器、纱管理器和介子管理器。

以下是 Spark 在高层次上的工作方式:

![](../images/00292.jpeg)

火花程序的主要入口点叫做`SparkContext`。`SparkContext`位于**驱动程序**组件中，代表与集群的连接，以及运行调度程序和任务分配与编排的代码。

In Spark 2.x, a new variable called `SparkSession` has been introduced. `SparkContext`, `SQLContext`, and `HiveContext` are now member variables of the `SparkSession`.

当您启动**驱动程序**程序时，使用`SparkContext`向集群发出命令，然后**执行器**将执行指令。一旦执行完成，**驱动程序**程序完成作业。此时，您可以发出更多命令并执行更多作业。

The ability to maintain and reuse the `SparkContext` is a key advantage of the Apache Spark architecture, unlike the Hadoop framework where every `MapReduce` job or Hive query or Pig Script starts entire processing from scratch for each task we want to execute that too using expensive disk instead of memory.

`SparkContext`可用于在集群上创建 RDD、累加器和广播变量。每个 JVM/Java 进程只能有一个`SparkContext`处于活动状态。在创建新的之前，您必须`stop()`激活`SparkContext`。

**驱动程序**解析代码，并将字节级代码序列化到要执行的执行器。当我们执行任何计算时，计算实际上将由每个节点使用内存处理在本地完成。

解析代码和计划执行的过程是**驱动程序**过程实现的关键方面。

以下是 Spark **驱动程序**如何协调集群中的计算:

![](../images/00298.jpeg)

**有向无环图** ( **DAG** )是 Spark 框架的秘方。**驱动程序**进程为您试图使用分布式处理框架运行的一段代码创建一组任务。然后，通过与**集群管理器**通信，任务调度器实际上在阶段和任务中执行 DAG，以获得运行执行器的资源。一个 DAG 代表一个作业，一个作业被分割成子集，也称为阶段，每个阶段作为任务执行，每个任务使用一个核心。

下面两张插图显示了一个简单的作业以及如何将 DAG 分成阶段和任务；第一个图显示了作业本身，第二个图显示了作业和任务的各个阶段:

![](../images/00301.jpeg)

下图现在将作业/DAG 分解为阶段和任务:

![](../images/00304.jpeg)

阶段的数量和阶段由什么组成取决于操作的种类。通常，任何转换都会进入与前一个转换相同的阶段，但每个操作(如 reduce 或 shuffle)总是会创建一个新的执行阶段。任务是阶段的一部分，与在执行器上执行操作的内核直接相关。

If you use YARN or Mesos as the cluster manager, you can use dynamic YARN scheduler to increase the number of executors when more work needs to be done, as well as killing idle executors.

因此，驱动程序管理整个执行过程的容错性。驱动程序完成作业后，输出可以写入文件、数据库或控制台。

Remember that the code in the Driver program itself has to be completely serializable including all the variables and objects.
The often seen exception is a not a serializable exception, which is a result of including global variables from outside the block.

因此，驱动程序进程负责整个执行过程，同时监控和管理所使用的资源，如执行器、阶段和任务，确保一切都按计划进行，并从执行器节点或整个执行器节点上的任务故障等故障中恢复。

# 阿帕奇火花安装

Apache Spark 是一个跨平台的框架，只要我们在机器上安装了 Java，它就可以部署在 Linux、Windows 和一台 Mac Machine 上。在本节中，我们将了解如何安装 Apache Spark。

Apache Spark can be downloaded from [http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html)

首先，让我们看看机器上必须具备的先决条件:

*   Java 8+(强制要求，因为所有 Spark 软件都作为 JVM 进程运行)
*   Python 3.4+(可选，仅在您想要使用 PySpark 时使用)
*   R 3.1+(可选，仅当您想使用 SparkR 时使用)
*   Scala 2.11+(可选，仅用于为 Spark 编写程序)

Spark 可以在三种主要部署模式下部署，我们将介绍这三种模式:

*   独立火花
*   纱线上的火花
*   介子上的火花

# 独立火花

Spark standalone 使用内置的调度程序，而不依赖任何外部调度程序，如 Spark 或 Mesos。要以独立模式安装 spark，您必须将 Spark 二进制安装包复制到集群中的所有计算机上。

在独立模式下，客户端可以通过 spark-submit 或 Spark shell 与集群交互。在任一种情况下，驱动程序都与 Spark 主节点通信，以获取工作节点，在这些节点上可以为此应用程序启动执行器。

Multiple clients interacting with the cluster create their own executors on the Worker Nodes. Also, each client will have its own Driver component.

以下是使用主节点和工作节点的 Spark 独立部署:

![](../images/00307.jpeg)

现在，让我们使用 Linux/Mac 以独立模式下载并安装 Spark:

1.  从链接[http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html)下载 Apache Spark:

![](../images/00313.jpeg)

2.  提取本地目录中的包:

```
 tar -xvzf spark-2.2.0-bin-hadoop2.7.tgz

```

3.  将目录更改为新创建的目录:

```
 cd spark-2.2.0-bin-hadoop2.7

```

4.  通过执行以下步骤为`JAVA_HOME`和`SPARK_HOME`设置环境变量:
    1.  `JAVA_HOME`应该是你安装 Java 的地方。在我的苹果终端上，这被设置为:

```
 export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_65.jdk/Contents/Home/

```

```
 export SPARK_HOME= /Users/myuser/spark-2.2.0-bin-hadoop2.7

```

5.  运行 Spark shell，看看这是否有效。如果不起作用，检查`JAVA_HOME`和`SPARK_HOME`环境变量:`./bin/spark-shell`
6.  You will now see the shell as shown in the following:

    ![](../images/00316.jpeg)

7.  您将在最后看到 Scala/ Spark shell，现在您已经准备好与 Spark 集群交互了:

```
 scala>

```

现在，我们有一个连接到运行 Spark 的自动设置本地集群的 Spark-shell。这是在本地机器上启动 Spark 的最快方法。但是，您仍然可以控制工作者/执行者，以及连接到任何集群(独立/纱/介子)。这就是 Spark 的强大之处，使您能够快速地从交互式测试转移到集群上的测试，并随后在大型集群上部署您的作业。无缝集成提供了许多好处，这是使用 Hadoop 和其他技术无法实现的。

You can refer to the official documentation in case you want to understand all the settings [http://spark.apache.org/docs/latest/](http://spark.apache.org/docs/latest/).

有几种方法可以启动 Spark shell，如下面的代码片段所示。我们将在后面的部分看到更多选项，更详细地展示 Spark shell。：

*   本地计算机上的默认 shell 自动选择本地计算机作为主计算机:

```
 ./bin/spark-shell

```

*   本地机器上的默认外壳，指定本地机器为主机器，带有`n`线程:

```
 ./bin/spark-shell --master local[n]

```

*   连接到指定火花主设备的本地机器上的默认外壳:

```
 ./bin/spark-shell --master spark://<IP>:<Port>

```

*   使用客户端模式连接到纱簇的本地机器上的默认外壳:

```
 ./bin/spark-shell --master yarn --deploy-mode client

```

*   使用集群模式连接到纱集群的本地机器上的默认外壳:

```
 ./bin/spark-shell --master yarn --deploy-mode cluster

```

Spark Driver 还有一个 Web UI，可以帮助您了解 Spark 集群的所有内容，正在运行的执行器、作业和任务、环境变量以及缓存。当然，最重要的用途是监控工作。

Launch the Web UI for the local Spark cluster at `http://127.0.0.1:4040/jobs/`

以下是网络用户界面中的“作业”选项卡:

![](../images/00322.jpeg)

以下是显示集群所有执行者的选项卡:

![](../images/00200.jpeg)

# 纱线上的火花

在纱模式下，客户端与纱资源管理器通信，并获得运行火花执行的容器。您可以将其视为专为您部署的迷你 Spark 集群。

Multiple clients interacting with the cluster create their own executors on the cluster nodes (node managers). Also, each client will have its own Driver component.

使用纱线运行时，Spark 可以在纱线客户端模式或纱线集群模式下运行。

# 纱客户端模式

在纱客户端模式下，驱动程序运行在集群之外的节点上(通常是客户端所在的位置)。驱动程序首先联系资源管理器，请求资源来运行 Spark 作业。资源管理器分配一个容器(容器 0)并响应驱动程序。然后，驱动程序在容器零中启动 Spark 应用程序主文件。然后，Spark 应用程序主机在资源管理器分配的容器上创建执行器。纱容器可以在由节点管理器控制的集群中的任何节点上。因此，所有分配都由资源管理器管理。

甚至 Spark 应用程序主也需要与资源管理器对话，以获得后续容器来启动执行器。

以下是 Spark 的纱客户端模式部署:

![](../images/00203.jpeg)

# 纱簇模式

在纱簇模式下，驱动程序运行在簇内的一个节点上(通常是应用程序主节点)。客户端首先联系资源管理器，请求资源来运行 Spark 作业。资源管理器分配一个容器(容器 0)并响应客户端。客户端然后将代码提交给集群，然后在容器 zero 中启动驱动程序和 Spark 应用主程序。驱动程序与应用主程序和 Spark 应用主程序一起运行，然后在资源管理器分配的容器上创建执行器。纱容器可以在由节点管理器控制的集群中的任何节点上。因此，所有分配都由资源管理器管理。

甚至 Spark 应用程序主也需要与资源管理器对话，以获得后续容器来启动执行器。

以下是 Spark 的纱簇模式部署:

![](../images/00206.jpeg)

There is no shell mode in YARN cluster mode, since the Driver itself is running inside YARN.

# 介子上的火花

Mesos 部署类似于 Spark 独立模式，驱动程序与 Mesos 主机通信，然后主机分配运行执行器所需的资源。如在独立模式中所见，驱动程序然后与执行器通信以运行作业。因此，Mesos 部署中的驱动程序首先与主节点对话，然后在所有 Mesos 从节点上确保容器的请求。

当容器被分配给火花作业时，驱动程序启动执行器，然后在执行器中运行代码。当火花作业完成并且驱动程序退出时，会通知 Mesos 主节点，并且 Mesos 从节点上所有容器形式的资源都会被回收。

Multiple clients interacting with the cluster create their own executors on the slave nodes. Also, each client will have its own Driver component. Both client and cluster mode are possible just like YARN mode

以下是 Spark 基于 mesos 的部署，描述了连接到 **Mesos 主节点**的**驱动程序**，该节点还拥有所有 Mesos 从节点上所有资源的集群管理器:

![](../images/00209.jpeg)

# RDDs 简介

一个**弹性分布式数据集** ( **RDD** )是一个不可变的分布式对象集合。Spark RDDs 具有弹性或容错能力，这使得 Spark 能够在出现故障时恢复 RDD。不变性使得 RDDs 一旦创建就成为只读的。转型允许在 RDD 上的操作创建一个新的 RDD，但是最初的 RDD 一旦创建就不会被修改。这使得 RDDs 不受竞争条件和其他同步问题的影响。

RDDs 的分布式特性起作用，因为 RDD 只包含对数据的引用，而实际数据包含在集群中节点的分区中。

Conceptually, a RDD is a distributed collection of elements spread out across multiple nodes in the cluster. We can simplify a RDD to better understand by thinking of a RDD as a large array of integers distributed across machines.

RDD 实际上是一个跨集群分区的数据集，分区数据可能来自 **HDFS** ( **Hadoop 分布式文件系统**)、HBase 表、Cassandra 表、亚马逊 S3。

在内部，每个 RDD 都有五个主要特征:

*   分区列表
*   用于计算每个分割的函数
*   其他关系数据库的依赖列表
*   可选地，键值 RDDs 的分区器(例如，假设 RDD 是散列分区的)
*   (可选)计算每个分割的首选位置列表(例如，HDFS 文件的块位置)

请看下图:

![](../images/00212.jpeg)

在您的程序中，驱动程序将 RDD 对象视为分布式数据的句柄。它类似于指向数据的指针，而不是实际使用的数据，以便在需要时到达实际数据。

默认情况下，RDD 使用哈希分区器在整个集群中对数据进行分区。分区的数量与集群中的节点数量无关。集群中的单个节点很可能有几个数据分区。现有数据分区的数量完全取决于您的集群有多少节点以及数据的大小。如果您查看节点上任务的执行情况，那么在工作节点上的执行器上运行的任务可能正在处理同一本地节点或远程节点上可用的数据。这被称为数据的局部性，执行任务尽可能选择最局部的数据。

The locality affects the performance of your job significantly. The order of preference of locality by default can be shown as
`PROCESS_LOCAL > NODE_LOCAL > NO_PREF > RACK_LOCAL > ANY`

无法保证一个节点可以获得多少分区。这会影响任何执行器的处理效率，因为如果单个节点上有太多的分区处理多个分区，那么处理所有分区所花费的时间也会增加，使执行器上的内核过载，从而减慢整个处理阶段，这直接减慢了整个作业的速度。事实上，分区是提高 Spark 作业性能的主要调整因素之一。请参考以下命令:

```
class RDD[T: ClassTag]

```

让我们进一步研究一下，当我们加载数据时，RDD 会是什么样子。以下是 Spark 如何使用不同的工作人员加载不同的数据分区或拆分的示例:

![](../images/00218.jpeg)

无论 RDD 是如何创建的，最初的 RDD 通常被称为 RDD 基地，而由各种操作创建的任何后续 rdd 都是 rdd 谱系的一部分。这是需要记住的另一个非常重要的方面，因为容错和恢复的秘密是**驱动程序**维护 RDDs 的谱系，并且可以执行该谱系来恢复 RDDs 的任何丢失的块。

下面的示例显示了由于操作而创建的多个关系数据库。我们从拥有 24 个项目的**RDD 基地**开始，衍生出另一个 RDD**carsdd**，它只包含与汽车匹配的项目(3):

![](../images/00227.jpeg)

The number of partitions does not change during such operations, as each executor applies the filter transformation in-memory, generating a new RDD partition corresponding to the original RDD partition.

接下来，我们将看到如何创建 rdd

# RDD 创作

RDD 是 Apache Spark 中使用的基本对象。它们是代表数据集的不可变集合，具有内在的可靠性和故障恢复能力。从本质上讲，关系数据库在任何操作(如转换或操作)下都会创建新的关系数据库。RDDs 还存储用于从故障中恢复的沿袭。我们在前面的章节中也看到了一些关于如何创建关系数据库以及什么样的操作可以应用于关系数据库的细节。

RDD 可以通过几种方式创建:

*   并行化集合
*   从外部源读取数据
*   改造现有的 RDD
*   流式应用编程接口

# 并行化集合

通过在驱动程序内部调用集合上的`parallelize()`，可以实现集合的并行化。当驱动程序尝试并行化集合时，它会将集合拆分为分区，并将数据分区分布在整个集群中。

下面是一个 RDD，使用 SparkContext 和`parallelize()`函数从一系列数字中创建一个 RDD。`parallelize()`函数本质上是将数字序列分割成一个分布式集合，也称为 RDD。

```
scala> val rdd_one = sc.parallelize(Seq(1,2,3))rdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24scala> rdd_one.take(10)res0: Array[Int] = Array(1, 2, 3)

```

# 从外部源读取数据

创建 RDD 的第二种方法是从外部分布式源读取数据，如亚马逊 S3、卡珊德拉、HDFS 等。例如，如果您正在从 HDFS 创建一个 RDD，那么 HDFS 的分布式数据块将全部由 Spark 集群中的单个节点读取。

Spark 集群中的每个节点本质上都在执行自己的输入输出操作，每个节点都独立地从 HDFS 块中读取一个或多个块。总的来说，Spark 尽最大努力将尽可能多的 RDD 放入记忆中。通过使火花簇中的节点能够避免重复的读取操作，例如来自可能远离火花簇的 HDFS 块的读取操作，能够`cache`数据以减少输入输出操作。在您的 Spark 程序中有一大堆可以使用的缓存策略，我们将在后面的缓存部分中研究这些策略。

以下是使用火花上下文和`textFile()`函数从文本文件加载文本行的 RDD。`textFile`函数将输入数据加载为文本文件(每一个换行符`\n`终止的部分成为 RDD 中的一个元素)。该函数调用还自动使用 HadoopRDD(在下一章中显示)来检测数据，并根据需要以几个分区的形式加载数据，这些分区分布在集群中。

```
scala> val rdd_two = sc.textFile("wiki1.txt")rdd_two: org.apache.spark.rdd.RDD[String] = wiki1.txt MapPartitionsRDD[8] at textFile at <console>:24scala> rdd_two.countres6: Long = 9scala> rdd_two.firstres7: String = Apache Spark provides programmers with an application programming interface centered on a data structure called the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way.

```

# 改造现有的 RDD

rdd 本质上是不可变的；因此，您的关系数据库可以通过对任何现有的 RDD 应用转换来创建。过滤器是转换的一个典型例子。

下面是一个简单的整数`rdd`和每个整数乘以`2`的变换。同样，我们使用`SparkContext`和并行化函数，通过以分区的形式分布序列，将整数序列创建成 RDD。然后，我们使用`map()`函数将每个数字乘以`2`，将 RDD 变成另一个 RDD。

```
scala> val rdd_one = sc.parallelize(Seq(1,2,3))rdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24scala> rdd_one.take(10)res0: Array[Int] = Array(1, 2, 3)scala> val rdd_one_x2 = rdd_one.map(i => i * 2)rdd_one_x2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[9] at map at <console>:26scala> rdd_one_x2.take(10)res9: Array[Int] = Array(2, 4, 6)

```

# 流式应用编程接口

rdd 也可以通过火花流创建。这些关系数据库被称为离散流关系数据库。

我们将在[第 9 章](09.html#8IL201-21aec46d8593429cacea59dbdcd64e1c)、*流我，斯科特-火花流*中进一步探讨这一点。

在下一节中，我们将创建 rdd，并探索使用 Spark-Shell 的一些操作。

# 使用火花外壳

Spark shell 提供了一种对数据进行交互式分析的简单方法。它还使您能够通过快速试用各种 API 来学习 Spark APIs。此外，与 Scala 外壳的相似性和对 Scala APIs 的支持也让您能够快速适应 Scala 语言结构，并更好地利用 Spark APIs。

Spark shell implements the concept of **read-evaluate-print-loop** (**REPL**), which allows you to interact with the shell by typing in code which is evaluated. The result is then printed on the console, without needing to be compiled, so building executable code.

通过在安装 Spark 的目录中运行以下命令来启动它:

```
./bin/spark-shell

```

火花壳启动，火花壳自动创建`SparkSession`和`SparkContext`对象。`SparkSession`作为星火提供，`SparkContext`作为 sc 提供。

`spark-shell`可以用几个选项启动，如下图所示(最重要的是粗体):

```
./bin/spark-shell --helpUsage: ./bin/spark-shell [options]Options:--master MASTER_URL spark://host:port, mesos://host:port, yarn, or local.--deploy-mode DEPLOY_MODE Whether to launch the driver program locally ("client") oron one of the worker machines inside the cluster ("cluster")(Default: client).--class CLASS_NAME Your application's main class (for Java / Scala apps).--name NAME A name of your application.--jars JARS Comma-separated list of local jars to include on the driverand executor classpaths.--packages Comma-separated list of maven coordinates of jars to includeon the driver and executor classpaths. Will search the localmaven repo, then maven central and any additional remoterepositories given by --repositories. The format for thecoordinates should be groupId:artifactId:version.--exclude-packages Comma-separated list of groupId:artifactId, to exclude whileresolving the dependencies provided in --packages to avoiddependency conflicts.--repositories Comma-separated list of additional remote repositories tosearch for the maven coordinates given with --packages.--py-files PY_FILES Comma-separated list of .zip, .egg, or .py files to placeon the PYTHONPATH for Python apps.--files FILES Comma-separated list of files to be placed in the workingdirectory of each executor.--conf PROP=VALUE Arbitrary Spark configuration property.--properties-file FILE Path to a file from which to load extra properties. If notspecified, this will look for conf/spark-defaults.conf.--driver-memory MEM Memory for driver (e.g. 1000M, 2G) (Default: 1024M).--driver-Java-options Extra Java options to pass to the driver.--driver-library-path Extra library path entries to pass to the driver.--driver-class-path Extra class path entries to pass to the driver. Note thatjars added with --jars are automatically included in theclasspath.--executor-memory MEM Memory per executor (e.g. 1000M, 2G) (Default: 1G).--proxy-user NAME User to impersonate when submitting the application.This argument does not work with --principal / --keytab.--help, -h Show this help message and exit.--verbose, -v Print additional debug output.--version, Print the version of current Spark.Spark standalone with cluster deploy mode only:--driver-cores NUM Cores for driver (Default: 1).Spark standalone or Mesos with cluster deploy mode only:--supervise If given, restarts the driver on failure.--kill SUBMISSION_ID If given, kills the driver specified.--status SUBMISSION_ID If given, requests the status of the driver specified.Spark standalone and Mesos only:--total-executor-cores NUM Total cores for all executors.Spark standalone and YARN only:--executor-cores NUM Number of cores per executor. (Default: 1 in YARN mode,or all available cores on the worker in standalone mode)YARN-only:--driver-cores NUM Number of cores used by the driver, only in cluster mode(Default: 1).--queue QUEUE_NAME The YARN queue to submit to (Default: "default").--num-executors NUM Number of executors to launch (Default: 2).If dynamic allocation is enabled, the initial number ofexecutors will be at least NUM.--archives ARCHIVES Comma separated list of archives to be extracted into theworking directory of each executor.--principal PRINCIPAL Principal to be used to login to KDC, while running onsecure HDFS.--keytab KEYTAB The full path to the file that contains the keytab for theprincipal specified above. This keytab will be copied tothe node running the Application Master via the SecureDistributed Cache, for renewing the login tickets and thedelegation tokens periodically.

```

您还可以以可执行 Java jars 的形式提交 Spark 代码，以便在集群中执行作业。通常，一旦您使用 shell 找到了可行的解决方案，您就会这样做。

Use `./bin/spark-submit` when submitting a Spark job to a cluster (local, YARN, and Mesos).

以下是 Shell 命令(最重要的命令以粗体显示):

```
scala> :helpAll commands can be abbreviated, e.g., :he instead of :help.:edit <id>|<line> edit history:help [command] print this summary or command-specific help:history [num] show the history (optional num is commands to show):h? <string> search the history:imports [name name ...] show import history, identifying sources of names:implicits [-v] show the implicits in scope:javap <path|class> disassemble a file or class name:line <id>|<line> place line(s) at the end of history:load <path> interpret lines in a file:paste [-raw] [path] enter paste mode or paste a file:power enable power user mode:quit exit the interpreter:replay [options] reset the repl and replay all previous commands:require <path> add a jar to the classpath:reset [options] reset the repl to its initial state, forgetting all session entries:save <path> save replayable session to a file:sh <command line> run a shell command (result is implicitly => List[String]):settings <options> update compiler options, if possible; see reset:silent disable/enable automatic printing of results:type [-v] <expr> display the type of an expression without evaluating it:kind [-v] <expr> display the kind of expression's type:warnings show the suppressed warnings from the most recent line which had any

```

使用火花壳，我们现在将加载一些数据作为 RDD:

```
scala> val rdd_one = sc.parallelize(Seq(1,2,3))rdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24scala> rdd_one.take(10)res0: Array[Int] = Array(1, 2, 3)

```

如您所见，我们正在一个接一个地运行命令。或者，我们也可以粘贴命令:

```
scala> :paste// Entering paste mode (ctrl-D to finish)val rdd_one = sc.parallelize(Seq(1,2,3))rdd_one.take(10)// Exiting paste mode, now interpreting.rdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[10] at parallelize at <console>:26res10: Array[Int] = Array(1, 2, 3)

```

在下一节中，我们将深入探讨操作。

# 动作和转换

RDD 是不可改变的，每一次操作都会创造一个新的 RDD。现在，您可以在 RDD 上执行的两个主要操作是**变换**和**动作**。

**变换**改变 RDD 中的元素，例如分割输入元素、过滤掉元素以及执行某种计算。可以按顺序执行几个转换；然而，在计划期间没有执行。

For transformations, Spark adds them to a DAG of computation and, only when driver requests some data, does this DAG actually gets executed. This is called *lazy* evaluation.

惰性评估背后的推理是，Spark 可以查看所有的转换并计划执行，利用驱动程序对所有操作的理解。例如，如果在某个其他转换之后立即应用过滤器转换，Spark 将优化执行，以便每个 Executor 在数据的每个分区上高效地执行转换。现在，只有当 Spark 在等待需要执行的事情时，这才是可能的。

**动作**是实际触发计算的操作。在遇到操作之前，spark 程序中的执行计划是以 DAG 的形式创建的，不做任何事情。显然，在执行计划中可能会有各种各样的转换，但是在您执行一个操作之前什么都不会发生。

以下是对一些任意数据的各种操作的描述，我们只是想移除所有的笔和自行车，只计算汽车**。**每个打印语句都是一个动作，它触发基于 DAG 的执行计划中所有转换步骤的执行，直到如下图所示:

![](../images/00230.jpeg)

例如，转换的有向无环图上的动作计数触发转换的执行，一直到基本 RDD。如果执行了另一个操作，那么可能会发生新的执行链。这清楚地说明了为什么在有向无环图的不同阶段进行的任何缓存都会大大加快程序的下一次执行。优化执行的另一种方法是通过重用前一次执行的无序播放文件。

另一个示例是收集操作，该操作从所有节点收集所有数据或将所有数据拉至驱动程序。您可以在调用 collect 时使用分部函数来有选择地提取数据。

# 转换

**变换**通过对现有 RDD 中的每个元素应用变换逻辑，从现有 RDD 创建新 RDD。一些转换函数涉及拆分元素、过滤掉元素以及执行某种计算。可以按顺序执行几个转换。然而，在计划期间没有执行。

转换可以分为四类，如下所示。

# 一般变换

**通用转换**是处理大多数通用用例的转换函数，将转换逻辑应用于现有的关系数据库，并生成新的 RDD。聚合、过滤等常见操作都被称为一般转换。

一般转换函数的例子有:

*   `map`
*   `filter`
*   `flatMap`
*   `groupByKey`
*   `sortByKey`
*   `combineByKey`

# 数学/统计转换

数学或统计变换是处理一些统计功能的变换函数，通常对现有的关系数据库进行一些数学或统计运算，生成新的 RDD。采样就是一个很好的例子，经常在 Spark 程序中使用。

这种转换的例子有:

*   `sampleByKey`
*   ``randomSplit``

# 集合论/关系变换

集合论/关系转换是转换函数，它处理像数据集连接这样的转换和其他关系代数功能，如`cogroup`。这些功能通过将转换逻辑应用于现有的关系数据库并生成新的 RDD 来发挥作用。

这种转换的例子有:

*   `cogroup`
*   `join`
*   `subtractByKey`
*   `fullOuterJoin`
*   `leftOuterJoin`
*   `rightOuterJoin`

# 基于数据结构的转换

基于数据结构的转换是对 RDD 的底层数据结构(RDD 的分区)进行操作的转换函数。在这些函数中，您可以直接处理分区，而无需直接接触 RDD 内部的元素/数据。除了简单的程序之外，这些在任何 Spark 程序中都是必不可少的，在简单的程序中，您需要更多地控制分区和分区在集群中的分布。通常，可以通过根据集群状态和数据大小以及确切的用例需求重新分配数据分区来实现性能的提高。

这种转换的例子有:

*   `partitionBy`
*   `repartition`
*   `zipwithIndex`
*   `coalesce`

以下是最新的 Spark 2.1.1 中可用的转换函数列表:

| 转换 | 意义 |
| `map(func)` | 返回通过函数`func`传递源的每个元素而形成的新的分布式数据集。 |
| `filter(func)` | 返回通过选择 func 返回 true 的源元素而形成的新数据集。 |
| `flatMap(func)` | 类似于 map，但是每个输入项可以映射到 0 个或更多的输出项(所以`func`应该返回一个`Seq`而不是单个项)。 |
| `mapPartitions(func)` | 类似于地图，但在 RDD 的每个分区(区块)上分别运行，因此当在类型为`T`的 RDD 上运行时，`func`必须是类型为`Iterator<T> => Iterator<U>`。 |
| `mapPartitionsWithIndex(func)` | 类似于`mapPartitions`，但也为`func`提供了一个代表分区索引的整数值，所以在`T`类型的 RDD 上运行时，`func`必须是`(Int, Iterator<T>) => Iterator<U>`类型。 |
| `sample(withReplacement, fraction, seed)` | 使用给定的随机数生成器种子，对数据的一小部分进行采样，无论有无替换。 |
| `union(otherDataset)` | 返回一个新数据集，该数据集包含源数据集中的元素和参数的并集。 |
| `intersection(otherDataset)` | 返回一个新的 RDD，其中包含源数据集中元素和参数的交集。 |
| `distinct([numTasks]))` | 返回包含源数据集的不同元素的新数据集。 |
| `groupByKey([numTasks])` | 当在`(K, V)`对的数据集上调用时，返回一个`(K, Iterable<V>)`对的数据集。注意:如果为了对每个键执行聚合(如求和或求平均值)而进行分组，使用`reduceByKey`或`aggregateByKey`会产生更好的性能。注意:默认情况下，输出中的并行级别取决于父 RDD 的分区数量。您可以通过可选的`numTasks`参数来设置不同数量的任务。 |
| reduceByKey(func，[numTasks]) | 当在由`(K, V)`对组成的数据集上调用时，返回一个由`(K, V)`对组成的数据集，其中每个键的值使用给定的`reduce`函数`func`进行聚合，该函数的类型必须为`(V,V) => V`。与`groupByKey`一样，减少任务的数量可以通过可选的第二个参数进行配置。 |
| `aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])` | 当在由`(K, V)`对组成的数据集上调用时，返回一个由`(K, U)`对组成的数据集，其中每个键的值使用给定的组合函数和一个中性的*零*值进行聚合。允许不同于输入值类型的聚合值类型，同时避免不必要的分配。如同在`groupByKey`中一样，减少任务的数量可以通过可选的第二个参数来配置。 |
| `sortByKey([ascending], [numTasks])` | 当在`K`实现有序的`(K, V)`对的数据集上调用时，按照布尔升序参数中的指定，返回按键升序或降序排序的`(K, V)`对的数据集。 |
| `join(otherDataset, [numTasks])` | 当在类型为`(K, V)`和`(K, W)`的数据集上调用时，返回一个包含每个键的所有元素对的`(K, (V, W))`对的数据集。通过`leftOuterJoin`、`rightOuterJoin`和`fullOuterJoin`支持外部连接。 |
| `cogroup(otherDataset, [numTasks])` | 当在类型为`(K, V)`和`(K, W)`的数据集上调用时，返回一个包含`(K, (Iterable<V>, Iterable<W>))`元组的数据集。这个操作也叫`groupWith`。 |
| `cartesian(otherDataset)` | 当在类型为`T`和`U`的数据集上调用时，返回一个由`(T, U)`对(所有元素对)组成的数据集。 |
| `pipe(command, [envVars])` | 通过一个 shell 命令(例如，一个 Perl 或 bash 脚本)来传输 RDD 的每个分区。RDD 元素被写入进程的`stdin`，输出到其`stdout`的行作为字符串的 RDD 返回。 |
| `coalesce(numPartitions)` | 将 RDD 的分区数量减少到`numPartitions`。用于在筛选大型数据集后更有效地运行操作。 |
| `repartition(numPartitions)` | 随机重组 RDD 中的数据，以创建更多或更少的分区，并在它们之间进行平衡。这总是会打乱网络上的所有数据。 |
| `repartitionAndSortWithinPartitions(partitioner)` | 根据给定的分区器对 RDD 进行重新分区，并在每个结果分区内，根据记录的关键字对记录进行排序。这比调用`repartition`然后在每个分区内进行排序更有效，因为它可以将排序下推到洗牌机器中。 |

我们将说明最常见的转换:

# 地图功能

`map`对输入分区应用变换函数，在输出 RDD 中生成输出分区。

如下面的代码片段所示，这就是我们如何将文本文件的 RDD 映射到具有文本行长度的 RDD:

```
scala> val rdd_two = sc.textFile("wiki1.txt")rdd_two: org.apache.spark.rdd.RDD[String] = wiki1.txt MapPartitionsRDD[8] at textFile at <console>:24scala> rdd_two.countres6: Long = 9scala> rdd_two.firstres7: String = Apache Spark provides programmers with an application programming interface centered on a data structure called the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way.scala> val rdd_three = rdd_two.map(line => line.length)res12: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[11] at map at <console>:2scala> rdd_three.take(10)res13: Array[Int] = Array(271, 165, 146, 138, 231, 159, 159, 410, 281)

```

下图解释了`map()`的工作原理。您可以看到，RDD 的每个分区都会在新的 RDD 中产生一个新的分区，本质上将转换应用于 RDD 的所有元素:

![](../images/00236.jpeg)

# 平面地图功能

`flatMap()`对输入分区应用变换函数，生成输出 RDD 中的输出分区，就像`map()`函数一样。然而，`flatMap()`也展平了输入 RDD 元素中的任何集合。

如下面的片段所示，我们可以在文本文件的 RDD 上使用`flatMap()`将文本中的行转换为包含单个单词的 RDD。我们还展示了`map()`之前对同一个 RDD 的称呼`flatMap()`的称呼只是为了展示行为上的差异:

```
scala> val rdd_two = sc.textFile("wiki1.txt")rdd_two: org.apache.spark.rdd.RDD[String] = wiki1.txt MapPartitionsRDD[8] at textFile at <console>:24scala> rdd_two.countres6: Long = 9scala> rdd_two.firstres7: String = Apache Spark provides programmers with an application programming interface centered on a data structure called the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way.scala> val rdd_three = rdd_two.map(line => line.split(" "))rdd_three: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[16] at map at <console>:26scala> rdd_three.take(1)res18: Array[Array[String]] = Array(Array(Apache, Spark, provides, programmers, with, an, application, programming, interface, centered, on, a, data, structure, called, the, resilient, distributed, dataset, (RDD),, a, read-only, multiset, of, data, items, distributed, over, a, cluster, of, machines,, that, is, maintained, in, a, fault-tolerant, way.)scala> val rdd_three = rdd_two.flatMap(line => line.split(" "))rdd_three: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[17] at flatMap at <console>:26scala> rdd_three.take(10)res19: Array[String] = Array(Apache, Spark, provides, programmers, with, an, application, programming, interface, centered)

```

下图解释了`flatMap()`的工作原理。您可以看到，RDD 的每个分区都会在新 RDD 中产生一个新分区，本质上是将转换应用于 RDD 的所有元素:

![](../images/00239.jpeg)

# 滤波函数

`filter`对输入分区应用变换函数，在输出 RDD 中生成过滤后的输出分区。

下面的代码片段展示了我们如何将文本文件的 RDD 过滤成只有包含单词`Spark`的行的 RDD:

```
scala> val rdd_two = sc.textFile("wiki1.txt")rdd_two: org.apache.spark.rdd.RDD[String] = wiki1.txt MapPartitionsRDD[8] at textFile at <console>:24scala> rdd_two.countres6: Long = 9scala> rdd_two.firstres7: String = Apache Spark provides programmers with an application programming interface centered on a data structure called the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way.scala> val rdd_three = rdd_two.filter(line => line.contains("Spark"))rdd_three: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[20] at filter at <console>:26scala>rdd_three.countres20: Long = 5

```

下图解释了`filter`的工作原理。您可以看到，RDD 的每个分区都会在新 RDD 中产生一个新分区，本质上是对 RDD 的所有元素应用滤镜变换。

Note that the partitions do not change, and some partitions could be empty too, when applying filter

![](../images/00242.jpeg)

# 联合

`coalesce`将`transformation`函数应用于输入分区，以将输入分区组合成输出 RDD 中较少的分区。

如下面的代码片段所示，这就是我们如何将所有分区组合成一个分区:

```
scala> val rdd_two = sc.textFile("wiki1.txt")rdd_two: org.apache.spark.rdd.RDD[String] = wiki1.txt MapPartitionsRDD[8] at textFile at <console>:24scala> rdd_two.partitions.lengthres21: Int = 2scala> val rdd_three = rdd_two.coalesce(1)rdd_three: org.apache.spark.rdd.RDD[String] = CoalescedRDD[21] at coalesce at <console>:26scala> rdd_three.partitions.lengthres22: Int = 1

```

下图解释了`coalesce`的工作原理。您可以看到，一个新的 RDD 是从原来的 RDD 创建的，通过根据需要组合分区来减少分区数量:

![](../images/00248.jpeg)

# 再分

`repartition`应用`transformation`功能将输入分区`repartition`输入到输出 RDD 中更少或更多的输出分区。

如下面的代码片段所示，这是我们如何将文本文件的 RDD 映射到具有更多分区的 RDD:

```
scala> val rdd_two = sc.textFile("wiki1.txt")rdd_two: org.apache.spark.rdd.RDD[String] = wiki1.txt MapPartitionsRDD[8] at textFile at <console>:24scala> rdd_two.partitions.lengthres21: Int = 2scala> val rdd_three = rdd_two.repartition(5)rdd_three: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[25] at repartition at <console>:26scala> rdd_three.partitions.lengthres23: Int = 5

```

下图解释了`repartition`的工作原理。您可以看到，一个新的 RDD 是从原来的 RDD 创建的，本质上是通过根据需要组合/拆分分区来重新分配分区:

![](../images/00254.jpeg)

# 行动

动作触发整个 **DAG** ( **有向无环图**)的转换，这些转换是通过运行代码块和函数来实现的。所有操作现在都按照 DAG 指定的方式执行。

有两种操作操作:

*   **驾驶员**:一种动作是收集计数、按键计数等驾驶员动作。每个这样的操作都会在远程执行器上执行一些计算，并将数据拉回到驱动程序中。

Driver-based action has the problem that actions on large datasets can easily overwhelm the memory available on the driver taking down the application, so you should use the driver involved actions judiciously

*   **分布式**:另一种动作是分布式动作，在集群中的节点上执行。这种分布式动作的一个例子是`saveAsTextfile`。由于操作的分布式特性，这是最常见的操作。

以下是最新的 Spark 2.1.1 中可用的操作功能列表:

| 行动 | 意义 |
| `reduce(func)` | 使用函数`func`聚合数据集的元素(该函数接受两个参数并返回一个)。该函数应该是可交换的和关联的，以便能够正确地并行计算。 |
| `collect()` | 在驱动程序中将数据集的所有元素作为数组返回。这通常在返回足够小的数据子集的过滤器或其他操作之后有用。 |
| `count()` | 返回数据集中的元素数量。 |
| `first()` | 返回数据集的第一个元素(类似于`take(1)`)。 |
| `take(n)` | 返回数据集第一个`n` lemon 的数组。 |
| `takeSample(withReplacement, num, [seed])` | 返回一个带有数据集`num`元素随机样本的数组，有或没有替换，可选地预先指定一个随机数生成器种子。 |
| `takeOrdered(n, [ordering])` | 使用自然顺序或自定义比较器返回 RDD 的第一个`n`元素。 |
| `saveAsTextFile(path)` | 将数据集的元素作为文本文件(或文本文件集)写入本地文件系统、HDFS 或任何其他 Hadoop 支持的文件系统中的给定目录。Spark 会在每个元素上调用`toString`将其转换为文件中的一行文本。 |
| `saveAsSequenceFile(path)` (Java 和 Scala) | 将数据集的元素作为 Hadoop 序列写入本地文件系统、HDFS 或任何其他 Hadoop 支持的文件系统中给定路径的文件中。这在实现 Hadoop`Writable`接口的键值对的 RDD 上是可用的。在 Scala 中，它也适用于隐式转换为`Writable`的类型(Spark 包括像`Int`、`Double`、`String`等基本类型的转换)。 |
| `saveAsObjectFile(path)` (Java 和 Scala) | 使用 Java 序列化以简单的格式编写数据集的元素，然后可以使用`SparkContext.objectFile()`加载。 |
| `countByKey()` | 仅在`(K, V)`类型的 rdd 上可用。返回带有每个键的计数的`(K, Int)`对的散列表。 |
| `foreach(func)` | 对数据集的每个元素运行一个函数`func`。这通常是为了避免副作用，例如更新累加器或与外部存储系统交互。注意:在`foreach()`之外修改累加器以外的变量可能会导致未定义的行为。更多详情请参见理解闭包([http://spark . Apache . org/docs/latest/programming-guide . html #理解-闭包-a-nameclosurelinka](http://spark.apache.org/docs/latest/programming-guide.html#understanding-closures-a-nameclosureslinka))[。](http://spark.apache.org/docs/latest/programming-guide.html#understanding-closures-a-nameclosureslinka) |

# 减少

`reduce()`将减少功能应用于 RDD 中的所有元素，并将其发送给驾驶员。

下面是示例代码来说明这一点。您可以使用`SparkContext`和并行化函数从整数序列创建 RDD。然后，您可以使用 RDD 上的`reduce`功能将 RDD 的所有数字相加。

Since this is an action, the results are printed as soon as you run the `reduce` function.

下面显示的代码是从一个小的数字数组构建一个简单的 RDD，然后在 RDD 上执行一个约简操作:

```
scala> val rdd_one = sc.parallelize(Seq(1,2,3,4,5,6))rdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[26] at parallelize at <console>:24scala> rdd_one.take(10)res28: Array[Int] = Array(1, 2, 3, 4, 5, 6)scala> rdd_one.reduce((a,b) => a +b)res29: Int = 21

```

下图是`reduce()`的图解。驱动程序在执行器上运行 reduce 函数，并最终收集结果。

![](../images/00257.jpeg)

# 数数

`count()`简单地计算 RDD 中的元素数量，并将其发送给驱动程序。

下面是这个函数的一个例子。我们使用 SparkContext 和并行化函数从整数序列中创建了一个 RDD，然后调用 count on the RDD 来打印 RDD 的元素数量。

```
scala> val rdd_one = sc.parallelize(Seq(1,2,3,4,5,6))rdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[26] at parallelize at <console>:24scala> rdd_one.countres24: Long = 6

```

以下是`count()`的图解。驱动程序要求每个执行器/任务计算任务正在处理的分区中的元素数量，然后在驱动程序级别将所有任务的数量加在一起。

![](../images/00260.jpeg)

# 收集

`collect()`简单地收集 RDD 的所有元素并将其发送给驱动程序。

这里显示了一个示例，展示了 collect 函数的基本功能。当您在 RDD 上呼叫对方付费时，司机通过将 RDD 的所有元素拉入司机中来收集它们。

Calling collect on large RDDs will cause out-of-memory issues on the Driver.

下面显示的是收集和显示 RDD 内容的代码:

```
scala> rdd_two.collectres25: Array[String] = Array(Apache Spark provides programmers with an application programming interface centered on a data structure called the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way., It was developed in response to limitations in the MapReduce cluster computing paradigm, which forces a particular linear dataflow structure on distributed programs., "MapReduce programs read input data from disk, map a function across the data, reduce the results of the map, and store reduction results on disk. ", Spark's RDDs function as a working set for distributed programs that offers a (deliberately) restricted form of distributed shared memory., The availability of RDDs facilitates t...

```

以下是`collect()`的图解。使用收集，驱动程序从所有分区中提取 RDD 的所有元素。

![](../images/00027.jpeg)

# 贮藏

缓存使 Spark 能够跨计算和操作保存数据。事实上，这是 Spark 中加快计算速度的最重要的技术之一，尤其是在处理迭代计算时。

缓存的工作原理是将 RDD 尽可能多地存储在内存中。如果没有足够的内存，那么按照 LRU 的策略，存储中的当前数据将被逐出。如果要求缓存的数据大于可用内存，性能将会下降，因为将使用磁盘而不是内存。

您可以使用`persist()`或`cache()`将 RDD 标记为缓存

`cache()` is simply a synonym for persist`(MEMORY_ONLY)`

`persist`可以使用内存或磁盘或两者:

```
persist(newLevel: StorageLevel) 

```

以下是存储级别的可能值:

| 存储级别 | 意义 |
| `MEMORY_ONLY` | 将 RDD 作为反序列化的 Java 对象存储在 JVM 中。如果 RDD 不适合内存，一些分区将不会被缓存，并且会在每次需要时被重新计算。这是默认级别。 |
| `MEMORY_AND_DISK` | 将 RDD 作为反序列化的 Java 对象存储在 JVM 中。如果 RDD 不适合内存，将不适合的分区存储在磁盘上，并在需要时从那里读取它们。 |
| `MEMORY_ONLY_SER` (Java 和 Scala) | 将 RDD 存储为序列化的 Java 对象(每个分区一个字节数组)。这通常比反序列化的对象更节省空间，尤其是在使用快速序列化程序时，但读取起来更耗费 CPU。 |
| `MEMORY_AND_DISK_SER` (Java 和 Scala) | 类似于`MEMORY_ONLY_SER`，但是将不适合内存的分区溢出到磁盘，而不是在每次需要的时候动态地重新计算它们。 |
| `DISK_ONLY` | 仅将 RDD 分区存储在磁盘上。 |
| `MEMORY_ONLY_2`、`MEMORY_AND_DISK_2`等等。 | 与前面的级别相同，但在两个群集节点上复制每个分区。 |
| `OFF_HEAP`(实验) | 类似于`MEMORY_ONLY_SER`，但是将数据存储在堆外内存中。这需要启用堆外内存。 |

存储级别的选择取决于具体情况

*   如果 RDDs 适合内存，使用`MEMORY_ONLY`作为执行性能的最快选项
*   尝试`MEMORY_ONLY_SER`是否有可序列化的对象被使用，以使对象更小
*   `DISK`除非你的计算很昂贵，否则不应该使用。
*   如果可以腾出所需的额外内存，请使用复制存储以获得最佳容错能力。这将防止为获得最佳可用性而重新计算丢失的分区。

`unpersist()` simply frees up the cached content.

以下是如何使用不同类型的存储器(内存或磁盘)调用`persist()`函数的示例:

```
scala> import org.apache.spark.storage.StorageLevelimport org.apache.spark.storage.StorageLevelscala> rdd_one.persist(StorageLevel.MEMORY_ONLY)res37: rdd_one.type = ParallelCollectionRDD[26] at parallelize at <console>:24scala> rdd_one.unpersist()res39: rdd_one.type = ParallelCollectionRDD[26] at parallelize at <console>:24scala> rdd_one.persist(StorageLevel.DISK_ONLY)res40: rdd_one.type = ParallelCollectionRDD[26] at parallelize at <console>:24scala> rdd_one.unpersist()res41: rdd_one.type = ParallelCollectionRDD[26] at parallelize at <console>:24

```

下面举例说明了我们通过缓存获得的性能提升。

首先，我们将运行代码:

```
scala> val rdd_one = sc.parallelize(Seq(1,2,3,4,5,6))rdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24scala> rdd_one.countres0: Long = 6scala> rdd_one.cacheres1: rdd_one.type = ParallelCollectionRDD[0] at parallelize at <console>:24scala> rdd_one.countres2: Long = 6

```

您可以使用网络界面来查看实现的改进，如以下截图所示:

![](../images/00052.jpeg)

# 加载和保存数据

将数据加载到 RDD 和将 RDD 保存到输出系统都支持几种不同的方法。我们将在这一部分讨论最常见的问题。

# 正在加载数据

使用`SparkContext`可以将数据载入 RDD。一些最常见的方法是:。

*   `textFile`
*   `wholeTextFiles`
*   `load`来自 JDBC 数据源

# 文本文件

`textFile()`可用于将文本文件加载到 RDD 中，每一行都成为 RDD 中的一个元素。

```
sc.textFile(name, minPartitions=None, use_unicode=True)

```

以下是使用`textFile()`将`textfile`载入 RDD 的示例:

```
scala> val rdd_two = sc.textFile("wiki1.txt")rdd_two: org.apache.spark.rdd.RDD[String] = wiki1.txt MapPartitionsRDD[8] at textFile at <console>:24scala> rdd_two.countres6: Long = 9

```

# wholatextfiles

`wholeTextFiles()`可用于将多个文本文件加载到成对的 RDD 文件中，该文件包含代表文件名和文件全部内容的对`<filename, textOfFile>`。这在加载多个小文本文件时很有用，并且与`textFile` API 不同，因为当使用整体`TextFiles()`时，文件的全部内容作为单个记录加载:

```
sc.wholeTextFiles(path, minPartitions=None, use_unicode=True)

```

以下是使用`wholeTextFiles()`将`textfile`载入 RDD 的示例:

```
scala> val rdd_whole = sc.wholeTextFiles("wiki1.txt")rdd_whole: org.apache.spark.rdd.RDD[(String, String)] = wiki1.txt MapPartitionsRDD[37] at wholeTextFiles at <console>:25scala> rdd_whole.take(10)res56: Array[(String, String)] =Array((file:/Users/salla/spark-2.1.1-bin-hadoop2.7/wiki1.txt,Apache Spark provides programmers with an application programming interface centered on a data structure called the resilient distributed dataset (RDD), a read-only multiset of data 

```

# 从 JDBC 数据源加载

您可以从支持 **Java 数据库连接** ( **JDBC** )的外部数据源加载数据。使用 JDBC 驱动程序，您可以连接到关系数据库(如 Mysql)并将表的内容加载到 Spark 中，如以下代码片段所示:

```
 sqlContext.load(path=None, source=None, schema=None, **options)

```

以下是从 JDBC 数据源加载的示例:

```
val dbContent = sqlContext.load(source="jdbc",  url="jdbc:mysql://localhost:3306/test",  dbtable="test",  partitionColumn="id")

```

# 拯救 RDD

将数据从 RDD 保存到文件系统可以通过以下任一方式完成:

*   `saveAsTextFile`
*   `saveAsObjectFile`

以下是将 RDD 保存到文本文件的示例

```
scala> rdd_one.saveAsTextFile("out.txt")

```

加载和保存数据的方式还有很多，尤其是在与 HBase、Cassandra 等集成时。

# 摘要

在这一章中，我们讨论了 Apache Spark 的内部，什么是关系数据库，关系数据库的 Dag 和谱系，转换和动作。我们还研究了 Apache Spark 的各种部署模式，包括独立部署、纱式部署和 Mesos 部署。我们还在本地机器上进行了本地安装，然后查看了 Spark shell 以及如何使用它与 Spark 进行交互。

此外，我们还研究了将数据加载到 rdd 和将 rdd 保存到外部系统，以及 Spark 非凡性能的秘密酱料、缓存功能，以及我们如何使用内存和/或磁盘来优化性能。

在下一章中，我们将在[第 7 章](07.html#6A5N81-21aec46d8593429cacea59dbdcd64e1c)、*特别 RDD 行动*中深入挖掘 RDD API 及其工作原理。