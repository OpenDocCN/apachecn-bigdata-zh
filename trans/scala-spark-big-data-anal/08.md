# 介绍一个小结构——Spark SQL

"One machine can do the work of fifty ordinary men. No machine can do the work of one extraordinary man."

-埃尔伯特·赫伯德

在本章中，您将学习如何使用 Spark 分析结构化数据(非结构化数据，如包含任意文本或其他某种格式的文档必须转换为结构化形式)；我们将在这里看到数据框架/数据集是如何成为基石的，以及 Spark SQL 的 API 如何使查询结构化数据变得简单而健壮。此外，我们介绍数据集，并了解数据集、数据框架和关系数据库之间的区别。简而言之，本章将涵盖以下主题:

*   Spark SQL 和数据帧
*   数据框架和 SQL 应用编程接口
*   数据框模式
*   数据集和编码器
*   加载和保存数据
*   聚集
*   连接

# Spark SQL 和数据帧

在 Apache Spark 之前，每当有人想要对大量数据运行类似于 SQL 的查询时，Apache Hive 都是首选技术。Apache Hive 本质上是将 SQL 查询转换成类似 MapReduce 的逻辑，自动使对大数据执行多种分析变得非常容易，而无需实际学习用 Java 和 Scala 编写复杂的代码。

随着 Apache Spark 的出现，我们如何在大数据规模上执行分析发生了范式转变。Spark SQL 在 Apache Spark 的分布式计算能力之上提供了一个易于使用的类似 SQL 的层。事实上，Spark SQL 可以用作在线分析处理数据库。

![](img/00297.jpeg)

Spark SQL 的工作原理是将类似 SQL 的语句解析为**抽象语法树** ( **AST** )，随后将该计划转换为逻辑计划，然后将逻辑计划优化为可执行的物理计划。最终的执行使用底层的数据框架应用编程接口，通过简单地使用类似于 SQL 的接口，而不是学习所有的内部，任何人都可以非常容易地使用数据框架应用编程接口。由于本书深入探讨了各种应用编程接口的技术细节，我们将主要介绍数据框架应用编程接口，展示一些地方的Spark SQL 应用编程接口，以对比使用这些应用编程接口的不同方式。

因此，数据框架应用编程接口是Spark SQL 下面的底层。在本章中，我们将向您展示如何使用各种技术创建数据框，包括 SQL 查询和对数据框执行操作。

数据框架是**弹性分布式数据集** ( **RDD** )的抽象，处理使用催化剂优化器优化的更高级功能，并且通过钨计划具有高性能。您可以将数据集看作是一个高效的 RDD 表，它具有高度优化的数据二进制表示。二进制表示是使用编码器实现的，编码器将各种对象序列化为二进制结构，性能比 RDD 表示好得多。因为数据框在内部使用 RDD，所以数据框/数据集也像 RDD 一样分布，因此也是一个分布式数据集。显然，这也意味着数据集是不可变的。

以下是数据的二进制表示的说明:

![](img/00037.jpeg)

数据集是在 Spark 1.6 中添加的，并在 DataFrames 之上提供了强类型化的好处。事实上，自 Spark 2.0 以来，数据框只是数据集的别名。

`org.apache.spark.sql` defines type `DataFrame` as a `dataset[Row]`, which means that most of the APIs will work well with both datasets and `DataFrames`
**type DataFrame = dataset[Row]**

数据框在概念上类似于关系数据库中的表。因此，数据帧包含多行数据，每行由几列组成。

我们需要记住的第一件事是，就像关系数据库一样，数据帧是不可变的。数据帧不可变的这一特性意味着每次转换或操作都会创建一个新的数据帧。

![](img/00034.jpeg)

让我们开始更多地研究数据帧，以及它们与关系数据库有什么不同。如前所述，RDD 代表了 Apache Spark 中数据操作的低级 API。数据框架是在关系数据库之上创建的，以抽象关系数据库的低级内部工作方式，并公开高级应用编程接口，这些接口更容易使用，并提供了许多现成的功能。DataFrame 是遵循 Python pandas 包、R 语言、Julia 语言等中的类似概念创建的。

正如我们之前提到的，数据框架将 SQL 代码和特定于域的语言表达式转换为优化的执行计划，在 Spark Core APIs 之上运行，以便 SQL 语句执行各种各样的操作。数据框支持许多不同类型的输入数据源和许多类型的操作。这包括所有类型的 SQL 操作，如联接、分组依据、聚合和窗口函数，就像大多数数据库一样。Spark SQL 也与 Hive 查询语言非常相似，由于 Spark 为 Apache Hive 提供了一个自然的适配器，所以一直在 Apache Hive 工作的用户可以轻松地转移他们的知识，将其应用到 Spark SQL 中，从而最大限度地减少转换时间。

如前所述，数据帧本质上依赖于表的概念。该表的操作方式非常类似于 Apache Hive 的工作方式。事实上，Apache Spark 中对表的许多操作类似于 Apache Hive 处理表和对这些表进行操作的方式。一旦您有了一个作为数据框的表，数据框就可以注册为一个表，并且您可以使用 Spark SQL 语句代替数据框 API 来操作数据。

数据帧取决于催化剂优化器和钨性能的提高，所以让我们简单地研究一下催化剂优化器是如何工作的。catalyst 优化器根据输入的 SQL 创建一个解析的逻辑计划，然后通过查看 SQL 语句中使用的所有不同属性和列来分析该逻辑计划。一旦分析的逻辑计划被创建，catalyst optimizer 会进一步尝试通过组合几个操作并重新排列逻辑来优化计划，以获得更好的性能。

In order to understand the catalyst optimizer, think about it as a common sense logic Optimizer which can reorder operations such as filters and transformations, sometimes grouping several operations into one so as to minimize the amount of data that is shuffled across the worker nodes. For example, catalyst optimizer may decide to broadcast the smaller datasets when performing joint operations between different datasets. Use explain to look at the execution plan of any data frame. The catalyst optimizer also computes statistics of the DataFrame's columns and partitions, improving the speed of execution.

例如，如果数据分区上有转换和过滤器，那么我们过滤数据和应用转换的顺序对操作的整体性能非常重要。作为所有优化的结果，生成优化的逻辑计划，然后将其转换为物理计划。显然，几个物理计划是执行相同的 SQL 语句并生成相同结果的可能性。成本优化逻辑根据成本优化和估算来确定和选择一个好的物理计划。

与之前的版本(如 Spark 1.6 及更早版本)相比，Spark 2.x 提供了显著的性能提升，钨性能的提升是其背后的另一个关键因素。钨实现了对内存管理和其他性能改进的全面检修。最重要的内存管理改进使用对象的二进制编码，并在堆外和堆内内存中引用它们。因此，钨允许使用办公室堆内存使用二进制编码机制来编码所有的对象。二进制编码对象占用的内存少得多。项目钨也提高了洗牌性能。

数据通常通过`DataFrameReader`加载到数据框中，数据通过`DataFrameWriter`从数据框中保存。

# 数据框架应用编程接口和 SQL 应用编程接口

数据框的创建可以通过几种方式完成:

*   通过执行 SQL 查询
*   加载外部数据，如拼花、JSON、CSV、文本、Hive、JDBC 等
*   将 RDDs 转换为数据帧

可以通过加载 CSV 文件来创建数据框。我们将看一个 CSV `statesPopulation.csv`，它作为一个数据帧被加载。

从 2010 年到 2016 年，美国各州人口的 CSV 格式如下。

| **状态** | **年** | **人口** |
| 亚拉巴马州 | Two thousand and ten | Four million seven hundred and eighty-five thousand four hundred and ninety-two |
| 阿拉斯加 | Two thousand and ten | Seven hundred and fourteen thousand and thirty-one |
| 亚利桑那州 | Two thousand and ten | Six million four hundred and eight thousand three hundred and twelve |
| 阿肯色州 | Two thousand and ten | Two million nine hundred and twenty-one thousand nine hundred and ninety-five |
| 加利福尼亚 | Two thousand and ten | Thirty-seven million three hundred and thirty-two thousand six hundred and eighty-five |

由于这个 CSV 有一个头，我们可以使用它来快速加载到带有隐式模式检测的数据帧中。

```scala
scala> val statesDF = spark.read.option("header", "true").option("inferschema", "true").option("sep", ",").csv("statesPopulation.csv")statesDF: org.apache.spark.sql.DataFrame = [State: string, Year: int ... 1 more field]

```

一旦加载了数据帧，就可以检查它的模式:

```scala
scala> statesDF.printSchemaroot|-- State: string (nullable = true)|-- Year: integer (nullable = true)|-- Population: integer (nullable = true)

```

`option("header", "true").option("inferschema", "true").option("sep", ",")` tells Spark that the CSV has a `header`; a comma separator is used to separate the fields/columns and also that schema can be inferred implicitly.

DataFrame 的工作原理是解析逻辑计划、分析逻辑计划、优化计划，然后最终执行执行的物理计划。

使用数据框上的解释显示执行计划:

```scala
scala> statesDF.explain(true)== Parsed Logical Plan ==Relation[State#0,Year#1,Population#2] csv== Analyzed Logical Plan ==State: string, Year: int, Population: intRelation[State#0,Year#1,Population#2] csv== Optimized Logical Plan ==Relation[State#0,Year#1,Population#2] csv== Physical Plan ==*FileScan csv [State#0,Year#1,Population#2] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/salla/states.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<State:string,Year:int,Population:int>

```

数据框也可以注册为表名(如下所示)，这将允许您像关系数据库一样键入 SQL 语句。

```scala
scala> statesDF.createOrReplaceTempView("states")

```

一旦我们将数据框作为结构化数据框或表，我们就可以运行命令对数据进行操作:

```scala
scala> statesDF.show(5)scala> spark.sql("select * from states limit 5").show+----------+----+----------+| State|Year|Population|+----------+----+----------+| Alabama|2010| 4785492|| Alaska|2010| 714031|| Arizona|2010| 6408312|| Arkansas|2010| 2921995||California|2010| 37332685|+----------+----+----------+

```

如果您在前面的代码中看到，我们已经编写了一个类似于 SQL 的语句，并使用`spark.sql` API 执行了它。

Note that the Spark SQL is simply converted to the DataFrame API for execution and the SQL is only a DSL for ease of use.

使用数据框上的`sort`操作，可以按任意列对数据框中的行进行排序。我们看到使用`Population`列下降`sort`的效果如下。这些行按人口降序排列。

```scala
scala> statesDF.sort(col("Population").desc).show(5)scala> spark.sql("select * from states order by Population desc limit 5").show+----------+----+----------+| State|Year|Population|+----------+----+----------+|California|2016| 39250017||California|2015| 38993940||California|2014| 38680810||California|2013| 38335203||California|2012| 38011074|+----------+----+----------+

```

使用`groupBy`我们可以按任意列对数据帧进行分组。以下是通过`State`对行进行分组，然后对每个`State`累加`Population`计数的代码。

```scala
scala> statesDF.groupBy("State").sum("Population").show(5)scala> spark.sql("select State, sum(Population) from states group by State limit 5").show+---------+---------------+| State|sum(Population)|+---------+---------------+| Utah| 20333580|| Hawaii| 9810173||Minnesota| 37914011|| Ohio| 81020539|| Arkansas| 20703849|+---------+---------------+

```

使用`agg`操作，您可以对数据框的列执行许多不同的操作，例如查找列的`min`、`max`和`avg`。您还可以同时执行该操作和重命名列，以适合您的用例。

```scala
scala> statesDF.groupBy("State").agg(sum("Population").alias("Total")).show(5)scala> spark.sql("select State, sum(Population) as Total from states group by State limit 5").show+---------+--------+| State| Total|+---------+--------+| Utah|20333580|| Hawaii| 9810173||Minnesota|37914011|| Ohio|81020539|| Arkansas|20703849|+---------+--------+

```

自然，逻辑越复杂，执行计划也就越复杂。让我们看看`groupBy`和`agg` API 调用的前一个操作的计划，以便更好地理解幕后到底发生了什么。以下代码显示了集团的执行计划和每个`State`的人口总和:

```scala
scala> statesDF.groupBy("State").agg(sum("Population").alias("Total")).explain(true)== Parsed Logical Plan =='Aggregate [State#0], [State#0, sum('Population) AS Total#31886]+- Relation[State#0,Year#1,Population#2] csv== Analyzed Logical Plan ==State: string, Total: bigintAggregate [State#0], [State#0, sum(cast(Population#2 as bigint)) AS Total#31886L]+- Relation[State#0,Year#1,Population#2] csv== Optimized Logical Plan ==Aggregate [State#0], [State#0, sum(cast(Population#2 as bigint)) AS Total#31886L]+- Project [State#0, Population#2]+- Relation[State#0,Year#1,Population#2] csv== Physical Plan ==*HashAggregate(keys=[State#0], functions=[sum(cast(Population#2 as bigint))], output=[State#0, Total#31886L])+- Exchange hashpartitioning(State#0, 200)+- *HashAggregate(keys=[State#0], functions=[partial_sum(cast(Population#2 as bigint))], output=[State#0, sum#31892L])+- *FileScan csv [State#0,Population#2] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/salla/states.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<State:string,Population:int> 
```

 **数据框操作可以很好地链接在一起，这样执行就可以利用成本优化(钨性能改进和催化剂优化器一起工作)。

我们还可以在一条语句中将操作链接在一起，如下所示，其中我们不仅按照`State`列对数据进行分组，然后对`Population`值求和，还按照求和列对数据帧进行排序:

```scala
scala> statesDF.groupBy("State").agg(sum("Population").alias("Total")).sort(col("Total").desc).show(5)scala> spark.sql("select State, sum(Population) as Total from states group by State order by Total desc limit 5").show+----------+---------+| State| Total|+----------+---------+|California|268280590|| Texas|185672865|| Florida|137618322|| New York|137409471|| Illinois| 89960023|+----------+---------+

```

前面的链式操作由多个转换和动作组成，可以使用下图进行可视化:

![](img/00042.jpeg)

也可以同时创建多个聚合，如下所示:

```scala
scala> statesDF.groupBy("State").agg(min("Population").alias("minTotal"), max("Population").alias("maxTotal"),        avg("Population").alias("avgTotal")).sort(col("minTotal").desc).show(5)scala> spark.sql("select State, min(Population) as minTotal, max(Population) as maxTotal, avg(Population) as avgTotal from states group by State order by minTotal desc limit 5").show+----------+--------+--------+--------------------+| State|minTotal|maxTotal| avgTotal|+----------+--------+--------+--------------------+|California|37332685|39250017|3.8325798571428575E7|| Texas|25244310|27862596| 2.6524695E7|| New York|19402640|19747183| 1.962992442857143E7|| Florida|18849098|20612439|1.9659760285714287E7|| Illinois|12801539|12879505|1.2851431857142856E7|+----------+--------+--------+--------------------+

```

# 中心

透视是转换表以创建不同视图的一种很好的方式，更适合于进行许多汇总和聚合。这是通过获取一列的值并使每个值成为一个实际的列来实现的。

为了更好地理解这一点，让我们通过`Year`来透视数据框的行，并检查结果，结果显示，现在，列`Year`通过将每个唯一值转换为实际列来创建几个新列。这样做的最终结果是，现在，我们可以使用`Year`创建的年度列来汇总和聚合，而不仅仅是查看年度列。

```scala
scala> statesDF.groupBy("State").pivot("Year").sum("Population").show(5)+---------+--------+--------+--------+--------+--------+--------+--------+| State| 2010| 2011| 2012| 2013| 2014| 2015| 2016|+---------+--------+--------+--------+--------+--------+--------+--------+| Utah| 2775326| 2816124| 2855782| 2902663| 2941836| 2990632| 3051217|| Hawaii| 1363945| 1377864| 1391820| 1406481| 1416349| 1425157| 1428557||Minnesota| 5311147| 5348562| 5380285| 5418521| 5453109| 5482435| 5519952|| Ohio|11540983|11544824|11550839|11570022|11594408|11605090|11614373|| Arkansas| 2921995| 2939493| 2950685| 2958663| 2966912| 2977853| 2988248|+---------+--------+--------+--------+--------+--------+--------+--------+

```

# 过滤

数据框还支持过滤器，可用于快速过滤数据框行以生成新的数据框。过滤器支持非常重要的数据转换，以将数据框架缩小到我们的用例。例如，如果您只想分析加利福尼亚州，那么使用`filter`应用编程接口可以消除每个数据分区上不匹配的行，从而提高操作的性能。

让我们看看过滤数据帧的执行计划，只考虑加利福尼亚州。

```scala
scala> statesDF.filter("State == 'California'").explain(true)== Parsed Logical Plan =='Filter ('State = California)+- Relation[State#0,Year#1,Population#2] csv== Analyzed Logical Plan ==State: string, Year: int, Population: intFilter (State#0 = California)+- Relation[State#0,Year#1,Population#2] csv== Optimized Logical Plan ==Filter (isnotnull(State#0) && (State#0 = California))+- Relation[State#0,Year#1,Population#2] csv== Physical Plan ==*Project [State#0, Year#1, Population#2]+- *Filter (isnotnull(State#0) && (State#0 = California))+- *FileScan csv [State#0,Year#1,Population#2] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/salla/states.csv], PartitionFilters: [], PushedFilters: [IsNotNull(State), EqualTo(State,California)], ReadSchema: struct<State:string,Year:int,Population:int>

```

现在我们已经看到了执行计划，现在让我们执行`filter`命令，如下所示:

```scala
scala> statesDF.filter("State == 'California'").show+----------+----+----------+| State|Year|Population|+----------+----+----------+|California|2010| 37332685||California|2011| 37676861||California|2012| 38011074||California|2013| 38335203||California|2014| 38680810||California|2015| 38993940||California|2016| 39250017|+----------+----+----------+

```

# 用户定义函数

UDF 定义了新的基于列的函数，扩展了 Spark SQL 的功能。通常，Spark 中提供的内置功能不能满足我们的确切需求。在这种情况下，Apache Spark 支持创建可以使用的 UDF。

`udf()` internally calls a case class User-Defined Function, which itself calls ScalaUDF internally.

让我们看一个简单地将州列值转换为大写的 UDF 的例子。

首先，我们在 Scala 中创建我们需要的函数。

```scala
import org.apache.spark.sql.functions._scala> val toUpper: String => String = _.toUpperCasetoUpper: String => String = <function1>

```

然后，我们必须将创建的函数封装在`udf`中，以创建 UDF。

```scala
scala> val toUpperUDF = udf(toUpper)toUpperUDF: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StringType,Some(List(StringType)))

```

现在我们已经创建了`udf`，我们可以使用它将 State 列转换为大写。

```scala
scala> statesDF.withColumn("StateUpperCase", toUpperUDF(col("State"))).show(5)+----------+----+----------+--------------+| State|Year|Population|StateUpperCase|+----------+----+----------+--------------+| Alabama|2010| 4785492| ALABAMA|| Alaska|2010| 714031| ALASKA|| Arizona|2010| 6408312| ARIZONA|| Arkansas|2010| 2921995| ARKANSAS||California|2010| 37332685| CALIFORNIA|+----------+----+----------+--------------+

```

# 数据的模式结构

模式是对数据结构的描述，可以是隐式的，也可以是显式的。

由于数据框在内部基于 RDD，因此将现有关系数据库转换为数据集有两种主要方法。通过使用反射来推断 RDD 的模式，可以将 RDD 转换为数据集。创建数据集的第二种方法是通过编程接口，使用该接口，您可以获取一个现有的 RDD，并提供一个模式来将 RDD 转换为具有模式的数据集。

为了通过使用反射来推断模式，从而从 RDD 创建一个数据帧，Scala Spark 应用编程接口提供了可用于定义表模式的案例类。数据框架是从 RDD 以编程方式创建的，因为案例类并不容易在所有情况下使用。例如，在一个 1000 列的表上创建案例类是很耗时的。

# 隐式模式

让我们看一个将一个 **CSV** (c **omma 分隔值**)文件加载到数据框中的例子。每当文本文件包含标题时，read API 可以通过读取标题行来推断模式。我们还可以选择指定用于拆分文本文件行的分隔符。

我们从标题行读取`csv`推断模式，并使用逗号(`,`)作为分隔符。我们还展示了使用`schema`命令和`printSchema`命令来验证输入文件的模式。

```scala
scala> val statesDF = spark.read.option("header", "true").option("inferschema", "true").option("sep", ",").csv("statesPopulation.csv")statesDF: org.apache.spark.sql.DataFrame = [State: string, Year: int ... 1 more field]scala> statesDF.schemares92: org.apache.spark.sql.types.StructType = StructType(StructField(State,StringType,true),StructField(Year,IntegerType,true),StructField(Population,IntegerType,true))**scala> statesDF.printSchema**root|-- State: string (nullable = true)|-- Year: integer (nullable = true)|-- Population: integer (nullable = true) 
```

 **# 显式模式

使用`StructType`描述模式，T0 是`StructField`对象的集合。

`StructType` and `StructField` belong to the `org.apache.spark.sql.types` package.
DataTypes such as `IntegerType`, `StringType` also belong to the `org.apache.spark.sql.types` package.

使用这些导入，我们可以定义一个自定义的显式模式。

首先，导入必要的类:

```scala
scala> import org.apache.spark.sql.types.{StructType, IntegerType, StringType}import org.apache.spark.sql.types.{StructType, IntegerType, StringType}

```

定义一个包含两个列/字段的模式-一个`Integer`后跟一个`String`:

```scala
scala> val schema = new StructType().add("i", IntegerType).add("s", StringType)schema: org.apache.spark.sql.types.StructType = StructType(StructField(i,IntegerType,true), StructField(s,StringType,true))

```

打印新创建的`schema`很容易:

```scala
scala> schema.printTreeStringroot|-- i: integer (nullable = true)|-- s: string (nullable = true)

```

还有一个打印 JSON 的选项，如下，使用`prettyJson`功能:

```scala
scala> schema.prettyJsonres85: String ={"type" : "struct","fields" : [ {"name" : "i","type" : "integer","nullable" : true,"metadata" : { }}, {"name" : "s","type" : "string","nullable" : true,"metadata" : { }} ]}

```

Spark SQL 的所有数据类型都位于包`org.apache.spark.sql.types`中。您可以通过以下方式访问它们:

```scala
import org.apache.spark.sql.types._

```

# 编码器

Spark 2.x 支持为复杂数据类型定义模式的不同方式。首先，让我们看一个简单的例子。

必须使用 import 语句导入编码器，才能使用编码器:

```scala
import org.apache.spark.sql.Encoders

```

让我们看一个简单的例子，将元组定义为要在数据集 API 中使用的数据类型:

```scala
scala> Encoders.product[(Integer, String)].schema.printTreeStringroot|-- _1: integer (nullable = true)|-- _2: string (nullable = true)

```

前面的代码看起来总是很复杂，所以我们也可以根据需要定义一个 case 类，然后使用它。我们可以用两个字段定义一个案例类`Record`-一个`Integer`和一个`String`:

```scala
scala> case class Record(i: Integer, s: String)defined class Record

```

使用`Encoders`，我们可以很容易地在 case 类之上创建一个`schema`，从而让我们可以轻松地使用各种 API:

```scala
scala> Encoders.product[Record].schema.printTreeStringroot|-- i: integer (nullable = true)|-- s: string (nullable = true)

```

Spark SQL 的所有数据类型都位于包 **`org.apache.spark.sql.types`** 中。您可以通过以下方式访问它们:

```scala
import org.apache.spark.sql.types._

```

您应该在代码中使用`DataTypes`对象来创建复杂的 Spark SQL 类型，如数组或映射，如下所示:

```scala
scala> import org.apache.spark.sql.types.DataTypesimport org.apache.spark.sql.types.DataTypesscala> val arrayType = DataTypes.createArrayType(IntegerType)arrayType: org.apache.spark.sql.types.ArrayType = ArrayType(IntegerType,true)

```

以下是 Spark SQL APIs 支持的数据类型:

| **数据类型** | **Scala 中的值类型** | **访问或创建数据类型的应用编程接口** |
| `ByteType` | `Byte` | `ByteType` |
| `ShortType` | `Short` | `ShortType` |
| `IntegerType` | `Int` | `IntegerType` |
| `LongType` | `Long` | `LongType` |
| `FloatType` | `Float` | `FloatType` |
| `DoubleType` | `Double` | `DoubleType` |
| `DecimalType` | `java.math.BigDecimal` | `DecimalType` |
| `StringType` | `String` | `StringType` |
| `BinaryType` | `Array[Byte]` | `BinaryType` |
| `BooleanType` | `Boolean` | `BooleanType` |
| `TimestampType` | `java.sql.Timestamp` | `TimestampType` |
| `DateType` | `java.sql.Date` | `DateType` |
| `ArrayType` | `scala.collection.Seq` | `ArrayType(elementType, [containsNull])` |
| `MapType` | `scala.collection.Map` | `MapType(keyType, valueType, [valueContainsNull])`注意:`valueContainsNull`的默认值为`true`。 |
| `StructType` | `org.apache.spark.sql.Row` | `StructType(fields)`注:菲尔兹是`StructFields`的`Seq`。此外，不允许两个字段同名。 |

# 加载和保存数据集

我们需要将数据作为输入和输出读入集群，或者将结果写回存储，以便用我们的代码做任何实际的事情。输入数据可以从各种数据集和来源中读取，如文件、亚马逊 S3 存储、数据库、NoSQLs 和 Hive，输出也可以类似地保存到文件、S3、数据库、Hive 等。

有几个系统通过连接器支持 Spark，随着越来越多的系统锁定 Spark 处理框架，这个数字日益增长。

# 正在加载数据集

Spark SQL 可以通过`DataFrameReader`界面从文件、Hive 表、JDBC 数据库等外部存储系统读取数据。

API 调用的格式为`spark.read.inputtype`

*   镶木地板
*   战斗支援车
*   蜂巢表
*   JDBC
*   妖魔
*   文本
*   JSON

让我们看几个将 CSV 文件读入数据帧的简单例子:

```scala
scala> val statesPopulationDF = spark.read.option("header", "true").option("inferschema", "true").option("sep", ",").csv("statesPopulation.csv")statesPopulationDF: org.apache.spark.sql.DataFrame = [State: string, Year: int ... 1 more field]scala> val statesTaxRatesDF = spark.read.option("header", "true").option("inferschema", "true").option("sep", ",").csv("statesTaxRates.csv")statesTaxRatesDF: org.apache.spark.sql.DataFrame = [State: string, TaxRate: double]

```

# 保存数据集

Spark SQL 可以通过`DataFrameWriter`界面将数据保存到文件、Hive 表、JDBC 数据库等外部存储系统中。

API 调用的格式为`dataframe` `.write.outputtype`

*   镶木地板
*   妖魔
*   文本
*   蜂巢表
*   JSON
*   战斗支援车
*   JDBC

让我们看几个将数据帧写入或保存到 CSV 文件的例子:

```scala
scala> statesPopulationDF.write.option("header", "true").csv("statesPopulation_dup.csv")scala> statesTaxRatesDF.write.option("header", "true").csv("statesTaxRates_dup.csv")

```

# 聚集

聚合是根据条件收集数据并对数据进行分析的方法。聚合对于理解各种大小的数据非常重要，因为仅仅拥有原始数据记录对于大多数用例来说并不那么有用。

例如，如果您先查看下表，然后查看聚合视图，很明显，仅仅是原始记录并不能帮助您理解数据。

Imagine a table containing one temperature measurement per day for every city in the world for five years.

下表显示了每个城市每天的平均温度记录:

| **城市** | **日期** | **温度** |
| 波士顿 | 12/23/2016 | Thirty-two |
| 纽约 | 12/24/2016 | Thirty-six |
| 波士顿 | 12/24/2016 | Thirty |
| 费城 | 12/25/2016 | Thirty-four |
| 波士顿 | 12/25/2016 | Twenty-eight |

如果我们想计算上表中所有测量日的每个城市的平均温度，我们可以看到类似于下表的结果:

| **城市** | **平均温度** |
| 波士顿 | 30 - *(32 + 30 + 28)/3* |
| 纽约 | Thirty-six |
| 费城 | Thirty-four |

# 聚合函数

大多数聚合可以使用`org.apache.spark.sql.functions`包中的函数来完成。此外，还可以创建自定义聚合函数，也称为**用户自定义聚合函数** ( **UDAF** )。

Each grouping operation returns a `RelationalGroupeddataset`, on which you can specify aggregations.

我们将加载示例数据来说明本节中所有不同类型的聚合函数:

```scala
val statesPopulationDF = spark.read.option("header", "true").option("inferschema", "true").option("sep", ",").csv("statesPopulation.csv")

```

# 数数

Count 是最基本的聚合函数，它只计算指定列的行数。扩展是`countDistinct`，也消除了重复。

`count` API 有如下几种实现方式。具体使用的应用编程接口取决于具体的用例:

```scala
def count(columnName: String): TypedColumn[Any, Long]Aggregate function: returns the number of items in a group.def count(e: Column): ColumnAggregate function: returns the number of items in a group.def countDistinct(columnName: String, columnNames: String*): ColumnAggregate function: returns the number of distinct items in a group.def countDistinct(expr: Column, exprs: Column*): ColumnAggregate function: returns the number of distinct items in a group.

```

让我们看一下在数据帧上调用`count`和`countDistinct`来打印行数的例子:

```scala
import org.apache.spark.sql.functions._scala> statesPopulationDF.select(col("*")).agg(count("State")).showscala> statesPopulationDF.select(count("State")).show+------------+|count(State)|+------------+| 350|+------------+scala> statesPopulationDF.select(col("*")).agg(countDistinct("State")).showscala> statesPopulationDF.select(countDistinct("State")).show+---------------------+|count(DISTINCT State)|+---------------------+| 50|

```

# 第一

获取`RelationalGroupeddataset.`中的第一条记录

`first` API 有如下几种实现方式。具体使用的应用编程接口取决于具体的用例:

```scala
def first(columnName: String): ColumnAggregate function: returns the first value of a column in a group.def first(e: Column): ColumnAggregate function: returns the first value in a group.def first(columnName: String, ignoreNulls: Boolean): ColumnAggregate function: returns the first value of a column in a group.def first(e: Column, ignoreNulls: Boolean): ColumnAggregate function: returns the first value in a group.

```

让我们看一个在数据帧上调用`first`来输出第一行的例子:

```scala
import org.apache.spark.sql.functions._scala> statesPopulationDF.select(first("State")).show+-------------------+|first(State, false)|+-------------------+| Alabama|+-------------------+

```

# 最后的

获取`RelationalGroupeddataset`中的最后一条记录。

`last` API 有如下几种实现方式。具体使用的应用编程接口取决于具体的用例:

```scala
def last(columnName: String): ColumnAggregate function: returns the last value of the column in a group.def last(e: Column): ColumnAggregate function: returns the last value in a group.def last(columnName: String, ignoreNulls: Boolean): ColumnAggregate function: returns the last value of the column in a group.def last(e: Column, ignoreNulls: Boolean): ColumnAggregate function: returns the last value in a group.

```

让我们看一个在数据帧上调用`last`来输出最后一行的例子。

```scala
import org.apache.spark.sql.functions._scala> statesPopulationDF.select(last("State")).show+------------------+|last(State, false)|+------------------+| Wyoming|+------------------+

```

# 近似 _ 计数 _ 独特

近似非重复计数在近似计算非重复记录方面比精确计数要快得多，后者通常需要大量的洗牌和其他操作。虽然近似的计数并不是 100%准确，但是即使没有精确的计数，许多用例也可以同样出色地执行。

`approx_count_distinct` API 有如下几种实现方式。具体使用的 API 取决于具体的用例。

```scala
def approx_count_distinct(columnName: String, rsd: Double): ColumnAggregate function: returns the approximate number of distinct items in a group.def approx_count_distinct(e: Column, rsd: Double): ColumnAggregate function: returns the approximate number of distinct items in a group.def approx_count_distinct(columnName: String): ColumnAggregate function: returns the approximate number of distinct items in a group.def approx_count_distinct(e: Column): ColumnAggregate function: returns the approximate number of distinct items in a group.

```

让我们看一个在数据帧上调用`approx_count_distinct`来打印数据帧的大概计数的例子:

```scala
import org.apache.spark.sql.functions._scala> statesPopulationDF.select(col("*")).agg(approx_count_distinct("State")).show+----------------------------+|approx_count_distinct(State)|+----------------------------+| 48|+----------------------------+scala> statesPopulationDF.select(approx_count_distinct("State", 0.2)).show+----------------------------+|approx_count_distinct(State)|+----------------------------+| 49|+----------------------------+

```

# 福建话

数据框中某一列的最小值。一个例子是如果你想找到一个城市的最低温度。

`min` API 有如下几种实现方式。具体使用的应用编程接口取决于具体的用例:

```scala
def min(columnName: String): ColumnAggregate function: returns the minimum value of the column in a group.def min(e: Column): ColumnAggregate function: returns the minimum value of the expression in a group.

```

让我们看一个在数据框上调用`min`来打印最小人口的例子:

```scala
import org.apache.spark.sql.functions._scala> statesPopulationDF.select(min("Population")).show+---------------+|min(Population)|+---------------+| 564513|+---------------+

```

# 最大

数据框中某列的最大列值。举个例子，如果你想找到一个城市的最高温度。

`max` API 有如下几种实现方式。具体使用的 API 取决于具体的用例。

```scala
def max(columnName: String): ColumnAggregate function: returns the maximum value of the column in a group.def max(e: Column): ColumnAggregate function: returns the maximum value of the expression in a group.

```

让我们看一个在数据框上调用`max`来打印最大人口的例子:

```scala
import org.apache.spark.sql.functions._scala> statesPopulationDF.select(max("Population")).show+---------------+|max(Population)|+---------------+| 39250017|+---------------+

```

# 平均的

这些值的平均值通过将这些值相加并除以值的数量来计算。

Average of 1,2,3 is (1 + 2 + 3) / 3 = 6/3 = 2

`avg` API 有如下几种实现方式。具体使用的应用编程接口取决于具体的用例:

```scala
def avg(columnName: String): ColumnAggregate function: returns the average of the values in a group.def avg(e: Column): ColumnAggregate function: returns the average of the values in a group.

```

让我们看一个在数据框上调用`avg`来打印平均人口的例子:

```scala
import org.apache.spark.sql.functions._scala> statesPopulationDF.select(avg("Population")).show+-----------------+| avg(Population)|+-----------------+|6253399.371428572|+-----------------+

```

# 总和

计算列值的总和。可选地，`sumDistinct`只能用于累加不同的值。

`sum` API 有如下几种实现方式。具体使用的应用编程接口取决于具体的用例:

```scala
def sum(columnName: String): ColumnAggregate function: returns the sum of all values in the given column.def sum(e: Column): ColumnAggregate function: returns the sum of all values in the expression.def sumDistinct(columnName: String): ColumnAggregate function: returns the sum of distinct values in the expressiondef sumDistinct(e: Column): ColumnAggregate function: returns the sum of distinct values in the expression.

```

让我们看一个在数据帧上调用`sum`来打印总和(总和)`Population`的例子。

```scala
import org.apache.spark.sql.functions._scala> statesPopulationDF.select(sum("Population")).show+---------------+|sum(Population)|+---------------+| 2188689780|+---------------+

```

# 峭度

峰度是一种量化分布形状差异的方法，在平均值和方差方面看起来非常相似，但实际上是不同的。在这种情况下，与分布中间相比，峰度成为分布尾部分布权重的良好度量。

`kurtosis` API 有如下几种实现方式。具体使用的 API 取决于具体的用例。

```scala
def kurtosis(columnName: String): ColumnAggregate function: returns the kurtosis of the values in a group.def kurtosis(e: Column): ColumnAggregate function: returns the kurtosis of the values in a group.

```

让我们看一个在`Population`列的数据框上调用`kurtosis`的例子:

```scala
import org.apache.spark.sql.functions._scala> statesPopulationDF.select(kurtosis("Population")).show+--------------------+|kurtosis(Population)|+--------------------+| 7.727421920829375|+--------------------+

```

# 歪斜

偏斜度衡量数据中的值围绕平均值的不对称性。

`skewness` API 有如下几种实现方式。具体使用的 API 取决于具体的用例。

```scala
def skewness(columnName: String): ColumnAggregate function: returns the skewness of the values in a group.def skewness(e: Column): ColumnAggregate function: returns the skewness of the values in a group.

```

让我们看一个在填充列的数据框上调用`skewness`的例子:

```scala
import org.apache.spark.sql.functions._scala> statesPopulationDF.select(skewness("Population")).show+--------------------+|skewness(Population)|+--------------------+| 2.5675329049100024|+--------------------+

```

# 差异

方差是每个值与平均值的平方差的平均值。

`var` API 有如下几种实现方式。具体使用的应用编程接口取决于具体的用例:

```scala
def var_pop(columnName: String): ColumnAggregate function: returns the population variance of the values in a group.def var_pop(e: Column): ColumnAggregate function: returns the population variance of the values in a group.def var_samp(columnName: String): ColumnAggregate function: returns the unbiased variance of the values in a group.def var_samp(e: Column): ColumnAggregate function: returns the unbiased variance of the values in a group.

```

现在，让我们看一个在测量`Population`方差的数据帧上调用`var_pop`的例子:

```scala
import org.apache.spark.sql.functions._scala> statesPopulationDF.select(var_pop("Population")).show+--------------------+| var_pop(Population)|+--------------------+|4.948359064356177E13|+--------------------+

```

# 标准偏差

标准差是方差的平方根(见前文)。

`stddev` API 有如下几种实现方式。具体使用的应用编程接口取决于具体的用例:

```scala
def stddev(columnName: String): ColumnAggregate function: alias for stddev_samp.def stddev(e: Column): ColumnAggregate function: alias for stddev_samp.def stddev_pop(columnName: String): ColumnAggregate function: returns the population standard deviation of the expression in a group.def stddev_pop(e: Column): ColumnAggregate function: returns the population standard deviation of the expression in a group.def stddev_samp(columnName: String): ColumnAggregate function: returns the sample standard deviation of the expression in a group.def stddev_samp(e: Column): ColumnAggregate function: returns the sample standard deviation of the expression in a group.

```

让我们看一个在数据框上调用`stddev`打印`Population`标准偏差的例子:

```scala
import org.apache.spark.sql.functions._scala> statesPopulationDF.select(stddev("Population")).show+-----------------------+|stddev_samp(Population)|+-----------------------+| 7044528.191173398|+-----------------------+

```

# 协方差

协方差是两个随机变量联合可变性的度量。如果一个变量的较大值主要对应于另一个变量的较大值，而较小值也是如此，那么这些变量往往表现出相似的行为，并且协方差为正。如果相反，一个变量的较大值对应于另一个变量的较小值，则协方差为负。

`covar` API 有如下几种实现方式。具体使用的 API 取决于具体的用例。

```scala
def covar_pop(columnName1: String, columnName2: String): ColumnAggregate function: returns the population covariance for two columns.def covar_pop(column1: Column, column2: Column): ColumnAggregate function: returns the population covariance for two columns.def covar_samp(columnName1: String, columnName2: String): ColumnAggregate function: returns the sample covariance for two columns.def covar_samp(column1: Column, column2: Column): ColumnAggregate function: returns the sample covariance for two columns.

```

让我们看一个在数据框上调用`covar_pop`来计算年份和人口列之间的协方差的例子:

```scala
import org.apache.spark.sql.functions._scala> statesPopulationDF.select(covar_pop("Year", "Population")).show+---------------------------+|covar_pop(Year, Population)|+---------------------------+| 183977.56000006935|+---------------------------+

```

# 群组依据

数据分析中常见的任务是将数据分组到分组的类别中，然后对结果数据组执行计算。

A quick way to understand grouping is to imagine being asked to assess what supplies you need for your office very quickly. You could start looking around you and just group different types of items, such as pens, paper, staplers, and analyze what you have and what you need.

让我们在`DataFrame`上运行`groupBy`功能，打印每个状态的总计数:

```scala
scala> statesPopulationDF.groupBy("State").count.show(5)+---------+-----+| State|count|+---------+-----+| Utah| 7|| Hawaii| 7||Minnesota| 7|| Ohio| 7|| Arkansas| 7|+---------+-----+
```

您也可以`groupBy`然后应用之前看到的任何聚合函数，如`min`、`max`、`avg`、`stddev`等:

```scala
import org.apache.spark.sql.functions._scala> statesPopulationDF.groupBy("State").agg(min("Population"), avg("Population")).show(5)+---------+---------------+--------------------+| State|min(Population)| avg(Population)|+---------+---------------+--------------------+| Utah| 2775326| 2904797.1428571427|| Hawaii| 1363945| 1401453.2857142857||Minnesota| 5311147| 5416287.285714285|| Ohio| 11540983|1.1574362714285715E7|| Arkansas| 2921995| 2957692.714285714|+---------+---------------+--------------------+
```

# 到达

Rollup 是用于执行分层或嵌套计算的多维聚合。例如，如果我们想要显示每个州+年份组的记录数量，以及每个州的记录数量(汇总所有年份以给出每个`State`的总计，而不考虑`Year`，我们可以使用`rollup`如下:

```scala
scala> statesPopulationDF.rollup("State", "Year").count.show(5)+------------+----+-----+| State|Year|count|+------------+----+-----+|South Dakota|2010| 1|| New York|2012| 1|| California|2014| 1|| Wyoming|2014| 1|| Hawaii|null| 7|+------------+----+-----+

```

`rollup`计算各州和年份的计数，如加州+2014，以及加州州(所有年份的总和)。

# 立方

Cube 是一种多维聚合，用于执行分层或嵌套计算，就像 rollup 一样，但不同之处在于 cube 对所有维度执行相同的操作。例如，如果我们想要显示每个`State`和`Year`组以及每个`State`的记录数量(汇总所有年份以给出每个州的总计，与`Year`无关)，我们可以使用汇总如下。此外，`cube`还显示了每年的总计(不考虑`State`):

```scala
scala> statesPopulationDF.cube("State", "Year").count.show(5)+------------+----+-----+| State|Year|count|+------------+----+-----+|South Dakota|2010| 1|| New York|2012| 1|| null|2014| 50|| Wyoming|2014| 1|| Hawaii|null| 7|+------------+----+-----+

```

# 窗口功能

窗口函数允许您在一个数据窗口上执行聚合，而不是在整个数据或某些筛选数据上执行聚合。此类窗口函数的用例有:

*   累计总和
*   同一个键的上一个值的增量
*   加权移动平均线

理解窗口函数的最好方法是在更大的数据集范围内想象一个滑动窗口。您可以通过执行简单的计算，指定一个窗口来查看三行 T-1、T 和 T+1。您还可以指定一个包含最近/最近十个值的窗口:

![](img/00047.jpeg)

窗口规范的应用编程接口需要三个属性，即`partitionBy()`、`orderBy()`和`rowsBetween()`。`partitionBy`按照`partitionBy()`的规定将数据分块到分区/组中。`orderBy()`用于对每个数据分区内的数据进行排序。

`rowsBetween()`指定执行计算的窗框或滑动窗口的跨度。

要试用 windows 功能，需要某些软件包。您可以使用导入指令导入必要的包，如下所示:

```scala
import org.apache.spark.sql.expressions.Windowimport org.apache.spark.sql.functions.colimport org.apache.spark.sql.functions.max

```

现在，您已经准备好编写一些代码来学习窗口函数。让我们为按`Population`排序和按`State`分区的分区创建一个窗口规范。另外，指定我们要考虑所有行，直到当前行作为`Window`的一部分。

```scala
 val windowSpec = Window .partitionBy("State") .orderBy(col("Population").desc) .rowsBetween(Window.unboundedPreceding, Window.currentRow)

```

计算窗口规格上的`rank`。只要在指定的`Window`范围内，结果将是添加到每一行的等级(行号)。在本例中，我们选择按`State`进行划分，然后按降序对每个`State`的行进行排序。因此，所有州行都分配了自己的等级编号。

```scala
import org.apache.spark.sql.functions._scala> statesPopulationDF.select(col("State"), col("Year"), max("Population").over(windowSpec), rank().over(windowSpec)).sort("State", "Year").show(10)+-------+----+------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+| State|Year|max(Population) OVER (PARTITION BY State ORDER BY Population DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)|RANK() OVER (PARTITION BY State ORDER BY Population DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)|+-------+----+------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+|Alabama|2010| 4863300| 6||Alabama|2011| 4863300| 7||Alabama|2012| 4863300| 5||Alabama|2013| 4863300| 4||Alabama|2014| 4863300| 3|

```

# 奈尔斯

ntiles 是一种流行的窗口聚合，通常用于将输入数据集分成 n 个部分。例如，在预测分析中，十分位数(10 个部分)通常用于首先对数据进行分组，然后将其分为 10 个部分，以获得公平的数据分布。这是窗口函数方法的一个自然函数，因此 ntiles 是窗口函数如何提供帮助的一个很好的例子。

例如，如果我们想将`statesPopulationDF`划分为`State`(之前显示了窗口规范)，按人口排序，然后分成两部分，我们可以在`windowspec`上使用`ntile`:

```scala
import org.apache.spark.sql.functions._scala> statesPopulationDF.select(col("State"), col("Year"), ntile(2).over(windowSpec), rank().over(windowSpec)).sort("State", "Year").show(10)+-------+----+-----------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+| State|Year|ntile(2) OVER (PARTITION BY State ORDER BY Population DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)|RANK() OVER (PARTITION BY State ORDER BY Population DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)|+-------+----+-----------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+|Alabama|2010| 2| 6||Alabama|2011| 2| 7||Alabama|2012| 2| 5||Alabama|2013| 1| 4||Alabama|2014| 1| 3||Alabama|2015| 1| 2||Alabama|2016| 1| 1|| Alaska|2010| 2| 7|| Alaska|2011| 2| 6|| Alaska|2012| 2| 5|+-------+----+-----------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------

```

如前所示，我们使用`Window`函数和`ntile()`一起将每个`State`的行分成两个相等的部分。

A popular use of this function is to compute deciles used in data science Models.

# 连接

在传统数据库中，连接用于将一个事务表与另一个查找表连接起来，以生成更完整的视图。例如，如果您有一个按客户标识列出的在线交易表和另一个包含客户城市和客户标识的表，则可以使用联接来生成按城市列出的交易报告。

**交易表**:下表有三栏:**客户编号**、**采购项目、**以及客户为该项目支付的金额:

| **客户名称** | **采购项目** | **支付的价格** |
| one | 双耳式耳机 | Twenty-five |
| Two | 看 | One hundred |
| three | 键盘 | Twenty |
| one | 老鼠 | Ten |
| four | 电缆 | Ten |
| three | 双耳式耳机 | Thirty |

**客户信息表:**下表有两栏，**客户信息**和客户居住的**城市**:

| **客户名称** | **城市** |
| one | 波士顿 |
| Two | 纽约 |
| three | 费城 |
| four | 波士顿 |

将交易表与客户信息表连接起来将生成如下视图:

| **客户名称** | **采购项目** | **支付的价格** | **城市** |
| one | 双耳式耳机 | Twenty-five | 波士顿 |
| Two | 看 | One hundred | 纽约 |
| three | 键盘 | Twenty | 费城 |
| one | 老鼠 | Ten | 波士顿 |
| four | 电缆 | Ten | 波士顿 |
| three | 双耳式耳机 | Thirty | 费城 |

现在，我们可以使用此联合视图生成**城市**的**销售总价**报告:

| **城市** | **#项** | **销售总价** |
| 波士顿 | three | Forty-five |
| 费城 | Two | Fifty |
| 纽约 | one | One hundred |

连接是 Spark SQL 的一个重要功能，因为它们使您能够将两个数据集结合在一起，如前所述。当然，Spark 不仅仅是用来生成报告，它还被用来处理 1pb 规模的数据，以处理实时流用例、机器学习算法或简单分析。为了实现这些目标，Spark 提供了所需的 API 函数。

两个数据集之间的典型连接使用左右数据集的一个或多个键进行，然后将键集上的条件表达式计算为布尔表达式。如果布尔表达式的结果返回真，则连接成功，否则连接的数据帧将不包含相应的连接。

连接应用编程接口有 6 种不同的实现:

```scala
join(right: dataset[_]): DataFrameCondition-less inner joinjoin(right: dataset[_], usingColumn: String): DataFrameInner join with a single columnjoin(right: dataset[_], usingColumns: Seq[String]): DataFrame Inner join with multiple columnsjoin(right: dataset[_], usingColumns: Seq[String], joinType: String): DataFrameJoin with multiple columns and a join type (inner, outer,....)join(right: dataset[_], joinExprs: Column): DataFrameInner Join using a join expressionjoin(right: dataset[_], joinExprs: Column, joinType: String): DataFrame Join using a Join expression and a join type (inner, outer, ...)

```

我们将使用其中一个 API 来了解如何使用 join APIs 但是，您可以根据用例选择使用其他 API:

```scala
def   join(right: dataset[_], joinExprs: Column, joinType: String): DataFrame Join with another DataFrame using the given join expressionright: Right side of the join.joinExprs: Join expression.joinType : Type of join to perform. Default is *inner* join// Scala:import org.apache.spark.sql.functions._import spark.implicits._df1.join(df2, $"df1Key" === $"df2Key", "outer")

```

请注意，在接下来的几节中将详细介绍连接。

# 连接的内部工作方式

Join 通过使用多个执行器对数据帧的分区进行操作来工作。但是，实际操作和后续性能取决于`join`的类型和要连接的数据集的性质。在下一节中，我们将研究连接的类型。

# 随机加入

两个大数据集之间的连接包括无序连接，其中左右数据集的分区分布在执行器中。混洗是昂贵的，分析逻辑以确保分区和混洗的分布是最佳的是很重要的。以下是 shuffle join 内部工作原理的说明:

![](img/00166.jpeg)

# 广播加入

一个大数据集和一个小数据集之间的连接可以通过将小数据集广播给所有存在左数据集的分区的执行器来完成。以下是广播连接如何在内部工作的说明:

![](img/00194.jpeg)

# 连接类型

下面是不同类型联接的表格。这一点很重要，因为连接两个数据集时所做的选择对输出和性能都有影响。

| **连接类型** | **描述** |
| **内** | 内部连接比较从*左侧*到*右侧*的每一行，并且仅当两个数据集都具有非空值时，才组合来自*左侧*和*右侧*数据集的匹配行对。 |
| **十字** | 交叉连接将从*左侧*开始的每一行与从*右侧*开始的每一行进行匹配，生成笛卡尔叉积。 |
| **外侧、全外侧、全外侧** | 如果仅在*右侧*或*左侧*中填写空值，则完全外部连接会给出*左侧*和*右侧*中的所有行。 |
| **左撇子** | 基于*右侧*不存在，左反连接只给出*左侧*的行。 |
| **左，左外侧** | 左外部连接给出了*左侧*中的所有行加上*左侧*和*右侧*的公共行(内部连接)。如果不在*右侧*，则填写空。 |
| **左半** | 基于*右侧*的存在，左半连接只给出*左侧*的行。不包括*右侧-* 侧值。 |
| **右，右外侧** | 右外部连接给出了*右侧*中的所有行加上*左侧*和*右侧*的公共行(内部连接)。如果不在*左侧*，则填写空。 |

我们将通过使用示例数据集来研究不同的连接类型是如何工作的。

```scala
scala> val statesPopulationDF = spark.read.option("header", "true").option("inferschema", "true").option("sep", ",").csv("statesPopulation.csv")statesPopulationDF: org.apache.spark.sql.DataFrame = [State: string, Year: int ... 1 more field]scala> val statesTaxRatesDF = spark.read.option("header", "true").option("inferschema", "true").option("sep", ",").csv("statesTaxRates.csv")statesTaxRatesDF: org.apache.spark.sql.DataFrame = [State: string, TaxRate: double]scala> statesPopulationDF.countres21: Long = 357scala> statesTaxRatesDF.countres32: Long = 47%sqlstatesPopulationDF.createOrReplaceTempView("statesPopulationDF")statesTaxRatesDF.createOrReplaceTempView("statesTaxRatesDF")
```

# 内部连接

当两个数据集中的状态都为非空时，内部连接会产生来自`statesPopulationDF`和`statesTaxRatesDF`的行。

![](img/00095.jpeg)

按状态列连接两个数据集，如下所示:

```scala
val joinDF = statesPopulationDF.join(statesTaxRatesDF, statesPopulationDF("State") === statesTaxRatesDF("State"), "inner")%sqlval joinDF = spark.sql("SELECT * FROM statesPopulationDF INNER JOIN statesTaxRatesDF ON statesPopulationDF.State = statesTaxRatesDF.State")scala> joinDF.countres22: Long = 329scala> joinDF.show+--------------------+----+----------+--------------------+-------+| State|Year|Population| State|TaxRate|+--------------------+----+----------+--------------------+-------+| Alabama|2010| 4785492| Alabama| 4.0|| Arizona|2010| 6408312| Arizona| 5.6|| Arkansas|2010| 2921995| Arkansas| 6.5|| California|2010| 37332685| California| 7.5|| Colorado|2010| 5048644| Colorado| 2.9|| Connecticut|2010| 3579899| Connecticut| 6.35|

```

你可以在`joinDF`上运行`explain()`查看执行计划:

```scala
scala> joinDF.explain== Physical Plan ==*BroadcastHashJoin [State#570], [State#577], Inner, BuildRight:- *Project [State#570, Year#571, Population#572]: +- *Filter isnotnull(State#570): +- *FileScan csv [State#570,Year#571,Population#572] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/salla/spark-2.1.0-bin-hadoop2.7/statesPopulation.csv], PartitionFilters: [], PushedFilters: [IsNotNull(State)], ReadSchema: struct<State:string,Year:int,Population:int>+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))+- *Project [State#577, TaxRate#578]+- *Filter isnotnull(State#577)+- *FileScan csv [State#577,TaxRate#578] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/salla/spark-2.1.0-bin-hadoop2.7/statesTaxRates.csv], PartitionFilters: [], PushedFilters: [IsNotNull(State)], ReadSchema: struct<State:string,TaxRate:double>

```

# 左外连接

左外连接导致从`statesPopulationDF`开始的所有行，包括在`statesPopulationDF`和`statesTaxRatesDF`中常见的任何行。

![](img/00273.jpeg)

按状态列连接两个数据集，如下所示:

```scala
val joinDF = statesPopulationDF.join(statesTaxRatesDF, statesPopulationDF("State") === statesTaxRatesDF("State"), "leftouter")%sqlval joinDF = spark.sql("SELECT * FROM statesPopulationDF LEFT OUTER JOIN statesTaxRatesDF ON statesPopulationDF.State = statesTaxRatesDF.State")scala> joinDF.countres22: Long = 357scala> joinDF.show(5)+----------+----+----------+----------+-------+| State|Year|Population| State|TaxRate|+----------+----+----------+----------+-------+| Alabama|2010| 4785492| Alabama| 4.0|| Alaska|2010| 714031| null| null|| Arizona|2010| 6408312| Arizona| 5.6|| Arkansas|2010| 2921995| Arkansas| 6.5||California|2010| 37332685|California| 7.5|+----------+----+----------+----------+-------+

```

# 右外连接

右外连接导致从`statesTaxRatesDF`开始的所有行，包括在`statesPopulationDF`和`statesTaxRatesDF`中常见的任何行。

![](img/00319.jpeg)

通过`State`列连接两个数据集，如下所示:

```scala
val joinDF = statesPopulationDF.join(statesTaxRatesDF, statesPopulationDF("State") === statesTaxRatesDF("State"), "rightouter")%sqlval joinDF = spark.sql("SELECT * FROM statesPopulationDF RIGHT OUTER JOIN statesTaxRatesDF ON statesPopulationDF.State = statesTaxRatesDF.State")scala> joinDF.countres22: Long = 323scala> joinDF.show+--------------------+----+----------+--------------------+-------+| State|Year|Population| State|TaxRate|+--------------------+----+----------+--------------------+-------+| Colorado|2011| 5118360| Colorado| 2.9|| Colorado|2010| 5048644| Colorado| 2.9|| null|null| null|Connecticut| 6.35|| Florida|2016| 20612439| Florida| 6.0|| Florida|2015| 20244914| Florida| 6.0|| Florida|2014| 19888741| Florida| 6.0|

```

# 外部连接

外部连接导致从`statesPopulationDF`到`statesTaxRatesDF`的所有行。

![](img/00245.jpeg)

通过`State`列连接两个数据集，如下所示:

```scala
val joinDF = statesPopulationDF.join(statesTaxRatesDF, statesPopulationDF("State") === statesTaxRatesDF("State"), "fullouter")%sqlval joinDF = spark.sql("SELECT * FROM statesPopulationDF FULL OUTER JOIN statesTaxRatesDF ON statesPopulationDF.State = statesTaxRatesDF.State")scala> joinDF.countres22: Long = 351scala> joinDF.show+--------------------+----+----------+--------------------+-------+| State|Year|Population| State|TaxRate|+--------------------+----+----------+--------------------+-------+| Delaware|2010| 899816| null| null|| Delaware|2011| 907924| null| null|| West Virginia|2010| 1854230| West Virginia| 6.0|| West Virginia|2011| 1854972| West Virginia| 6.0|| Missouri|2010| 5996118| Missouri| 4.225|| null|null| null|  Connecticut|   6.35|

```

# 左反连接

左反连接仅导致从`statesPopulationDF`开始的行，如果且仅当在`statesTaxRatesDF`中没有对应的行。

![](img/00072.jpeg)

通过`State`列连接两个数据集，如下所示:

```scala
val joinDF = statesPopulationDF.join(statesTaxRatesDF, statesPopulationDF("State") === statesTaxRatesDF("State"), "leftanti")%sqlval joinDF = spark.sql("SELECT * FROM statesPopulationDF LEFT ANTI JOIN statesTaxRatesDF ON statesPopulationDF.State = statesTaxRatesDF.State")scala> joinDF.countres22: Long = 28scala> joinDF.show(5)+--------+----+----------+| State|Year|Population|+--------+----+----------+| Alaska|2010| 714031||Delaware|2010| 899816|| Montana|2010| 990641|| Oregon|2010| 3838048|| Alaska|2011| 722713|+--------+----+----------+

```

# 左半连接

左半连接仅产生来自`statesPopulationDF`的行，如果且仅当在`statesTaxRatesDF`中有相应的行。

![](img/00097.jpeg)

按状态列连接两个数据集，如下所示:

```scala
val joinDF = statesPopulationDF.join(statesTaxRatesDF, statesPopulationDF("State") === statesTaxRatesDF("State"), "leftsemi")%sqlval joinDF = spark.sql("SELECT * FROM statesPopulationDF LEFT SEMI JOIN statesTaxRatesDF ON statesPopulationDF.State = statesTaxRatesDF.State")scala> joinDF.countres22: Long = 322scala> joinDF.show(5)+----------+----+----------+| State|Year|Population|+----------+----+----------+| Alabama|2010| 4785492|| Arizona|2010| 6408312|| Arkansas|2010| 2921995||California|2010| 37332685|| Colorado|2010| 5048644|+----------+----+----------+

```

# 交叉连接

交叉连接将从*左侧*开始的每一行与从*右侧*开始的每一行进行匹配，生成笛卡尔叉积。

![](img/00312.jpeg)

通过`State`列连接两个数据集，如下所示:

```scala
scala> val joinDF=statesPopulationDF.crossJoin(statesTaxRatesDF)joinDF: org.apache.spark.sql.DataFrame = [State: string, Year: int ... 3 more fields]%sqlval joinDF = spark.sql("SELECT * FROM statesPopulationDF CROSS JOIN statesTaxRatesDF")scala> joinDF.countres46: Long = 16450scala> joinDF.show(10)+-------+----+----------+-----------+-------+| State|Year|Population| State|TaxRate|+-------+----+----------+-----------+-------+|Alabama|2010| 4785492| Alabama| 4.0||Alabama|2010| 4785492| Arizona| 5.6||Alabama|2010| 4785492| Arkansas| 6.5||Alabama|2010| 4785492| California| 7.5||Alabama|2010| 4785492| Colorado| 2.9||Alabama|2010| 4785492|Connecticut| 6.35||Alabama|2010| 4785492| Florida| 6.0||Alabama|2010| 4785492| Georgia| 4.0||Alabama|2010| 4785492| Hawaii| 4.0||Alabama|2010| 4785492| Idaho| 6.0|+-------+----+----------+-----------+-------+

```

You can also use join with cross jointype instead of calling the cross join API. `statesPopulationDF.join(statesTaxRatesDF, statesPopulationDF("State").isNotNull, "cross").count`.

# 加入对性能的影响

选择的连接类型直接影响连接的性能。这是因为联接需要在执行器之间进行数据洗牌来执行任务，因此在使用联接时需要考虑不同的联接，甚至联接的顺序。

以下是您在编写`Join`代码时可以参考的表格:

| **连接类型** | **性能注意事项和提示** |
| **内** | 内部联接要求左右表具有相同的列。如果您在左侧或右侧有重复或多个键的副本，连接将很快变成一种笛卡尔连接，比正确设计以最小化多个键花费更长的时间来完成。 |
| **十字** | 交叉连接将从*左侧*开始的每一行与从*右侧*开始的每一行进行匹配，生成笛卡尔叉积。这需要谨慎使用，因为这是性能最差的连接，只能在特定的用例中使用。 |
| **外侧、全外侧、全外侧** | Fullouter Join 给出左*和右*中的所有行，如果只在右*或左*中填充空。如果在几乎没有共同点的表上使用，会导致非常大的结果，从而降低性能。**** |
| **左撇子** | 基于*右侧*不存在，Leftanti Join 只给出*左侧*的行。非常好的性能，因为只充分考虑了一个表，并且只检查了另一个表的连接条件。 |
| **左，左外侧** | 左外部连接给出了*左侧*中的所有行加上*左侧*和*右侧*的公共行(内部连接)。如果不在*右侧*，则填写空。如果在几乎没有共同点的表上使用，会导致非常大的结果，从而降低性能。 |
| **左半** | 基于*右侧*的存在，左半连接只给出*左侧*的行。不包括*右侧*侧值。非常好的性能，因为只充分考虑了一个表，只检查了另一个表的连接条件。 |
| **右，右外侧** | 右外连接给出了*右侧*中的所有行加上*左侧*和*右侧*的公共行(内连接)。如果不在*左侧*，则填写空。性能类似于本表前面提到的 leftouter 联接。 |

# 摘要

在本章中，我们讨论了数据框的起源以及 Spark SQL 如何在数据框之上提供 SQL 接口。数据帧的强大之处在于，与最初基于 RDD 的计算相比，执行时间减少了许多倍。拥有这样一个功能强大的层和一个简单的类似于 SQL 的接口使它们变得更加强大。我们还研究了创建和操作数据帧的各种 API，以及深入挖掘聚合的复杂特性，包括`groupBy`、`Window`、`rollup`和`cubes`。最后，我们还研究了连接数据集的概念以及各种可能的连接类型，例如内部、外部、交叉等等。

在下一章中，我们将在[第 9 章](09.html#8IL201-21aec46d8593429cacea59dbdcd64e1c)、*流我、Scotty - Spark Streaming* 中探索令人兴奋的实时数据处理和分析世界。****