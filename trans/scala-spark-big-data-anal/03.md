# 函数式编程概念

"Object-oriented programming makes code understandable by encapsulating moving parts. Functional programming makes code understandable by minimizing moving parts."

-迈克尔·费哲

使用 Scala 和 Spark 是学习大数据分析的一个非常好的组合。然而，随着面向对象的范例，我们也需要知道为什么功能概念对于编写最终分析您的数据的 Spark 应用程序很重要。如前几章所述，Scala 支持两种编程范式:面向对象编程范式和函数式编程概念。在[第 2 章](02.html#1NA0K1-21aec46d8593429cacea59dbdcd64e1c)、*面向对象的 Scala* 中，我们探索了 OOP 范式，在该范式中，我们已经看到了如何在蓝图(类)中表示现实世界的对象，然后将它们实例化为具有真实内存表示的对象。

在本章中，我们将集中讨论第二种范式(即函数式编程)。我们将看到什么是函数式编程，Scala 如何支持它，为什么它很重要，以及使用这个概念的相关优势。更具体地说，我们将学习几个主题，例如为什么 Scala 是数据科学家的武器库，为什么学习 Spark 范式、纯函数和**高阶函数** ( **HOFs** )很重要。本章还将展示一个使用 HOF 的真实用例。然后，我们将看到如何使用 Scala 的标准库来处理集合之外的高阶函数中的异常。最后，我们将学习函数式 Scala 如何影响对象的可变性。

简而言之，本章将涵盖以下主题:

*   函数式编程导论
*   数据科学家的功能 Scala
*   为什么函数式编程和 Scala 对学习 Spark 很重要？
*   纯函数和高阶函数
*   使用高阶函数:一个真实的用例
*   函数 Scala 中的错误处理
*   函数式编程和数据可变性

# 函数式编程导论

在计算机科学中，`functional programming` (FP)是一种编程范式，是构建计算机程序结构和元素的独特风格。这种唯一性有助于将计算视为数学函数的求值，并避免改变状态和可变数据。因此，通过使用 FP 概念，您可以学习以自己的风格编码，以确保数据的不变性。换句话说，FP 是关于编写纯函数的，关于尽可能地移除隐藏的输入和输出，以便我们的代码尽可能多地*只是*描述输入和输出之间的关系。

这不是一个新概念，而是`Lambda Calculus`，它提供了 FP 的基础，最早是在 20 世纪 30 年代引入的。然而，在编程语言领域，术语函数式编程指的是一种新型的声明式编程范式，这意味着编程可以借助控制、声明或表达式来完成，而不是像 c 语言这样的旧编程语言中常用的经典语句。

# 函数式编程的优点

FP 范式中有一些令人兴奋和酷的特性，比如`composition`、`pipelining`和`higher order functions`，它们有助于避免编写非功能性代码。或者，至少在以后，这有助于将一个非功能性的程序转化为一个功能性的程序，使之成为一个命令性的程序。最后，现在让我们看看如何从计算机科学的角度定义函数式编程这个术语。函数式编程是一种常见的计算机科学概念，在这种概念中，计算和程序的构建结构被视为您正在评估支持不可变数据并避免状态变化的数学函数。在函数式编程中，对于相同的输入参数值，每个函数都有相同的映射或输出。

随着对复杂软件的需求而来的是对好的结构化程序和软件的需求，这些程序和软件并不难编写并且是可调试的。我们还需要编写可扩展的代码，这将节省我们未来的编程成本，并有助于代码的轻松编写和调试；更模块化的软件，易于扩展，编程工作量更少。由于函数式编程的后一个贡献，模块化，函数式编程被认为是软件开发的一大优势。

在函数式编程中，它的结构中有一个基本的构建块，叫做函数，在大多数代码中没有副作用(或者至少很少)。没有副作用，评价的顺序真的不重要。说到编程语言视图，有一些方法可以强制特定的顺序。在一些对参数没有求值顺序的 FP 语言(例如 Scheme 这样的急切语言)中，您可以将这些表达式嵌套在它们自己的 lambda 形式中，如下所示:

```scala
((lambda (val1) ((lambda (val2) ((lambda (val3) (/ (* val1 val2) val3)) expression3)) ; evaluated thirdexpression2))   ; evaluated secondexpression1)      ; evaluated first

```

在函数式编程中，编写执行顺序无关紧要的数学函数通常会使代码更易读。有时候，有人会说我们也需要有副作用的功能。实际上，这是大多数函数式编程语言的主要缺点之一，因为通常很难编写不需要任何输入/输出的函数；另一方面，这些需要 I/O 的功能在函数式编程中很难实现。从*图 1* 可以看出，Scala 也是从 Java 等命令式语言和 Lisp 等函数式语言中取长补短进化而来的混合语言。

但幸运的是，这里我们处理的是一种混合语言，其中允许面向对象和函数式编程范例，因此编写这种需要输入/输出的函数非常容易。函数式编程相对于基础编程也有很大的优势，比如理解和缓存。

函数式编程的主要优点之一是简洁，因为使用函数式编程，您可以编写更紧凑和简洁的代码。此外，并发性被认为是主要优势之一，这在函数式编程中更容易实现。因此，像 Scala 这样的函数式语言提供了许多其他特性和工具，鼓励程序员将整个范式转变为更数学的思维方式。

![](../images/00062.jpeg)

**Figure 1:** Shows a conceptual view of using functional programming concepts

通过将焦点缩小到少数可组合的抽象概念，例如函数、函数组合和抽象代数，FP 概念提供了优于其他范例的几个优点。例如:

*   **更接近数学思维:**你倾向于用接近数学定义的格式而不是迭代程序来阐述你的想法。
*   **没有(或至少更少)副作用:**你的函数不影响其他函数，这对于并发和并行化，以及调试都是很大的。
*   **在不牺牲概念清晰性的情况下减少代码行:** Lisp 比非函数语言更强大。虽然你确实需要花更多的时间思考项目而不是写作，但你可能会发现你最终会更有效率。

对于这些令人兴奋的特性，函数式编程实现了显著的表达能力。例如，机器学习算法可能需要数百行命令式代码才能实现，但它们可以在几个等式中定义。

# 数据科学家的功能 Scala

为了执行交互式数据清理、处理、管理和分析，许多数据科学家使用 R 或 Python 作为他们最喜欢的工具。然而，有许多数据科学家倾向于非常依赖他们最喜欢的工具——也就是 Python 或 R，并试图使用该工具解决所有数据分析问题或工作。因此，在大多数情况下，向他们介绍一个新工具可能非常具有挑战性，因为在使用新工具解决他们的目的之前，新工具有更多的语法和一组新的模式要学习。

Spark 中还有其他用 Python 和 R 编写的 API，如 PySpark 和 SparkR，分别允许您从 Python 或 R 中使用它们。然而，大多数 Spark 书籍和在线示例都是用 Scala 编写的。可以说，作为一名数据科学家，我们认为学习如何使用与编写 Spark 代码相同的语言与 Spark 一起工作将为您带来许多优于 Java、Python 或 R 的优势:

*   更好的性能并消除数据处理开销
*   提供对 Spark 最新和最大功能的访问
*   有助于以透明的方式理解火花哲学

分析数据意味着您正在编写 Scala 代码，使用 Spark 及其 API(即 SparkR、SparkSQL、Spark Streaming、Spark MLlib 和 Spark GraphX)从集群中检索数据。或者，您正在使用 Scala 开发一个 Spark 应用程序，以便在自己的机器上本地操作这些数据。在这两种情况下，Scala 都是你真正的朋友，会及时给你分红。

# 为什么要用 FP 和 Scala 来学习 Spark？

在本节中，我们将讨论为什么我们将学习 Spark 来解决我们的数据分析问题。然后我们将讨论为什么 Scala 中的函数式编程概念对于让数据科学家更容易进行数据分析特别重要。我们还将讨论 Spark 编程模型及其生态系统，以使它们更加清晰。

# 为什么是火花？

Spark 是一个闪电般快速的集群计算框架，主要设计用于快速计算。Spark 基于 Hadoop MapReduce 模型，在更多形式和类型的计算中使用 MapReduce，例如交互式查询和流处理。Spark 的主要功能之一是内存处理，这有助于提高应用程序的性能和处理速度。Spark 支持多种应用和工作负载，例如:

*   基于批处理的应用程序
*   以前不可能快速运行的迭代算法
*   交互式查询和流式传输

此外，学习 Spark 并在应用程序中实现它不需要太多时间，也不需要了解并发和分布式系统的内部细节。Spark 于 2009 年在加州大学伯克利分校的 AMPLab 实施。2010 年，他们决定将其开源。然后，Spark 在 2013 年成为 Apache 发行版，从那以后 Spark 被认为是最著名/使用最多的 Apache 发行软件。Apache Spark 因其特点而变得非常出名:

*   **快速计算** : Spark 帮助你运行比 Hadoop 更快的应用，因为它的黄金特性——内存处理。
*   **支持多种编程语言** : Apache Spark 提供了 Scala、Java、Python 甚至 r 等不同语言的包装器和内置 API。
*   **更多分析**:如前所述，Spark 支持 MapReduce 操作，还支持更高级的分析，如**机器学习** ( **MLlib** )、数据流和图形处理算法。

如前所述，Spark 构建在 Hadoop 软件之上，您可以通过不同的方式部署 Spark:

*   **独立集群**:这意味着 Spark 将运行在 **Hadoop 分布式文件系统** ( **HDFS** )之上，空间将实际分配给 HDFS。Spark 和 MapReduce 将并行运行，为所有 Spark 作业提供服务。
*   **Hadoop 纱簇**:这意味着 Spark 只是在纱上运行，没有任何根权限或预装。
*   **Mesos 集群**:当驱动程序创建一个 Spark 作业并开始分配相关任务进行调度时，Mesos 决定哪些计算节点将处理哪些任务。我们假设您已经在机器上配置并安装了 Mesos。
*   **在现收现付集群上部署**:可以在 AWS EC2 上以真实集群模式部署 Spark 作业。为了让您的应用程序在 Spark 集群模式下运行并获得更好的可扩展性，您可以将**亚马逊弹性计算云** ( **EC2** )服务视为**基础架构即服务** ( **IaaS** )或**平台即服务** ( **PaaS** )。

Refer to [Chapter 17](17.html#F7AFE1-21aec46d8593429cacea59dbdcd64e1c), *Time to Go to ClusterLand - Deploying Spark on a Cluster* and [Chapter 18](18.html#GCGLC1-21aec46d8593429cacea59dbdcd64e1c), *Testing and Debugging Spark* for how to deploy your data analytics application using Scala and Spark on a real cluster.

# Scala 和 Spark 编程模型

Spark 编程从一个或几个数据集开始，通常驻留在某种形式的分布式持久存储中，如 HDFS。Spark 提供的典型 RDD 编程模型可以描述如下:

*   从一个环境变量，Spark context(Spark shell 为您提供了一个 Spark Context，或者您可以自己创建，这将在本章后面描述)创建一个初始数据引用 RDD 对象。
*   按照函数式编程风格(稍后讨论)，转换初始 RDD，创建更多 RDD 对象。
*   将驱动程序中的代码、算法或应用程序发送到集群管理器节点。然后，集群管理器向每个计算节点提供一个副本。
*   计算节点持有对其分区中 RDDs 的引用(同样，驱动程序也持有数据引用)。然而，计算节点也可以拥有由集群管理器提供的输入数据集。
*   经过一次改造(通过狭义或广义的改造)，产生的结果是一个全新的 RDD，因为原来的不会变异。
*   最后，通过将 RDD 转储到存储中的操作来具体化 RDD 对象或更多对象(具体来说，数据引用)。
*   驱动程序可以向计算节点请求一大块结果，用于程序的分析或可视化。

等等！到目前为止，我们进展顺利。我们假设您将把应用程序代码发送到集群中的计算节点。尽管如此，您仍必须将输入数据集上传或发送到集群，以便在计算节点之间分发。即使在批量上传期间，您也必须通过网络传输数据。我们还认为应用程序代码和结果的大小可以忽略或微不足道。另一个障碍是，如果您希望 Spark 以规模计算的方式处理数据，它可能需要首先从多个分区中合并数据对象。这意味着我们将需要在工作/计算节点之间洗牌，这通常由`partition()`、`intersection()`和`join()`转换操作来完成。

# 斯卡拉和火花生态系统

为了提供更多的增强和额外的大数据处理能力，可以在现有的基于 Hadoop 的集群上配置和运行 Spark。另一方面，Spark 中的核心 API 是用 Java、Scala、Python 和 r 编写的，与 MapReduce 相比，凭借更通用、更强大的编程模型，Spark 还提供了几个作为 Spark 生态系统一部分的库，用于通用数据处理和分析、图形处理、大规模结构化 SQL 和**机器学习** ( **ML** )领域的附加功能。

火花生态系统由以下组件组成，如图所示(详情请参考[第 16 章](16.html#E9OE01-21aec46d8593429cacea59dbdcd64e1c)、*火花调谐*):

*   **Apache Spark 核心**:这是 Spark 平台的底层引擎，所有其他功能都是建立在这个基础上的。此外，它还提供内存处理。
*   **Spark SQL** :如前所述，Spark core 是底层引擎，所有其他组件或功能都建立在其上。Spark SQL 是 Spark 组件，为不同的数据结构(结构化和半结构化数据)提供支持。
*   **Spark streaming** :这个组件负责为分析流式传输数据，并将其转换为小批量，以便以后用于分析。
*   **MLlib(机器学习库)** : MLlib 是一个机器学习框架，以分布式的方式支持大量的 ML 算法。
*   **GraphX** :建立在 Spark 之上的分布式图框架，以并行方式表达用户定义的图组件。

如前所述，大多数函数式编程语言允许用户编写漂亮的、模块化的和可扩展的代码。此外，函数式编程通过编写看起来像数学函数的函数来鼓励安全的编程方式。那么，Spark 是如何让所有的 API 作为一个单元工作的呢？这是可能的，因为硬件的进步，当然还有功能编程的概念。由于添加语法糖来轻松实现 lambda 表达式不足以使语言具有功能，所以这只是一个开始。

尽管 Spark 中的 RDD 概念工作得很好，但是由于它的不变性，有很多用例有点复杂。对于以下计算平均值的经典示例，使源代码健壮且可读；当然，为了降低总体成本，即使数据缓存在主内存中，也不希望先计算总数，然后再进行计数。

```scala
val data: RDD[People] = ...data.map(person => (person.name, (person.age, 1))).reduceByKey(_ |+| _).mapValues { case (total, count) =>total.toDouble / count}.collect()

```

数据框架应用编程接口(这将在后面的章节中详细讨论)产生同样简洁可读的代码，其中功能性应用编程接口非常适合大多数用例，并最大限度地减少了 MapReduce 阶段；有许多洗牌会造成巨大的成本，其主要原因如下:

*   大型代码库需要静态输入来消除琐碎的错误，例如 *aeg* 而不是 *age* 瞬间
*   复杂的代码需要透明的 API 来清晰地传达设计
*   通过面向对象封装状态以及使用映射分区和组合密钥，同样可以通过幕后变异实现数据帧应用编程接口的 2 倍加速
*   快速构建功能需要灵活性和 Scala 特性

OOP 和 FP 与 Spark 的结合可以让巴克莱的难题变得更加容易。例如，在巴克莱，最近开发了一个名为 Insights Engine 的应用程序，可以执行任意 N 个近乎任意的类似 SQL 的查询。应用程序可以通过随 n 增加而扩展的方式来执行它们

现在我们来谈谈纯函数、高阶函数和匿名函数，这是 Scala 函数编程中的三个重要概念。

# 纯函数和高阶函数

从计算机科学的角度来看，函数可以有许多形式，如一阶函数、高阶函数或纯函数。从数学的角度来看也是如此。使用高阶函数是可以执行以下操作之一的函数:

*   接受一个或多个函数作为参数来执行一些操作
*   返回一个函数作为其结果

除了高阶函数之外，所有其他函数都是一阶函数。然而，从数学的角度来看，高阶函数也被称为**算子**或**泛函**。另一方面，如果一个函数的返回值只由它的输入决定，当然没有可观察到的副作用，它被称为**纯函数**。

在本节中，我们将简要讨论为什么以及如何在 Scala 中使用不同的函数范式。尤其是纯函数和高阶函数。在本节的最后，还将提供使用匿名函数的简要概述，因为这在使用 Scala 开发 Spark 应用程序时经常使用。

# 纯函数

函数式编程最重要的原则之一是纯函数。那么什么是纯函数，我们为什么要关心它们呢？在本节中，我们将讨论函数式编程的这一重要特性。函数式编程的最佳实践之一是实现您的程序，使您的程序/应用程序的核心由纯函数组成，并且所有的输入/输出功能或副作用(如网络开销和异常)都在一个暴露的外层中。

那么纯函数有什么好处呢？纯函数通常比正常函数小(尽管这取决于编程语言等其他因素)，对于人脑来说甚至更容易解释和理解，因为它看起来像一个数学函数。

然而，你可能会反对这一点，因为大多数开发人员仍然觉得命令式编程更容易理解！纯函数更容易实现和测试。让我们通过一个例子来证明这一点。假设我们有以下两个独立的函数:

```scala
def pureFunc(cityName: String) = s"I live in $cityName"def notpureFunc(cityName: String) = println(s"I live in $cityName")

```

所以在前面的两个例子中，如果你想测试`pureFunc`纯函数，我们只需要根据我们的输入断言来自纯函数的返回值，比如:

```scala
assert(pureFunc("Dublin") == "I live in Dublin")

```

但另一方面，如果我们想测试我们的`notpureFunc`不纯函数，那么我们需要重定向标准输出，然后对其应用断言。下一个实用技巧是函数式编程使程序员更有效率，因为，如前所述，纯函数更小，更容易编写，您可以轻松地将它们组合在一起。此外，代码的重复很少，您可以轻松地重用您的代码。现在让我们用一个更好的例子来证明这个优势。考虑这两个函数:

```scala
scala> def pureMul(x: Int, y: Int) = x * ypureMul: (x: Int, y: Int)Intscala> def notpureMul(x: Int, y: Int) = println(x * y)notpureMul: (x: Int, y: Int)Unit

```

然而，可变性可能有副作用；使用纯函数(即没有可变性)有助于我们推理和测试代码:

```scala
def pureIncrease(x: Int) = x + 1

```

这一个是有利的，非常容易解释和使用。但是，让我们看另一个例子:

```scala
varinc = 0def impureIncrease() = {inc += 1inc}

```

现在，考虑一下这会有多混乱:多线程环境中的输出会是什么？正如你所看到的，我们可以很容易地使用我们的纯函数`pureMul`来乘以任何数字序列，不像我们的`notpureMul`不纯函数。让我们通过以下示例来演示这一点:

```scala
scala> Seq.range(1,10).reduce(pureMul)res0: Int = 362880

```

前面示例的完整代码如下所示(方法是使用一些实值调用的):

```scala
package com.chapter3.ScalaFPobject PureAndNonPureFunction {def pureFunc(cityName: String) = s"I live in $cityName"def notpureFunc(cityName: String) = println(s"I live in $cityName")def pureMul(x: Int, y: Int) = x * ydef notpureMul(x: Int, y: Int) = println(x * y)  def main(args: Array[String]) {//Now call all the methods with some real valuespureFunc("Galway") //Does not print anythingnotpureFunc("Dublin") //Prints I live in DublinpureMul(10, 25) //Again does not print anythingnotpureMul(10, 25) // Prints the multiplicaiton -i.e. 250   //Now call pureMul method in a different wayval data = Seq.range(1,10).reduce(pureMul)println(s"My sequence is: " + data)}}

```

前面代码的输出如下:

```scala
I live in Dublin 250 My sequence is: 362880

```

如前所述，您可以将纯函数视为函数式编程最重要的特性之一，并将其视为最佳实践；您需要使用纯函数构建应用程序的核心。

Functions versus methods:
In the programming realm, a **function** is a piece of code called by a name. Data (as an argument or as a parameter) can be passed to operate on and can return data (optionally). All data passed to a function is passed explicitly. A **method,** on the other hand, is also a piece of code that is called by a name too. However, a method is always associated with an object. Sounds similar? Well! In most cases, a method is identical to a function except for two key differences:
1\. A method is implicitly passed the object on which it was called.
2\. A method is able to operate on data that is contained within the class.

It is already stated in the previous chapter that an object is an instance of a class--the class is the definition, the object is an instance of that data.

现在是学习高阶函数的时候了。然而，在此之前，我们应该学习函数 Scala 中一个更重要的概念- **匿名函数**。通过这个，我们还将学习如何将 lambda 表达式与函数式 Scala 一起使用。

# 匿名函数

有时在代码中，您不想在使用之前定义函数，可能是因为您将在一个地方使用它。在函数式编程中，有一种函数非常适合这种情况。这叫做匿名函数。让我们使用前面的转账示例来演示匿名函数的使用:

```scala
def TransferMoney(money: Double, bankFee: Double => Double): Double = {money + bankFee(money)}

```

现在，让我们用一些实值来调用`TransferMoney()`方法，如下所示:

```scala
 TransferMoney(100, (amount: Double) => amount * 0.05)

```

Lambda expression:
As already stated, Scala supports first-class functions, which means functions can be expressed in function-literal syntax as well; functions can be represented by objects, called function values. Try the following expression, it creates a successor function for integers:
`scala> var apply = (x:Int) => x+1`
`apply: Int => Int = <function1>`
The apply variable is now a function that can be used in the usual way as follows:
`scala> var x = apply(7)`
`x: Int = 8`
What we have done here is simply use the core of a function: the argument list followed by the function arrow and the body of the function. This one is not black magic but a full-fledged function, only without a given name--that is, anonymous. If you define a function this way, there will be no way to refer to that function afterward and hence you couldn't call that function afterward because without a name it's an anonymous one. Also, we have a so-called **lambda expression**! It's just the pure, anonymous definition of a function.

前面代码的输出如下:

```scala
105.0

```

因此，在前面的例子中，我们没有声明一个单独的`callback`函数，而是直接传递了一个匿名函数，它做了和`bankFee`函数一样的工作。您也可以省略匿名函数中的类型，它将根据传递的参数直接推断出来，如下所示:

```scala
TransferMoney(100, amount => amount * 0.05)

```

前面代码的输出如下:

```scala
105.0

```

让我们在 Scala 外壳上演示前面的例子，如下面的截图所示:

![](../images/00066.jpeg)

**Figure 6:** Use of the anonymous function in Scala

一些具有函数支持的编程语言使用名称 lambda 函数而不是匿名函数。

# 高阶函数

在 Scala 的函数式编程中，您可以将函数作为参数传递，甚至可以将一个函数作为另一个函数的结果返回；这定义了所谓的高阶函数。

让我们通过一个例子来演示这个特性。考虑以下函数`testHOF`，该函数采用另一个函数`func`，然后将该函数应用于其第二个参数值:

```scala
object Test {def main(args: Array[String]) {println( testHOF( paramFunc, 10) )}def testHOF(func: Int => String, value: Int) = func(value)def paramFunc[A](x: A) = "[" + x.toString() + "]"}

```

在演示了 Scala 函数式编程的基础之后，现在我们准备转向更复杂的函数式编程案例。如前所述，我们可以将高阶函数定义为接受其他函数作为参数并将其作为结果返回的函数。如果你来自面向对象的编程背景，你会发现这是一种非常不同的方法，但是随着我们的深入，它会变得更容易理解。

让我们从定义一个简单的函数开始:

```scala
def quarterMaker(value: Int): Double = value.toDouble/4

```

前面的函数非常简单。这是一个函数，接受一个 Int 值，然后以`Double`类型返回这个值的四分之一。让我们定义另一个简单的函数:

```scala
def addTwo(value: Int): Int = value + 2

```

第二个函数`addTwo`比第一个更琐碎。它接受一个`Int`值，然后加 2。如您所见，这两个函数有一些共同点。它们都接受`Int`并返回另一个我们可以称之为`AnyVal`的处理值。现在，让我们定义一个接受其参数中另一个函数的高阶函数:

```scala
def applyFuncOnRange(begin: Int, end: Int, func: Int => AnyVal): Unit = {for (i <- begin to end)println(func(i))}

```

如您所见，前面的函数`applyFuncOnRange`接受两个`Int`值，作为序列的开始和结束，并且它接受一个具有`Int => AnyVal`签名的函数，就像前面定义的简单函数(`quarterMakder`和`addTwo`)一样。现在让我们通过将两个简单函数中的一个作为第三个参数传递给它来演示我们之前的高阶函数(如果您想要传递您自己的函数，那么请确保它具有相同的签名`Int => AnyVal`)。

**Scala syntax for loop with ranges:** The simplest syntax of using a for loop with ranges in Scala is:
`for( var x <- range ){`
`statement(s)`
`}`
Here, the `range` could be a range of numbers and is represented as `i` to `j` or sometimes like `i` until `j`. The left-arrow `←` operator is called a generator because it's generating individual values from a range. Let's see a concrete example of this feature:
`object UsingRangeWithForLoop {`
`def main(args: Array[String]):Unit= {`
`var i = 0;`
`// for loop execution with a range`
`for( i <- 1 to 10){`
`println( "Value of i: " + i )`
`}`
`}`
`}`
The output of the preceding code is as follows:
`Value of i: 1`
`Value of i: 2`
`Value of i: 3`
`Value of i: 4`
`Value of i: 5`
`Value of i: 6`
`Value of i: 7`
`Value of i: 8`
`Value of i: 9`
`Value of i: 10`

让我们首先定义我们的函数，然后再开始使用它们，如下图所示:

![](../images/00070.jpeg)

**Figure 2:** An example of defining a higher-order function in Scala

现在，让我们从调用我们的高阶函数`applyFuncOnRange`开始，并将`quarterMaker`函数作为第三个参数传递:

![](../images/00074.jpeg)

**Figure 3:** Calling a higher-order function

我们甚至可以应用另一个函数`addTwo`，因为它具有如下截图所示的相同签名:

![](../images/00079.jpeg)

**Figure 4:** An alternative way of calling a higher-order function

在进入更多示例之前，让我们定义一下什么叫做回调函数。回调函数是可以作为参数传递给另一个函数的函数。其他功能只是普通功能。让我们演示更多使用不同回调函数的例子。考虑下面的高阶函数，它负责从您的帐户中转移特定数量的钱:

```scala
def TransferMoney(money: Double, bankFee: Double => Double): Double = {money + bankFee(money)}def bankFee(amount: Double) = amount * 0.05

```

在 100 上调用`TransferMoney`功能后:

```scala
TransferMoney(100, bankFee)

```

前面代码的输出如下:

```scala
105.0

```

从功能编程的角度来看，该代码还没有准备好集成到银行系统中，因为您需要对货币参数应用不同的验证，例如它必须是正数并且大于银行指定的特定金额。然而，这里我们只是演示高阶函数和回调函数的使用。

因此，这个例子的工作原理如下:您想要将特定金额的钱转移到另一个银行帐户或货币代理。银行会根据您转账的金额收取特定的费用，回调函数的作用就来了。它需要转账的金额，并向其收取银行费用，以便得出总额。

`TransferMoney`函数取两个参数:第一个是要转账的钱，第二个是带有签名`Double => Double`的回调函数，该函数应用于货币参数，以确定转账的钱的银行费用。

![](../images/00083.jpeg)

**Figure 5:** Calling and giving extra power to the higher-order function

前面例子的完整源代码可以看如下(我们使用一些实值调用方法):

```scala
package com.chapter3.ScalaFPobject HigherOrderFunction {def quarterMaker(value: Int): Double = value.toDouble / 4def testHOF(func: Int => String, value: Int) = func(value)def paramFunc[A](x: A) = "[" + x.toString() + "]"def addTwo(value: Int): Int = value + 2def applyFuncOnRange(begin: Int, end: Int, func: Int => AnyVal): Unit = {for (i <- begin to end)println(func(i))}def transferMoney(money: Double, bankFee: Double => Double): Double = {money + bankFee(money)}def bankFee(amount: Double) = amount * 0.05def main(args: Array[String]) {//Now call all the methods with some real valuesprintln(testHOF(paramFunc, 10)) // Prints [10]println(quarterMaker(20)) // Prints 5.0println(paramFunc(100)) //Prints [100]println(addTwo(90)) // Prints 92println(applyFuncOnRange(1, 20, addTwo)) // Prints 3 to 22 and ()println(TransferMoney(105.0, bankFee)) //prints 110.25}}

```

前面代码的输出如下:

```scala
[10] 5.0 [100] 92 3 4 5 6 7 8 9 10 11 12 13 14 15 16 1718 19 20 21 22 () 110.25

```

通过使用回调函数，你给了高阶函数额外的能力；因此，这是一个非常强大的机制，可以使您的程序更加优雅、灵活和高效。

# 用作返回值

如上所述，高阶函数也支持返回函数结果。让我们通过一个例子来证明这一点:

```scala
def transferMoney(money: Double) = {if (money > 1000)(money: Double) => "Dear customer we are going to add the followingamount as Fee: "+money * 0.05else(money: Double) => "Dear customer we are going to add the followingamount as Fee: "+money * 0.1} val returnedFunction = TransferMoney(1500)returnedFunction(1500)

```

前面的代码段将产生以下输出:

```scala
Dear customer, we are going to add the following amount as Fee: 75.0

```

让我们运行前面的例子，如下面的截图所示；它显示了如何使用函数作为返回值:

![](../images/00087.jpeg)

**Figure 7:** Function as a return value

前面示例的完整代码如下所示:

```scala
package com.chapter3.ScalaFPobject FunctionAsReturnValue {def transferMoney(money: Double) = {if (money > 1000)(money: Double) => "Dear customer, we are going to add followingamount as Fee: " + money * 0.05else(money: Double) => "Dear customer, we are going to add followingamount as Fee: " + money * 0.1}  def main(args: Array[String]) {val returnedFunction = transferMoney(1500.0)println(returnedFunction(1500)) //Prints Dear customer, we are going to add following amount as Fee: 75.0}}

```

前面代码的输出如下:

```scala
Dear customer, we are going to add following amount as Fee: 75.0

```

在停止讨论 HFO 之前，让我们看一个真实的例子，那就是利用 HFO 来讨好。

# 使用高阶函数

假设你在一家餐厅做厨师，你的一个同事问你一个问题:实现一个 **HOF** ( **高阶函数**)来执行讨好。寻找线索？假设您的 HOF 有以下两个签名:

```scala
def curry[X,Y,Z](f:(X,Y) => Z) : X => Y => Z

```

类似地，实现一个如下执行解绕的函数:

```scala
def uncurry[X,Y,Z](f:X => Y => Z): (X,Y) => Z

```

现在，你怎么能用 HOFs 来做这个手术呢？好吧，你可以创建一个特性，封装两个 HOFs(即 curry 和 uncurry)的签名，如下所示:

```scala
trait Curry {def curry[A, B, C](f: (A, B) => C): A => B => Cdef uncurry[A, B, C](f: A => B => C): (A, B) => C}

```

现在，您可以将这个特性作为一个对象来实现和扩展，如下所示:

```scala
object CurryImplement extends Curry {def uncurry[X, Y, Z](f: X => Y => Z): (X, Y) => Z = { (a: X, b: Y) => f(a)(b) }def curry[X, Y, Z](f: (X, Y) => Z): X => Y => Z = { (a: X) => { (b: Y) => f(a, b) } }}

```

这里我先实现了 uncurry，因为它更容易。等号后面的两个花括号是一个匿名函数字面量，用于接受两个参数(即分别为类型`X`和`Y`的`a`和`b`)。然后，这两个参数可以用在同样返回函数的函数中。然后，它将第二个参数传递给返回的函数。最后，它返回第二个函数的值。第二个函数文本接受一个参数并返回一个新函数，即`curry()`。最终，它在被调用时返回一个函数，并返回另一个函数。

现在来了:如何在现实实现中使用前面的扩展基本特性的对象。这里有一个例子:

```scala
object CurryingHigherOrderFunction {def main(args: Array[String]): Unit = {def add(x: Int, y: Long): Double = x.toDouble + yval addSpicy = CurryImplement.curry(add) println(addSpicy(3)(1L)) // prints "4.0"    val increment = addSpicy(2) println(increment(1L)) // prints "3.0"    val unspicedAdd = CurryImplement.uncurry(addSpicy) println(unspicedAdd(1, 6L)) // prints "7.0"}}

```

在前面的对象和主方法中:

*   `addSpicy`保存一个函数，该函数将一个 long 作为类型，并向其添加 1，然后打印 4.0。
*   `increment`保存一个函数，该函数将一个 long 作为类型，并向其添加 2，最后打印 3.0。
*   `unspicedAdd`保存一个加 1 并取长整型的函数。最后，它打印 7.0。

前面代码的输出如下:

```scala
4.03.07.0
```

In mathematics and computer science, currying is the technique of translating the evaluation of a function that takes multiple arguments (or a tuple of arguments) into evaluating a sequence of functions, each with a a single argument. Currying is related to, but not the same as, partial application:
**Currying:** Currying is useful in both practical and theoretical settings. In functional programming languages, and many others, it provides a way of automatically managing how arguments are passed to functions and exceptions. In theoretical computer science, it provides a way to study functions with multiple arguments in simpler theoretical models, which provide only one argument.
**Uncurrying:** Uncurrying is the dual transformation to currying, and can be seen as a form of defunctionalization. It takes a function `f` whose return value is another function `g` and yields a new function `f′` that takes as parameters the arguments for both `f` and `g`, and returns, as a result, the application of `f` and subsequently, `g`, to those arguments. The process can be iterated.

到目前为止，我们已经看到了如何在 Scala 中处理纯函数、高阶函数和匿名函数。现在，让我们在下一节中简要概述如何使用`Throw`、`Try`、`Either`和`Future`扩展高阶函数。

# 函数 Scala 中的错误处理

到目前为止，我们专注于确保 Scala 函数的主体做它应该做的事情，而不做任何其他事情(也就是说，一个错误或异常)。现在，为了利用任何编程并避免产生容易出错的代码，您需要知道如何用这种语言捕捉异常并处理错误。我们将看到如何使用 Scala 的一些特殊特性来扩展集合之外的高阶函数，例如`Try`、`Either`和`Future`。

# Scala 中的失败和异常

首先，让我们定义一下我们通常所说的故障是什么意思(来源:[https://ters systems . com/2012/12/27/Scala 中的错误处理/](https://tersesystems.com/2012/12/27/error-handling-in-scala/) ):

*   **意外的内部故障**:由于未实现的期望，如空指针引用、违反的断言或简单的坏状态，操作失败
*   **预期内部故障**:由于内部状态，即黑名单或断路器，操作故意失败
*   **预期外部故障**:操作失败是因为被告知要处理一些原始输入，如果无法处理原始输入就会失败
*   **意外外部故障**:操作失败，因为系统依赖的资源不在:文件句柄松动，数据库连接失败，或者网络故障

不幸的是，没有具体的方法来阻止失败，除非失败是由于一些可管理的异常。另一方面，Scala 使*检查和未检查*变得非常简单:它没有检查异常。所有的异常在 Scala 中都是不被检查的，甚至是`SQLException`和`IOException`等等。现在让我们看看至少如何处理这样的异常。

# 引发异常

Scala 方法可能会因为意外的工作流而引发异常。创建一个异常对象，然后用 throw 关键字抛出它，如下所示。例如:

```scala
//code somethingthrow new IllegalArgumentException("arg 2 was wrong...");//nothing will be executed from here.

```

请注意，使用异常处理的主要目标不是产生友好的消息，而是退出 Scala 程序的正常流程。

# 使用尝试和捕获来捕获异常

Scala 允许您尝试/捕获单个块中的任何异常，然后使用案例块对其执行模式匹配。Scala 中使用`try...catch`的基本语法如下:

```scala
try{// your scala code should go here} catch{case foo: FooException => handleFooException(foo)case bar: BarException => handleBarException(bar)case _: Throwable => println("Got some other kind of exception")}finally{// your scala code should go here, such as to close a database connection }

```

因此，如果您抛出一个异常，那么您需要使用`try...catch`块来很好地处理它，而不会因内部异常消息而崩溃:

```scala
package com.chapter3.ScalaFPimport java.io.IOExceptionimport java.io.FileReaderimport java.io.FileNotFoundExceptionobject TryCatch {def main(args: Array[String]) {try {val f = new FileReader("data/data.txt")} catch {case ex: FileNotFoundException => println("File not found exception")case ex: IOException => println("IO Exception") } }}

```

如果您的项目树下的路径/数据中没有名为`data.txt`的文件，您将会遇到如下的`FileNotFoundException`:

前面代码的输出如下:

```scala
File not found exception

```

现在，我们来看一个简单的例子，在 Scala 中使用`finally`子句来完成`try...catch`块。

# 最后

假设无论是否抛出异常，您都希望执行代码，那么您应该使用`finally`子句。您可以将其放入`try block`内，如下所示。这里有一个例子:

```scala
try {val f = new FileReader("data/data.txt")} catch {case ex: FileNotFoundException => println("File not found exception")} finally { println("Dude! this code always executes") }}

```

下面是使用`try...catch...finally`的完整例子:

```scala
package com.chapter3.ScalaFPimport java.io.IOExceptionimport java.io.FileReaderimport java.io.FileNotFoundExceptionobject TryCatch {def main(args: Array[String]) {try {val f = new FileReader("data/data.txt")} catch {case ex: FileNotFoundException => println("File not found exception")case ex: IOException => println("IO Exception") } finally {println("Finally block always executes!")}}}

```

前面代码的输出如下:

```scala
File not found exception Finally block always executes!

```

接下来，我们将讨论 Scala 中另一个名为`Either`的强大特性。

# 创建任一

`Either[X, Y]`是包含`X`的一个实例或`Y`的一个实例的实例，但不是两者都包含。我们称这些子类型为“任一”的左和右。创建一个非此即彼是微不足道的。但是有时在你的程序中使用它是非常强大的:

```scala
package com.chapter3.ScalaFPimport java.net.URLimport scala.io.Sourceobject Either {def getData(dataURL: URL): Either[String, Source] =if (dataURL.getHost.contains("xxx"))Left("Requested URL is blocked or prohibited!")elseRight(Source.fromURL(dataURL))      def main(args: Array[String]) {val either1 = getData(new URL("http://www.xxx.com"))    println(either1)      val either2 = getData(new URL("http://www.google.com"))    println(either2)}}

```

现在，如果我们传递任何不包含`xxx`的任意 URL，那么我们将得到一个包装在`Right`子类型中的`Scala.io.Source`。如果网址包含`xxx`，那么我们将得到一个包裹在`Left`子类型中的`String`。为了使前面的语句更加清晰，让我们看看前面代码段的输出:

```scala
Left(Requested URL is blocked or prohibited!) Right(non-empty iterator)

```

接下来，我们将探索 Scala 的另一个有趣的特性`Future`，它被用来以非阻塞的方式执行任务。这也是当他们完成时处理结果的更好方法。

# 将来的

如果您只想以非阻塞的方式运行任务，并且需要一种在任务完成时处理结果的方法，Scala 为您提供了 Futures，例如，如果您想以并行的方式进行多个 web 服务调用，并在 web 服务处理完所有这些调用后处理结果。下一节提供了一个使用未来的示例。

# 运行一个任务，但阻塞

下面的示例演示了如何创建一个未来，然后阻塞执行序列以等待其结果。创造未来是微不足道的。你只需要把它传递给你想要的代码。以下示例在将来执行 2+2，然后返回结果:

```scala
package com.chapter3.ScalaFPimport scala.concurrent.ExecutionContext.Implicits.globalimport scala.concurrent.duration._import scala.concurrent.{Await, Future}object RunOneTaskbutBlock {def main(args: Array[String]) {// Getting the current time in Millisecondsimplicit val baseTime = System.currentTimeMillis    // Future creationval testFuture = Future {Thread.sleep(300)2 + 2}    // this is the blocking partval finalOutput = Await.result(testFuture, 2 second)println(finalOutput)}}

```

`Await.result`方法最多等待 2 秒，直到`Future`返回结果；如果它在 2 秒钟内没有返回结果，它将引发您可能想要处理或捕获的以下异常:

```scala
java.util.concurrent.TimeoutException

```

该结束这一章了。然而，我想借此机会讨论我对 Scala 函数编程和对象可变性的一个重要观点。

# 函数式编程和数据可变性

纯函数式编程是函数式编程的最佳实践之一，您应该坚持下去。编写纯函数将使您的编程生活更加容易，并且您将能够编写易于维护和扩展的代码。此外，如果您想要并行化代码，那么如果您编写纯函数，那么这样做将会更容易。

如果你是一个 FP 纯粹主义者，在 Scala 中使用函数式编程的一个缺点是 Scala 同时支持 OOP 和 FP(见*图 1* )，因此可以在同一个代码库中混合两种编码风格。在本章中，我们看到了几个例子，表明编写纯函数很容易。然而，将它们组合成一个完整的应用程序是困难的。你可能会同意像单子这样的高级主题会让 FP 变得令人生畏。

我和很多人谈过，他们认为递归感觉不太自然。当您使用不可变对象时，您永远不能用其他东西来改变它们。没有什么时候你可以这样做。这就是不可变对象的全部意义！有时我所经历的是，一个纯粹的函数和数据输入或输出真的混淆了。但是，当您需要变异时，您可以创建一个包含变异字段的对象副本。因此，理论上没有必要将*和*混为一谈。最后，仅使用不可变值和递归可能会导致 CPU 使用和内存方面的性能问题。

# 摘要

在本章中，我们探讨了 Scala 中的一些函数式编程概念。我们已经看到了什么是函数式编程，Scala 如何支持它，为什么它很重要，以及使用函数式概念的优势。我们已经看到了为什么学习 FP 概念对于学习 Spark 范式很重要。用适当的例子讨论了纯函数、匿名函数和高阶函数。在本章的后面，我们看到了如何使用 Scala 的标准库来处理集合之外的高阶函数中的异常。最后，我们讨论了功能 Scala 如何影响对象可变性。

在下一章中，我们将对 Collections API 进行深入分析，这是标准库最突出的特性之一。