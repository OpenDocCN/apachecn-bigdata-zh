# Apache Spark MLlib

MLlib 是最初的机器学习库，由基于内存集群的开源数据处理系统 Apache Spark 提供。该库仍然基于 RDD 应用编程接口。在本章中，我们将从回归、分类和神经网络处理等方面来研究 MLlib 库提供的功能。在提供解决实际问题的工作示例之前，我们将研究每种算法背后的理论。web 上的示例代码和文档可能很少，也很混乱。

我们将采取循序渐进的方法来描述如何使用以下算法以及它们能够做什么:

*   体系结构
*   朴素贝叶斯分类
*   用 K-均值进行聚类
*   利用**人工神经网络**进行图像分类

# 体系结构

请记住，虽然 Spark 用于其内存中分布式处理的速度，但它不提供存储。您可以使用主机(本地)文件系统来读写您的数据，但是如果您的数据量大到足以被描述为大数据，那么使用基于云的分布式存储系统是有意义的，例如 OpenStack Swift 对象存储，它可以在许多云环境中找到，也可以安装在私有数据中心。

In case very high I/O is needed, HDFS would also be an option. More information on HDFS can be found here: [http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html).

# 发展环境

Scala 语言将用于本书的编码示例。这是因为，作为一种脚本语言，它产生的代码比 Java 少。它也可以从 Spark shell 中使用，也可以用 Apache Spark 应用程序编译。我们将使用 **sbt 工具**来编译 Scala 代码，我们已经将其安装到 Hortonworks HDP 2.6 沙盒中，如下所示:

```scala
[hadoop@hc2nn ~]# sudo su -
[root@hc2nn ~]# cd /tmp
[root@hc2nn ~]#wget http://repo.scala-sbt.org/scalasbt/sbt-native-packages/org/scala-sbt/sbt/0.13.1/sbt.rpm
[root@hc2nn ~]# rpm -ivh sbt.rpm
```

以下网址提供了在包括 Windows、Linux 和 macOS 在内的其他操作系统上安装 sbt 的说明:[http://www.scala-sbt.org/0.13/docs/Setup.html](http://www.scala-sbt.org/0.13/docs/Setup.html)。

我们使用了一个名为 **Hadoop** 的通用 Linux 帐户。如前面的命令所示，我们需要安装`sbt`作为根帐户，我们已经通过`sudo su -l`(切换用户)访问了该帐户。然后我们使用`wget`从名为`repo.scala-sbt.org`的网络服务器下载`sbt.rpm`文件到`/tmp`目录。最后，我们使用`rpm`命令安装了`rpm`文件，选项为`i`用于安装，`v`用于验证，以及`h`用于在安装软件包时打印散列标记。

我们使用 Linux Hadoop 帐户，在 Linux 服务器上开发了本章中 Apache Spark 的所有 Scala 代码。我们将每组代码放在`/home/hadoop/spark`下的子目录中。例如，下面的`sbt`结构图显示了 MLlib 朴素贝叶斯代码存储在 Spark 目录下名为`nbayes`的子目录中。图中还显示，Scala 代码是在`nbayes`目录下名为`src/main/scala`的子目录结构中开发的。名为`bayes1.scala`和`convert.scala`的文件包含将在下一节中使用的朴素贝叶斯代码:

![](Images/4e6ee246-91df-45b4-b62d-91322e97ea0f.png)

`bayes.sbt`文件是`sbt`工具使用的配置文件，描述了如何在 Scala 目录下编译 Scala 文件。(注意，如果你用 Java 开发，你会使用`nbayes/src/main/java`形式的路径。)接下来显示`bayes.sbt`文件的内容。`pwd`和`cat` Linux 命令提醒您文件位置，也提醒您转储文件内容。

`name`、`version`和`scalaVersion`选项设置了项目的细节和要使用的 Scala 版本。`libraryDependencies`选项定义了 Hadoop 和 Spark 库的位置。

```scala
[hadoop@hc2nn nbayes]$ pwd
/home/hadoop/spark/nbayes
[hadoop@hc2nn nbayes]$ cat bayes.sbt 
name := "Naive Bayes"
version := "1.0"
scalaVersion := "2.11.2"
libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "2.8.1"
libraryDependencies += "org.apache.spark" %% "spark-core" % "2.6.0"
libraryDependencies += "org.apache.spark" %% "spark-mllib" % "2.1.1"
```

可以使用以下命令从`nbayes`子目录中编译 Scala `nbayes`项目代码:

```scala
[hadoop@hc2nn nbayes]$ sbt compile
```

`sbt compile`命令用于将代码编译成类。然后这些类被放在`nbayes/target/scala-2.10/classes`目录中。编译后的类可以用以下命令打包到一个 JAR 文件中:

```scala
[hadoop@hc2nn nbayes]$ sbt package
```

`sbt package`命令将在`nbayes/target/scala-2.10`目录下创建一个 JAR 文件。正如我们在 **sbt 结构图**的例子中看到的，名为`naive-bayes_2.10-1.0.jar`的 JAR 文件是在成功编译和打包之后创建的。这个 JAR 文件及其包含的类可以在`spark-submit`命令中使用。这将在后面随着 Apache Spark MLlib 模块中的功能的探索而描述。

# 朴素贝叶斯分类

本节将提供一个 Apache Spark MLlib 朴素贝叶斯算法的工作示例。它将描述算法背后的理论，并将在 Scala 中提供一个分步示例来展示如何使用该算法。

# 分类理论

为了使用朴素贝叶斯算法对数据集进行分类，数据必须是线性可分的；也就是说，数据中的类必须能被类边界线性整除。下图用虚线显示的三个数据集和两个类边界直观地解释了这一点:

![](Images/5d57d6e1-50f8-4545-adac-23de9f528038.png)

朴素贝叶斯假设数据集内的特征(或维度)彼此独立；也就是说，它们对彼此没有影响。以下示例将电子邮件分类为垃圾邮件。如果您有 100 封电子邮件，请执行以下操作:

```scala
60% of emails are spam
80% of spam emails contain the word buy
20% of spam emails don't contain the word buy
40% of emails are not spam
10% of non spam emails contain the word buy
90% of non spam emails don't contain the word buy
```

让我们将这个例子转换成条件概率，这样朴素贝叶斯分类器就可以提取它:

```scala
P(Spam) = the probability that an email is spam = 0.6
P(Not Spam) = the probability that an email is not spam = 0.4
P(Buy|Spam) = the probability that an email that is spam has the word buy = 0.8
P(Buy|Not Spam) = the probability that an email that is not spam has the word buy = 0.1
```

包含“购买”字样的电子邮件是垃圾邮件的概率有多大？嗯，这可以写成 *P(垃圾邮件|购买)*。朴素贝叶斯说它由下图中的等式描述:

![](Images/3adfb0ba-fb3c-4349-9460-d2957e12edc5.png)

因此，使用前面的百分比数字，我们得到以下结果:

*P(Spam | Buy)=(0.8 * 0.6)/((0.8 * 0.6)+(0.1 * 0.4))=(. 48)/(. 48+. 04)*

*= .48 / .52 = .923*

这意味着包含“购买”一词的电子邮件是垃圾邮件的可能性增加了 92%。这是对理论的一种审视；现在是时候尝试一个使用 Apache Spark MLlib 朴素贝叶斯算法的真实例子了。

# 实践中的朴素贝叶斯

第一步是选择一些将用于分类的数据。我们从英国政府数据网站[http://data.gov.uk/dataset/road-accidents-safety-data](http://data.gov.uk/dataset/road-accidents-safety-data)上选择了一些数据。

数据集名为**道路安全-2013 年数字呼气测试数据**，该数据集下载了一个名为`DigitalBreathTestData2013.txt`的压缩文本文件。该文件包含大约 50 万行。数据如下:

```scala
Reason,Month,Year,WeekType,TimeBand,BreathAlcohol,AgeBand,GenderSuspicion of Alcohol,Jan,2013,Weekday,12am-4am,75,30-39,MaleMoving Traffic Violation,Jan,2013,Weekday,12am-4am,0,20-24,MaleRoad Traffic Collision,Jan,2013,Weekend,12pm-4pm,0,20-24,Female
```

为了对数据进行分类，我们修改了这两个列...

# 用 K-均值进行聚类

本示例将使用与上一示例相同的测试数据，但是我们将尝试使用 MLlib K-Means 算法在数据中查找聚类。

# 聚类理论

K-Means 算法迭代地试图通过最小化聚类中心向量的平均值和新的候选聚类成员向量之间的距离来确定测试数据内的聚类。以下等式假设数据集成员的范围从 *X1* 到*Xn*；它还假设 *K* 簇集的范围从 *S1* 到 *Sk* ，其中 *K < = n* 。

![](Images/55533695-d1e3-486e-a661-261ea6f75809.png)

# 实践中的 k-均值

K-Means MLlib 功能使用`LabeledPoint`结构来处理其数据，因此它需要数字输入数据。由于上一节中的相同数据正在被重用，我们将不再解释数据转换。本节中数据术语的唯一变化是 HDFS 的处理将在`/data/spark/kmeans/`目录**下进行。**此外，K-Means 示例的转换 Scala 脚本生成了一个全逗号分隔的记录。

K-Means 示例的开发和处理发生在`/home/hadoop/spark/kmeans`目录下，以将工作与其他开发分开。`sbt`配置文件现在称为`kmeans.sbt`，除了项目名称外，与上一个示例相同:

```scala
name := "K-Means"
```

这部分的代码可以在`chapter7\K-Means`下的软件包中找到。因此，查看存储在`kmeans/src/main/scala`下的`kmeans1.scala`的代码，会发生一些类似的动作。导入语句引用了 Spark 上下文和配置。然而，这一次，K-Means 功能是从 MLlib 导入的。此外，本例中的应用类名已更改为`kmeans1`:

```scala
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.clustering.{KMeans,KMeansModel}

object kmeans1 extends App {
```

正在采取与上一个示例中相同的操作来定义数据文件-定义火花配置并创建火花上下文:

```scala
 val hdfsServer = "hdfs://localhost:8020"
 val hdfsPath   = "/data/spark/kmeans/" 
 val dataFile   = hdfsServer + hdfsPath + "DigitalBreathTestData2013-MALE2a.csv"
 val sparkMaster = "spark://localhost:7077"
 val appName = "K-Means 1"
 val conf = new SparkConf()
 conf.setMaster(sparkMaster)
 conf.setAppName(appName)
 val sparkCxt = new SparkContext(conf)
```

接下来，从数据文件中加载 CSV 数据，并用逗号字符分割成`VectorData`变量:

```scala
 val csvData = sparkCxt.textFile(dataFile)
 val VectorData = csvData.map {
   csvLine =>
     Vectors.dense( csvLine.split(',').map(_.toDouble))
 }
```

初始化一个`KMeans`对象，设置参数定义聚类数和最大迭代次数来确定它们:

```scala
 val kMeans = new KMeans
 val numClusters         = 3
 val maxIterations       = 50
```

一些默认值是为初始化模式、运行次数和ε定义的，这是我们在 K-Means 调用中需要的，但在处理过程中没有变化。最后，这些参数是针对`KMeans`对象设置的:

```scala
 val initializationMode = KMeans.K_MEANS_PARALLEL
 val numRuns            = 1
 val numEpsilon         = 1e-4 
 kMeans.setK( numClusters )
 kMeans.setMaxIterations( maxIterations )
 kMeans.setInitializationMode( initializationMode )
 kMeans.setRuns( numRuns )
 kMeans.setEpsilon( numEpsilon )
```

我们缓存了训练向量数据以提高性能，并使用向量数据训练`KMeans`对象以创建训练好的 K-Means 模型:

```scala
 VectorData.cache
 val kMeansModel = kMeans.run( VectorData )
```

我们已经计算了 K-Means 成本和输入数据行数，并且必须通过`println`语句输出结果。成本值表示集群的紧密程度以及集群的分离程度:

```scala
 val kMeansCost = kMeansModel.computeCost( VectorData ) 
 println( "Input data rows : " + VectorData.count() )
 println( "K-Means Cost   : " + kMeansCost )
```

接下来，我们使用 K-Means 模型将聚类中心打印为计算出的三个聚类的向量:

```scala
 kMeansModel.clusterCenters.foreach{ println }
```

最后，我们使用 K-Means 模型预测函数来创建一个集群成员预测列表。然后，我们按值对这些预测进行计数，以给出每个集群中数据点的计数。这显示了哪些集群更大，以及是否真的有三个集群:

```scala
 val clusterRddInt = kMeansModel.predict( VectorData ) 
 val clusterCount = clusterRddInt.countByValue
  clusterCount.toList.foreach{ println }
} // end object kmeans1
```

因此，为了运行这个应用程序，它必须从`kmeans`子目录编译和打包，如 Linux `pwd`命令所示:

```scala
[hadoop@hc2nn kmeans]$ pwd
/home/hadoop/spark/kmeans
[hadoop@hc2nn kmeans]$ sbt package
Loading /usr/share/sbt/bin/sbt-launch-lib.bash
[info] Set current project to K-Means (in build file:/home/hadoop/spark/kmeans/)
[info] Compiling 2 Scala sources to /home/hadoop/spark/kmeans/target/scala-2.10/classes...
[info] Packaging /home/hadoop/spark/kmeans/target/scala-2.10/k-means_2.10-1.0.jar ...
[info] Done packaging.
[success] Total time: 20 s, completed Feb 19, 2015 5:02:07 PM
```

一旦包装成功，我们将检查 HDFS，以确保测试数据准备就绪。如最后一个例子，我们使用软件包中提供的`convert.scala`文件将数据转换成数字形式。我们将处理`/data/spark/kmeans`HDFS 目录中的`DigitalBreathTestData2013-MALE2a.csv`数据文件，如下所示:

```scala
[hadoop@hc2nn nbayes]$ hdfs dfs -ls /data/spark/kmeans
Found 3 items
-rw-r--r--   3 hadoop supergroup   24645166 2015-02-05 21:11 /data/spark/kmeans/DigitalBreathTestData2013-MALE2.csv
-rw-r--r--   3 hadoop supergroup   5694226 2015-02-05 21:48 /data/spark/kmeans/DigitalBreathTestData2013-MALE2a.csv
drwxr-xr-x   - hadoop supergroup         0 2015-02-05 21:46 /data/spark/kmeans/result
```

`spark-submit`工具用于运行 K-Means 应用程序。这个命令唯一的变化就是现在的类是`kmeans1`:

```scala
spark-submit \
 --class kmeans1 \
 --master spark://localhost:7077 \
 --executor-memory 700M \
 --total-executor-cores 100 \
 /home/hadoop/spark/kmeans/target/scala-2.10/k-means_2.10-1.0.jar
```

火花集群运行的输出如下所示:

```scala
Input data rows : 467054
K-Means Cost   : 5.40312223450789E7
```

前面的输出显示了输入数据量，看起来是正确的；它还显示`K-Means cost`值。该成本是基于的集内误差平方和**，它基本上给出了找到的聚类中心与数据点分布匹配程度的度量。它们匹配得越好，成本就越低。以下链接[https://datasciencelab . WordPress . com/2013/12/27/find-the-k-in-k-means-clustering/](https://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/)更详细地解释了 WSSSE 以及如何为 **k** 找到一个好的值。**

接下来是三个向量，它们用正确的维数描述了数据集群中心。请记住，这些聚类质心向量将具有与原始向量数据相同的列数:

```scala
[0.24698249738061878,1.3015883142472253,0.005830116872250263,2.9173747788555207,1.156645130895448,3.4400290524342454] 
[0.3321793984152627,1.784137241326256,0.007615970459266097,2.5831987075928917,119.58366028156011,3.8379106085083468] 
[0.25247226760684494,1.702510963969387,0.006384899819416975,2.231404248000688,52.202897927594805,3.551509158139135]
```

最后，给定聚类 1 至 3 的聚类成员资格，其中聚类 1(索引 0)在`407539`成员向量处具有最大的成员资格:

```scala
(0,407539)
(1,12999)
(2,46516)
```

这两个例子展示了如何使用朴素贝叶斯和 K-Means 对数据进行分类和聚类。如果我想对图像或更复杂的图案进行分类，并使用黑盒方法进行分类，该怎么办？下一节使用**神经网络**或**人工神经网络**来检查基于火花的分类。

# 人工神经网络

下图左边是一个简单的生物神经元。神经元有树突，接收来自其他神经元的信号。细胞体控制激活，轴突将电脉冲传递到其他神经元的树突。右边的人工神经元有一系列加权输入:对输入进行分组的求和函数和**触发机制** ( **F(Net)** )，该机制决定输入是否达到阈值，如果达到阈值，神经元将触发:

![](Images/663b4884-b77e-4a07-8718-7f41d2f5fdf1.png)

神经网络能够容忍有噪声的图像和失真，因此在可能需要黑盒分类方法时非常有用...

# 人工神经网络在实践中的应用

为了开始人工神经网络训练，需要测试数据。鉴于这种类型的分类方法应该能够很好地对失真或有噪声的图像进行分类，我们决定尝试在这里对图像进行分类:

![](Images/19b4278b-afec-4d67-826c-20e4b767737f.png)

它们是手工制作的文本文件，包含由字符 1 和 0 创建的形状块。当它们存储在 HDFS 时，回车符将被删除，这样图像将呈现为单行向量。因此，人工神经网络将对一系列形状图像进行分类，然后针对添加了噪声的相同图像进行测试，以确定分类是否仍然有效。共有六个训练图像，每个图像将被赋予一个从 0.1 到 0.6 的任意训练标签。因此，如果人工神经网络呈现一个封闭的正方形，它应该返回一个 0.1 的标签。下图显示了添加了噪声的测试图像的示例。

通过在图像中添加额外的零(0)个字符而产生的噪点已突出显示:

![](Images/9a151e3d-12c8-4d62-9c5b-2b7c4c9674f9.png)

和以前一样，ANN 代码是在一个名为`spark/ann`的子目录中使用 Linux Hadoop 帐户开发的。`ann.sbt`文件存在于`ann`目录中:

```scala
[hadoop@hc2nn ann]$ pwd
/home/hadoop/spark/ann

[hadoop@hc2nn ann]$ ls
ann.sbt   project src target
```

`ann.sbt`文件的内容已更改为使用火花依赖项的 JAR 库文件的完整路径:

```scala
name := "A N N"
version := "1.0"
scalaVersion := "2.11.2"
libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "2.8.1"
libraryDependencies += "org.apache.spark" % "spark-core" % "2.6.0"
libraryDependencies += "org.apache.spark" % "spark-mllib" % "2.1.1"
libraryDependencies += "org.apache.spark" % "akka" % "2.5.3"
```

与前面的例子一样，要编译的实际 Scala 代码存在于名为`src/main/scala`的子目录中。我们已经创建了两个 Scala 程序。首先使用输入数据进行训练，然后用相同的输入数据测试人工神经网络模型。第二种方法用噪声数据测试训练好的模型，以测试失真的数据分类:

```scala
[hadoop@hc2nn scala]$ pwd
/home/hadoop/spark/ann/src/main/scala 
[hadoop@hc2nn scala]$ ls
test_ann1.scala test_ann2.scala
```

我们将检查第一个 Scala 文件，然后我们将只显示第二个文件的额外特性，因为这两个例子在训练神经网络方面非常相似。这里显示的代码示例可以在本书提供的软件包中的路径`chapter2\ANN`下找到。因此，为了检查第一个 Scala 示例，导入语句类似于前面的示例。正在导入火花上下文、配置、矢量和`LabeledPoint`。这次正在进口用于 RDD 加工的`RDD`级，以及新的 ANN 级`ANNClassifier`。请注意，MLlib/分类例程广泛使用输入数据的`LabeledPoint`结构，该结构将包含应该针对以下内容进行训练的特征和标签:

```scala
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf 
import org.apache.spark.mllib.classification.ANNClassifier
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.linalg._
import org.apache.spark.rdd.RDD 

object testann1 extends App {
```

这个例子中的应用类被称为`testann1`。待处理的 HDFS 文件已根据 HDFS `server`、`path`和文件名进行了定义:

```scala
 val server = "hdfs://localhost:8020"
 val path   = "/data/spark/ann/"

 val data1 = server + path + "close_square.img"
 val data2 = server + path + "close_triangle.img"
 val data3 = server + path + "lines.img"
 val data4 = server + path + "open_square.img"
 val data5 = server + path + "open_triangle.img"
 val data6 = server + path + "plus.img"
```

火花上下文是用火花实例的网址创建的，现在它有了不同的端口号- `8077`。应用名称为`ANN 1`。当应用程序运行时，这将出现在 Spark 网络用户界面上:

```scala
 val sparkMaster = "spark://localhost:8077"
 val appName = "ANN 1"
 val conf = new SparkConf()

 conf.setMaster(sparkMaster)
 conf.setAppName(appName)

 val sparkCxt = new SparkContext(conf)
```

加载基于 HDFS 的输入训练和测试数据文件。每行上的值都用空格字符分隔，数值已经转换为双精度值。包含这些数据的变量存储在一个名为**输入**的数组中。同时，创建一个名为 outputs 的数组，包含从`0.1`到`0.6`的标签。这些值将用于对输入模式进行分类:

```scala
 val rData1 = sparkCxt.textFile(data1).map(_.split(" ").map(_.toDouble)).collect
 val rData2 = sparkCxt.textFile(data2).map(_.split(" ").map(_.toDouble)).collect
 val rData3 = sparkCxt.textFile(data3).map(_.split(" ").map(_.toDouble)).collect
 val rData4 = sparkCxt.textFile(data4).map(_.split(" ").map(_.toDouble)).collect
 val rData5 = sparkCxt.textFile(data5).map(_.split(" ").map(_.toDouble)).collect
 val rData6 = sparkCxt.textFile(data6).map(_.split(" ").map(_.toDouble)).collect 
 val inputs = Array[Array[Double]] (
     rData1(0), rData2(0), rData3(0), rData4(0), rData5(0), rData6(0) ) 
 val outputs = Array[Double]( 0.1, 0.2, 0.3, 0.4, 0.5, 0.6 )
```

然后，代表输入数据特征和标签的输入和输出数据被组合并转换成`LabeledPoint`结构。最后，数据被并行化，以便对其进行分区，从而实现最佳的并行处理:

```scala
 val ioData = inputs.zip( outputs )
 val lpData = ioData.map{ case(features,label) =>

   LabeledPoint( label, Vectors.dense(features) )
 }
 val rddData = sparkCxt.parallelize( lpData )
```

创建变量来定义神经网络的隐藏层拓扑。在这种情况下，我们选择了两个隐藏层，每个层有 100 个神经元。定义了最大迭代次数以及批次大小(六种模式)和收敛容差。公差是指在我们认为训练已经奏效之前，训练误差能有多大。然后，使用这些配置参数和输入数据创建人工神经网络模型:

```scala
 val hiddenTopology : Array[Int] = Array( 100, 100 )
 val maxNumIterations = 1000
 val convTolerance   = 1e-4
 val batchSize       = 6
 val annModel = ANNClassifier.train(rddData,
                                    batchSize,
                                    hiddenTopology,
                                    maxNumIterations,
                                    convTolerance)
```

为了测试训练好的人工神经网络模型，使用相同的输入训练数据作为测试数据来获得预测标签。首先，创建一个名为`rPredictData`的输入数据变量。然后，数据被分割，最后，使用训练好的神经网络模型获得预测。这个模型要工作，必须输出标签，`0.1`到`0.6`:

```scala
 val rPredictData = inputs.map{ case(features) => 
   ( Vectors.dense(features) )
 }
 val rddPredictData = sparkCxt.parallelize( rPredictData )
 val predictions = annModel.predict( rddPredictData )
```

标签预测被打印，脚本以一个右括号结束:

```scala
 predictions.toArray().foreach( value => println( "prediction > " + value ) )
} // end ann1
```

因此，为了运行这个代码示例，必须首先对其进行编译和打包。现在，您必须熟悉从`ann`子目录执行的`sbt`命令:

```scala
[hadoop@hc2nn ann]$ pwd
/home/hadoop/spark/ann
[hadoop@hc2nn ann]$ sbt package
```

然后在新的`spark/spark`路径中使用`spark-submit`命令，在端口`8077`使用新的基于 Spark 的网址运行应用程序`testann1`:

```scala
/home/hadoop/spark/spark/bin/spark-submit \
 --class testann1 \
 --master spark://localhost:8077 \
 --executor-memory 700M \
 --total-executor-cores 100 \
 /home/hadoop/spark/ann/target/scala-2.10/a-n-n_2.10-1.0.jar
```

通过查看`http://localhost:19080/`处的 Apache Spark 网址，现在可以看到应用程序正在运行。下图显示了运行的`ANN 1`应用程序以及先前完成的执行:

![](Images/e5e83e52-c0db-4506-8eeb-3ea0c81a5c5e.png)

通过选择其中一个集群主机工作者实例，可以看到实际为该工作者执行集群处理的执行器列表:

![](Images/689d4c67-406b-4b41-8e9a-e437d9354d48.png)

最后，通过选择一个执行器，可以看到它的历史和配置，以及日志文件和错误信息的链接。在这个级别上，通过提供日志信息，调试是可能的。可以检查这些日志文件来处理错误消息:

![](Images/467af2aa-e9ae-4b6b-927b-8b44c9d32c6d.png)

`ANN 1`应用程序提供以下输出，以表明它已经正确地重新分类了相同的输入数据。重新分类是成功的，因为每个输入模式都被赋予了与训练时相同的标签:

```scala
prediction > 0.1
prediction > 0.2
prediction > 0.3
prediction > 0.4
prediction > 0.5
prediction > 0.6
```

因此，这表明人工神经网络训练和测试预测将在相同的数据下工作。现在，我们将使用相同的数据进行训练，但使用失真或有噪声的数据进行测试，我们已经演示了一个例子。这个例子可以在你的软件包中名为`test_ann2.scala`的文件中找到。它与第一个示例非常相似，因此我们将只演示更改后的代码。该应用程序现在被称为`testann2`:

```scala
object testann2 extends App
```

使用训练数据创建人工神经网络模型后，会创建一组额外的测试数据。该测试数据包含噪声:

```scala
 val tData1 = server + path + "close_square_test.img"
 val tData2 = server + path + "close_triangle_test.img"
 val tData3 = server + path + "lines_test.img"
 val tData4 = server + path + "open_square_test.img"
 val tData5 = server + path + "open_triangle_test.img"
 val tData6 = server + path + "plus_test.img"
```

这些数据被处理成输入数组，并进行分区以进行集群处理:

```scala
 val rtData1 = sparkCxt.textFile(tData1).map(_.split(" ").map(_.toDouble)).collect
 val rtData2 = sparkCxt.textFile(tData2).map(_.split(" ").map(_.toDouble)).collect
 val rtData3 = sparkCxt.textFile(tData3).map(_.split(" ").map(_.toDouble)).collect
 val rtData4 = sparkCxt.textFile(tData4).map(_.split(" ").map(_.toDouble)).collect
 val rtData5 = sparkCxt.textFile(tData5).map(_.split(" ").map(_.toDouble)).collect
 val rtData6 = sparkCxt.textFile(tData6).map(_.split(" ").map(_.toDouble)).collect 
 val tInputs = Array[Array[Double]] (
     rtData1(0), rtData2(0), rtData3(0), rtData4(0), rtData5(0), rtData6(0) )

 val rTestPredictData = tInputs.map{ case(features) => ( Vectors.dense(features) ) }
 val rddTestPredictData = sparkCxt.parallelize( rTestPredictData )
```

然后使用它以与第一个示例相同的方式生成标签预测。如果模型对数据分类正确，那么从`0.1`到`0.6`应打印相同的标签值:

```scala
 val testPredictions = annModel.predict( rddTestPredictData )
 testPredictions.toArray().foreach( value => println( "test prediction > " + value ) )
```

代码已经编译完毕，可以使用`spark-submit`命令运行:

```scala
/home/hadoop/spark/spark/bin/spark-submit \
 --class testann2 \
 --master spark://localhost:8077 \
 --executor-memory 700M \
 --total-executor-cores 100 \
 /home/hadoop/spark/ann/target/scala-2.10/a-n-n_2.10-1.0.jar
```

这是这个脚本的集群输出，显示了使用训练好的人工神经网络模型和一些噪声测试数据的成功分类。噪声数据已被正确分类。例如，如果训练好的模型变得混乱，它可能会给位置一的噪声`close_square_test.img`测试图像一个 0.15 的值，而不是像它那样返回`0.1`:

```scala
test prediction > 0.1
test prediction > 0.2
test prediction > 0.3
test prediction > 0.4
test prediction > 0.5
test prediction > 0.6
```

# 摘要

本章试图向您概述 Apache Spark MLlib 模块中的一些可用功能。它还展示了神经网络或人工神经网络即将实现的功能。你可能会对人工神经网络的良好运作印象深刻。由于本章允许的时间和空间，不可能涵盖 MLlib 的所有领域。此外，我们现在希望在下一章中更多地关注 SparkML 库，它通过支持数据帧和底层的 Catalyst 和 wow 优化来加速机器学习。

我们看到了如何为朴素贝叶斯分类、K-Means 聚类和人工神经网络开发基于 Scala 的例子。你学会了如何准备考试...