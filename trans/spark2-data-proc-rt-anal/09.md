# 火花的测试和调试

在一个理想的世界里，我们编写完美的 Spark 代码，一切都一直完美运行，对吗？开玩笑的；在实践中，我们知道处理大规模数据集几乎从来都没有那么容易，不可避免地会有一些数据点会暴露您的代码的任何角落情况。

因此，考虑到前面提到的挑战，在本章中，我们将看到如果应用程序是分布式的，测试它会有多困难；然后，我们将看到解决这个问题的一些方法。简而言之，本章将涵盖以下主题:

*   分布式环境中的测试
*   测试火花应用
*   调试 Spark 应用程序

# 分布式环境中的测试

莱斯利·兰波特对分布式系统的定义如下:

"A distributed system is one in which I cannot get any work done because some machine I have never heard of has crashed."

通过**万维网**(又名 **WWW** )的资源共享，这是一个连接的计算机网络(又名集群)，是分布式系统的一个很好的例子。这些分布式环境通常很复杂，并且经常出现大量的异构性。在这种异构环境中进行测试也很有挑战性。在本节中，首先，我们将观察在使用这样一个系统时经常提出的一些共有问题。

# 分布式环境

分布式系统有许多定义。让我们看一些定义，然后我们将尝试关联上述类别。库仑将分布式系统定义为*一个系统，在该系统中，位于联网计算机上的硬件或软件组件仅通过消息传递*进行通信并协调它们的动作。另一方面，Tanenbaum 以几种方式定义了该术语:

*   *独立计算机的集合，在系统的用户看来是一台计算机。*
*   *由两台或多台独立计算机的集合组成的系统，这些计算机通过交换同步或异步消息传递来协调它们的处理。*
*   *分布式系统是通过网络链接的自治计算机的集合，这些计算机具有设计用于产生集成计算设施的软件。*

现在，基于前面的定义，分布式系统可以分类如下:

*   只有硬件和软件是分布式的:本地分布式系统通过局域网连接。
*   用户是分布式的，但是有运行后端的计算和硬件资源，例如 WWW。
*   用户和硬件/软件都是分布式的:通过广域网连接的分布式计算集群。例如，您可以在使用亚马逊 AWS、微软 Azure、谷歌云或数字海洋的水滴时获得这些类型的计算设施。

# 分布式系统中的问题

在这里，我们将讨论在软件和硬件测试期间需要注意的一些主要问题，以便 Spark 作业在集群计算中平稳运行，集群计算本质上是一个分布式计算环境。

请注意，所有的问题都是不可避免的，但我们至少可以调整它们以获得更好的结果。你应该遵循前一章给出的说明和建议。根据*Kamal Sheel Mishra**阿尼尔·库马尔特里帕蒂**分布式软件系统的一些问题、挑战和问题**国际计算机科学与信息技术杂志*，第 5 卷(4 期)，2014，4922-4925。网址:[https://pdf s . semanticschool . org/4c 6d/c4d 739 bad 13 BCD 0398 e 5180 c 1513 f 18275d 8 . pdf](https://pdfs.semanticscholar.org/4c6d/c4d739bad13bcd0398e5180c1513f18275d8.pdf)，在那里...

# 分布式环境中软件测试的挑战

敏捷软件开发中有一些与任务相关的常见挑战，在最终部署之前，在分布式环境中测试软件时，这些挑战会变得更加复杂。通常团队成员需要在 bug 激增后并行合并软件组件。然而，基于紧迫性，合并通常发生在测试阶段之前。有时，许多涉众分布在团队中。因此，误解的可能性很大，团队经常会在误解中失败。

例如，Cloud Foundry([https://www.cloudfoundry.org/](https://www.cloudfoundry.org/))是一个开源的高度分布式 PaaS 软件系统，用于管理云中应用程序的部署和可扩展性。它承诺了不同的功能，如可扩展性、可靠性和弹性，这些在云铸造上的部署固有地要求底层分布式系统实施措施来确保健壮性、弹性和故障转移。

众所周知，软件测试的过程包括*单元测试*、*集成测试*、*冒烟测试*、*验收测试*、*可扩展性测试*、*性能测试*、*服务质量测试*。在 Cloud Foundry 中，测试分布式系统的过程如下图所示:

![](Images/3c974ded-af48-4a29-a107-4dce0e42a32a.png)

**Figure 1:** An example of software testing in a distributed environment like Cloud

如上图(第一列)所示，在像 Cloud 这样的分布式环境中进行测试的过程从针对系统中最小的契约点运行单元测试开始。在成功执行所有单元测试之后，运行集成测试来验证交互组件作为在单个盒子(例如，**虚拟机** ( **虚拟机**)或裸机上运行的单个连贯软件系统(第二列)的一部分的行为。然而，尽管这些测试将系统的整体行为作为一个整体进行了验证，但它们并不能保证分布式部署中的系统有效性。一旦集成测试通过，下一步(第三列)是验证系统的分布式部署并运行冒烟测试。

如您所知，软件的成功配置和单元测试的执行为我们验证系统行为的可接受性做好了准备。该验证通过运行验收测试来完成(第四列)。现在，为了克服上述分布式环境中的问题和挑战，还有其他隐藏的挑战需要研究人员和大数据工程师解决，但这些实际上不在本书的范围内。

现在我们知道了分布式环境中软件测试的真正挑战是什么，现在让我们开始测试一下我们的 Spark 代码。下一节专门测试 Spark 应用程序。

# 测试火花应用

有许多方法可以尝试测试您的 Spark 代码，这取决于它是 Java(您可以做基本的 JUnit 测试来测试非 Spark 代码)还是 Scala 代码的 ScalaTest。您也可以通过在本地或小型测试集群上运行 Spark 来进行完全集成测试。霍尔登·卡劳的另一个令人敬畏的选择是使用火花测试基地。您可能知道，到目前为止，Spark 中还没有用于单元测试的本机库。然而，我们可以有以下两种选择来使用两个库:

*   ScalaTest
*   火花测试基地

然而，在开始测试用 Scala 编写的火花应用程序之前，一些关于单元测试和测试 Scala 方法的背景知识是一项任务。

# 测试 Scala 方法

在这里，我们将看到一些测试 Scala 方法的简单技术。对于 Scala 用户来说，这是最熟悉的单元测试框架(您也可以用它来测试 Java 代码，很快就可以用它来测试 JavaScript)。ScalaTest 支持多种不同的测试风格，每种风格都是为了支持特定类型的测试需求。详情见[http://www.scalatest.org/user_guide/selecting_a_style](http://www.scalatest.org/user_guide/selecting_a_style)ScalaTest 用户指南。虽然 ScalaTest 支持多种风格，但最快速的入门方法之一是使用以下 ScalaTest 特性，并以 **TDD** ( **测试驱动开发**)风格编写测试:

1.  `FunSuite`
2.  `Assertions`
3.  `BeforeAndAfter`

请随意浏览前面的网址，了解更多关于这些特征的信息；这将使本教程的其余部分顺利进行。

It is to be noted that the TDD is a programming technique to develop software, and it states that you should start development from tests. Hence, it doesn't affect how tests are written, but when tests are written. There is no trait or testing style to enforce or encourage TDD in `ScalaTest.FunSuite`, `Assertions`, and `BeforeAndAfter` are only more similar to the xUnit testing frameworks.

ScalaTest 中有三种风格特征的断言:

*   `assert`:这用于您的 Scala 程序中的一般断言。
*   `assertResult`:这有助于区分期望值和实际值。
*   `assertThrows`:这是用来保证一点代码抛出一个预期的异常。

ScalaTest 的断言被定义在特性`Assertions`中，这个特性又被`Suite`进一步扩展。简而言之，`Suite`特质是所有风格特质中的超级特质。根据[http://www.scalatest.org/user_guide/using_assertions](http://www.scalatest.org/user_guide/using_assertions)的 ScalaTest 文档，`Assertions`特性还提供了以下特性:

*   `assume`有条件取消测试
*   `fail`无条件通过测试
*   `cancel`无条件取消测试
*   `succeed`让测试无条件成功
*   `intercept`确保一点代码抛出一个预期的异常，然后对该异常进行断言
*   `assertDoesNotCompile`保证一位代码不编译
*   `assertCompiles`确保一点代码确实编译
*   `assertTypeError`确保一位代码不会因为类型(非解析)错误而编译
*   `withClue`添加故障的更多信息

从前面的列表中，我们将展示其中的一些。在您的 Scala 程序中，您可以通过调用`assert`并在中传递一个`Boolean`表达式来编写断言。您可以简单地使用`Assertions`开始编写您的简单单元测试用例。`Predef`是一个对象，在这里定义了断言的行为。请注意，`Predef`的所有成员都会被导入到你的每个 Scala 源文件中。以下源代码将为以下情况打印`Assertion success`:

```
package com.chapter16.SparkTesting
object SimpleScalaTest {
  def main(args: Array[String]):Unit= {
    val a = 5
    val b = 5
    assert(a == b)
      println("Assertion success")       
  }
}
```

但是，例如，如果您进行`a = 2`和`b = 1`，断言将失败，并且您将体验到以下输出:

![](Images/7f37e532-3351-4ae6-80a6-4679d0579aec.png)

**Figure 2:** An example of assertion fail

如果传递真表达式，assert 将正常返回。但是，如果提供的表达式为假，断言将突然终止，并出现断言错误。与`AssertionError`和`TestFailedException`不同，ScalaTest 的断言提供了更多的信息，可以准确地告诉你测试用例在哪一行失败了，或者哪个表达式失败了。因此，ScalaTest 的断言比 Scala 的断言提供了更好的错误消息。

例如，对于下面的源代码，您应该会体验到`TestFailedException`会告诉您 5 不等于 4:

```
package com.chapter16.SparkTesting
import org.scalatest.Assertions._
object SimpleScalaTest {
  def main(args: Array[String]):Unit= {
    val a = 5
    val b = 4
    assert(a == b)
      println("Assertion success")       
  }
}
```

下图显示了前面 Scala 测试的输出:

![](Images/2ac606f4-4805-4633-a8c5-5a99799c8f3e.png)

**Figure 3:** An example of TestFailedException

下面的源代码解释了如何使用`assertResult`单元测试来测试你的方法的结果:

```
package com.chapter16.SparkTesting
import org.scalatest.Assertions._
object AssertResult {
  def main(args: Array[String]):Unit= {
    val x = 10
    val y = 6
    assertResult(3) {
      x - y
    }
  }
}
```

前面的断言将失败，Scala 将抛出异常`TestFailedException`并打印`Expected 3 but got 4` ( *图 4* ):

![](Images/1b1bcd86-c72c-494a-b7b0-6ce8c22ddc47.png)

**Figure 4:** Another example of TestFailedException

现在，让我们看一个单元测试来显示预期的异常:

```
package com.chapter16.SparkTesting
import org.scalatest.Assertions._
object ExpectedException {
  def main(args: Array[String]):Unit= {
    val s = "Hello world!"
    try {
      s.charAt(0)
      fail()
    } catch {
      case _: IndexOutOfBoundsException => // Expected, so continue
    }
  }
}
```

如果您试图访问索引之外的数组元素，前面的代码将告诉您是否允许访问前面字符串`Hello world!`的第一个字符。如果您的 Scala 程序可以访问索引中的值，断言将会失败。这也意味着测试用例失败了。因此，前面的测试用例自然会失败，因为第一个索引包含字符`H`，您应该会遇到以下错误消息(*图 5* ):

![](Images/53b86f5b-c893-4daa-8061-2a9f025b803b.png)

**Figure 5:** Third example of TestFailedException

但是，现在让我们尝试访问位置`-1`处的索引，如下所示:

```
package com.chapter16.SparkTesting
import org.scalatest.Assertions._
object ExpectedException {
  def main(args: Array[String]):Unit= {
    val s = "Hello world!"
    try {
      s.charAt(-1)
      fail()
    } catch {
      case _: IndexOutOfBoundsException => // Expected, so continue
    }
  }
}
```

现在断言应该是真的，因此，测试用例将被通过。最后，代码将正常终止。现在，让我们检查一下我们的代码片段是否可以编译。通常，您可能希望确保代表出现的“用户错误”的某种代码顺序根本不会编译。目标是针对错误检查库的强度，以禁止不需要的结果和行为。ScalaTest 的`Assertions`特性包括以下语法:

```
assertDoesNotCompile("val a: String = 1")
```

如果希望确保代码片段不会因为类型错误(而不是语法错误)而编译，请使用以下方法:

```
assertTypeError("val a: String = 1")
```

语法错误仍然会导致抛出`TestFailedException`。最后，如果您想声明一段代码确实可以编译，那么您可以用下面的内容使它变得更加明显:

```
assertCompiles("val a: Int = 1")
```

完整的示例如下所示:

```
package com.chapter16.SparkTesting
import org.scalatest.Assertions._ 
object CompileOrNot {
  def main(args: Array[String]):Unit= {
    assertDoesNotCompile("val a: String = 1")
    println("assertDoesNotCompile True")

    assertTypeError("val a: String = 1")
    println("assertTypeError True")

    assertCompiles("val a: Int = 1")
    println("assertCompiles True")

    assertDoesNotCompile("val a: Int = 1")
    println("assertDoesNotCompile True")
  }
}
```

下图显示了上述代码的输出:

![](Images/2fbf984e-a29a-4dda-a9ff-389d870142c4.png)

**Figure 6:** Multiple tests together

现在，由于页面限制，我们希望完成基于 Scala 的单元测试。但是，对于其他单元测试案例，您可以参考位于[http://www.scalatest.org/user_guide](http://www.scalatest.org/user_guide)的 Scala 测试指南。

# 单元测试

在软件工程中，经常测试源代码的单个单元，以确定它们是否适合使用。这种方式的软件测试方法也被称为单元测试。该测试确保由软件工程师或开发人员开发的源代码符合设计规范并按预期工作。

另一方面，单元测试的目标是分离程序的每个部分(即以模块化的方式)。然后试着观察各个零件是否正常工作。在任何软件系统中，单元测试都有几个好处:

*   **早期发现问题:**它在开发周期早期发现规范的 bug 或缺失部分。
*   **促进变化:**有助于重构...

# 测试火花应用

我们已经看到了如何使用内置的 Scala`ScalaTest`包测试您的 Scala 代码。然而，在这一小节中，我们将看到如何测试用 Scala 编写的 Spark 应用程序。将讨论以下三种方法:

*   **方法 1:** 使用 JUnit 测试 Spark 应用程序
*   **方法 2:** 使用`ScalaTest`包测试火花应用
*   **方法 3:** 使用火花测试基地测试火花应用

方法 1 和 2 将在这里用一些实用的代码进行讨论。但是，下一小节将详细讨论方法 3。为了使理解简单明了，我们将使用著名的单词计数应用程序来演示方法 1 和 2。

# 方法 1:使用 Scala JUnit 测试

假设您用 Scala 编写了一个应用程序，可以告诉您文档或文本文件中有多少个单词，如下所示:

```
package com.chapter16.SparkTestingimport org.apache.spark._import org.apache.spark.sql.SparkSessionclass wordCounterTestDemo {  val spark = SparkSession    .builder    .master("local[*]")    .config("spark.sql.warehouse.dir", "E:/Exp/")    .appName(s"OneVsRestExample")    .getOrCreate()  def myWordCounter(fileName: String): Long = {    val input = spark.sparkContext.textFile(fileName)    val counts = input.flatMap(_.split(" ")).distinct()    val counter = counts.count()    counter  }}
```

前面的代码只是解析一个文本文件，并通过简单地拆分单词来执行`flatMap`操作。然后，它执行...

# 方法 2:使用 FunSuite 测试 Scala 代码

现在，让我们重新设计前面的测试用例，只返回文档中文本的 RDD，如下所示:

```
package com.chapter16.SparkTesting
import org.apache.spark._
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession
class wordCountRDD {
  def prepareWordCountRDD(file: String, spark: SparkSession): RDD[(String, Int)] = {
    val lines = spark.sparkContext.textFile(file)
    lines.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_ + _)
  }
}
```

因此，前面类中的`prepareWordCountRDD()`方法返回一个由字符串和整数值组成的 RDD。现在，如果我们想要测试`prepareWordCountRDD()`方法的功能，我们可以通过从 Scala 的`ScalaTest`包中扩展带有`FunSuite`和`BeforeAndAfterAll`的测试类来更明确地进行测试。测试以下列方式进行:

*   从 Scala 的`ScalaTest`包中用`FunSuite`和`BeforeAndAfterAll`扩展测试类
*   覆盖创建火花上下文的`beforeAll()`
*   使用`test()`方法进行测试，并在`test()`方法中使用`assert()`方法
*   覆盖停止火花上下文的`afterAll()`方法

基于前面的步骤，我们来看一个测试前面`prepareWordCountRDD()`方法的类:

```
package com.chapter16.SparkTesting
import org.scalatest.{ BeforeAndAfterAll, FunSuite }
import org.scalatest.Assertions._
import org.apache.spark.sql.SparkSession
import org.apache.spark.rdd.RDD
class wordCountTest2 extends FunSuite with BeforeAndAfterAll {
  var spark: SparkSession = null
  def tokenize(line: RDD[String]) = {
    line.map(x => x.split(' ')).collect()
  }
  override def beforeAll() {
    spark = SparkSession
      .builder
      .master("local[*]")
      .config("spark.sql.warehouse.dir", "E:/Exp/")
      .appName(s"OneVsRestExample")
      .getOrCreate()
  }  
  test("Test if two RDDs are equal") {
    val input = List("To be,", "or not to be:", "that is the question-", "William Shakespeare")
    val expected = Array(Array("To", "be,"), Array("or", "not", "to", "be:"), Array("that", "is", "the", "question-"), Array("William", "Shakespeare"))
    val transformed = tokenize(spark.sparkContext.parallelize(input))
    assert(transformed === expected)
  }  
  test("Test for word count RDD") {
    val fileName = "C:/Users/rezkar/Downloads/words.txt"
    val obj = new wordCountRDD
    val result = obj.prepareWordCountRDD(fileName, spark)    
    assert(result.count() === 214)
  }
  override def afterAll() {
    spark.stop()
  }
}
```

第一个测试说，如果两个 rdd 以两种不同的方式实现，那么内容应该是相同的。因此，第一次测试应该通过。我们将在下面的示例中看到这一点。现在，对于第二个测试，正如我们之前看到的，RDD 的字数是 214，但是让我们假设它暂时未知。如果是 214 巧合，那么测试用例应该通过，这是它的预期行为。

因此，我们希望这两项测试都能通过。现在，在 Eclipse 上，运行测试套件为`ScalaTest-File`，如下图所示:

![](Images/87342913-3a2b-4a9e-8a70-45cffdb44044.png)

图 10:作为 ScalaTest-File 运行测试套件

现在您应该观察到以下输出(*图 11* )。输出显示了我们执行了多少测试用例，以及其中有多少通过、失败、取消、被忽略或处于待定状态。它还显示了执行整个测试的时间。

![](Images/0db3c30f-5d87-43d8-a784-4cbbf13bb513.png)

**Figure 11:** Test result when running the two test suites as ScalaTest-file

太棒了！测试用例通过了。现在，让我们尝试使用`test()`方法在两个单独的测试中更改断言中的比较值，如下所示:

```
test("Test for word count RDD") { 
  val fileName = "data/words.txt"
  val obj = new wordCountRDD
  val result = obj.prepareWordCountRDD(fileName, spark)    
  assert(result.count() === 210)
}
test("Test if two RDDs are equal") {
  val input = List("To be", "or not to be:", "that is the question-", "William Shakespeare")
  val expected = Array(Array("To", "be,"), Array("or", "not", "to", "be:"), Array("that", "is", "the", "question-"), Array("William", "Shakespeare"))
  val transformed = tokenize(spark.sparkContext.parallelize(input))
  assert(transformed === expected)
}
```

现在，您应该预料到测试用例将会失败。现在运行前面的类为`ScalaTest-File` ( *图 12* ):

![](Images/7ca0967a-280e-46f0-a6e8-934dbcc0e93f.png)

**Figure 12:** Test result when running the preceding two test suites as ScalaTest-File

干得好！我们已经学习了如何使用 Scala 的 FunSuite 执行单元测试。然而，如果你仔细评估前面的方法，你应该同意有几个缺点。例如，您需要确保对`SparkContext`的创建和销毁进行明确的管理。作为开发人员或程序员，您必须编写更多的代码行来测试示例方法。有时，代码重复会发生，因为在所有测试套件中必须重复之前的*和*之后的*步骤。然而，这是有争议的，因为共同的代码可以放在一个共同的特点。*

现在的问题是我们如何改善我们的体验？我的建议是使用 Spark 测试基础，让生活变得更简单和直接。我们将讨论如何在 Spark 测试基地执行单元测试。

# 方法 3:使用 Spark 测试基础让生活变得更轻松

Spark 测试库帮助您轻松测试大多数 Spark 代码。那么，这种方法的优点是什么呢？其实有很多。例如，使用这个代码并不冗长，但是我们可以得到一个非常简洁的代码。该应用编程接口本身比 ScalaTest 或 JUnit 更丰富。多种语言支持，例如，Scala、Java 和 Python。它支持内置的 RDD 比较器。您也可以使用它来测试流应用程序。最后，也是最重要的，它支持本地和集群模式测试。这对于分布式环境中的测试来说是最重要的。

The GitHub repo is located at [https://github.com/holdenk/spark-testing-base](https://github.com/holdenk/spark-testing-base).

开始之前...

# 在 Windows 上配置 Hadoop 运行时

我们已经看到了如何在 Eclipse 或 IntelliJ 上测试用 Scala 编写的 Spark 应用程序，但是还有另一个不应该被忽视的潜在问题。虽然 Spark 在 Windows 上工作，但 Spark 被设计为在类似 UNIX 的操作系统上运行。因此，如果您在 Windows 环境中工作，则需要格外小心。

在使用 Eclipse 或 IntelliJ 开发您的 Spark 应用程序以解决 Windows 上的数据分析、机器学习、数据科学或深度学习应用程序时，您可能会遇到输入/输出异常错误，并且您的应用程序可能无法成功编译或可能会中断。事实上，Spark 希望在 Windows 上也有一个运行时环境。例如，如果您第一次在 Eclipse 上运行一个 Spark 应用程序，比如说`KMeansDemo.scala`，您将会遇到一个输入/输出异常，比如说:

```
17/02/26 13:22:00 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
```

原因是，默认情况下，Hadoop 是为 Linux 环境开发的，如果您在 Windows 平台上开发您的 Spark 应用程序，则需要一个桥，该桥将为 Hadoop 运行时提供一个环境，以便 Spark 正确执行。输入/输出异常的详细信息如下图所示:

![](Images/32afa337-6d87-4b24-859d-c30a53ab627f.png)

**Figure 14:** I/O exception occurred due to the failure of not to locate the winutils binary in the Hadoop binary path

那么，如何解决这个问题呢？解决办法很简单。正如错误信息所说，我们需要有一个可执行文件，即`winutils.exe`。现在从[https://github . com/steveloughran/winutils/tree/master/Hadoop-2 . 7 . 1/bin](https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin)下载`winutils.exe`文件，粘贴到 Spark 发行目录，配置 Eclipse。更具体地说，假设你的包含 Hadoop 的 Spark 发行版位于`C:/Users/spark-2.1.0-bin-hadoop2.7`。在 Spark 发行版内部，有一个名为 bin 的目录。现在，将可执行文件粘贴在那里(即`path = C:/Users/spark-2.1.0-binhadoop2.7/bin/`)。

解决方案的第二阶段是转到 Eclipse，然后选择主类(在这种情况下是`KMeansDemo.scala`)，然后转到 Run 菜单。从运行菜单中，转到运行配置选项，并从该选项中选择环境选项卡，如下图所示:

![](Images/b5e41aab-8cce-4deb-8f05-446767d78561.png)

**Figure 15:** Solving the I/O exception occurred due to the absence of winutils binary in the Hadoop binary path

如果您选择该选项卡，您将可以选择使用 JVM 为 Eclipse 创建一个新的环境变量。现在创建一个名为`HADOOP_HOME`的新环境变量，并将该值设为`C:/Users/spark-2.1.0-bin-hadoop2.7/`。现在按下应用按钮并重新运行您的应用程序，您的问题应该会得到解决。

需要注意的是，在 PySpark 中使用 Windows 上的 Spark 时，`winutils.exe`文件也是必需的。

请注意，前面的解决方案也适用于调试应用程序。有时，即使发生了前面的错误，您的 Spark 应用程序也会正常运行。但是，如果数据集的大小很大，很可能会出现前面的错误。

# 调试火花应用程序

在这一节中，我们将看到如何调试在本地(在 Eclipse 或 IntelliJ 上)、独立或集群模式下运行的 Spark 应用程序。但是，在深入研究之前，有必要了解 Spark 应用程序中的日志记录。

# log4j 测井，带火花回顾

如前所述，Spark 使用 log4j 进行自己的日志记录。如果您正确配置了 Spark，Spark 会将所有操作记录到 shell 控制台。从下图中可以看到该文件的示例快照:

![](Images/e5aa3075-1e4f-4fd6-8594-01e12076c1ce.png)

**Figure 16:** A snap of the log4j.properties file

将默认火花壳日志级别设置为 WARN。运行 spark-shell 时，此类的日志级别用于覆盖根记录器的日志级别，以便用户可以对 shell 和常规 spark 应用程序使用不同的默认值。当启动由执行器执行并由驱动程序管理的作业时，我们还需要附加 JVM 参数。为此，您应该编辑`conf/spark-defaults.conf`。简而言之，可以添加以下选项:

```
spark.executor.extraJavaOptions=-Dlog4j.configuration=file:/usr/local/spark-2.1.1/conf/log4j.properties spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/usr/local/spark-2.1.1/conf/log4j.properties
```

为了让讨论更清晰，我们需要隐藏 Spark 生成的所有日志。然后，我们可以将它们重定向到文件系统中进行记录。另一方面，我们希望我们自己的日志记录在 shell 和一个单独的文件中，这样它们就不会与 Spark 中的日志混淆。从这里，我们将把 Spark 指向我们自己的日志所在的文件，在这个特殊的例子中是`/var/log/sparkU.log`。这个`log4j.properties`文件然后在应用程序启动时被 Spark 拾取，所以除了把它放在提到的位置之外，我们不需要做任何事情:

```
package com.chapter14.Serilazition
import org.apache.log4j.LogManager
import org.apache.log4j.Level
import org.apache.spark.sql.SparkSession
object myCustomLog {
  def main(args: Array[String]): Unit = {   
    val log = LogManager.getRootLogger    
    //Everything is printed as INFO once the log level is set to INFO untill you set the level to new level for example WARN. 
    log.setLevel(Level.INFO)
    log.info("Let's get started!")    
    // Setting logger level as WARN: after that nothing prints other than WARN
    log.setLevel(Level.WARN)    
    // Creating Spark Session
    val spark = SparkSession
      .builder
      .master("local[*]")
      .config("spark.sql.warehouse.dir", "E:/Exp/")
      .appName("Logging")
      .getOrCreate()
    // These will note be printed!
    log.info("Get prepared!")
    log.trace("Show if there is any ERROR!")
    //Started the computation and printing the logging information
    log.warn("Started")
    spark.sparkContext.parallelize(1 to 20).foreach(println)
    log.warn("Finished")
  }
}
```

在前面的代码中，一旦日志级别设置为`INFO`，所有内容都会打印为信息，直到您将级别设置为新的级别，例如`WARN`。然而，在那之后没有信息或痕迹等等，那将不会被打印。除此之外，log4j 和 Spark 还支持几种有效的日志记录级别。前面代码的成功执行应该会生成以下输出:

```
17/05/13 16:39:14 INFO root: Let's get started!
17/05/13 16:39:15 WARN root: Started
4 
1 
2 
5 
3 
17/05/13 16:39:16 WARN root: Finished
```

您也可以在`conf/log4j.properties`中设置火花外壳的默认日志记录。Spark 提供了一个 log4j 的模板作为属性文件，我们可以扩展和修改该文件来登录 Spark。移动到`SPARK_HOME/conf`目录，你会看到`log4j.properties.template`文件。更名为`log4j.properties`后，应使用以下`conf/log4j.properties.template`。在开发您的 Spark 应用程序时，您可以将`log4j.properties`文件放在您的项目目录下，同时在基于 IDE 的环境(如 Eclipse)中工作。但是，要完全禁用日志记录，只需将`log4j.logger.org`标志设置为`OFF`，如下所示:

```
log4j.logger.org=OFF
```

到目前为止，一切都很容易。然而，在前面的代码段中，有一个问题我们还没有注意到。`org.apache.log4j.Logger`类的一个缺点是它不可序列化，这意味着我们不能在对 Spark API 的某些部分进行操作时在闭包内部使用它。例如，假设我们在 Spark 代码中执行以下操作:

```
object myCustomLogger {
  def main(args: Array[String]):Unit= {
    // Setting logger level as WARN
    val log = LogManager.getRootLogger
    log.setLevel(Level.WARN)
    // Creating Spark Context
    val conf = new SparkConf().setAppName("My App").setMaster("local[*]")
    val sc = new SparkContext(conf)
    //Started the computation and printing the logging information
    //log.warn("Started")
    val i = 0
    val data = sc.parallelize(i to 100000)
    data.map{number =>
      log.info(“My number”+ i)
      number.toString
    }
    //log.warn("Finished")
  }
}
```

您应该会遇到一个异常，表示`Task`不可序列化，如下所示:

```
org.apache.spark.SparkException: Job aborted due to stage failure: Task not serializable: java.io.NotSerializableException: ...
Exception in thread "main" org.apache.spark.SparkException: Task not serializable 
Caused by: java.io.NotSerializableException: org.apache.log4j.spi.RootLogger
Serialization stack: object not serializable
```

起初，我们可以尝试用一种天真的方式来解决这个问题。你能做的就是用`extends Serializable`做 Scala 类(做实际操作)`Serializable`。例如，代码如下所示:

```
class MyMapper(n: Int) extends Serializable {
  @transient lazy val log = org.apache.log4j.LogManager.getLogger("myLogger")
  def logMapper(rdd: RDD[Int]): RDD[String] =
    rdd.map { i =>
      log.warn("mapping: " + i)
      (i + n).toString
    }
  }
```

This section is intended for carrying out a discussion on logging. However, we take the opportunity to make it more versatile for general purpose Spark programming and issues. In order to overcome the `task not serializable` error in a more efficient way, the compiler will try to send the whole object (not only the lambda) by making it serializable and forces SPark to accept that. However, it increases shuffling significantly, especially for big objects! The other ways are making the whole class `Serializable` or by declaring the instance only within the lambda function passed in the map operation. Sometimes, keeping the not `Serializable` objects across the nodes can work. Lastly, use the `forEachPartition()` or `mapPartitions()` instead of just `map()` and create the not `Serializable` objects. In summary, these are the ways to solve the problem around:

*   可序列化该类
*   仅在映射中传递的 lambda 函数内声明实例
*   将 NotSerializable 对象作为静态对象，并在每台机器上创建一次
*   调用`forEachPartition ()`或`mapPartitions()`而不是`map()`并创建不可列化的对象

在前面的代码中，我们使用了注释`@transient lazy`，它将`Logger`类标记为非持久性的。另一方面，包含实例化`MyMapper`类的对象的方法 apply(即`MyMapperObject`)的对象如下:

```
//Companion object 
object MyMapper {
  def apply(n: Int): MyMapper = new MyMapper(n)
}
```

最后，包含`main()`方法的对象如下:

```
//Main object
object myCustomLogwithClosureSerializable {
  def main(args: Array[String]) {
    val log = LogManager.getRootLogger
    log.setLevel(Level.WARN)
    val spark = SparkSession
      .builder
      .master("local[*]")
      .config("spark.sql.warehouse.dir", "E:/Exp/")
      .appName("Testing")
      .getOrCreate()
    log.warn("Started")
    val data = spark.sparkContext.parallelize(1 to 100000)
    val mapper = MyMapper(1)
    val other = mapper.logMapper(data)
    other.collect()
    log.warn("Finished")
  }
```

现在，让我们看看另一个例子，它提供了更好的洞察力来继续解决我们正在讨论的问题。假设我们有以下计算两个整数相乘的类:

```
class MultiplicaitonOfTwoNumber {
  def multiply(a: Int, b: Int): Int = {
    val product = a * b
    product
  }
}
```

现在，本质上，如果您尝试使用这个类来计算使用`map()`的 lambda 闭包中的乘法，您将得到我们前面描述的`Task Not Serializable`错误。现在我们可以简单地使用`foreachPartition()`和里面的λ如下:

```
val myRDD = spark.sparkContext.parallelize(0 to 1000)
    myRDD.foreachPartition(s => {
      val notSerializable = new MultiplicaitonOfTwoNumber
      println(notSerializable.multiply(s.next(), s.next()))
    })
```

现在，如果您编译它，它应该会返回期望的结果。为了方便起见，使用`main()`方法的完整代码如下:

```
package com.chapter16.SparkTesting
import org.apache.spark.sql.SparkSession
class MultiplicaitonOfTwoNumber {
  def multiply(a: Int, b: Int): Int = {
    val product = a * b
    product
  }
}
```

```

object MakingTaskSerilazible {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .master("local[*]")
      .config("spark.sql.warehouse.dir", "E:/Exp/")
      .appName("MakingTaskSerilazible")
      .getOrCreate()
 val myRDD = spark.sparkContext.parallelize(0 to 1000)
    myRDD.foreachPartition(s => {
      val notSerializable = new MultiplicaitonOfTwoNumber
      println(notSerializable.multiply(s.next(), s.next()))
    })
  }
}
```

输出如下:

```
0
5700
1406
156
4032
7832
2550
650
```

# 调试火花应用程序

在这一节中，我们将讨论如何在 Eclipse 或 IntelliJ 上调试本地运行的 Spark 应用程序，作为独立模式或集群模式。在开始之前，您还可以在[https://hortonworks . com/Hadoop-教程/设置-spark-开发-环境-scala/](https://hortonworks.com/hadoop-tutorial/setting-spark-development-environment-scala/) 处阅读调试文档。

# 在 Eclipse 上调试 Spark 应用程序作为 Scala 调试

要做到这一点，只需将您的 Eclipse 配置为像常规的 Scala 代码调试一样调试您的 Spark 应用程序。要进行配置，请选择运行|调试配置| Scala 应用程序，如下图所示:

![](Images/eb20cffd-d0b0-4286-96bb-2ebb36b82048.png)

**Figure 17:** Configuring Eclipse to debug Spark applications as a regular Scala code debug

假设我们想要调试我们的`KMeansDemo.scala`并要求 Eclipse(您可以在 InteliJ IDE 上有类似的选项)在第 56 行开始执行并在第 95 行设置断点。为此，在调试时运行您的 Scala 代码，您应该在 Eclipse 上观察到以下场景:

![](Images/bb345bf6-ae2b-4a92-9811-37038227fc01.png)

**Figure 18:** Debugging Spark applications on Eclipse

然后，Eclipse 将在您要求它停止执行的第 95 行暂停，如下图所示:

![](Images/1d4e11ff-3edf-456e-be9f-fa62cacac199.png)

**Figure 19:** Debugging Spark applications on Eclipse (breakpoint)

总之，为了简化前面的例子，如果第 56 行和第 95 行之间有任何错误，Eclipse 将显示错误实际发生的位置。否则，如果不中断，它将遵循正常的工作流程。

# 调试以本地和独立模式运行的火花作业

在本地或以独立模式调试您的 Spark 应用程序时，您应该知道调试驱动程序和调试其中一个执行器是不同的，因为使用这两种类型的节点需要不同的提交参数传递给`spark-submit`。在本节中，我将使用端口 4000 作为地址。例如，如果您想要调试驱动程序，您可以将以下内容添加到您的`spark-submit`命令中:

```
--driver-java-options -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=4000
```

之后，您应该将远程调试器设置为连接到提交驱动程序的节点。对于前面的情况，端口号 4000 是...

# 调试在纱簇或介子簇上的火花应用

在纱线上运行火花应用程序时，可以通过修改`yarn-env.sh`来启用一个选项:

```
YARN_OPTS="-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=4000 $YARN_OPTS"
```

现在，远程调试将通过您的 Eclipse 或 IntelliJ IDE 上的端口 4000 可用。第二个选项是通过设置`SPARK_SUBMIT_OPTS`。您可以使用 Eclipse 或 IntelliJ 来开发您的 Spark 应用程序，这些应用程序可以提交到远程多节点纱集群上执行。我所做的是在 Eclipse 或 IntelliJ 上创建一个 Maven 项目，并将我的 Java 或 Scala 应用程序打包成 jar 文件，然后作为 Spark 作业提交。但是，为了将您的 IDE(如 Eclipse 或 IntelliJ 调试器)附加到您的 Spark 应用程序，您可以使用`SPARK_SUBMIT_OPTS`环境变量定义所有提交参数，如下所示:

```
$ export SPARK_SUBMIT_OPTS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=4000
```

然后提交您的火花作业如下(请根据您的要求和设置相应地更改值):

```
$ SPARK_HOME/bin/spark-submit \
--class "com.chapter13.Clustering.KMeansDemo" \
--master yarn \
--deploy-mode cluster \
--driver-memory 16g \
--executor-memory 4g \
--executor-cores 4 \
--queue the_queue \
--num-executors 1\
--executor-cores 1 \
--conf "spark.executor.extraJavaOptions=-agentlib:jdwp=transport=dt_socket,server=n,address= host_name_to_your_computer.org:4000,suspend=n" \
--driver-java-options -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=4000 \
 KMeans-0.0.1-SNAPSHOT-jar-with-dependencies.jar \
Saratoga_NY_Homes.txt
```

运行前面的命令后，它会一直等到您连接调试器，如下所示:`Listening for transport dt_socket at address: 4000`。现在，您可以在 IntelliJ 调试器上配置您的 Java 远程应用程序(Scala 应用程序也可以工作)，如下图所示:

![](Images/15c30522-5742-4de4-885f-fc568d44748e.png)

**Figure 20:** Configuring remote debugger on IntelliJ

对于前面的情况，10.200.1.101 是基本上运行 Spark 作业的远程计算节点的 IP 地址。最后，您必须通过单击 IntelliJ 的“运行”菜单下的“调试”来启动调试器。然后，如果调试器连接到您的远程 Spark 应用程序，您将在 IntelliJ 上的应用程序控制台中看到日志信息。现在如果你能设置断点，剩下的都是正常调试。

下图显示了一个示例，当使用断点暂停火花作业时，您将如何在智能界面上看到:

![](Images/2d8b430a-9583-4009-b8ba-bebcc6dfbd13.png)

**Figure 21:** An example how will you see on the IntelliJ when pausing a Spark job with a breakpoint

虽然效果不错，但有时候我体验到在 Eclipse 甚至 IntelliJ 上使用`SPARK_JAVA_OPTS`在调试过程中帮不了你多少。相反，在真实集群(纱、纱或 AWS)上运行火花作业时，使用并导出`SPARK_WORKER_OPTS`和`SPARK_MASTER_OPTS`，如下所示:

```
$ export SPARK_WORKER_OPTS="-Xdebug -Xrunjdwp:server=y,transport=dt_socket,address=4000,suspend=n"
$ export SPARK_MASTER_OPTS="-Xdebug -Xrunjdwp:server=y,transport=dt_socket,address=4000,suspend=n"
```

然后按如下方式启动主节点:

```
$ SPARKH_HOME/sbin/start-master.sh
```

现在打开到您的远程机器的 SSH 连接，在那里 Spark 作业实际上正在运行，并且将您的本地主机在 4000(也就是`localhost:4000`)映射到`host_name_to_your_computer.org:5000`，假设集群在`host_name_to_your_computer.org:5000`并且监听端口 5000。现在，您的 Eclipse 将认为您只是在调试作为本地 Spark 应用程序或进程的 Spark 应用程序。但是，要实现这一点，您必须在 Eclipse 上配置远程调试器，如下图所示:

![](Images/591d8d5c-0a0a-4e5d-9fd0-d38bf0b71f89.png)

**Figure 22:** Connecting remote host on Eclipse for debugging Spark application

就这样！现在，您可以在实时集群上进行调试，就像在桌面上一样。前面的例子是用火花主控器作为纱客户端运行的。然而，当在 Mesos 集群上运行时，它也应该工作。如果您使用纱簇模式运行，您可能必须将驱动程序设置为附加到您的调试器，而不是将您的调试器附加到驱动程序，因为您不一定事先知道驱动程序将在什么模式下执行。

# 使用 SBT 调试 Spark 应用程序

上述设置主要在使用 Maven 项目的 Eclipse 或 IntelliJ 上工作。假设您已经完成了应用程序，并且正在使用您喜欢的 ide，如 IntelliJ 或 Eclipse，如下所示:

```
object DebugTestSBT {  def main(args: Array[String]): Unit = {    val spark = SparkSession      .builder      .master("local[*]")      .config("spark.sql.warehouse.dir", "C:/Exp/")      .appName("Logging")      .getOrCreate()          spark.sparkContext.setCheckpointDir("C:/Exp/")    println("-------------Attach debugger now!--------------")    Thread.sleep(8000)    // code goes here, with breakpoints set on the lines you want to pause  }}
```

现在，如果您想将此作业转移到本地集群(独立)，第一步就是打包...

# 摘要

在本章中，您看到了测试和调试 Spark 应用程序有多困难。这些在分布式环境中甚至更为关键。我们还讨论了一些先进的方法来解决它们。总之，您学习了在分布式环境中进行测试的方法。然后，您学习了测试 Spark 应用程序的更好方法。最后，我们讨论了调试 Spark 应用程序的一些高级方法。

这或多或少是我们关于火花的高级主题的小旅程的结束。现在，作为读者，或者如果你对数据科学、数据分析、机器学习、Scala 或 Spark 相对较新，我们给你的一个一般性建议是，你应该首先尝试理解你想要执行什么类型的分析。更具体地说，例如，如果您的问题是机器学习问题，请尝试猜测哪种类型的学习算法应该是最适合的，即分类、聚类、回归、推荐或频繁模式挖掘。然后定义和表述问题，之后，您应该基于我们前面讨论的 Spark 的特性工程概念生成或下载适当的数据。另一方面，如果你认为使用深度学习算法或 API 就可以解决你的问题，那么你应该使用其他第三方算法，并与 Spark 集成，直接工作。

我们给读者的最终建议是定期浏览 Spark 网站(位于[http://spark.apache.org/](http://spark.apache.org/))以获取更新，并尝试将 Spark 提供的常规 API 与其他第三方应用程序或工具结合，以获得最佳的协作效果。