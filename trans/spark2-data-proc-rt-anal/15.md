# 用 Spark 2.0 ML 库实现文本分析

在本章中，我们将介绍以下食谱:

*   用 Spark 做词频——所有重要的事情
*   使用 Word2Vec 显示与 Spark 相似的单词
*   为现实生活中的星火 ML 项目下载维基百科的完整转储
*   在 Spark 2.0 中使用潜在语义分析进行文本分析
*   Spark 2.0 中基于潜在狄利克雷分配的主题建模

# 介绍

文本分析处于机器学习、数学、语言学和自然语言处理的交叉点。文本分析，在旧文献中称为文本挖掘，试图从非结构化和半结构化数据中提取信息并推断更高级别的概念、情感和语义细节。需要注意的是，传统的关键词搜索不足以处理嘈杂、模糊和不相关的标记和概念，这些标记和概念需要根据实际上下文进行过滤。

最终，我们试图为给定的一组文档(文本、推文、网络和社交媒体)做的是，确定通信的主旨是什么，以及它试图传达什么概念(主题和...

# 用 Spark 做词频——所有重要的事情

对于这个食谱，我们将从古腾堡计划下载一本书的文本格式，来自[http://www.gutenberg.org/cache/epub/62/pg62.txt](http://www.gutenberg.org/cache/epub/62/pg62.txt)。

古腾堡计划提供超过 50，000 本各种格式的免费电子书供人类消费。请阅读他们的使用条款；让我们不要使用命令行工具来下载任何书籍。

当你看文件的内容时，你会注意到这本书的标题和作者是*《火星公主的古腾堡计划》电子书*，作者是埃德加·赖斯·巴勒斯。

This eBook is for the use of anyone, anywhere, at no cost, and with almost no restrictions whatsoever. You may copy it, give it away, or reuse it under the terms of the Project Gutenberg License included with this eBook online at [http://www.gutenberg.org/](http://www.gutenberg.org/).

然后，我们使用下载的书来演示 Scala 和 Spark 的经典字数统计程序。这个例子刚开始看起来有点简单，但是我们开始了文本处理的特征提取过程。此外，对计算文档中单词出现次数的一般理解将大大有助于我们理解 TF-IDF 的概念。

# 怎么做...

1.  在 IntelliJ 或您选择的 IDE 中启动一个新项目。确保包含必要的 JAR 文件。

2.  配方的`package`说明如下:

```
package spark.ml.cookbook.chapter12
```

3.  为 Scala、Spark 和 JFreeChart 导入必要的包:

```
import org.apache.log4j.{Level, Logger}import org.apache.spark.sql.SQLContextimport org.apache.spark.{SparkConf, SparkContext}import org.jfree.chart.axis.{CategoryAxis, CategoryLabelPositions}import org.jfree.chart.{ChartFactory, ChartFrame, JFreeChart}import org.jfree.chart.plot.{CategoryPlot, PlotOrientation}import org.jfree.data.category.DefaultCategoryDataset
```

4.  我们将定义一个函数，在窗口中显示我们的 JFreeChart:

```
def show(chart: JFreeChart) ...
```

# 它是如何工作的...

我们首先加载下载的书籍，并通过正则表达式对其进行标记。下一步是将所有标记转换为小写，并从标记列表中排除停止单词，然后过滤掉任何长度小于两个字符的单词。

停止词和一定长度的词的去除减少了我们必须处理的特征的数量。这可能看起来不明显，但是基于各种处理标准的特定单词的移除减少了我们的机器学习算法稍后将处理的维数。

最后，我们按照降序对结果字数进行排序，取前 25 名，我们为其显示了一个条形图。

# 还有更多...

在这个食谱中，我们有了关键词搜索的基础。理解主题建模和关键词搜索之间的区别很重要。在关键字搜索中，我们尝试根据出现的情况将短语与给定的文档相关联。在这种情况下，我们将向用户指出出现次数最多的一组文档。

# 请参见

开发人员可以尝试作为扩展的该算法的下一步是添加权重并得出加权平均值，但是 Spark 提供了一个工具，我们将在即将到来的食谱中对其进行探索。

# 使用 Word2Vec 显示与 Spark 相似的单词

在这个食谱中，我们将探索 Word2Vec，这是 Spark 评估单词相似性的工具。Word2Vec 算法的灵感来自于普通语言学中的*分布假设*。在核心，它试图说的是，在相同的上下文(即，与目标的距离)中出现的标记倾向于支持相同的原始概念/意义。

Word2Vec 算法是由谷歌的一组研究人员发明的。请参考*中提到的白皮书，还有更多...*更详细描述 Word2Vec 的配方部分。

# 怎么做...

1.  在 IntelliJ 或您选择的 IDE 中启动一个新项目。确保包含必要的 JAR 文件。

2.  配方的`package`说明如下:

```
package spark.ml.cookbook.chapter12
```

3.  为 Scala 和 Spark 导入必要的包:

```
import org.apache.log4j.{Level, Logger}
import org.apache.spark.ml.feature.{RegexTokenizer, StopWordsRemover, Word2Vec}
import org.apache.spark.sql.{SQLContext, SparkSession}
import org.apache.spark.{SparkConf, SparkContext}
```

4.  让我们定义图书文件的位置:

```
val input = "../data/sparkml2/chapter12/pg62.txt"
```

5.  使用工厂生成器模式创建带有配置的火花会话:

```
val spark = SparkSession
         .builder
.master("local[*]")
         .appName("Word2Vec App")
         .config("spark.sql.warehouse.dir", ".")
         .getOrCreate()
import spark.implicits._
```

6.  我们应该将日志级别设置为警告，否则输出将很难遵循:

```
Logger.getRootLogger.setLevel(Level.WARN)
```

7.  我们载入书籍并将其转换为数据帧:

```
val df = spark.read.text(input).toDF("text")
```

8.  现在，我们利用 Spark 的正则表达式标记器将每一行转换成一个单词包，将每个术语转换成小写，并过滤掉字符长度小于 4 的任何术语:

```
val tokenizer = new RegexTokenizer()
 .setPattern("\\W+")
 .setToLowercase(true)
 .setMinTokenLength(4)
 .setInputCol("text")
 .setOutputCol("raw")
 val rawWords = tokenizer.transform(df)
```

9.  我们通过使用 Spark 的`StopWordRemover`类删除停止词:

```
val stopWords = new StopWordsRemover()
 .setInputCol("raw")
 .setOutputCol("terms")
 .setCaseSensitive(false)
 val wordTerms = stopWords.transform(rawWords)
```

10.  我们应用 Word2Vec 机器学习算法来提取特征:

```
val word2Vec = new Word2Vec()
 .setInputCol("terms")
 .setOutputCol("result")
 .setVectorSize(3)
 .setMinCount(0)
val model = word2Vec.fit(wordTerms)
```

11.  我们从书中找到了火星人的十个同义词:

```
val synonyms = model.findSynonyms("martian", 10)
```

12.  显示模型找到的十个同义词的结果:

```
synonyms.show(false)
```

![](Images/7d3b2158-a771-4656-af50-1a4492e191f7.png)

13.  我们通过停止 SparkContext 来关闭程序:

```
spark.stop()
```

# 它是如何工作的...

Spark 中的 Word2Vec 使用的是跳跃式语法，而不是**连续词包** ( **CBOW** )更适合一个**神经网络** ( **NN** )。其核心是，我们试图计算单词的表示。强烈建议用户理解本地表示和分布式表示之间的区别，这与单词本身的明显含义非常不同。

如果我们对单词使用分布式向量表示，相似的单词自然会在向量空间中紧密地落在一起，这是一种理想的模式抽象和操作的概括技术(也就是说，我们将问题简化为向量算法)。

对于给定的一组单词 *{Word <sub>1、</sub> Word <sub>2，.......</sub>*

# 还有更多...

无论如何你会发现类似的词？能解决这个问题的算法有多少种，又是如何变化的？Word2Vec 算法已经存在了一段时间，并且有一个名为 CBOW 的对应物。请记住，Spark 提供了 skip-gram 方法作为实现技术。

Word2Vec 算法的变化如下:

*   **连续词包(CBOW)** :给定一个中心词，周围有哪些词？
*   **Skip-gram** :如果我们知道周围的单词，我们能猜出缺少的单词吗？

该算法有一个变种叫做**负采样跳格模型** ( **SGNS** )，似乎比其他变种表现更好。

共现是 CBOW 和 skip-gram 的基本概念。即使 skip-gram 不直接使用共现矩阵，它也在间接使用它。

在这个食谱中，我们使用了自然语言处理中的*停止词*技术，在运行我们的算法之前有一个更干净的语料库。停止词是英语单词，如“*”，需要删除，因为它们无助于改善结果。*

 *另一个重要的概念是*词干*，这里不涉及，但将在以后的食谱中演示。词干去除多余的语言构件，将单词还原为词根(例如*工程*、*工程师*、*工程师*变成*工程*，这就是词根)。

在以下网址找到的白皮书应该会对 Word2Vec 提供更深入的解释:

[http://arxiv.org/pdf/1301.3781.pdf](http://arxiv.org/pdf/1301.3781.pdf)

# 请参见

Word2Vec 配方的文档:

*   `Word2Vec()`:[http://spark . Apache . org/docs/latest/API/Scala/index . html # org . Apache . spark . ml . feature . word 2 vec](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.Word2Vec)
*   `Word2VecModel()`:[http://spark . Apache . org/docs/latest/API/Scala/index . html # org . Apache . spark . ml . feature . word 2 vec model](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.Word2VecModel)
*   `StopWordsRemover()`:[http://spark . Apache . org/docs/latest/API/Scala/index . html # org . Apache . spark . ml . feature . stop words remover](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.StopWordsRemover)

# 为现实生活中的星火 ML 项目下载维基百科的完整转储

在这个食谱中，我们将下载并探索维基百科的一个转储，这样我们就可以有一个真实的例子。我们将在本食谱中下载的数据集是维基百科文章的转储。你要么需要命令行工具**卷曲**，要么需要浏览器来检索一个压缩文件，这个时候大约是 13.6 GB。由于大小的原因，我们推荐使用 curl 命令行工具。

# 怎么做...

1.  您可以使用以下命令开始下载数据集:

```
curl -L -O http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles-multistream.xml.bz2
```

2.  现在您想要解压缩 ZIP 文件:

```
bunzip2 enwiki-latest-pages-articles-multistream.xml.bz2
```

这将创建一个名为`enwiki-latest-pages-articles-multistream.xml`的未压缩文件，大约 56 GB。

3.  让我们看看维基百科的 XML 文件:

```
head -n50 enwiki-latest-pages-articles-multistream.xml<mediawiki xmlns=http://www.mediawiki.org/xml/export-0.10/ xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd" version="0.10" ...
```

# 还有更多...

我们建议分块处理 XML 文件，并在实验中使用采样，直到准备好提交最终作业。这将节省大量的时间和精力。

# 请参见

维基下载的文档可在[https://en.wikipedia.org/wiki/Wikipedia:Database_download](https://en.wikipedia.org/wiki/Wikipedia:Database_download)获得。

# 在 Spark 2.0 中使用潜在语义分析进行文本分析

在这个食谱中，我们将利用维基百科上的文章数据来探索 LSA。LSA 翻译成分析一个文档语料库，以发现这些文档中隐藏的意义或概念。

在本章的第一个食谱中，我们介绍了 TF(即术语频率)技术的基础。在这个配方中，我们使用哈希函数来计算 TF，并使用 IDF 将模型拟合到计算的 TF 中。其核心是，LSA 对词频文档使用**奇异值分解** ( **奇异值分解**)来降维，从而提取最重要的概念。还有其他我们需要做的清理步骤(例如，停止单词和词干)，这些步骤将在我们开始分析之前清理单词包。

# 怎么做...

1.  在 IntelliJ 或您选择的 IDE 中启动一个新项目。确保包含必要的 JAR 文件。

2.  配方的包装说明如下:

```
package spark.ml.cookbook.chapter12
```

3.  为 Scala 和 Spark 导入必要的包:

```
import edu.umd.cloud9.collection.wikipedia.WikipediaPage import edu.umd.cloud9.collection.wikipedia.language.EnglishWikipediaPage import org.apache.hadoop.fs.Path import org.apache.hadoop.io.Text import org.apache.hadoop.mapred.{FileInputFormat, JobConf} import org.apache.log4j.{Level, Logger} import org.apache.spark.mllib.feature.{HashingTF, IDF} import org.apache.spark.mllib.linalg.distributed.RowMatrix import org.apache.spark.sql.SparkSession import org.tartarus.snowball.ext.PorterStemmer ...
```

# 它是如何工作的...

该示例首先使用 Cloud9 Hadoop XML 流工具加载维基百科 XML 转储，以处理庞大的 XML 文档。一旦我们解析出页面文本，标记化阶段就调用将我们的维基百科页面文本流转换成标记。在标记化阶段，我们使用了波特词干分析器来帮助将单词简化为通用的基本形式。

更多关于炮泥的详细信息可在[https://en.wikipedia.org/wiki/Stemming](https://en.wikipedia.org/wiki/Stemming)获得。

下一步是在每个页面标记上使用 Spark HashingTF 来计算术语频率。此阶段完成后，我们利用 Spark 的 IDF 生成反向文档频率。

最后，我们使用 TF-IDF API 并应用奇异值分解来处理因子分解和降维。

下面的截图显示了配方的步骤和流程:

![](Images/ff9337d4-120e-4985-a45a-d4a273732f96.png)

Cloud9 Hadoop XML 工具和其他几个必需的依赖项可以在以下位置找到:

*   [:http://central . maven . org/maven 2/info/bliki/wiki/bliki-core/3 . 0 . 19/bliki-core-3 . 0 . 19 . jar](http://central.maven.org/maven2/info/bliki/wiki/bliki-core/3.0.19/bliki-core-3.0.19.jar)
*   `cloud9-2.0.1.jar`:[http://central . maven . org/maven 2/edu/UMD/cloud 9/2 . 0 . 1/cloud 9-2 . 0 . 1 . jar](http://central.maven.org/maven2/edu/umd/cloud9/2.0.1/cloud9-2.0.1.jar)
*   `hadoop-streaming-2.7.4.jar`:[http://central . maven . org/maven 2/org/Apache/Hadoop/Hadoop-streaming/2 . 7 . 4/Hadoop-streaming-2 . 7 . 4 . jar](http://central.maven.org/maven2/org/apache/hadoop/hadoop-streaming/2.7.4/hadoop-streaming-2.7.4.jar)
*   `lucene-snowball-3.0.3.jar`:[http://central . maven . org/maven 2/org/Apache/Lucene/Lucene-雪球/3 . 0 . 3/Lucene-雪球-3.0.3.jar](http://central.maven.org/maven2/org/apache/lucene/lucene-snowball/3.0.3/lucene-snowball-3.0.3.jar)

# 还有更多...

现在应该很明显，即使 Spark 没有提供直接的 LSA 实现，TF-IDF 和 SVD 的结合将让我们构建大语料库矩阵，然后将其分解为三个矩阵，这可以帮助我们通过 SVD 应用降维来解释结果。我们可以专注于最有意义的聚类(类似于推荐算法)。

奇异值分解将把术语频率文档(即按属性排列的文档)分解为三个不同的矩阵，这三个矩阵从一个难以处理且成本高昂的大矩阵中提取 *N* 个概念(即我们示例中的 *N=27* )的效率要高得多。在 ML 中，我们总是更喜欢又高又瘦的矩阵(即本例中的 *U* 矩阵)...

# 请参见

更多关于`SingularValueDecomposition()`的详细信息可以在[http://spark . Apache . org/docs/latest/API/Scala/index . html # org . Apache . spark . mllib . linalg . singularvaluedecomposition](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.SingularValueDecomposition)上找到。

`RowMatrix()`详情请参考[。](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.RowMatrix)

# Spark 2.0 中基于潜在狄利克雷分配的主题建模

在本食谱中，我们将通过利用潜在狄利克雷分配从一组文档中推断主题来演示主题模型的生成。

在前面的章节中，我们已经介绍了 LDA 在聚类和主题建模中的应用，但是在这一章中，我们将展示一个更详细的例子，展示它在使用更真实和复杂的数据集进行文本分析中的应用。

我们还应用了自然语言处理技术，如词干和停止词，为线性判别分析问题的解决提供了一种更现实的方法。我们试图做的是发现一组潜在的因素(也就是说，不同于原始的因素)，这些因素能够以更有效的方式在简化的...

# 怎么做...

1.  在 IntelliJ 或您选择的 IDE 中启动一个新项目。确保包含必要的 JAR 文件。

2.  配方的`package`说明如下:

```
package spark.ml.cookbook.chapter12
```

3.  为 Scala 和 Spark 导入必要的包:

```
import edu.umd.cloud9.collection.wikipedia.WikipediaPage
import edu.umd.cloud9.collection.wikipedia.language.EnglishWikipediaPage
import org.apache.hadoop.fs.Path
import org.apache.hadoop.io.Text
import org.apache.hadoop.mapred.{FileInputFormat, JobConf}
import org.apache.log4j.{Level, Logger}
import org.apache.spark.ml.clustering.LDA
import org.apache.spark.ml.feature._
import org.apache.spark.sql.SparkSession
```

4.  我们定义了一个函数来解析维基百科页面并返回页面的标题和内容文本:

```
def parseWikiPage(rawPage: String): Option[(String, String)] = {
 val wikiPage = new EnglishWikipediaPage()
 WikipediaPage.*readPage*(wikiPage, rawPage)

 if (wikiPage.isEmpty
 || wikiPage.isDisambiguation
 || wikiPage.isRedirect
 || !wikiPage.isArticle) {
 None
 } else {
 *Some*(wikiPage.getTitle, wikiPage.getContent)
 }
 }
```

5.  让我们定义维基百科数据转储的位置:

```
val input = "../data/sparkml2/chapter12/enwiki_dump.xml" 
```

6.  我们为 Hadoop XML 流创建了一个作业配置:

```
val jobConf = new JobConf()
 jobConf.set("stream.recordreader.class", "org.apache.hadoop.streaming.StreamXmlRecordReader")
 jobConf.set("stream.recordreader.begin", "<page>")
 jobConf.set("stream.recordreader.end", "</page>")
```

7.  我们为 Hadoop XML 流处理设置了数据路径:

```
FileInputFormat.addInputPath(jobConf, new Path(input))
```

8.  使用工厂构建器模式创建带有配置的`SparkSession`:

```
val spark = SparkSession
    .builder
.master("local[*]")
    .appName("ProcessLDA App")
    .config("spark.serializer",   "org.apache.spark.serializer.KryoSerializer")
    .config("spark.sql.warehouse.dir", ".")
    .getOrCreate()
```

9.  我们应该将日志记录级别设置为警告，否则，输出将很难遵循:

```
Logger.getRootLogger.setLevel(Level.WARN)
```

10.  我们开始将庞大的维基百科数据转储到文章页面中，并提取文件样本:

```
val wikiData = spark.sparkContext.hadoopRDD(
 jobConf,
 classOf[org.apache.hadoop.streaming.StreamInputFormat],
 classOf[Text],
 classOf[Text]).sample(false, .1)
```

11.  接下来，我们将样本数据处理成一个包含标题和页面上下文文本元组的 RDD，最终生成一个数据帧:

```
val df = wiki.map(_._1.toString)
 .flatMap(parseWikiPage)
 .toDF("title", "text")
```

12.  现在，我们使用 Spark 的`RegexTokenizer`为每个维基百科页面将数据框的文本列转换为原始单词:

```
val tokenizer = new RegexTokenizer()
 .setPattern("\\W+")
 .setToLowercase(true)
 .setMinTokenLength(4)
 .setInputCol("text")
 .setOutputCol("raw")
 val rawWords = tokenizer.transform(df)
```

13.  下一步是通过从标记中移除所有停止单词来过滤原始单词:

```
val stopWords = new StopWordsRemover()
 .setInputCol("raw")
 .setOutputCol("words")
 .setCaseSensitive(false)

 val wordData = stopWords.transform(rawWords)
```

14.  我们通过使用 Spark 的`CountVectorizer`类为过滤后的标记生成术语计数，从而生成包含列特征的新数据框:

```
val cvModel = new CountVectorizer()
 .setInputCol("words")
 .setOutputCol("features")
 .setMinDF(2)
 .fit(wordData)
 val cv = cvModel.transform(wordData)
 cv.cache()
```

“MinDF”指定了为了包含在词汇表中必须出现的不同文档术语的最小数量。

15.  我们现在调用 Spark 的 LDA 类来生成主题和主题的令牌分布:

```
val lda = new LDA()
 .setK(5)
 .setMaxIter(10)
 .setFeaturesCol("features")
 val model = lda.fit(tf)
 val transformed = model.transform(tf)
```

“K”指的是要执行的主题数量和“最大”迭代次数。

16.  我们最后描述了生成的前五个主题并显示:

```
val topics = model.describeTopics(5)
 topics.show(false)
```

![](Images/65c0cc6a-8015-4bdb-85e2-c753056bb79c.png)

17.  现在显示与它们相关的主题和术语:

```
val vocaList = cvModel.vocabulary
topics.collect().foreach { r => {
 println("\nTopic: " + r.get(r.fieldIndex("topic")))
 val y = r.getSeq[Int](r.fieldIndex("termIndices")).map(vocaList(_))
 .zip(r.getSeq[Double](r.fieldIndex("termWeights")))
 y.foreach(println)

 }
}
```

控制台输出如下:

![](Images/d6907078-900d-4d60-9db9-7697c384eb19.png)

18.  我们通过停止 SparkContext 来关闭程序:

```
spark.stop()
```

# 它是如何工作的...

我们从加载维基百科文章的转储开始，并使用 Hadoop XML 利用流工具应用编程接口将页面文本解析为令牌。特征提取过程利用几个类来设置 LDA 类的最终处理，让令牌从 Spark 的`RegexTokenize`、`StopwordsRemover`和`HashingTF`流出。一旦我们有了术语频率，数据就被传递到 LDA 类，用于将文章聚集在几个主题下。

Hadoop XML 工具和其他几个必需的依赖项可以在以下网址找到:

*   [:http://central . maven . org/maven 2/info/bliki/wiki/bliki-core/3 . 0 . 19/bliki-core-3 . 0 . 19 . jar](http://central.maven.org/maven2/info/bliki/wiki/bliki-core/3.0.19/bliki-core-3.0.19.jar)
*   `cloud9-2.0.1.jar`:http://central . maven . org/maven 2/edu/UMD/cloud 9/2 . 0 . 1/cloud 9-2 . 0 . 1 . jar...

# 还有更多...

请参见[第 8 章](15.html)、*使用 Apache Spark 2.0* 的无监督聚类将文档和文本分类为主题的方法 LDA，了解 LDA 算法本身的更详细解释。

以下来自*机器学习研究杂志(JMLR)* 的白皮书为那些想要进行广泛分析的人提供了全面的治疗方法。这是一篇写得很好的论文，一个有统计学和数学基础背景的人应该能够毫无问题地完成它。

关于 JMLR 的更多详情，请参考[http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)链接；另一个链接是[。](https://www.cs.colorado.edu/~mozer/Teaching/syllabi/ProbabilisticModels/readings/BleiNgJordan2003.pdf)

# 请参见

*   构造器的文档可在[http://spark . Apache . org/docs/latest/API/Scala/index . html # org . Apache . spark . ml . clustering . LDA](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA)获得
*   LDAModel 的文档可在[http://spark . Apache . org/docs/latest/API/Scala/index . html # org . Apache . spark . ml . clustering . LDAModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDAModel)获得

有关以下内容，请参见 Spark 的 Scala 应用编程接口文档:

*   分布式数据模型
*   EMLDAOptimizer
*   lanoptimizer
*   LocalLDAModel
*   在线优化器*