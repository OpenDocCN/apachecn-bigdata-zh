# 更多数据挖掘和机器学习技术

在本章中，我们将讨论更多的数据挖掘和机器学习技术。 我们将讨论一种非常简单的技术，称为**k 近邻**(**KNN**)。 然后，我们将使用 kNN 来预测电影的评级。 之后，我们将继续讨论降维和主成分分析。 我们还将查看 PCA 的一个示例，在该示例中，我们将把 4D 数据减少到二维，同时仍然保留其方差。

然后，我们将介绍数据仓库的概念，并了解较新的 ELT 流程相对于 ETL 流程的优势。 我们将学习强化学习的有趣概念，并了解 Pac-Man 游戏的智能 Pac-Man 代理背后使用的技术。 最后，我们将看到一些用于强化学习的花哨术语。

我们将介绍以下主题：

*   K 近邻的概念
*   KNN 在电影评分预测中的实现
*   降维与主成分分析
*   使用虹膜数据集的 PCA 示例
*   数据仓库和 ETL 与 ELT
*   什么是强化学习？
*   智能吃豆人游戏背后的工作
*   一些用于强化学习的花哨词汇

# K 近邻-概念

让我们来谈谈雇主希望你了解的一些数据挖掘和机器学习技术。 我们将从一个非常简单的名字开始，简称为 KNN。 你会惊讶于一个好的有监督的机器学习技术是多么简单。 我们来看看吧！

KNN 听起来很奇特，但它实际上是最简单的技术之一！ 假设您有一个散点图，您可以计算该散点图上任意两点之间的距离。 让我们假设您有一堆已经分类的数据，您可以从中训练系统。 如果我有一个新的数据点，我所要做的就是查看基于该距离度量的 kNN，并让他们全部投票决定该新点的分类。

让我们假设下面的散点图正在策划电影。 正方形代表科幻电影，三角形代表戏剧电影。 我们会说，这是在绘制收视率与人气的对比图，或者你能想象到的任何其他东西：

![](img/cdac4f24-ef67-463c-ade8-14188ba538e5.jpg)

在这里，我们可以根据散点图上任意两点之间的评分和受欢迎程度来计算某种距离。 让我们假设一个新的观点出现了，一部我们不知道是哪种类型的新电影。 我们可以做的是将*K*设置为*3*，并将最近的*3*取到散点图上的这一点；然后他们都可以对新点/电影的分类进行投票。

你可以看到，如果我取最近的三个邻居(*K=3*)，我有 2 部剧情片和 1 部科幻片。 然后我会让他们所有人投票，然后我们会根据这 3 个最近的邻居来选择这个新点的戏剧分类。 现在，如果我把这个圆扩大到包括 5 个最近的邻居，也就是*K=5*，我会得到一个不同的答案。 所以，在这种情况下，我选择了 3 部科幻小说和 2 部剧情片。 如果我让他们全部投票，我最终会得到新电影的科幻分类。

我们对 K 的选择可能非常重要。 你想要确保它足够小，这样你就不会走得太远，开始挑选不相关的邻居，但它必须足够大，以包含足够的数据点来获得有意义的样本。 因此，通常您必须使用训练/测试或类似的技术来实际确定给定数据集的*K*的正确值是多少。 但是，在一天结束的时候，你必须从你的直觉开始，并从那里开始工作。

事情就是这样，就这么简单。 所以，这是一种非常简单的技术。 你所做的就是把 k 个最近的邻居放在一个散点图上，然后让他们对一个分类进行投票。 它确实符合监督学习的条件，因为它使用一组已知点(即已知分类)的训练数据来通知新点的分类。

但让我们做一些更复杂的事情，实际操作电影，仅仅基于它们的元数据。 让我们看看我们是否真的能仅仅根据电影的内在价值来计算出电影的最近邻居，例如，它的评级，它的流派信息：

![](img/22a520fc-4ebf-43d3-9427-5974ced51f65.jpg)

从理论上讲，我们可以使用 k 近邻重新创建类似于观看此商品的*客户也观看了*(上图是来自亚马逊的屏幕截图)的内容。 而且，我可以更进一步：一旦我基于 k-近邻算法识别出与给定电影相似的电影，我就可以让他们对该电影的预测评级进行投票。

这就是我们在下一个示例中要做的。 现在你有了 knn，k-近邻的概念。 让我们继续将其应用于一个例子，即实际找到彼此相似的电影，并使用这些最近的电影来预测另一部我们以前没有看过的电影的评分。

# 使用 KNN 预测电影的评分

好的，我们将把 KNN 这个简单的概念应用到一个更复杂的问题上，那就是预测一部电影的评级，只给出它的类型和评级信息。 因此，让我们潜入并实际尝试预测电影评分仅仅基于 KNN 算法，看看我们得到了什么。 所以，如果你想跟我一起去，那就去打开`KNN.ipynb`吧，你可以和我一起玩。

我们要做的是仅仅根据电影的元数据来定义电影之间的距离度量。 我所说的元数据只是指电影固有的信息，即与电影相关的信息。 具体地说，我们将看看这部电影的类型分类。

我们的`MovieLens`数据集中的每部电影都有关于其所属类型的附加信息。 一部电影可以属于不止一种类型，一种类型可以是科幻、戏剧、喜剧或动画。 我们还将看看这部电影的整体受欢迎程度，这是根据评分人数得出的，我们还知道每部电影的平均评分。 我可以将所有这些信息结合在一起，基本上只根据分级信息和类型信息来创建两部电影之间的距离度量。 让我们看看我们能拿到什么。

我们将再次使用 PANDA 来简化工作，如果您继续学习，请再次确保将`MovieLens`数据集的路径更改为您安装它的位置，这几乎肯定不会是本 Python 笔记本中的路径。

如果您想跟上的话，请先换一下。 与前面一样，我们只需要导入实际的评级数据文件本身，即在 PANAAS 中使用`read_csv()`函数的`u.data`。 我们将告诉您，它实际上有一个制表符分隔符，而不是逗号。 我们将为数据集中的每个电影评级导入前 3 列，它们分别表示`user_id`、`movie_id`和评级：

```py
import pandas as pd 

r_cols = ['user_id', 'movie_id', 'rating'] 
ratings = pd.read_csv('C:\DataScience\ml-100k\u.data', sep='\t', names=r_cols, usecols=range(3)) 
ratings.head()ratings.head() 

```

如果我们继续运行它并查看顶部，我们可以看到它正在工作，下面是输出应该是什么样子：

![](img/e4c042b5-3659-4827-aa99-624eaac5abda.png)

我们最终得到一个具有`user_id`、`movie_id`和`rating`的`DataFrame`。 例如，`user_id 0`评分`movie_id 50`，我认为是《星球大战》、《5 星》等等。

下一件事我们必须弄清楚的是关于每部电影的评分的汇总信息。 我们在熊猫中使用`groupby()`函数来实际按`movie_id`对所有内容进行分组。 我们将把每部电影的所有评分合并在一起，我们将输出每部电影的评分数量和平均评分，也就是平均值：

```py
movieProperties = ratings.groupby('movie_id').agg({'rating':  [np.size, np.mean]}) 
movieProperties.head() 

```

让我们继续这样做-很快就会返回，下面是输出的样子：

![](img/cfe272d1-c206-460d-9ebb-759a700f39ec.png)

这给我们提供了另一个`DataFrame`，它告诉我们，例如，`movie_id 1`拥有**452**评级(这是衡量其受欢迎程度的指标，即有多少人实际观看了它并对其进行了评级)，以及 3.8 分的平均评论分数。 所以，**452**人看了`movie_id 1`，他们给它的平均评价是 3.87，这是相当不错的。

现在，收视率的原始数字对我们来说并不是那么有用。 我的意思是，我不知道**452**是否意味着它很受欢迎。 因此，为了使这一点正常化，我们要做的基本上是根据每部电影的最高和最低收视率来衡量这一点。 我们可以使用`lambda`函数来实现这一点。 因此，我们可以通过这种方式将函数应用于整个`DataFrame`。

我们要做的是使用`np.min()`和`np.max()`函数来查找在整个数据集中找到的最大评级数和最小评级数。 因此，我们将选择最受欢迎的电影和最不受欢迎的电影，找出那里的范围，并根据该范围将所有内容正常化：

```py
movieNumRatings = pd.DataFrame(movieProperties['rating']['size']) 
movieNormalizedNumRatings = movieNumRatings.apply(lambda x: (x - np.min(x)) / (np.max(x) - np.min(x))) 
movieNormalizedNumRatings.head() 

```

当我们运行它时，它为我们提供了以下内容：

![](img/5efb3fdc-8c1e-4a15-8978-7a3bc3e3a954.png)

这基本上是衡量每部电影的受欢迎程度，从 0 到 1。所以，0 分意味着没有人看过它，它是最不受欢迎的电影，而`1`分意味着每个人都看了它，这是最受欢迎的电影，或者更确切地说，是最多人看的电影。 所以，我们现在有了一个衡量电影受欢迎程度的指标，我们可以用它来衡量我们的距离。

接下来，我们来摘录一些大体信息。 因此，有一个`u.item`文件，它不仅包含电影名称，还包含每部电影所属的所有类型：

```py
movieDict = {} 
with open(r'c:/DataScience/ml-100k/u.item') as f: 
    temp = '' 
    for line in f: 
        fields = line.rstrip('\n').split('|') 
        movieID = int(fields[0]) 
        name = fields[1] 
        genres = fields[5:25] 
        genres = map(int, genres) 
        movieDict[movieID] = (name, genres,      
        movieNormalizedNumRatings.loc[movieID].get('size'),movieProperties.loc[movieID].rating.get('mean')) 

```

上面的代码实际上将遍历`u.item`的每一行。 我们用很难的方式来做这件事；我们不使用任何 PANDA 函数；这一次我们只使用直接的 Python。 同样，请确保将该路径更改为安装此信息的位置。

接下来，我们打开`u.item`文件，然后一次遍历文件中的每一行。 我们去掉末尾的新行，并根据该文件中的管道分隔符将其拆分。 然后，我们提取`movieID`、电影名称和所有单独的类型字段。 基本上，在这个源数据的 19 个不同字段中有一堆 0 和 1，其中每个字段代表一个给定的类型。 然后，我们最后构建一个 Python 字典，将电影 ID 映射到它们的名称、流派，然后我们还重新放回我们的评级信息。 因此，我们将有名字，流派，受欢迎程度从 0 到 1，以及平均评分。 这就是这段小代码的作用。 让我们来运行它！ 为了看看我们最终得到的是什么，我们可以提取`movie_id 1`的值：

```py
movieDict[1] 

```

以下是前面代码的输出：

![](img/d0121c0b-60c7-4e78-a982-88ea83f656be.png)

在我们的字典里，`movie_id 1`的第一个词条恰好是“玩具总动员”，这是一部你可能听说过的皮克斯 1995 年的老电影。 接下来是所有流派的列表，其中 0 表示它不是该流派的一部分，1 表示它是该流派的一部分。 在`MovieLens`数据集中有一个数据文件，它将告诉您这些流派字段中的每一个实际上对应的是什么。

对我们来说，这其实并不重要，对吧？ 我们只是试着根据类型来衡量电影之间的距离。 所以，从数学上讲，最重要的是这种类型的载体与另一部电影有多相似，好吗？ 真正的流派本身，并不重要！ 我们只是想看看两部电影在类型分类上有什么相同或不同之处。 所以我们有类型列表，我们有我们计算的受欢迎程度分数，我们有玩具总动员的平均评分。 好的，让我们继续找出如何将所有这些信息组合到一个距离度量中，这样我们就可以找到 k 个最近的邻居，比如玩具总动员(Toy Story)。

我相当随意地计算了这个`ComputeDistance()`函数，它接受两个电影 ID 并计算两者之间的距离分数。 首先，我们将基于两个流派向量之间的相似性，使用余弦相似性度量。 就像我说的，我们只需要列出每部电影的类型列表，看看它们之间有多相似。 同样，`0`表示它不是该流派的一部分，`1`表示它是该流派的一部分。

然后，我们将比较受欢迎程度分数，只取原始差异，即这两个受欢迎程度分数之间的差异的绝对值，并将其用于距离度量。 然后，我们将仅使用该信息来定义两部电影之间的距离。 例如，如果我们计算电影 ID 2 和 4 之间的距离，该函数将仅根据该电影的受欢迎程度和这些电影的流派返回某个距离函数。

现在，如果你愿意，想象一个散点图，就像我们在前面几节的例子中看到的，其中一个轴可能是体裁相似性的度量，基于余弦度量，另一个轴可能是流行度，好吗？ 我们只是想找出这两件事之间的距离：

```py
from scipy import spatial 

def ComputeDistance(a, b): 
    genresA = a[1] 
    genresB = b[1] 
    genreDistance = spatial.distance.cosine(genresA, genresB) 
    popularityA = a[2] 
    popularityB = b[2] 
    popularityDistance = abs(popularityA - popularityB) 
    return genreDistance + popularityDistance 

ComputeDistance(movieDict[2], movieDict[4]) 

```

在本例中，我们尝试使用电影 2 和电影 4 之间的距离度量来计算距离，最终得到的分数为 0.8：

![](img/feff2ac4-5127-44d7-b8b2-63e72bfbb485.png)

记住，距离远意味着它们不相似，对吗？ 我们想要距离最近、距离最小的邻居。 因此，从 0 到 1 的分数来看，0.8 分是一个相当高的数字。这告诉我，这两部电影真的不太相似。 让我们做一个快速的理智检查，看看这些电影到底是什么：

```py
print movieDict[2] 
print movieDict[4] 

```

原来是电影“黄金眼”和“矮子”是两部截然不同的电影：

![](img/475c25af-41f0-4812-80dc-56ede2b9baf1.png)

你知道，你有詹姆斯·邦德的动作冒险电影和喜剧电影-一点也不像！ 实际上，他们在受欢迎程度上是可以比较的，但类型的差异造成了这一点。 好吧!。 那么，让我们把这一切放在一起吧！

接下来，我们将编写一些代码来实际获取一些给定的 MovieID 并找到 knn。 因此，我们所要做的就是计算“玩具总动员”和我们电影词典中所有其他电影之间的距离，并根据它们的距离得分对结果进行排序。 下面的一小段代码就是这样做的。 如果你想花点时间来理解它，那就相当简单了。

我们有一个小的`getNeighbors()`函数，它将获取我们感兴趣的电影和我们想要查找的 K 个邻居。 它将遍历我们拥有每一部电影；如果它实际上是一部与我们正在观看的电影不同的电影，它将计算之前的距离分数，将其附加到我们已有的结果列表中，并对结果进行排序。 然后我们将揭晓 K 个排名靠前的结果。

在本例中，我们将*K*设置为 10，找出最近的 10 个邻居。 我们将使用`getNeighbors()`找到 10 个最近的邻居，然后遍历所有这 10 个最近的邻居，并计算每个邻居的平均评分。 这个平均收视率将告诉我们我们对这部电影的收视率预测。

As a side effect, we also get the 10 nearest neighbors based on our distance function, which we could call similar movies. So, that information itself is useful. Going back to that "Customers Who Watched Also Watched" example, if you wanted to do a similar feature that was just based on this distance metric and not actual behavior data, this might be a reasonable place to start, right?

```py
import operator 

def getNeighbors(movieID, K): 
    distances = [] 
    for movie in movieDict: 
        if (movie != movieID): 
            dist = ComputeDistance(movieDict[movieID],  movieDict[movie]) 
            distances.append((movie, dist)) 
    distances.sort(key=operator.itemgetter(1)) 
    neighbors = [] 
    for x in range(K): 
        neighbors.append(distances[x][0]) 
    return neighbors 

K = 10 
avgRating = 0 
neighbors = getNeighbors(1, K) 
for neighbor in neighbors: 
    avgRating += movieDict[neighbor][3] 
    print movieDict[neighbor][0] + " " +  str(movieDict[neighbor][3]) 
    avgRating /= float(K) 

```

那么，让我们继续运行它，看看我们最终会得到什么。 以下代码的输出如下：

![](img/4e0fbf04-f04f-4b07-aff6-6d7f67fb0c17.png)

结果并不是那么不合理。 所以，我们以电影“玩具总动员”为例，它是 MovieID1，我们带回来的，排名前 10 位的邻居，都是相当不错的喜剧和儿童电影的精选集。 因此，鉴于“玩具总动员”是一部受欢迎的喜剧和儿童电影，我们还有其他一些受欢迎的喜剧和儿童电影；所以，它似乎奏效了！ 我们不需要使用一堆花哨的协同过滤算法，这些结果并没有那么糟糕。

接下来，让我们使用 kNN 来预测评级，在此示例中，我们将评级视为分类：

```py
avgRating 

```

以下是前面代码的输出：

![](img/92dc0277-702f-4801-bda5-ae21498f7a55.png)

我们最终的预测评分是 3.34，这实际上与那部电影的实际评分 3.87 相差不大。 所以不算太好，但也不算太差！ 我的意思是，考虑到这个算法是多么简单，它实际上工作得出奇地好！

# 状况 / 活动 / 活性 / 行为

本例中的大部分复杂性只是确定我们的距离度量，您知道我们故意在这里做了一点花哨，只是为了让它有趣，但是您可以做任何其他您想做的事情。 所以，如果你想摆弄这件事，我绝对鼓励你这么做。 我们给 K 选 10 的选择完全是凭空而来的，这是我编出来的。 这对不同的 K 值有何反应？ K 值越高，效果越好吗？ 或者使用较低的 K 值？ 有关系吗？

如果你真的想做一个更复杂的练习，你可以试着把它应用到训练/测试中，找出最能根据 kNN 预测给定电影评分的 K 值。 而且，你可以使用不同的距离度量，这在某种程度上也是我编造的！ 所以，玩玩距离度量，也许你可以使用不同的信息来源，或者用不同的权重。 这可能是一件有趣的事。 也许，受欢迎程度并不像类型信息那么重要，或者恰恰相反。 也看看这对你的结果有什么影响。 所以，继续扰乱这些算法，扰乱代码并运行它，看看你能得到什么！ 而且，如果你确实找到了改进这一点的重要方法，那就和你的同学们分享吧。

这就是 KNN 在行动！ 这是一个非常简单的概念，但实际上它可能非常强大。 所以，你就知道了：类似的电影仅仅是基于类型和受欢迎程度，而不是其他。 效果出奇的好！ 而且，我们使用 KNN 的概念来实际使用最近的邻居来预测一部新电影的评分，这实际上也是非常有效的。 所以，这是 KNN 在行动，非常简单的技术，但通常它工作得非常好！

# 降维与主成分分析

好了，是时候全力以赴了！ 我们将讨论更高的维度，以及降维。 听起来很可怕！ 这涉及到一些花哨的数学运算，但从概念上讲并不像您想象的那么难掌握。 接下来，我们来谈谈降维和主成分分析。 听起来很戏剧化！ 通常，当人们谈论这一点时，他们谈论的是一种称为主成分分析(PCA)的技术，以及一种称为奇异值分解(SVD)的特定技术。 因此，主成分分析和奇异值分解是这一部分的主题。 让我们一头扎进去吧！

# 降维

那么，维度的诅咒是什么呢？ 嗯，很多问题可以被认为有很多不同的维度。 例如，当我们做电影推荐时，我们有各种电影的属性，每一部电影都可以被认为是它在数据空间中自己的维度。

如果你有很多电影，那是很多的维度，你不可能真的想超过 3 个，因为那是我们成长过程中进化的东西。 您可能拥有某些类型的数据，这些数据具有许多您关心的不同功能。 你知道，稍后我们将看一个我们想要分类的花朵的例子，这个分类是基于 4 种不同的花朵尺寸。 这 4 个不同的特征，这 4 个不同的测量可以代表 4 个维度，同样，这是很难想象的。

为此，存在降维技术以找到一种将高维信息降维为低维信息的方法。 这不仅可以使查看和分类变得更容易，而且对压缩数据之类的事情也很有用。 因此，通过保留最大方差，在减少维数的同时，我们可以更紧凑地表示数据集。 降维的一个非常常见的应用不仅仅是可视化，还包括压缩和特征提取。 稍后我们会更详细地讨论这一点。

降维的一个非常简单的例子可以被认为是 k-均值聚类：

![](img/8b8a47d6-5330-471d-b6c9-e36d861d9247.png)

例如，我们可能会从代表数据集中许多不同维度的很多点开始。 但是，最终，我们可以把它归结为 K 个不同的质心，以及你到这些质心的距离。 这是将数据浓缩到低维表示的一种方式。

# 主分量分析 / 主成分分析

通常，当人们谈论降维时，他们谈论的是一种叫做主成分分析的技术。 这是一种更花哨的技术，它涉及到一些相当复杂的数学。 但是，在更高的层次上，你所需要知道的就是它需要更高维度的数据空间，并且它会在该数据空间和更高维度中找到平面。

这些高维平面被称为超平面，它们由被称为特征向量的东西定义。 最终，您可以获得任意多个维度的平面，将数据投影到这些超平面上，这些超平面就会成为低维数据空间中的新轴：

![](img/f45171af-5601-499c-a64b-2e13f263c348.png)

你知道，除非你熟悉高维数学，而且你以前也想过，否则你很难理解！ 但是，归根结底，这意味着我们选择了高维空间中的平面，这些平面仍然保留了我们数据中的最大方差，并将数据投影到那些高维平面上，然后我们把这些平面带到低维空间，好吗？

您不需要真正理解所有的数学知识就可以使用它；重要的是，它是一种非常有原则的方法，可以将数据集降低到更低的维度空间，同时仍然保持其中的方差。 我们将图像压缩作为这方面的一种应用。 所以你知道，如果我想要降低图像的维度，我可以使用 PCA 将其归结为本质。

面部识别是另一个例子。 所以，如果我有一个人脸的数据集，也许每个人脸代表了 2D 图像的第三个维度，我想总结一下，奇异值分解(SVD)和主成分分析(Master Component Analysis)可以成为识别人脸真正重要特征的一种方法。 因此，它可能最终会更多地关注眼睛和嘴巴，例如，那些在数据集中保留方差所必需的重要特征。 因此，它可以产生一些非常有趣和非常有用的结果，这些结果是从数据中自然产生的，这是一种很酷的结果！

为了让它成为现实，我们将使用一个更简单的例子，使用所谓的虹膜数据集。 这是一个包含在 SCRICKIT-LEARN 中的数据集。 这是非常常用的例子，这是它背后的想法：所以，一朵虹膜的花上实际上有两种不同的花瓣。 一种叫做花瓣，这是你熟悉的花瓣，它还有一种叫做萼片的东西，这是花朵上支撑性的下层花瓣。

我们可以取一些不同种类的蝴蝶花，测量花瓣的长度和宽度，以及萼片的长度和宽度。 因此，在我们的数据集中，花瓣的长度和宽度以及萼片的长度和宽度是 4 个不同的测量值，对应着 4 个不同的维度。 我想用它来对虹膜可能属于什么物种进行分类。 现在，PCA 将允许我们在 2 维而不是 4 维上可视化这一点，同时仍然保留该数据集中的方差。 因此，让我们看看这有多好，并实际编写一些 Python 代码来使 PCA 在 Iris 数据集上发生。

这些就是降维、主成分分析和奇异值分解的概念。 所有华丽的字眼，是的，这是一种华丽的东西。 你知道，我们正在处理的问题是将高维空间缩小到较小的维度空间，同时保持它们的方差。 幸运的是，SCRKIT-LINE 让这件事变得非常容易，就像你实际应用 PCA 所需要的 3 行代码一样。 所以，让我们来实现这一目标吧！

# 虹膜数据集的 PCA 示例

让我们将主成分分析应用于虹膜数据集。 这是一个四维数据集，我们要将其缩减到二维。 我们将看到，即使丢弃一半的维度，我们实际上仍然可以保留该数据集中的大部分信息。 这是很酷的东西，也很简单。 让我们一头扎进去，做一些主成分分析，治愈维度的魔咒。 继续打开`PCA.ipynb`文件。

实际上，使用 SCRICKIT 很容易做到这一点--像往常一样学习！ 同样，PCA 是一种降维技术。 这听起来很科幻，所有这些关于更高维度的讨论。 但是，为了让它再次变得更加具体和真实，一个常见的应用程序是图像压缩。 你可以把黑白图片想象成 3 维的图像，你有宽度，x 轴，y 轴的高度，每个单元格都有 0 到 1 的亮度值，也就是黑色或白色，或者介于两者之间的某个值。 这就是 3D 数据，你有 2 个空间维度，然后是亮度和强度维度。

如果你把它提取到 2 维，那就是压缩的图像，如果你用一种技术尽可能地保留图像中的方差，你仍然可以重建图像，理论上不会有太大的损失。 这就是降维，归结为一个实际的例子。

现在，我们将在这里使用一个不同的例子，使用虹膜数据集，而 SCRKIT-LEARN 包括了这一点。 它只是一个各种虹膜花尺寸的数据集，以及该数据集中每个虹膜的物种分类。 就像我之前说的，它还测量了每个虹膜标本的花瓣和萼片的长度和宽度。 因此，在花瓣的长度和宽度以及萼片的长度和宽度之间，我们的数据集中有 4 维的特征数据。

我们想把它提炼成我们可以真正看到和理解的东西，因为你的大脑不能很好地处理 4 维，但你可以很容易地看到一张纸上的 2 维。 让我们继续把它装载起来：

```py
from sklearn.datasets import load_iris 
from sklearn.decomposition import PCA 
import pylab as pl 
from itertools import cycle 

iris = load_iris() 

numSamples, numFeatures = iris.data.shape 
print numSamples 
print numFeatures 
print list(iris.target_names) 

```

SCRICKIT 中内置了一个非常方便的花花公子`load_iris()`函数--学习它不需要额外的工作就可以为您加载，因此您只需专注于有趣的部分即可。 让我们看看该数据集是什么样子，前面代码的输出如下所示：

![](img/3c9a69f5-391b-47b4-9fe1-38deb4e9b270.png)

您可以看到，我们正在提取该数据集的形状，这意味着我们在其中有多少个数据点，即`150`，以及该数据集有多少个特征或多少维，即`4`。 因此，我们的数据集中有`150`个虹膜样本，包含 4 维信息。 同样，这是萼片的长度和宽度，以及花瓣的长度和宽度，总共有`4`个特征，我们可以把它们看作`4`个维度。

我们还可以打印出这个数据集中的目标名称列表，这是分类，我们可以看到每个虹膜属于三个不同的物种之一：Setosa、Versicolor 或 Virginica。 这就是我们正在处理的数据：150 个虹膜样本，分为 3 个物种之一，每个虹膜都有 4 个特征。

让我们看看 PCA 有多简单。 尽管这是一项非常复杂的技术，但只需几行代码即可。 我们将分配整个虹膜数据集，并将其命名为 X。然后，我们将创建一个 PCA 模型，我们将保留`n_components=2`，因为我们需要 2 维，也就是说，我们将从 4 维变为 2 维。

我们将使用`whiten=True`，这意味着我们将对所有数据进行标准化，并确保所有数据都是好的和可比较的。 通常情况下，您会想要这样做以获得好的结果。 然后，我们将 PCA 模型拟合到我们的虹膜数据集`X`。 然后，我们可以使用该模型，将该数据集向下转换为 2 维。 让我们继续运行它。 事情发生得太快了！

```py
X = iris.data 
pca = PCA(n_components=2, whiten=True).fit(X) 
X_pca = pca.transform(X) 

```

请想一想刚才那里发生了什么事。 我们实际上创建了一个 PCA 模型，将 4 维数据降低到`2`，它通过选择 2 个 4 维矢量，在周围创建超平面，将 4 维数据投影到 2 维，从而做到了这一点。 你可以通过打印出主成分分析的实际分量，来实际看到这些 4D 矢量，这些特征矢量。 因此，**PCA**代表**主成分分析**，这些主成分是我们选择用来定义平面的特征向量：

```py
print pca.components_ 

```

上述代码的输出如下所示：

![](img/481b0278-59d5-4c69-a59c-8dafa30717de.png)

你可以实际查看这些值，它们对你来说不会有太大意义，因为你不可能真的描绘出 4 维，但我们这样做只是为了让你能看到它实际上在主成分上做了一些事情。 那么，让我们来评估一下我们的结果：

```py
print pca.explained_variance_ratio_ 
print sum(pca.explained_variance_ratio_) 

```

PCA 模型给我们带回了一种叫做`explained_variance_ratio`的东西。 基本上，这告诉您原始 4D 数据中的方差在我将其降低到 2 维时保留了多少。 那么，让我们继续来看看这一点：

![](img/b5dd7d8a-29b5-4e7e-be53-21821ac9418c.png)

实际上，它返回的是我们保留的 2 个维度的 2 个项目的列表。 这告诉我，在第一个维度中，我实际上可以保留数据中 92%的方差，而第二个维度只给了我额外 5%的方差。 如果我把我的数据投射到的这两个维度加在一起，我仍然保留了源数据中 97%以上的方差。 我们可以看到，捕获此数据集中的所有信息实际上并不需要 4 维，这非常有趣。 这是相当酷的东西！

如果你仔细想想，你觉得为什么会这样呢？ 嗯，也许花的整体大小与其中心的物种有一定的关系。 可能是花瓣和萼片的长宽比。 你知道，对于给定的物种，或者对于给定的花的整体大小，这些东西中的一些可能会彼此协调一致地移动。 因此，也许 PCA 自己提取的这 4 个维度之间存在关系。 这是相当酷，非常强大的东西。 让我们继续想象一下这一点。

将其降低到 2 维的全部目的是为了我们可以制作一个很好的二维散点图，至少这是我们在这里这个小例子中的目标。 因此，我们将在这里表演一点 Matplotlib 魔术来做到这一点。 这里发生了一些奇特的事情，我至少应该提一下。 因此，我们要做的是创建一个颜色列表：红色、绿色和蓝色。 我们将创建一个目标 ID 列表，以便值 0、1 和 2 映射到我们拥有的不同虹膜种类。

我们要做的就是用每个物种的实际名称把所有这些都压缩起来。 For 循环将遍历 3 个不同的 Iris 物种，在此过程中，我们将获得该物种的索引、与其关联的颜色以及该物种的实际人类可读名称。 我们一次只取一个物种，然后用给定的颜色和给定的标签将其绘制在我们的散点图上。 然后我们将添加我们的图例并显示结果：

```py
colors = cycle('rgb') 
target_ids = range(len(iris.target_names)) 
pl.figure() 
for i, c, label in zip(target_ids, colors, iris.target_names): 
    pl.scatter(X_pca[iris.target == i, 0], X_pca[iris.target == i, 1], 
        c=c, label=label) 
pl.legend() 
pl.show() 

```

以下是我们最终得出的结论：

![](img/c4cb8972-5c7e-4b7a-b42b-2f03a0c0fa57.png)

这就是我们的 4D 虹膜数据向下投影到 2 维。 非常有趣的东西！ 你可以看到它仍然很好地聚集在一起。 你知道，你有所有的处女座坐在一起，所有的 Versicolors 坐在中间，而 Setosas 坐在左边很远的地方。 真的很难想象这些实际的价值代表了什么。 但是，重要的是，我们已经将 4D 数据投影到 2D，并且以这样一种方式，我们仍然保留了方差。 我们仍然可以看到这 3 个物种之间清晰的界限。 有点混杂在一起，你知道，这不是完美的。 但总的来说，这是相当有效的。

# 状况 / 活动 / 活性 / 行为

正如您在`explained_variance_ratio`中回忆的那样，我们实际上捕获了单个维度中的大部分差异。 也许花的整体大小才是对它进行分类的真正重要的因素；你可以用一个特征来指定这一点。 所以，如果你觉得可以的话，那就继续修改结果吧。 看看你能不能用 2 维，或者用 1 维代替 2 维！ 所以，把`n_components`改成`1`，看看你会得到什么样的变异率。

会发生什么事？ 这有意义吗？ 玩一玩，熟悉一下。 也就是说，降维、主成分分析和奇异值分解都在起作用。 非常非常花哨的术语，你知道，公平地说，这是一些隐藏在引擎盖下的非常花哨的数学。 但正如你所看到的，这是一项非常强大的技术，有了科学工具包学习，它的应用并不困难。 所以，把它放在你的工具箱里。

现在你就知道了！ 花卉信息的 4D 数据集可以归结为 2 个维度，我们既可以很容易地将其可视化，也可以在我们感兴趣的分类之间看到清晰的轮廓。 因此，PCA 在这个例子中工作得非常好。 同样，对于压缩、特征提取或面部识别等方面，它也是一个有用的工具。 所以，把它放在你的工具箱里。

# 数据仓库概述

接下来，我们将稍微谈一谈数据仓库。 这是一个最近被 Hadoop、一些大数据技术和云计算的出现所颠覆的领域。 这里有很多很时髦的词，但概念对你来说很重要。

让我们深入探讨这些概念！ 让我们讨论一下 ELT 和 ETL，以及一般的数据仓库。 这更多的是一个概念，而不是一种具体的实用技术，所以我们将从概念上来讨论它。 但是，这很可能会出现在求职面试的场景中。 因此，让我们确保您理解这些概念。

我们将从一般的数据仓库开始讨论。 什么是数据仓库？ 嗯，它基本上是一个巨大的数据库，包含来自许多不同来源的信息，并为您将它们联系在一起。 例如，可能你在一家大型电子商务公司工作，他们可能有一个订购系统，将人们购买的商品的信息反馈到你的数据仓库中。

您还可能拥有来自 Web 服务器日志的信息，这些信息会被吸收到数据仓库中。 例如，这将允许你将网站上的浏览信息与人们最终订购的信息捆绑在一起。 也许你还可以从你的客户服务系统中获取信息，并衡量浏览行为与客户在一天结束时的幸福感之间是否存在关系。

数据仓库面临的挑战是从许多不同的来源获取数据，将它们转换为某种模式，使我们能够同时查询这些不同的数据源，并让我们通过数据分析获得洞察力。 所以，大公司和组织通常都有这样的事情。 我们将在这里介绍大数据的概念。 例如，你可以有一个巨大的 Oracle 数据库，其中包含了所有这些内容，也许它以某种方式进行了分区，并进行了复制，它具有各种各样的复杂性。 您只需通过 SQL、结构化查询语言或通过图形工具(如 Tableau，这是目前非常流行的一种工具)就可以对其进行查询。 这就是数据分析师所做的，他们使用 Tableau 之类的东西来查询大型数据集。

这就是数据分析师和数据科学家之间的区别。 您可能实际上是在编写代码，以便对接近人工智能的数据执行更高级的技术，而不是仅仅使用工具从数据仓库中提取图形和关系。 这是一个非常复杂的问题。 在亚马逊，我们有一个完整的数据仓库部门全职处理这些东西，而他们从来没有足够的人员，我可以告诉你，这是一项艰巨的工作！

你知道，做数据仓库有很多挑战。 一个是数据规范化：因此，您必须弄清楚这些不同数据源中的所有字段实际上是如何相互关联的？ 如何使用相同的术语实际确保一个数据源中的列与另一个数据源中的列具有可比性，并且具有相同比例的相同数据集？ 我如何处理丢失的数据？ 我如何处理损坏的数据或来自异常值或来自机器人之类的数据？ 这些都是非常大的挑战。 维护这些数据馈送也是一个非常大的问题。

当您将所有这些信息导入到数据仓库中时，可能会出现很多问题，特别是当您需要进行非常大的转换时，需要将从 Web 日志保存的原始数据转换到可以导入到数据仓库中的实际结构化数据库表中。 当您处理单一的数据仓库时，伸缩也会变得棘手。 最终，您的数据将变得如此庞大，以至于这些转换本身开始成为问题。 这开始涉及到英语教学与对外汉语教学之间的关系。

# ETL 与 ELT

我们先来谈谈 ETL。 那代表什么？ 它代表提取、转换和加载--这是进行数据仓库的一种传统方式。

基本上，首先从您想要的操作系统中提取您想要的数据。 例如，我可能每天从我们的 Web 服务器中提取所有的 Web 日志。 然后，我需要将所有这些信息转换成一个实际的结构化数据库表，我可以将其导入到我的数据仓库中。

这个转换阶段可能会遍历这些 Web 服务器日志的每一行，将其转换为实际的表，在那里我从每个 Web 日志行中提取诸如会话 ID、他们查看的页面、时间、推荐人是什么之类的信息，我可以将其组织成表格结构，然后我可以将其加载到数据仓库本身中，作为数据库中的实际表格。 因此，随着数据变得越来越大，转换步骤可能会成为一个真正的问题。 想想看，要浏览 Google、Amazon 或任何大型网站上的所有 Web 日志，并将其转换为数据库可以摄取的内容，需要进行多少处理工作。 这本身就是一个可伸缩性挑战，可能会给整个数据仓库管道带来稳定性问题。

这就是英语教学概念的用武之地，它几乎颠覆了一切。 它说，“如果我们不使用大型 Oracle 实例怎么办？如果我们使用一些新的技术，让我们在 Hadoop 群集上拥有更分布式的数据库，让我们利用这些分布式数据库(如配置单元、Spark 或 MapReduce)的能力，并在加载后实际执行转换，会怎么样？”

这里的想法是我们将像以前一样提取我们想要的信息，比如从一组 Web 服务器日志中提取。 然后，我们将直接将其加载到我们的数据存储库中，并且我们将使用存储库本身的功能来进行适当的转换。 因此，这里的想法是，不是进行离线过程来将我的网络日志转换为结构化格式，例如，我只需要将它们作为原始文本文件输入，然后使用 Hadoop 之类的工具逐行查看它们，从而真正将这些转换为更结构化的格式，然后我可以在整个数据仓库解决方案中进行查询。

像蜂窝这样的东西可以让你在 Hadoop 集群上托管一个庞大的数据库。 像 Spark SQL 这样的东西还可以让您以一种非常类似 SQL 的数据仓库方式对实际分布在 Hadoop 集群上的数据仓库进行查询。 还有一些分布式 NoSQL 数据存储，可以使用 Spark 和 MapReduce 进行查询。 我们的想法是，不是使用单一的数据库作为数据仓库，而是使用构建在 Hadoop 之上的东西，或者某种类型的集群，这些东西实际上不仅可以扩展数据的处理和查询，而且还可以扩展数据的转换。

同样，您首先提取原始数据，然后我们将按原样将其加载到数据仓库系统本身。 然后，使用数据仓库的强大功能(可能构建在 Hadoop 上)作为第三步进行转换。 那我就可以一起查询了。 所以，这是一个非常大的项目，非常大的主题。 您知道，数据仓库本身就是一门完整的学科。 我们很快就会在这本书中更多地讨论 Spark，这是处理这件事的一种方式-特别是有一种叫做 Spark SQL 的东西是相关的。

这里的总体概念是，如果您从构建在 Oracle 或 MySQL 上的单一数据库迁移到构建在 Hadoop 之上的这些更现代的分布式数据库之一，则可以在加载原始数据之后进行转换，而不是在加载之前。 这最终可以变得更简单、更具伸缩性，并利用当今可用的大型计算集群的强大功能。

这是 ETL 与 ELT 的对比，在基于云的计算中，这是一种传统的方式，到处都是集群，而今天的方式是有意义的，因为我们确实有大量的云计算可供我们用来转换大型数据集。 这就是我们的概念。

ETL 是一种老式的方法，在导入数据并将其加载到一个巨大的数据仓库、单片数据库之前，您可以离线转换大量数据。 但是，使用今天的技术，使用基于云的数据库、Hadoop、Have、Spark 和 MapReduce，您实际上可以更高效地执行此操作，并在将原始数据加载到数据仓库后利用群集的力量实际执行转换步骤。

这真的改变了这个领域，你知道这一点很重要。 再说一次，在这个主题上还有很多需要学习的地方，所以我鼓励您在这个主题上探索更多。 但是，这是基本的概念，现在你知道当人们谈论 ETL 和 ELT 时他们在谈论什么了。

# 强化学习

我们的下一个话题是一个有趣的话题：强化学习。 我们实际上可以把这个想法用在吃豆人的例子上。 我们实际上可以创建一个小的智能吃豆人代理，它可以独立很好地玩吃豆人游戏。 你会惊讶于这个智能吃豆人背后的聪明才智是如此简单。 我们来看看吧！

强化学习背后的想法是你有某种智能体，在这个例子中是吃豆人，它探索某种空间，在我们的例子中，这个空间将是吃豆人所处的迷宫。 在此过程中，它学习不同状态在不同条件下的值变化。

![](img/d79e079f-b596-42a2-ad11-58c39caf615f.png)

例如，在上图中，吃豆人的状态可能是由这样一个事实来定义的，即它的南面有一个幽灵，西面有一堵墙，北面和东面都有空地，这可能定义了吃豆人的当前状态。 它可以采取的状态变化是朝着给定的方向移动。 然后我就可以学到朝某个方向走的价值。 因此，例如，如果我搬到北方，什么都不会发生，没有与之相关的真正的回报。 但是，如果我搬到南方去，我会被鬼魂摧毁，那将是一个负值。

当我去探索整个空间时，我可以建立一套吃豆人可能处于的所有状态，以及在每个状态下向给定方向移动的相关价值，这就是强化学习。 当它探索整个空间时，它会为给定的州细化这些奖励值，然后它可以使用这些存储的奖励值来选择在给定的一组当前条件下做出的最佳决策。 除了吃豆人，还有一款叫做猫和老鼠的游戏，这是一个常用的例子，我们稍后将会看到。

此技术的好处是，一旦您研究了代理可能所处的整个可能状态集，当您运行不同的迭代时，您可以很快获得非常好的性能。 所以，你知道，你基本上可以做一个智能吃豆人，通过运行强化学习，让它探索它在不同状态下做出的不同决定的价值，然后存储这些信息，以便非常迅速地做出正确的决定，给定它在未知条件下看到的未来状态。

# Q-学习

因此，强化学习的一个非常具体的实现称为 Q-学习，这将我们刚才更多地谈到的内容形式化了：

*   同样，您可以从代理的一组环境状态开始(我旁边有幽灵吗？ 我面前有药丸吗？ 诸如此类的事情)，我们称其为 s。
*   在这些状态下，我有一组可能的动作，我们将这组动作称为 a。在吃豆人的例子中，这些可能的动作是上移、下移、左移或右移。
*   然后每个状态/动作对都有一个值，我们称之为 Q；这就是我们称之为 Q-Learning 的原因。 因此，对于每个状态，即围绕 Pac-Man 的给定一组条件，给定的操作将具有值*Q*。 例如，向上移动可能会有一个给定值 q，向下移动可能会有负的*q*值，如果这意味着遇到鬼魂。

因此，我们从*q*值开始，对于 Pac-Man 可能处于的每个可能状态，值都是 0。 而且，当吃豆人探索迷宫时，当糟糕的事情发生在吃豆人身上时，我们降低了吃豆人当时所处状态的*Q*值。 因此，如果吃豆人最终被鬼魂吃掉，我们会惩罚他在当前状态下所做的一切。 当好事情发生在吃豆人身上时，当他吃异能药或吃鬼魂时，我们将增加他所处状态的动作的*Q*值。 然后，我们能做的就是使用那些*Q*值来指导吃豆人未来的选择，并在某种程度上建立一个能够最佳执行的小智能代理，从而成为一个完美的小吃豆人。 从我们刚才看到的吃豆人的图像中，我们可以进一步定义吃豆人的现状，定义他在西面有一堵墙，在北面和东面有一片空地，在南面有一个幽灵。

我们可以看看他可以采取的行动：他实际上根本不能向左移动，但他可以向上、向下或向右移动，我们可以为所有这些动作赋值。 通过向上或向右，什么都不会发生，没有能量药丸或点状物质可供消费。 但如果他往左走，那肯定是负值。 我们可以说，对于 Pac-Man 所处的当前状态，向下移动将是一个非常糟糕的选择；应该有一个负的*Q*值。 左转根本做不到。 向上或向右移动或保持中立，对于该给定状态的那些操作选择，*Q*值将保持为 0。

现在，你也可以往前看一点，来制造一个更智能的代理。 所以，实际上我离拿到强力药丸只有两步之遥。 所以，当吃豆人要探索这个状态时，如果我在下一个状态下吃了那个药丸，我实际上可以把它计入前一个状态的*Q*值中。 如果你只是有一些折扣率，基于你在时间上的距离，你有几步之遥，你可以把这些都考虑进去。 因此，这是一种在系统中实际构建少量内存的方式。 在计算 Q(这里*s*是以前的状态，*s*是当前状态)时，您可以通过使用折扣率“向前看”不止一步：

*Q(s,a) += discount * (reward(s,a) + max(Q(s')) - Q(s,a))*

所以，当我吃那个强效药丸时，我体验到的*Q*值实际上可能会提升我在此过程中遇到的之前的*Q*值。 所以，这是一种让 Q-Learning 做得更好的方法。

# 探险问题

我们在强化学习中遇到的一个问题是探索问题。 如何确保在探索阶段有效地覆盖这些州内的所有不同状态和操作？

# 简单的方法

一种简单的方法是始终选择我到目前为止计算出的*Q*值最高的给定状态的操作，如果存在平局，只需随机选择。 所以，最初我的所有*q*值可能都是 0，我将首先随机选择动作。

当我开始获得有关给定操作和给定状态的更好的*Q*值的信息时，我将开始使用这些值。 但是，这最终是非常低效的，如果我只是把自己绑在这个总是选择我到目前为止计算出的最佳*q*值的僵化算法中，那么我实际上可能会错过很多路径。

# 更好的方式

所以，一个更好的方法是在我探索的过程中，在我的行为中引入一点随机变化。 所以，我们称它为ε项。 假设我们有一些值，我掷骰子，我有一个随机数。 如果它最终小于这个 epsilon 值，我实际上不会遵循最高的*Q*值；我不会做有意义的事情，我只是随机选择一条路径来尝试，看看会发生什么。 这实际上让我探索了更广泛的可能性，更广泛的行动，在探索阶段更有效地为更广泛的州探索。

所以，我们刚才所做的可以用非常奇特的数学术语来描述，但从概念上来说，它是相当简单的。

# 花言巧语

我探索了一些我可以对给定的一组状态采取的行动，我用它来通知与给定的一组状态的给定行动相关的奖励，在探索完成之后，我可以使用这些信息，比如那些*Q*值，来智能地导航到一个全新的迷宫中。

这也可以称为马尔可夫决策过程。 所以再说一次，很多数据科学只是给简单的概念起花哨的、吓人的名字，强化学习中有很多这样的名字。

# 马尔可夫决策过程

因此，如果你查阅马尔可夫决策过程的定义，就会发现它是“一个数学框架，用于在结果部分随机、部分受决策者控制的情况下对决策进行建模”。

*   **决策**：给定给定状态的一组可能性，我们应该采取什么行动？
*   **在结果部分是随机的情况下**：嗯，有点像我们在那里的随机探索。
*   **部分受决策者控制**：决策者是我们计算的*q*值。

所以，MDP，马尔可夫决策过程，是描述我们刚才描述的强化学习探索算法的一种奇妙方式。 记号甚至是相似的，状态仍然被描述为 s，s‘是我们遇到的下一个状态。 对于给定的 s 和 s‘状态，我们有定义为*P<sub class="calibre50">a</sub>*的状态转移函数。 对于给定的 s 和 s‘，我们有我们的*Q*值(基本上表示为奖励函数)，一个*R<sub class="calibre50">a</sub>*值。 因此，从一个状态转移到另一个状态具有与之相关的给定奖励，并且从一个状态转移到另一个状态由状态转移函数定义：

*   状态仍然被描述为*s*和*s‘’*
*   状态转移函数被描述为*Pa(s，s‘)*
*   我们的*q*值被描述为奖励函数*Ra(s，s‘)*

所以，再次描述我们刚刚做的事情，只有一个数学符号，一个听起来更华丽的词，马尔可夫决策过程。 而且，如果你想听起来更聪明，你也可以用另一个名字来称呼马尔可夫决策过程：离散时间随机控制过程。 听起来很聪明！ 但是这个概念本身和我们刚才描述的是一样的。

# 动态规划

所以，更花哨的说法是：动态编程也可以用来描述我们刚才所做的事情。 哇!。 这听起来像人工智能，计算机自己编程，终结者 2，天网之类的东西。 但不是的，这就是我们刚刚做的。 如果您查看动态编程的定义，就会发现它是一种解决复杂问题的方法，方法是将其分解为一组更简单的子问题，每个子问题只解决一次，并使用基于内存的数据结构存储它们的解决方案。

下次出现相同的子问题时，无需重新计算其解，只需查找先前计算的解，从而节省计算时间，但代价是(希望)适度的存储空间开销：

*   **解决复杂问题的方法**：与创建智能吃豆人一样，这是一个相当复杂的最终结果。
*   **通过将其分解为一组更简单的子问题**：那么，例如，对于 Pac-Man 可能处于的给定状态，采取的最佳操作是什么。 吃豆人有很多不同的状态，但每一种状态都代表着一个更简单的子问题，在那里我可以做出的选择是有限的，而最好的选择只有一个正确的答案。
*   **存储它们的解决方案**：这些解决方案是 i 在每个状态下与每个可能的动作相关联的*q*值。
*   **理想情况下，使用基于内存的数据结构**：当然，我需要存储那些*q*值，并以某种方式将它们与状态相关联，对吗？
*   **下一次出现相同的子问题时**：下一次 Pac-Man 处于给定状态时，我有一组*q*值。
*   **不需要重新计算其解，只需查看之前计算的解**：我在探索阶段已有的*q*值。
*   **因此，以适度的存储空间开销为代价来节省计算时间**：这正是我们刚刚使用强化学习所做的事情。

我们有一个复杂的探索阶段，找到与给定状态的每个操作相关联的最佳回报。 一旦我们有了对给定状态采取正确行动的表格，我们就可以非常迅速地利用它来让我们的吃豆人在一个全新的迷宫中以最优的方式移动，这是他以前从未见过的。 因此，强化学习也是动态规划的一种形式。 哇!

简单地说，您可以通过让智能 Pac-Man 代理在给定不同条件的情况下半随机地探索不同的移动选择，其中这些选择是动作，而那些条件是状态，就可以创建一个智能的 Pac-Man 代理。 我们会跟踪与每种行为或状态相关的奖励或惩罚，如果你想让它变得更好，我们实际上可以打折，退回多个步骤。

然后我们存储最终与每个州相关联的那些*q*值，我们可以用它来通知它将来的选择。 因此，我们可以进入一个全新的迷宫，拥有一个真正聪明的吃豆人，它可以避开鬼魂，并相当有效地吃掉它们，一切都是自己完成的。 这是一个非常简单的概念，但非常强大。 你也可以说你理解了一大堆花哨的术语，因为它们都叫同一个东西。 Q-学习、强化学习、马尔可夫决策过程、动态规划：所有这些都与同一个概念联系在一起。

我不知道，我认为你可以通过这么简单的技术制造出一种人工智能的吃豆人，这真的很酷，而且真的很管用！ 如果您想更详细地了解它，下面是几个示例，您可以查看其中的一个实际源代码，**Python 马尔可夫决策过程工具箱**：[http://pymdptoolbox.readthedocs.org/en/latest/api/mdp.html](http://pymdptoolbox.readthedocs.org/en/latest/api/mdp.html)。

有一个 Python 马尔可夫决策过程工具箱，它用我们所讨论的所有术语来包装它。 这里有一个你可以看到的例子，一个猫捉老鼠的游戏的工作例子，它是相似的。 事实上，你也可以在网上看到一个吃豆人的例子，它与我们刚才讨论的内容更直接地联系在一起。 请随意浏览这些链接，并了解更多信息。

这就是强化学习。 更广泛地说，这是一项有用的技术，用于构建能够在可能的不同状态集之间导航的代理，这些状态集具有一组可以与每个状态相关联的操作。 所以，我们主要是在迷宫游戏的背景下讨论它。 但是，你可以更广泛地思考，当你遇到需要预测某事物行为的情况时，你就会知道，给定了一组当前条件和它可以采取的一系列行动。 强化学习和 Q 学习可能是这样做的一种方式。 所以，请记住这一点！

# 简略的 / 概括的 / 简易判罪的 / 简易的

在本章中，我们看到了一种最简单的机器学习技术，称为 k 近邻。 我们还看了一个 KNN 的例子，它预测了一部电影的评分。 我们分析了降维和主成分分析的概念，并给出了一个主成分分析的例子，它将四维数据降维为二维，同时保持其方差不变。

接下来，我们学习了数据仓库的概念，并了解了现在如何使用 ELT 流程而不是 ETL 更有意义。 我们介绍了强化学习的概念，并了解了它是如何在吃豆人游戏背后使用的。 最后，我们看到了一些用于强化学习(Q-学习、马尔可夫决策过程和动态学习)的花哨词汇。 在下一章中，我们将了解如何处理真实世界的数据。