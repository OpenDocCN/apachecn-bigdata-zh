# 五、使用 Python 的机器学习

在本章中，我们将介绍机器学习以及如何在 Python 中实际实现机器学习模型。

我们将研究监督学习和非监督学习的含义，以及它们之间有何不同。 我们将了解防止过度匹配的技术，然后查看一个有趣的示例，其中我们实现了垃圾邮件分类器。 我们将分析 K-Means 聚类还有很长的路要走，我们将用一个实际的例子来分析 K-Means 聚类，该示例使用 SCRICKIT-LEARN 根据人们的收入和年龄对其进行聚类！

我们还将介绍一个非常有趣的机器学习应用程序，称为**决策树**，我们还将用 Python 构建一个预测公司 Shiring 决策的工作示例。 最后，我们将介绍集成学习和支持向量机的有趣概念，它们是我最喜欢的机器学习领域！

更具体地说，我们将介绍以下主题：

*   有监督和无监督学习
*   使用列车/测试避免过度安装
*   贝叶斯方法
*   一种基于贝叶斯的垃圾邮件分类器的实现
*   K-均值聚类的概念
*   Python 中的集群示例
*   熵及其度量方法
*   决策树的概念及其在 Python 中的应用实例
*   什么是整体学习？
*   **支持向量机**(**SVM**)及其使用 SCRKIT-LEARN 的实例

# 机器学习和培训/测试

那么，什么是机器学习呢？ 嗯，如果你在维基百科或其他什么地方查一下，它会说是算法可以从观测数据中学习，并根据这些数据做出预测。 听起来很花哨，对吧？ 就像人工智能的东西，就像你的电脑里有一个跳动的大脑。 但在现实中，这些技术通常非常简单。

我们已经看过回归了，我们取了一组观测数据，我们给它拟合了一条直线，然后我们用这条直线来做预测。 所以根据我们的新定义，这就是机器学习！ 你的大脑也是这样工作的。

机器学习中的另一个基本概念是**Train**/**test**，它让我们非常聪明地评估我们建立的机器学习模型有多好。 当我们现在来看无监督学习和有监督学习时，您将会明白为什么训练/测试对机器学习如此重要。

# 无监督学习

现在让我们详细讨论两种不同类型的机器学习：监督学习和非监督学习。 有时两者之间可能有一条模糊的界限，但无监督学习的基本定义是，你没有给你的模型任何可供学习的答案。 你只是用一组数据来呈现它，机器学习算法试图在没有额外信息的情况下从这些数据中找出意义来：

![](img/54c8b166-bc40-4c03-8a14-a47bd74e93dc.jpg)

比方说，我给它一堆不同的物体，比如这些球、方块、骰子等等。 然后，让我们假设有一些算法，可以根据某些相似性度量将这些对象聚集到彼此相似的对象中。

现在我还没有提前告诉机器学习算法，某些对象属于什么类别。 我没有一个小抄，它可以从我在哪里有一组现有的对象和我对它的正确分类中了解到这一点。 机器学习算法必须自己推断这些类别。 这是一个无监督学习的例子，我没有一套让它学习的答案。 我只是试图让算法根据单独提供给它的数据来收集它自己的答案。

这样做的问题是，我们不一定知道算法会得出什么结果！ 如果我给它上一张图中显示的那一堆物体，它会不会把东西分成圆形的，大的和小的，红色和蓝色的，我不知道。 这主要取决于我给出的衡量项目之间相似度的标准。 但有时你会发现令人惊讶的星团，并出现你意想不到的景象。

这就是无监督学习的真正意义所在：如果你不知道你要找的是什么，它可以是一个强大的工具，用来发现你甚至不知道那里有的分类。 我们称它为**潜变量**。 你的数据中一些你原本不知道的属性，可以通过无监督的学习来梳理出来。

让我们再举一个关于无监督学习的例子。 比方说我把人聚集在一起，而不是球和骰子。 我正在写一个交友网站，我想看看什么样的人倾向于聚集在一起。 例如，有一些属性是人们倾向于聚集在一起的，它们决定了他们是否倾向于彼此喜欢对方和约会。 现在你可能会发现，出现的星团并不符合你预想的刻板印象。 也许这不是关于大学生和中年人，或者离婚的人和诸如此类的人，或者他们的宗教信仰的问题。 也许，如果你看看从分析中实际出现的集群，你会对你的用户了解一些新的东西，并真正发现有一些比你的用户现有的任何功能更重要的东西，来决定他们是否喜欢对方。 这是一个监督学习的例子，提供了有用的结果。

另一个例子可以是基于其属性对电影进行群集。 如果您要对 IMDB 或其他内容中的一组电影运行集群，结果可能会让您大吃一惊。 也许这不仅仅是关于电影的类型。 也许还有其他更重要的因素，比如电影的年龄、放映时长或在哪个国家上映。 只是你永远不会知道。 或者，我们可以分析产品描述的文本，试图找到对某一类别最有意义的术语。 同样，我们可能不一定提前知道哪些术语或词语最能表明某一产品属于某一类别；但通过无监督的学习，我们可以梳理出潜在的信息。

# 有监督的学习

现在与之形成对比的是，监督学习是一种我们有一套模型可以学习的答案的情况。 我们给它一组训练数据，模型从中学习。 然后，它可以推断出我们想要的特征和类别之间的关系，并将其应用于看不见的新值--并预测有关它们的信息。

回到我们之前的例子，我们试图根据汽车的属性来预测汽车价格。 这就是我们使用实际答案训练模型的一个例子。 我有一套已知的汽车及其实际售价。 我根据这套完整的答案训练模型，然后我可以创建一个模型，我可以用它来预测我以前从未见过的新车的价格。 这是监督学习的一个例子，你给了它一套可供学习的答案。 您已经为一组数据分配了类别或一些组织标准，然后您的算法使用该标准来构建一个模型，它可以根据该模型预测新值。

# 评估有监督的学习

那么，你如何评估有监督的学习呢？ 好的，有监督学习的好处是我们可以使用一种叫做训练/测试的技巧。 这里的想法是将我们的观测数据分成两组，一组是训练集，一组是测试集，我希望我的模型可以从中学习到这些数据。 因此，当我基于我拥有的数据训练/构建我的模型时，我只使用我称为训练集的部分数据来训练/构建我的模型，我保留了另一部分数据用于测试目的。

我可以使用训练数据的一个子集来构建我的模型，然后我就可以评估由此产生的模型，看看它是否能成功地预测我的测试数据的正确答案。

你看到我在那里做了什么吗？ 我有一组数据，我已经有了可以用来训练我的模型的答案，但我将保留其中的一部分数据，并实际使用这些数据来测试使用训练集生成的模型！ 这给了我一种非常具体的方式来测试我的模型在看不见的数据上有多好，因为我实际上有一些数据可以用来测试它。

然后，你可以使用 r 平方或其他一些度量，比如均方根误差，来定量衡量它做得有多好。 您可以使用它来测试一个模型与另一个模型，并查看针对给定问题的最佳模型是什么。 您可以调整该模型的参数，并使用训练/测试来最大限度地提高该模型在测试数据上的准确性。 所以这是一个防止过度贴身的好方法。

对有监督的学习有一些警告。 需要确保您的训练和测试数据集都足够大，能够真正代表您的数据。 您还需要确保在培训和测试中捕捉到您关心的所有不同类别和离群值，以便很好地衡量它的成功程度，并构建一个好的模型。

你必须确保你从这些数据集中随机选择，而不是把你的数据集一分为二，然后说这里剩下的一切都是训练，而这里就是测试。 你想随机抽样，因为你的数据中可能有一些你不知道的顺序模式。

现在，如果你的模型过度拟合，并且正在不遗余力地接受训练数据中的异常值，那么当你把它与未设定的测试数据场景进行对比时，这一点就会暴露出来。 这是因为所有这些异常值的旋转对它以前从未见过的异常值没有帮助。

让我们在这里明确一点，培训/测试并不完美，因此可能会得到误导性的结果。 也许你的样本规模太小，就像我们已经讨论过的那样，或者可能只是因为随机的机会，你的训练数据和你的测试数据看起来非常相似，它们实际上确实有一组相似的异常值-而你仍然可能是过度拟合。 正如您在下面的示例中看到的，这确实可能发生：

![](img/dfd6dacd-41fc-415b-ab5b-dda9abf05c09.jpg)

# K 重交叉验证

现在有一种绕过这个问题的方法，称为 k-折叠交叉验证，我们将在本书后面介绍一个例子，但基本概念是您要多次训练/测试。 所以你实际上不是把你的数据分成一个训练集和一个测试集，而是分成多个随机分配的片段，k 个片段。 这就是 k 的来源。 您保留其中一个片段作为测试数据，然后开始在其余片段上训练您的模型，并根据您的测试数据集测量它们的性能。 然后，从每个训练集的模型结果中提取平均性能，并取其 r 平方平均得分。

因此，通过这种方式，你实际上是在不同的数据切片上进行训练，根据相同的测试集来衡量它们，如果你有一个模型对你的训练数据的特定部分过度拟合，那么它将被其他有助于 k 重交叉验证的模型平均出来。

以下是 K 折交叉验证步骤：

1.  将您的数据拆分成 K 个随机分配的数据段
2.  保留一个数据段作为测试数据
3.  对剩余的 K-1 段进行训练，并根据测试集测量它们的性能
4.  取 K-1 r 平方分数的平均值

这将在本书后面的内容中更有意义，现在我只想让您知道，这个工具的存在实际上是为了使训练/测试比现在更健壮。 因此，接下来让我们实际操作一些数据，并使用 Train/TestNext 对其进行实际评估。

# 使用训练/测试防止多项式回归的过拟合

让我们把训练/测试付诸行动吧。 所以你可能还记得，回归可以被认为是有监督的机器学习的一种形式。 让我们采用前面介绍的多项式回归，并使用训练/测试来尝试找到合适的次数多项式来拟合给定的数据集。

就像我们之前的例子一样，我们将建立一个随机生成的页面速度和购买金额的假数据集，我将在它们之间创建一个奇怪的小关系，本质上是指数关系。

```py
%matplotlib inline 
import numpy as np 
from pylab import * 

np.random.seed(2) 

pageSpeeds = np.random.normal(3.0, 1.0, 100) 
purchaseAmount = np.random.normal(50.0, 30.0, 100) / pageSpeeds 

scatter(pageSpeeds, purchaseAmount) 

```

让我们继续生成该数据。 我们将使用如下截图所示的关系，对页面速度和购买金额使用随机数据的正态分布：

![](img/a215de99-d26a-4838-9482-274b254b3d20.jpg)

接下来，我们将拆分这些数据。 我们将获取 80%的数据，并将其保留用于我们的培训数据。 因此，这些点中只有 80%将用于训练模型，然后我们将保留另外 20%用于针对未知数据测试该模型。

在这里，我们将使用 Python 的语法来拆分列表。 前 80 分将进入训练集，后 20 分，所有超过 80 分的分数将进入测试集。 您可能还记得我们前面的 Python 基础章节中的这一点，我们在那一章中介绍了实现这一点的语法，我们将在这里对购买金额做同样的事情：

```py
trainX = pageSpeeds[:80] 
testX = pageSpeeds[80:] 

trainY = purchaseAmount[:80] 
testY = purchaseAmount[80:] 

```

在前面的部分中，我已经说过，您不应该像这样将数据集一分为二，而应该随机抽样进行培训和测试。 不过，在这种情况下，它是可行的，因为我的原始数据无论如何都是随机生成的，所以真的没有押韵或理由来判断事情发生在哪里。 但在实际数据中，您需要在拆分数据之前对其进行混洗。

现在，我们将介绍一种方便的方法，您可以使用该方法来混洗数据。 此外，如果您使用的是 PANAAS 包，其中有一些方便的函数可以自动为您创建训练和测试数据集。 但我们将在这里使用 Python 列表来完成此操作。 因此，让我们可视化我们最终得到的训练数据集。 我们会做一个培训页面速度和购买金额的散点图。

```py
scatter(trainX, trainY) 

```

下面是您的输出现在应该是什么样子：

![](img/d0fc1eec-b655-424f-956f-798327e5a2f8.jpg)

基本上，已经绘制了从原始完整数据集中随机选择的 80 个点。 它的形状基本上是一样的，所以这是件好事。 它代表了我们的数据。 这很重要！

现在，让我们绘制作为测试数据保留的剩余 20 个点。

```py
scatter(testX, testY) 

```

![](img/534acada-d2fb-4cc7-b4b7-82c40fb4fd64.jpg)

在这里，我们看到剩下的 20 个用于测试的数据也具有与原始数据相同的一般形状。 所以我认为这也是一个有代表性的测试集。 当然，它比你希望在现实世界中看到的要小一点。 例如，如果你有 1000 分而不是 100 分可供选择，保留 200 分而不是 20 分，你可能会得到更好的结果。

现在我们要试着用 8 次多项式来拟合这个数据，我们会随机选取数字`8`，因为我知道这是一个非常高的阶数，可能过拟合了。

让我们继续使用`np.poly1d(np.polyfit(x, y, 8))`拟合我们的 8 次多项式，其中*x*只是训练数据的数组，*y*只是训练数据的数组。 我们只用我们为训练预留的 80 分就找到了我们的模型。 现在我们有了这个`p4`函数，它的结果可以用来预测新值：

```py
x = np.array(trainX) 
y = np.array(trainY) 

p4 = np.poly1d(np.polyfit(x, y, 8)) 

```

现在，我们将根据训练数据绘制出多项式。 我们可以将原始数据分散到训练数据集，然后根据这些数据绘制预测值：

```py
import matplotlib.pyplot as plt 

xp = np.linspace(0, 7, 100) 
axes = plt.axes() 
axes.set_xlim([0,7]) 
axes.set_ylim([0, 200]) 
plt.scatter(x, y) 
plt.plot(xp, p4(xp), c='r') 
plt.show() 

```

您可以在下图中看到，它看起来非常适合，但您知道它显然是在做一些过度拟合：

![](img/16b521a0-d571-4cd7-a5a8-e5557dcb7c1f.jpg)

右手边的这是什么疯狂的东西？ 我非常肯定我们的真实数据，如果我们有它的话，不会像这个函数所暗示的那样高得离谱。 这是一个很好的数据过度拟合的例子。 它很好地符合你给出的数据，但在预测超出右图疯狂偏高的点的新值时，它会做得很糟糕。 所以让我们试着把它梳理出来。 让我们给它提供我们的测试数据集：

```py
testx = np.array(testX) 
testy = np.array(testY) 

axes = plt.axes() 
axes.set_xlim([0,7]) 
axes.set_ylim([0, 200]) 
plt.scatter(testx, testy) 
plt.plot(xp, p4(xp), c='r') 
plt.show() 

```

事实上，如果我们将我们的测试数据与该函数进行对比，那么实际上看起来并没有那么糟糕。

![](img/8b6b4a61-0d3b-45a7-b7fd-4bfb2d6ad3fa.jpg)

我们很幸运，我们的测试一开始都不在这里，但你可以看到它是一个合理的匹配，但远不是完美的。 事实上，如果你真的测量 r 平方分数，它会比你想象的更糟糕。 我们可以使用`sklearn.metrics`中的`r2_score()`函数进行测量。 我们只需给它我们的原始数据和预测值，它就会检查并测量预测的所有方差，并为您将它们全部平方：

```py
from sklearn.metrics import r2_score  
r2 = r2_score(testy, p4(testx))  
print r2 

```

我们最终得到的 r 平方分数只有`0.3`。 所以那并不是那么热！ 您可以看到，它更符合训练数据：

```py
from sklearn.metrics import r2_score  
r2 = r2_score(np.array(trainY), p4(np.array(trainX))) 
print r2 

```

R 平方值结果是`0.6`，这并不令人惊讶，因为我们根据训练数据对其进行了训练。 测试数据在某种程度上是未知的，它的测试，坦率地说，它确实没有通过测试。 30%，那是个 F！

这是一个例子，我们使用训练/测试来评估有监督的学习算法，就像我之前说的，熊猫有一些方法可以让这件事变得更容易。 我们将在稍后讨论这一点，我们还将在本书后面介绍更多的训练/测试示例，包括 k-折交叉验证。

# 状况 / 活动 / 活性 / 行为

你大概可以猜到你的家庭作业是什么。 所以我们知道 8 次多项式不是很有用。 你能做得更好吗？ 所以我希望你们回到我们的例子，用不同的值来表示你们要用来拟合的次数多项式。 将 8 更改为不同的值，看看是否可以使用训练/测试作为度量来计算出多项式实际得分最高。 你的测试数据从哪里得到最好的 r 平方分数？ 什么学位适合这里？ 去玩那个吧。 这应该是一个相当简单的练习，对你也是一个非常有启发性的练习。

这就是训练/测试，这是一项非常重要的技术，你要一次又一次地使用它，以确保你的结果与你拥有的模型很好地吻合，并且你的结果是对看不见的价值的很好的预测。 这是一个很好的方法，可以防止你在建模时穿得太合身。

# 贝叶斯方法-概念

你有没有想过你电子邮件中的垃圾邮件分类器是如何工作的？ 它怎么知道一封电子邮件可能是垃圾邮件呢？ 一种流行的技术叫做朴素贝叶斯，这就是贝叶斯方法的一个例子。 让我们更多地了解它是如何工作的。 让我们来讨论一下贝叶斯方法。

在这本书的前面，我们确实讨论了贝叶斯定理，当时我们谈到了像药检这样的事情在结果中会产生很大的误导性。 但实际上，您可以将相同的贝叶斯定理应用于更大的问题，比如垃圾邮件分类器。 所以让我们深入研究一下它是如何工作的，它被称为贝叶斯方法。

我们再来回顾一下贝叶斯定理--记住，给定 B 的 A 的概率等于 A 的总体概率乘以给定 A 的 B 的概率除以 B 的总体概率：

![](img/95ca9a4e-01b9-4dbe-a066-5ff2db89e08d.jpg)

我们如何才能将其用于机器学习呢？ 我实际上可以为此构建一个垃圾邮件分类器：一种算法，可以分析一组已知的垃圾电子邮件和一组已知的非垃圾电子邮件，并训练一个模型来实际预测新的电子邮件是否为垃圾邮件。 这是在现实世界中实际的垃圾邮件分类器中使用的真实技术。

例如，假设一封电子邮件包含单词“free”，让我们计算一下它成为垃圾邮件的概率。 如果人们承诺给你免费的东西，那很可能是垃圾邮件！ 所以让我们来解决这个问题。 假设一封电子邮件中包含“免费”一词，则该电子邮件成为垃圾邮件的概率等于该邮件成为垃圾邮件的总体概率乘以该邮件包含“免费”一词的概率(假设该邮件是垃圾邮件)除以该邮件的总体免费概率：

![](img/6992cb5a-12dd-4397-b787-571b1d13777c.jpg)

分子可以仅仅被认为是消息为`Spam`并且包含单词`Free`的概率。 但这与我们正在寻找的略有不同，因为这是整个数据集的赔率，而不仅仅是包含单词`Free`的事物的赔率。 分母就是包含单词`Free`的总体概率。 有时，从您拥有的数据中无法立即访问这些信息。 如果不是，如果需要派生，可以将其展开为以下表达式：

![](img/ca351b51-13d1-432c-814b-43bcf6a55db0.jpg)

这会给出包含单词“free”的电子邮件中垃圾邮件的百分比，当您试图确定它是否是垃圾邮件时，这将是一件很有用的事情。

然而，英语中的所有其他单词又是怎么回事呢？ 因此，我们的垃圾邮件分类器应该知道的不仅仅是“免费”这个词。 理想情况下，它应该自动提取邮件中的每个单词，并计算出这在多大程度上增加了特定电子邮件成为垃圾邮件的可能性。 所以我们能做的就是训练我们的模型学习我们在训练过程中遇到的每一个单词，去掉像“a”、“the”和“and”这样的无意义的单词。 然后，当我们检查一封新电子邮件中的所有单词时，我们可以将每个单词被垃圾邮件的概率相乘，我们就得到了该电子邮件是垃圾邮件的总体概率。

现在它被称为朴素贝叶斯是有原因的。 这是天真的，因为我们假设单词之间没有关系。 我们只是孤立地查看邮件中的每个单词，并基本上结合每个单词对其的贡献是否为垃圾邮件的所有概率。 我们不是在看单词之间的关系。 因此，一个更好的垃圾邮件分类器会做到这一点，但显然这要困难得多。

所以这听起来像是有很多工作要做。 但是总体想法并不那么难，而在 Python 中学习 SCRKIT 实际上让它变得相当容易。 它提供了一个名为**CountVectorizer**的功能，可以非常简单地将一封电子邮件拆分成其所有组成部分的单词，然后分别处理这些单词。 然后它有一个`MultinomialNB`函数，其中 NB 代表朴素贝叶斯，它将为我们完成朴素贝叶斯的所有繁重任务。

# 用朴素贝叶斯实现垃圾邮件分类器

让我们使用朴素贝叶斯编写一个垃圾邮件分类器。 你会惊讶于这是多么容易。 事实上，大多数工作最终只是读取我们将要训练的所有输入数据，并实际解析这些数据。 实际的垃圾邮件分类位，也就是机器学习位，本身只有几行代码。 所以通常就是这样工作的：读入、修改和清理数据通常是做数据科学时的大部分工作，所以要习惯这个想法！

```py
import os 
import io 
import numpy 
from pandas import DataFrame 
from sklearn.feature_extraction.text import CountVectorizer 
from sklearn.naive_bayes import MultinomialNB 

def readFiles(path): 
    for root, dirnames, filenames in os.walk(path): 
        for filename in filenames: 
            path = os.path.join(root, filename) 

            inBody = False 
            lines = [] 
            f = io.open(path, 'r', encoding='latin1') 
            for line in f: 
                if inBody: 
                    lines.append(line) 
                elif line == '\n': 
                    inBody = True 
            f.close() 
            message = '\n'.join(lines) 
            yield path, message 

def dataFrameFromDirectory(path, classification): 
    rows = [] 
    index = [] 
    for filename, message in readFiles(path): 
        rows.append({'message': message, 'class': classification}) 
        index.append(filename) 

    return DataFrame(rows, index=index) 

data = DataFrame({'message': [], 'class': []}) 

data = data.append(dataFrameFromDirectory(                   'e:/sundog-consult/Udemy/DataScience/emails/spam',                   'spam')) 
data = data.append(dataFrameFromDirectory(                   'e:/sundog-consult/Udemy/DataScience/emails/ham',                   'ham')) 

```

所以我们需要做的第一件事是以某种方式阅读所有这些电子邮件，然后我们将再次使用熊猫来让这件事变得更容易一些。 同样，熊猫是处理表格数据的有用工具。 我们导入了我们将在这里的示例中使用的所有不同的包，其中包括 os 库、io 库、numpy、pandas，以及 SCISKIT-LEARN 中的 CountVectorizer 和 MultinomialNB。

现在让我们详细了解一下这段代码。 现在我们可以跳过`readFiles()`和`dataFrameFromDirectory()`的函数定义，深入到我们的代码实际做的第一件事，那就是创建一个 Pandas DataFrame 对象。

我们将从一个字典构造它，该字典最初包含一个空类列表中的消息的小空列表。 所以这个语法是说，“我想要一个有两列的 DataFrame：一列包含消息，即每封电子邮件的实际文本；另一列包含每封电子邮件的类，也就是说，它是垃圾邮件还是火腿邮件”。 这就是说我想创建一个电子邮件的小数据库，这个数据库有两列：电子邮件的实际文本和它是否是垃圾邮件。

现在，我们需要使用 Python 语法在该数据库中放入一些内容，即放入该 DataFrame 中。 因此，我们调用两个方法`append()`和`dataFrameFromDirectory()`，将我的`spam`文件夹中的所有垃圾电子邮件和`ham`文件夹中的所有 ham 电子邮件实际投放到 DataFrame 中。

如果您正在尝试，请确保修改传递给`dataFrameFromDirectory()`函数的路径，使其与系统中安装图书资料的位置相匹配！ 再说一次，如果你使用的是 Mac 或 Linux，请注意反斜杠和正斜杠之类的东西。 在这种情况下，这无关紧要，但如果您不是在 Windows 上，就不会有驱动器号。 因此，只需确保这些路径实际上指向本例中的`spam`和`ham`文件夹所在的位置。

接下来，`dataFrameFromDirectory()`是我编写的一个函数，它基本上表示我有一个目录的路径，我知道它是分类的，垃圾邮件或火腿，然后它使用`readFiles()`函数，这个函数也是我编写的，它将遍历目录中的每个文件。 因此，`readFiles()`使用`os.walk()`函数查找目录中的所有文件。 然后，它为该目录中的每个单独文件构建完整的路径名，然后将其读入。 当它读入邮件时，它实际上会跳过每封电子邮件的标题，直接转到文本，并通过查找第一个空行来实现这一点。

它知道第一个空行之后的所有内容实际上都是邮件正文，而第一个空行前面的所有内容都只是一堆标题信息，我实际上并不想训练垃圾邮件分类器。 因此，它既返回了每个文件的完整路径，也返回了消息的正文。 这就是我们读入所有数据的方式，这是大部分代码！

因此，我最终得到的是一个 DataFrame 对象，基本上是一个包含两列的数据库，其中包含邮件正文，以及它是否是垃圾邮件。 我们可以继续运行它，并且可以从 DataFrame 使用`head`命令来实际预览它是什么样子：

```py
data.head() 

```

DataFrame 中的前几个条目如下所示：对于指向充满电子邮件的给定文件的每个路径，我们都有一个分类和邮件正文：

![](img/5eb289f1-dcfb-460a-984b-6b07d9c8dc56.jpg)

好了，现在有趣的是，我们将使用 SCRICKIT 中的`MultinomialNB()`函数-学习如何对我们已有的数据实际执行朴素贝叶斯(Naive Bayes)。

```py
vectorizer = CountVectorizer() 
counts = vectorizer.fit_transform(data['message'].values) 

classifier = MultinomialNB() 
targets = data['class'].values 
classifier.fit(counts, targets) 

```

下面是您的输出现在应该是什么样子：

![](img/389232ea-b526-4051-9297-89bc6f7f7d09.jpg)

一旦我们构建了`MultinomialNB`分类器，它需要两个输入。 它需要我们正在训练的实际数据(`counts`)，以及每件事的目标(`targets`)。 因此，`counts`基本上是每个电子邮件中的所有单词以及该单词出现的次数的列表。

所以这就是`CountVectorizer()`所做的：它从 DataFrame 中获取`message`列，并从中获取所有值。 我将调用`vectorizer.fit_transform`，它基本上将我的数据中看到的所有单个单词标记化或转换为数字或值。 然后计算每个单词出现的次数。

这是一种更简洁的方式来表示每个单词在电子邮件中出现的次数。 我不是实际保留单词本身，而是将这些单词表示为稀疏矩阵中的不同值，这基本上就是说，我将每个单词视为一个数字，作为一个数字索引，放入一个数组中。 它的作用是，简单地说，它将每条消息分成一个单词列表，并计算每个单词出现的次数。 所以我们称之为`counts`。 它基本上是每个单词在每个单独消息中出现次数的信息。 同时，`targets`是我遇到的每封电子邮件的实际分类数据。 因此，我可以使用我的`MultinomialNB()`函数调用`classifier.fit()`来使用朴素贝叶斯实际创建一个模型，该模型将根据我们给出的信息预测新的电子邮件是否为垃圾邮件。

让我们继续运行它。 它跑得真快！ 这里我将举几个例子。 让我们尝试一条消息正文，它只表示`Free Money now!!!`，这很明显是垃圾邮件，而另一条更无辜的消息，它只表示`"Hi Bob, how about a game of golf tomorrow?"`，所以我们将传递这些消息。

```py
examples = ['Free Money now!!!', "Hi Bob, how about a game of golf tomorrow?"] 
example_counts = vectorizer.transform(examples) 
predictions = classifier.predict(example_counts) 
predictions 

```

我们要做的第一件事是将消息转换为我训练模型时使用的相同格式。 因此，我使用创建模型时创建相同向量器将每条消息转换为单词及其频率列表，其中单词由数组中的位置表示。 然后，一旦我完成了转换，我就可以对我的分类器使用`predict()`函数，对已经转换成单词列表的示例数组使用`predict()`函数，然后看看我们得到了什么：

![](img/9cd78d24-18d0-4700-b154-aa2ac869322b.jpg)

```py
array(['spam', 'ham'], dtype='|S4') 

```

果然，它成功了！ 因此，给出这两个输入消息`Free Money now!!!`和`Hi Bob`的数组，它告诉我第一个结果返回为垃圾邮件，第二个结果返回为 HAME，这是我所期望的。 这真是太酷了。 现在你就知道了。

# 状况 / 活动 / 活性 / 行为

我们这里有一个相当小的数据集，所以如果您愿意，可以尝试运行一些不同的电子邮件，看看是否会得到不同的结果。 如果您真的想挑战自己，请尝试将培训/测试应用于此示例。 因此，衡量我的垃圾邮件分类器好不好的真正标准不仅仅是它是否能直观地判断出`Free Money now!!!`是垃圾邮件。 你想要定量地衡量这一点。

如果你想挑战一下，那就试着把这些数据分成训练集和测试数据集。 实际上，你可以在网上查找熊猫如何很容易地将数据划分为训练集和测试集，或者你也可以手动完成。 只要对你合适就行。 看看您是否可以将`MultinomialNB`分类器实际应用于测试数据集并测量其性能。 所以，如果你想做点运动，挑战一下，那就试一试吧。

多酷啊？ 我们只用了几行 Python 代码就编写了自己的垃圾邮件分类器。 使用 SCRICKIT-LINE 和 Python 非常容易。 这是天真的贝叶斯在行动，你实际上可以去分类一些垃圾邮件或火腿消息，因为你有你的腰带。 很酷的东西。 接下来让我们讨论一下集群。

# K-均值聚类

接下来，我们将讨论 k-均值聚类，这是一种无监督的学习技术，在这种技术中，你有一组你想要分组到不同簇中的东西。 也许是电影类型或人口统计，谁知道呢？ 但这实际上是一个非常简单的想法，所以让我们看看它是如何运作的。

K-Means 聚类是机器学习中的一种非常常见的技术，在机器学习中，您只需尝试获取一堆数据，然后根据数据本身的属性找到有趣的聚类。 听起来很奇特，但实际上很简单。 我们在 k-Means 聚类中所做的就是试图将我们的数据分成 K 个组--这就是 K 的来源，也就是你试图将你的数据分成多少个不同的组--它通过找到 K 个质心来实现这一点。

因此，基本上，给定的数据点属于哪个组是由它在散点图中最接近哪个质心点来定义的。 您可以在下图中直观地看到这一点：

![](img/df576023-6bcc-4691-9d5d-baa565c15445.jpg)

这是一个 k 均值聚类的示例，其中 K 为 3，正方形表示散点图中的数据点。 圆圈代表 k-均值聚类算法得出的质心，每个点根据它离哪个质心最近的位置被分配一个聚类。 所以这就是一切，真的。 这是无监督学习的一个例子。 这并不是说我们有一堆数据，而且我们已经知道一组给定训练数据的正确群集；相反，您只需要给定数据本身，它就会尝试仅根据数据的属性自然地收敛到这些群集上。 这也是一个示例，您正在尝试查找您甚至不知道存在的群集或分类。 就像大多数无监督学习技术一样，重点是发现潜在的价值，这些东西在算法向你展示之前，你并没有真正意识到它们在那里。

例如，百万富翁住在哪里？ 我不知道，也许有一些有趣的地理集群，富人倾向于居住在那里，而 k-Means 集群可以帮助你弄清楚这一点。 也许我真的不知道今天的音乐流派是否有意义。 如今成为另类是什么意思？ 不是很多，对吧？ 但是，通过对歌曲属性使用 k-Means 聚类，也许我可以找到彼此相关的有趣的歌曲群集，并为这些群集代表的内容想出新的名称。 或者我可以看看人口统计数据，也许现有的刻板印象不再有用。 也许西班牙裔已经失去了它的意义，实际上还有其他定义人群的属性，例如，我可以用集群来揭示。 听起来很奇特，不是吗？ 非常复杂的东西。 使用 K 个聚类的无监督机器学习听起来很奇特，但就像数据科学中的大多数技术一样，它实际上是一个非常简单的想法。

以下是简单易懂的算法：

1.  **随机选取 K 个质心(k-均值)：**我们从随机选择的一组质心开始。 因此，如果 K 为 3，我们将在组中寻找三个星团，并在散点图中分配三个随机位置的质心。
2.  **将每个数据点分配给它最接近的质心：**然后我们将每个数据点分配给它最接近的随机分配的质心。
3.  **根据每个质心点的平均位置重新计算质心**：然后重新计算我们得出的每个群集的质心。 也就是说，对于我们最终得到的给定星团，我们将移动该质心，使其成为所有这些点的实际中心。
4.  **反复迭代，直到点停止将分配更改为质心：**我们将再次重复，直到这些质心停止移动，我们达到了某个阈值，表示好的，我们已经收敛到某处了。
5.  **预测新点的群集：**要预测我以前没有见过的新点的群集，我们只需检查一下质心位置，然后找出它最接近哪个质心来预测它的群集。

为了更有意义，让我们看一个图形示例。 我们将下图中的第一个图形称为 A，第二个称为 B，第三个称为 C，第四个称为 D。

![](img/a1e5ec68-fe61-4e99-b6e6-395b3354fef1.jpg)

图像 A 中的灰色方块表示散点图中的数据点。 这些轴代表了某物的一些不同特征。 也许是年龄和收入的问题；这是我经常使用的一个例子，但它可以是任何东西。 灰色方块可能代表个别的人、个别的歌曲或个别的东西，我想找出它们之间的关系。

所以我从我的散点图上随机选取三个点开始。 可能在任何地方。 总得从某个地方开始，对吧？ 我选择的三个点(质心)在图像 A 中显示为圆圈。所以我下一步要做的是为每个质心计算它与哪个灰点最接近。 通过这样做，以蓝色着色的点与该蓝色质心相关联。 绿点最接近绿色质心，这个红色点最接近我挑选的那个红色随机点。

当然，你可以看到这并不能真正反映出星团的实际位置。 所以我要做的是取每个簇中的点，并计算这些点的实际中心。 例如，在绿色集群中，所有数据的实际中心都稍微低一点。 我们要把质心下移一点。 红色星团只有一个点，所以它的中心向下移动到那个点所在的位置。 蓝点实际上离中心很近，所以它只是稍微移动了一下。 在下一次迭代中，我们最终得到了类似于图 D 的东西。现在你可以看到，我们的红色物体的星团有了一点增长，物体也有了一点移动，也就是说，那些从绿色星团中拿到的物体。

如果我们再做一次，你大概可以预测下一步会发生什么。 绿色的质心会稍微移动一点，蓝色的质心仍然会在它所在的位置。 但在一天结束的时候，你最终会得到你可能会期待看到的星团。 这就是 k-Means 的工作原理。 所以它一直在迭代，试图找到正确的质心，直到事情开始移动，我们收敛到一个解。

# K-均值聚类的局限性

因此，k-均值聚类有一定的局限性。 它们是：

1.  **选择 K：**首先，我们需要选择正确的 K 值，这根本不是一件简单的事情。 选择 K 的主要方式是从低开始，根据你想要的组数不断增加 K 的值，直到你停止以平方误差大幅降低。 如果你看看每个点到它们质心的距离，你可以把它看作一个误差度量。 当您停止减少该错误度量时，您知道您的集群可能太多了。 因此，在这一点上添加额外的群集并不能真正获得更多信息。
2.  **避免局部极小：**此外，还存在局部极小的问题。 最初选择的质心可能会非常不走运，它们最终可能只会收敛到局部现象，而不是更多的全局群集，所以通常情况下，你会想要运行几次，也许会将结果平均在一起。 我们称之为整体学习。 我们稍后会更多地讨论这一点，但使用一组不同的随机初始值多次运行 k-Means 总是一个好主意，看看你是否真的得到了相同的整体结果。
3.  **标记群集：**最后，k-Means 聚类的主要问题是您得到的群集没有标签。 它只会告诉你，这组数据点是有某种关联的，但你不能给它起个名字。 它不能告诉你那个星团的实际意义。 假设我有一堆我正在看的电影，k-Means 聚类告诉我，这里有一堆科幻电影，但对我来说，它不会把它们称为“科幻”电影。 这取决于我去挖掘数据，找出这些东西到底有什么共同之处？ 我该怎么用英语来形容呢？ 这是最难的部分，K-Means 不会帮你做到这一点。 因此，再说一次，SCRKIT-LEARN 使这项工作变得非常容易。

现在，让我们来做一个例子，并将 k-Means 聚类付诸实施。

# 根据收入和年龄对人群进行聚类

让我们看看使用 SCRKIT-LEARN 和 Python 进行 k-Means 集群有多容易。

我们要做的第一件事是创建一些想要聚集的随机数据。 为了方便起见，我们实际上会在我们的假测试数据中构建一些群集。 所以让我们假设这些数据之间有一些真正的基本关系，其中存在一些真正的自然星团。

为此，我们可以使用 Python 中的这个小`createClusteredData()`函数：

```py
from numpy import random, array 

#Create fake income/age clusters for N people in k clusters 
def createClusteredData(N, k): 
    random.seed(10) 
    pointsPerCluster = float(N)/k 
    X = [] 
    for i in range (k): 
        incomeCentroid = random.uniform(20000.0, 200000.0) 
        ageCentroid = random.uniform(20.0, 70.0) 
        for j in range(int(pointsPerCluster)): 
            X.append([random.normal(incomeCentroid, 10000.0),             random.normal(ageCentroid, 2.0)]) 
    X = array(X) 
    return X 

```

该函数以一致的随机种子开始，因此每次都会得到相同的结果。 我们想要在 k 个集群中创建 N 个人的集群。 因此，我们将`N`和`k`传递给`createClusteredData().`

我们的代码计算出每个群集有多少个点，这等于 First，并将其存储在`pointsPerCluster`中。 然后，它构建从空开始的列表`X`。 对于每个集群，我们将在 20,000 到 200,000 美元之间创建一些随机的收入质心(`incomeCentroid`)，在 20 岁到 70 岁之间创建一些随机的年龄质心(`ageCentroid`)。

我们在这里做的是创建一个假的散点图，它将显示`N`人和`k`群的收入与年龄的关系。 因此，对于我们创建的每个随机质心，我将创建一组正态分布的随机数据，其中收入的标准差为 10000，年龄的标准差为 2。 这将给我们带来一堆年龄收入数据，这些数据被聚集到一些预先存在的集群中，我们可以随机选择。 好的，让我们继续运行它。

现在，要真正做 k-Means，你会发现这是多么容易。

```py
from sklearn.cluster import KMeans 
import matplotlib.pyplot as plt 
from sklearn.preprocessing import scale 
from numpy import random, float 

data = createClusteredData(100, 5) 

model = KMeans(n_clusters=5) 

# Note I'm scaling the data to normalize it! Important for good results. 
model = model.fit(scale(data)) 

# We can look at the clusters each data point was assigned to 
print model.labels_  

# And we'll visualize it: 
plt.figure(figsize=(8, 6)) 
plt.scatter(data[:,0], data[:,1], c=model.labels_.astype(float)) 
plt.show() 

```

您所需要做的就是从 SCRICKIT-LEARN 的`cluster`包中导入`KMeans`。 我们还将导入`matplotlib`，这样我们就可以将事物可视化，还可以导入`scale`，这样我们就可以看看它是如何工作的。

因此，我们使用我们的`createClusteredData()`函数来表示 5 个群集周围的 100 个随机人。 因此，我要创建的数据有 5 个自然群集。 然后我们创建一个模型，一个 k 为 5 的 KMeans 模型，所以我们选择 5 个群集，因为我们知道这是正确的答案。 但同样，在无监督学习中，您不一定知道`k`的真正价值是什么。 您需要自己对其进行迭代和收敛。 然后我们只需使用我的 KMeans`model`调用`model.fit`，使用我们已有的数据。

我之前提到的规模，就是数据的正常化。 K-Means 最重要的一点是，如果您的数据都是标准化的，那么它工作得最好。 这意味着所有东西的规模都是一样的。 所以我这里有一个问题，我的年龄从 20 岁到 70 岁不等，但我的收入一直到 20 万。 所以这些值并不是真正的可比性。 他们的收入比年龄值大得多。 `Scale`我会把所有这些数据放在一起，放到一个一致的尺度上，这样我就可以把这些东西像苹果和苹果一样进行比较，这将对你的 k-Means 结果有很大帮助。

因此，一旦我们在模型上实际调用了`fit`，我们就可以实际查看得到的标签了。 然后我们就可以使用一点`matplotlib`魔法将其可视化。 您可以在代码中看到，我们有一个小技巧，我们将颜色分配给标签，最终将其转换为某个浮点数。 这只是一个小技巧，您可以使用它为给定值指定任意颜色。 那么让我们来看看我们最终会得到什么：

![](img/5799932c-1451-4c6a-a10a-14372dbc79b6.jpg)

![](img/e988ba57-00c7-4e2e-a3c5-45cf7165bd4c.jpg)

没花那么长时间。 你看，结果基本上就是我把所有东西都分配到的集群。 我们知道我们的假数据已经预聚类了，所以它似乎很容易识别出第一类和第二类。 然而，在这一点之外，它变得有点混乱了，因为我们中间的集群实际上有点混在一起了。 它们并不是真的那么清晰，所以这对 k-Means 来说是一个挑战。 但不管怎样，它确实对星系团做出了一些合理的猜测。 这可能是四个集群更自然地适合数据的一个例子。

# 状况 / 活动 / 活性 / 行为

所以我想让你们做的是尝试不同的 k 值，然后看看你们最终得到的是什么。 只要看一下前面的图表，看起来四个就可以很好地工作了。 真的吗？ 如果我把 k 增加得太大会发生什么？ 我的结果会怎么样？ 它试图把东西分成什么，它有意义吗？ 所以，试着使用它，尝试不同的`k`值。 因此，在`n_clusters()`函数中，将 5 改为其他值。 再过一遍，你就会发现。

这就是 k-Means 聚类的全部内容。 就这么简单。 您可以从`cluster`使用 SCRICKIT-LEARN 的`KMeans`内容。 唯一真正的问题是：确保对数据进行缩放，并将其标准化。 您需要确保正在使用 k-均值的东西彼此具有可比性，而`scale()`函数将为您完成这项工作。 这就是 k-均值聚类的主要内容。 非常简单的概念，使用 SCRICKIT-LINE 做起来就更简单了。

非那样做不行。 这就是 k-均值聚类。 因此，如果你有一大堆未保密的数据，而且你事先并没有得到正确的答案，这是一个很好的方法来尝试自然地找到你的数据的有趣分组，也许这可以让你对这些数据是什么有一些洞察力。 这是一个很好的工具。 我以前在现实世界中用过它，它真的不难用，所以把它放在你的工具箱里。

# 测量熵

很快我们就会接触到机器学习中最酷的部分之一，至少我是这么认为的，那就是决策树。 但在我们谈论这一点之前，有必要先理解一下数据科学中的熵的概念。

因此，就像物理学和热力学中的熵一样，熵是数据集无序性的度量，也是数据集的相同或不同程度的度量。 假设我们有一个不同分类的数据集，例如动物。 比方说，我有一群按物种分类的动物。 现在，如果我的数据集中的所有动物都是鬣蜥，我的熵非常低，因为它们都是一样的。 但如果我的数据集中的每一种动物都是不同的动物，我有鬣蜥、猪和树懒，谁知道还有什么，那么我会有更高的熵，因为我的数据集中有更多的无序。 比起原来的情况，现在的情况更多的是不同。

熵只是一种量化数据中相同或不同之处的方法。 因此，熵为 0 意味着数据中的所有类都是相同的，而如果一切都不同，我就会有一个很高的熵，而介于两者之间的某个东西将是一个介于两者之间的数字。 熵只是描述数据集中的事物的相同或不同程度。

现在从数学上讲，它比这个复杂一点，所以当我实际计算一个熵的数字时，它是使用下面的表达式计算的：

![](img/7dd736b4-b705-487c-94e6-df5a8640a08a.jpg)

所以对于我的数据中的每一个不同的类，我会有其中一个 p 项，p<sub class="calibre50">1</sub>，p<sub class="calibre50">2</sub>，以此类推，直到 p<sub class="calibre50">n</sub>，对于我可能有的 n 个不同的类，我会有一个 p 项，p<sub class="calibre50">1</sub>，p<sub class="calibre50">2</sub>，以此类推。 P 只表示属于该类的数据的比例。 如果您实际绘制每一项的情况-`pi* ln * pi`，它看起来有点像下图：

![](img/b3ae9e6d-662c-4568-b28f-e9fed56739e7.jpg)

您可以为每个单独的类将它们相加。 例如，如果数据的比例，即给定类别的数据比例为 0，则对总体熵的贡献为 0。 如果所有的东西都是这个类，那么对整体熵的贡献也是 0，因为在任何一种情况下，如果没有任何东西是这个类，或者所有的东西都是这个类，那就不会对整体熵有任何贡献。

是中间的东西贡献了这门课的熵，在那里有一些分类和其他东西的混合。 当您将所有这些术语加在一起时，最终得到的是整个数据集的总体熵。 所以从数学上讲，这就是它的工作原理，但同样，这个概念非常简单。 它只是对数据集的无序程度、数据中的内容相同或不同程度的度量。

# 决策树-概念

信不信由你，给定一组训练数据，您实际上可以让 Python 生成一个流程图，让您做出决定。 因此，如果您想要对某个分类进行预测，您可以使用决策树来实际查看多个属性，这些属性可以在流程图中的每个级别上做出决定。 您可以打印出实际的流程图，以供您根据实际的机器学习做出决策。 多酷啊？ 让我们看看它是如何工作的。

我个人发现决策树是机器学习最有趣的应用之一。 决策树基本上给了你一个如何做决定的流程图。你有一些因变量，比如我今天是否应该根据天气去外面玩。 当您有一个依赖于多个属性或多个变量的决策时，决策树可能是一个很好的选择。

天气有很多不同的方面，可能会影响我是否应该出去玩的决定。 例如，这可能与湿度、温度有关，不管是不是晴天。 决策树可以查看所有这些不同的天气或其他属性，并决定阈值是什么？ 在我决定是否应该出去玩之前，我需要在这些属性中的每一个上做出什么决定？ 这就是决策树的全部意义所在。 所以这是一种监督学习的形式。

在本例中，它的工作方式如下所示。 我会有一些历史天气的数据集，以及关于人们在特定的一天是否出去玩的数据。 我会向模型提供这些数据，包括每天是否阳光明媚，湿度如何，是否刮风，以及是否适合外出玩耍。 在给定训练数据的情况下，决策树算法可以得到一棵树，它给出了我们可以打印出来的流程图。 它看起来就像下面的流程图。 您只需漫步其中，根据当前的属性判断是否适合在户外玩耍。 您可以用它来预测一组新值的决策：

![](img/f0d013a2-8886-4d48-8a56-3b4e166a551a.jpg)

多酷啊？ 我们有一种算法，可以根据观测数据自动为您绘制流程图。 更酷的是，一旦你了解了它的工作原理，它就会变得多么简单。

# 决策树示例

比方说，我想构建一个系统，根据简历中的信息自动过滤掉简历。 科技公司面临的一个大问题是，我们的职位会收到大量的简历。 我们必须决定我们真正带谁来面试，因为用飞机把某人送出去，然后抽出一天的时间进行面试，这可能是很昂贵的。 那么，如果有一种方法可以真正获取被录用人员的历史数据，并将其与简历上的内容对应起来，那会怎样呢？

我们可以构建一棵决策树，让我们浏览一份个人简历，然后说，“好吧，这个人实际上很有可能被录用，或者不被录用。” 我们可以根据历史数据训练决策树，并为将来的候选人遍历决策树。 那不是一件很棒的事吗？

因此，让我们制作一些完全捏造的招聘数据，我们将在本例中使用这些数据：

![](img/c4a0232c-b18c-41a8-a032-92b7466ca4a0.jpg)

在上表中，我们有一些仅由数字标识符标识的候选项。 我将挑选一些我认为可能有趣或有帮助的属性来预测他们是否是一名好员工。 他们有多少年的工作经验？ 他们现在有工作吗？ 在此之前，他们有过多少雇主？ 他们的教育水平是多少？ 他们有什么学位？ 他们上的是我们归类为一流学校的学校吗？ 他们在大学期间做过实习吗？ 我们可以看看这个历史数据，这里的因变量是`Hired`。 这个人是否真的根据这些信息得到了工作机会？

很明显，这个模型中有很多信息不是很重要，但我们从这些数据中训练的决策树在筛选一些候选人的初始阶段可能会很有用武之地。 我们最终得到的可能是一棵如下所示的树：

![](img/47031034-ef5a-4c31-b66f-63953d852abf.jpg)

*   所以事实证明，在我完全捏造的数据中，任何在大学实习的人最终都得到了一份工作机会。 所以我的第一个决定是“这个人有没有实习过？” 如果是，那就把他们带进来。 根据我的经验，实习实际上可以很好地预测一个人有多优秀。 如果他们主动走出去实习，并在实习中学到一些东西，这是一个好兆头。
*   他们现在有工作吗？ 嗯，如果他们现在有工作，在我非常小的假数据集中，事实证明他们是值得雇用的，只是因为其他人也认为他们值得雇用。 显然，在现实世界中，这将是一个更微妙的决定。
*   如果他们目前没有工作，他们之前的雇主是不是少于一个？ 如果是，这个人从来没有工作过，他们也从来没有实习过。 可能不是一个好的雇佣决定。 别雇那个人。
*   但如果他们确实有前雇主，他们至少上过一流的学校吗？ 如果不是，那就有点可疑了。 如果是，那么是的，我们应该根据我们培训的数据雇用这个人。

# 浏览决策树

这就是如何遍历决策树的结果。 这就像是在看流程图，算法能为你产生这样的结果，这真是太棒了。 算法本身实际上非常简单。 让我解释一下算法是如何工作的。

在决策树流程图的每一步，我们都会找到可以在其上划分数据的属性，该属性可以在下一步最小化数据的熵。 因此，我们有一个分类的结果集：在本例中，是雇用还是不雇用，并且我们希望在该步骤中选择属性决策，该属性决策将在下一步最小化熵。

在每一步，我们都希望做出所有剩余的选择，结果要么是尽可能多地不招人，要么就是尽可能多地做出聘用决定。 我们希望使这些数据变得越来越统一，以便我们在流程图中继续工作，最终得到一组候选人，这些候选人要么全部聘用，要么全部不聘用，这样我们就可以在决策树上分类为是/否决策。 所以我们只是沿着树往下走，通过选择正确的属性来最小化每一步的熵，然后我们一直走到用完为止。

这个算法有一个很棒的名字。 它被称为**ID3**(**迭代式二叉树分割器 3**)。 这就是众所周知的贪婪算法。 因此，当它沿着树往下走时，它只会选择在这一点上使熵最小的属性。 现在，这可能不会真正产生一棵最优的树来最小化您必须做出的选择的数量，但它将导致一棵能够正常工作的树，因为您已经给出了它的数据。

# 随机森林技术

现在决策树的一个问题是它们很容易过度拟合，所以你最终得到的决策树可以很好地处理你训练过的数据，但对于预测以前没有见过的新的人的正确分类来说，它可能不是那么好。 决策树就是为你提供的训练数据做出正确的决策，但也许你没有真正考虑到正确的属性，也许你没有给它足够多的有代表性的样本来学习。 这可能会导致真正的问题。

因此，为了解决这个问题，我们使用了一种叫做随机森林的技术，它的想法是我们对我们训练的数据进行采样，以不同的方式，为多个不同的决策树。 每个决策树从我们的训练数据集中提取一个不同的随机样本，并从它构建一棵树。 然后，每棵生成的树都可以对正确的结果进行投票。

现在，使用相同模型对数据进行随机重采样的技术被称为引导聚合(Bootstrap Aggregation)，或称打包(Bootstrap Aggregation)。 这是我们所说的整体学习的一种形式，我们稍后将更详细地介绍。 但基本的想法是，我们有多棵树，如果你愿意的话，可以说是一片树木森林，每棵树都使用我们必须训练的数据的随机子样本。 然后，这些树中的每一棵都可以对最终结果进行投票，这将帮助我们克服对给定训练数据集的过度拟合。

随机森林可以做的另一件事实际上是限制它在每个阶段可以选择的属性的数量，同时它试图在进行过程中将熵降到最低。 我们可以随机选择它可以在每个级别选择的属性。 这也让我们在不同的树之间有了更多的差异，因此我们得到了更多可以相互竞争的各种算法。 他们都可以使用略微不同的方法对最终结果进行投票，以得出相同的答案。

这就是随机森林的工作原理。 基本上，它是一个决策树森林，它们从不同的样本和每个阶段的不同属性集中提取数据，以供选择。

所以，有了这些，让我们去做一些决策树。 当我们完成后，我们也会使用随机森林，因为 SCRICKIT-LINE 使它变得非常容易，你很快就会看到。

# 决策树-使用 Python 预测招聘决策

事实证明，创建决策树很容易；事实上，只需要几行 Python 代码，就可以轻松地创建决策树，这简直太疯狂了。 所以让我们试一试吧。

我已经把`PastHires.csv`档案和你的图书材料放在一起了，其中只包括一些我编造的数据，这些数据是关于那些要么得到了工作机会，要么不是基于这些候选人的属性而获得工作机会的人。

```py
import numpy as np 
import pandas as pd 
from sklearn import tree 

input_file = "c:/spark/DataScience/PastHires.csv" 
df = pd.read_csv(input_file, header = 0) 

```

请立即将我在这里用于我自己系统的路径(`c:/spark/DataScience/PastHires.csv`)更改为您安装本书材料的位置。 我不知道你把它放在哪里了，但几乎可以肯定它不在那里。

我们将使用`pandas`读入 CSV，并从中创建一个 DataFrame 对象。 让我们继续运行我们的代码，我们可以在 DataFrame 上使用`head()`函数打印出前几行，并确保它看起来有意义。

```py
df.head() 

```

Sure enough we have some valid data in the output:

![](img/39aa9b7f-af63-4996-ae11-fc8d96174087.jpg)

因此，对于每个应聘者 ID，我们有他们过去多年的经验，无论他们是否被聘用，他们以前的雇主数量，他们的最高教育水平，他们是否上过一流的学校，他们是否做过实习；最后，在这里，在雇用专栏中，我们知道我们要么向这个人提供了工作机会，要么没有。

像往常一样，大部分工作只是在你实际运行算法之前，对你的数据进行修改，准备你的数据，这就是我们在这里需要做的。 现在，科学工具包-学习要求一切都是数字的，所以我们不能有 Y、NS、BSS、MSS 和博士学位。 我们必须将所有这些内容转换为数字，这样决策树模型才能正常工作。 做到这一点的方法是在熊猫身上使用一些速记，这使得这些事情变得容易。 例如：

```py
d = {'Y': 1, 'N': 0} 
df['Hired'] = df['Hired'].map(d) 
df['Employed?'] = df['Employed?'].map(d) 
df['Top-tier school'] = df['Top-tier school'].map(d) 
df['Interned'] = df['Interned'].map(d) 
d = {'BS': 0, 'MS': 1, 'PhD': 2} 
df['Level of Education'] = df['Level of Education'].map(d) 
df.head() 

```

基本上，我们用 Python 制作一个字典，将字母 Y 映射到数字 1，将字母 N 映射到值 0。 因此，我们希望将所有的 Y 转换为 1，将 NS 转换为 0。 所以 1 表示是，0 表示不是。 我们所做的就是从 DataFrame 中获取雇佣列，并使用字典对其调用`map()`。 这将遍历整个 DataFrame 中的整个雇用列，并使用该字典查找来转换该列中的所有条目。 它返回一个新的 DataFrame 列，我要将该列放回被雇用的列中。 这将用映射到 1 和 0 的列替换已雇用的列。

我们对就业人员、顶级学校和实习生都做了同样的事情，所以所有这些都会使用是/否字典进行映射。 所以，Y 和 NS 变成了 1 和 0。 对于教育水平，我们使用相同的技巧，我们只是创建了一个字典，将 BS 指定为 0，MS 指定为 1，PHD 指定为 2，并使用该字典将这些学位名称重新映射到实际数值。 因此，如果我继续运行它，并再次执行`head()`，您可以看到它起作用了：

![](img/8f975ef7-dde7-4e2c-988f-a115252ad95d.jpg)

我所有的“是”都是“1”，我的“否”是“0”，我的教育水平现在用一个真正有意义的数值来表示。

接下来，我们需要做好一切准备，以便真正进入我们的决策树分类器，这并不难。 要做到这一点，我们需要将我们的特征信息(即我们试图预测的属性)和我们的目标列(包含我们试图预测的内容)分开。要提取特征名称列的列表，我们只需创建一个列的列表，直到第 6 列。我们继续并将其打印出来。

```py
features = list(df.columns[:6]) 
features 

```

我们得到以下输出：

![](img/b121561b-c7b2-4418-aad6-bc33f5c608d2.jpg)

以上是包含我们的特征信息的列名：年限、受雇？、以前的雇主、教育程度、顶级学校和实习生。 这些是我们想要预测招聘的候选人的属性。

接下来，我们构造我们的*y*向量，该向量被赋予我们试图预测的内容，即我们雇佣的列：

```py
y = df["Hired"] 
X = df[features] 
clf = tree.DecisionTreeClassifier() 
clf = clf.fit(X,y) 

```

此代码提取整个雇用列并将其命名为`y`。 然后，它将我们所有的列用于特征数据，并将它们放入名为`X`的内容中。 这是所有数据和所有功能列的集合，`X`和`y`是我们的决策树分类器需要的两个东西。

要实际创建分类器本身，需要两行代码：我们调用`tree.DecisionTreeClassifier()`来创建分类器，然后将其与我们的特征数据(`X`)和答案(`y`)相匹配-无论是否雇用人员。 所以，让我们继续运行它。

显示图形数据有点棘手，我不想在这里用太多细节分散我们的注意力，所以请考虑下面的样板代码。 你不需要深入了解 Graph viz 在这里是如何工作的--还有点文件之类的东西：这对我们现在的旅程并不重要。 要实际显示决策树的最终结果，您需要的代码很简单：

```py
from IPython.display import Image   
from sklearn.externals.six import StringIO   
import pydot  

dot_data = StringIO()   
tree.export_graphviz(clf, out_file=dot_data,   
                         feature_names=features)   
graph = pydot.graph_from_dot_data(dot_data.getvalue())   
Image(graph.create_png()) 

```

那么让我们继续运行这个。

下面是您的输出现在应该是什么样子：

![](img/19fbf5b0-4476-46de-897e-ce3bb7a5237f.jpg)

我们拿到了！ 这是不是很酷？！ 我们这里有一个实际的流程图。

现在，让我教你怎么读。 在每个阶段，我们都有一个决定。 记住，我们的大多数数据，无论是是还是否，都将是**0**或**1**。 因此，第一个决策点变成：受雇了吗？ 小于**0.5**？ 这意味着如果我们的就业值为 0，那就是否，我们就会向左转，如果就业率是 1，那就是对的，我们就会向右转。

那么，他们以前有没有工作过呢？ 如果不是左转，如果是右转。 事实证明，在我的样本数据中，每个目前有工作的人实际上都得到了一份工作机会，所以我可以很快地说，如果你现在有工作，是的，你值得引进，我们将继续深入到第二个层面。

那么，你是如何解读这一点的呢？ 基尼指数基本上是它在每一步中使用的熵的衡量标准。 记住，当我们往下走时，算法试图最小化熵值。 样本是未被之前的决定分割的样本的剩余数量。

所以说这个人是被雇佣的。 读取右侧叶节点的方法是值列，它告诉您在这一点上，我们有 0 个候选人没有被雇用，5 个候选人是被雇用的。 那么，再说一次，解释第一个决策点的方式是如果使用呢？ 是 1，我要转到右边，这意味着他们目前都有工作，这把我带到了一个人人都有工作机会的世界。 所以，这意味着我应该雇用这个人。

现在假设这个人目前没有工作。 接下来我要看的是，他们有没有实习机会。 如果是，那么在我们的培训数据中，每个人都得到了一份工作机会。 所以，在这一点上，我们可以说我们的熵现在是 0(`gini=0.0000`)，因为每个人都是一样的，在这一点上他们都得到了一份工作。 然而，你知道，如果我们继续下降(这个人没有做过实习)，我们将处于熵值为 0.32 的点上。 它越来越低，这是一件好事。

下面我们来看看他们有多少经验，他们有没有不到一年的经验？ 而且，如果情况是他们确实有一些经验，而且他们已经走到了这一步，那么他们就是一个很好的不聘用员工的决定。 我们最终得到了零熵，但是，我们训练集中剩下的三个样本都是无人雇用的。 我们有 3 个不招人，0 个人。 但是，如果他们的经验确实较少，那么他们很可能是刚从大学毕业的人，他们可能仍然值得一看。

我们要看的最后一件事是他们是否上过一流的学校，如果是的话，他们最终是一个很好的被录用的预测。 如果不是，他们最终就会被拒之门外。 我们最终得到的结果是，一名应聘者没有被录用，0 名应聘者被录用。 然而，如果候选人确实去了一流的学校，我们没有招聘，只有 1 名。

所以，你可以看到，我们一直往前走，直到熵达到 0，如果可能的话，对于每一种情况。

# 整体学习-使用随机森林

现在，假设我们想要使用一个随机的森林，你知道，我们担心我们可能会过度拟合我们的训练数据。 实际上，创建多个决策树的随机森林分类器非常容易。

因此，要做到这一点，我们可以使用之前创建的相同数据。 您只需要您的`*X*`和`*y*`向量，即您试图预测的特性集和列：

```py
from sklearn.ensemble import RandomForestClassifier 

clf = RandomForestClassifier(n_estimators=10) 
clf = clf.fit(X, y) 

#Predict employment of an employed 10-year veteran 
print clf.predict([[10, 1, 4, 0, 0, 0]]) 
#...and an unemployed 10-year veteran 
print clf.predict([[10, 0, 4, 0, 0, 0]]) 

```

我们制作了一个随机的森林分类器，也可以从 SCRICIT-LEARN 获得，然后传递给它我们想要的森林中的树木数量。 因此，在上面的代码中，我们在随机森林中创建了 10 棵树。 然后我们将其与模型相匹配。

你不需要徒手穿过树木，当你处理随机的森林时，你无论如何都不能做到这一点。 因此，我们在模型上使用`predict()`函数，也就是在我们制作的分类器上使用`predict()`函数。 我们传入一个列表，其中列出了我们想要预测其就业的给定候选人的所有不同特征。

如果你还记得这些列：年限、工作经验、以前的雇主、教育程度、顶尖学校和实习生；解释为数字值。 我们预计会雇用一名有 10 年工作经验的老兵。 我们还预测了一位失业了 10 年的退伍军人的就业情况。 果然，我们得到了一个结果：

![](img/76760aeb-0024-4cec-8ca3-5cd9c64c2ebe.jpg)

因此，在这个特殊的案例中，我们最终做出了这两个方面的招聘决定。 但是，有趣的是，其中有一个随机成分。 你并不是每次都能得到相同的结果！ 通常情况下，失业者得不到工作机会，如果你继续这样做，你会发现情况通常是这样的。 但是，套袋的随机性，引导聚合每一棵树的本质，意味着你不会每次都得到相同的结果。 所以，也许 10 棵树还不够。 所以，不管怎样，这是一个很好的教训，在这里学习！

# 状况 / 活动 / 活性 / 行为

对于一项活动，如果您想要回去玩这个游戏，可以摆弄我的输入数据。 继续编辑我们一直在探索的代码，创建一个另一个世界，在那里它是一个颠倒的世界；例如，我现在给每个人提供的工作机会都没有得到，反之亦然。 看看这对你的决策树有什么影响。 把它弄乱，看看你能做些什么，然后试着解释结果。

所以，这就是决策树和随机森林，在我看来，这是机器学习中比较有趣的部分之一。 我一直认为，像这样凭空生成流程图是相当酷的。 所以，希望你会觉得这很有用。

# 集成学习

当我们谈到随机森林时，这是一个整体学习的例子，我们实际上将多个模型结合在一起，得出了比任何单一模型都更好的结果。 那么，让我们更深入地了解一下这一点。 让我们再多谈一下合奏学习。

还记得随机森林吗？ 我们有一组决策树，它们使用不同的输入数据子样本，并根据不同的属性集进行分支，当你在最后尝试对某样东西进行分类时，它们都会对最终结果进行投票。 这是整体学习的一个例子。 另一个例子：当我们讨论 k-Means 聚类时，我们的想法是可能使用具有不同初始随机质心的不同 k-Means 模型，并让它们都对最终结果进行投票。 这也是整体学习的一个例子。

基本上，我们的想法是，你有不止一个模型，它们可能是同一类型的模型，也可能是不同类型的模型，但你可以在你的训练数据集上运行它们，它们都会对最终结果进行投票，无论你试图预测的是什么。 通常情况下，你会发现这种不同模型的组合比任何一个单独的模型都能产生更好的效果。

几年前的一个很好的例子是 Netflix 奖。 Netflix 举办了一场竞赛，他们向任何能够超越他们现有电影推荐算法的研究人员提供 100 万美元的奖励。 获胜的是集成方法，他们实际上一次运行多个推荐算法，并让它们对最终结果进行投票。 因此，集成学习可以是一个非常强大而又简单的工具，用于提高机器学习的最终结果的质量。 现在让我们尝试探索各种类型的合奏学习：

*   **自举聚集或装袋：**现在，随机森林使用一种称为自举聚集的技术，即自举聚集的缩写。 这意味着我们随机抽取训练数据的子样本，将它们输入同一模型的不同版本，并让它们对最终结果进行投票。 如果你还记得，随机森林采用了许多不同的决策树，这些决策树使用不同的随机训练数据样本进行训练，然后它们最终都聚集在一起，对最终结果进行投票。 这就是装袋。
*   **提升：**提升是一种替代模型，这里的想法是您从一个模型开始，但每个后续模型都会提升解决前一个模型错误分类的区域的属性。 因此，你在一个模型上运行训练/测试，找出它基本上出错的属性，然后在后续模型中提升这些属性-希望后续模型会更多地关注这些属性，并将其纠正为正确的属性。 这就是助推器背后的大意。 你运行一个模型，找出它的弱点，在你前进的过程中放大对这些弱点的关注，并在前一个模型的弱点的基础上，不断构建越来越多的模型，不断完善这个模型。
*   **模型桶：**另一种技术，也就是 Netflix 获奖者所做的，被称为模型桶，你可能会有完全不同的模型试图预测某事。 也许我使用的是 k-均值、决策树和回归。 我可以在一组训练数据上一起运行这三个模型，当我试图预测某件事时，让它们都投票决定最终的分类结果。 也许这比单独使用这些模型中的任何一个都要好。
*   **堆叠：**堆叠也有同样的概念。 因此，您对数据运行多个模型，以某种方式将结果组合在一起。 模特桶和堆叠之间的微妙区别在于，你可以选择获胜的模特。 因此，您可以运行 Train/Test，找到最适合您的数据的模型，然后使用该模型。 相比之下，堆叠将把所有这些模型的结果组合在一起，以得出最终结果。

现在，有一整套关于合奏学习的研究试图找到进行合奏学习的最佳方式，如果你想听起来很聪明，通常需要经常使用贝叶斯这个词。 所以，有一些非常先进的方法来做合奏学习，但它们都有弱点，我认为这是另一个教训，因为我们应该总是使用最简单的技术，对我们来说效果很好。

这些都是非常复杂的技术，我不能真正进入本书的范围，但归根结底，很难超越我们已经讨论过的简单技术。 下面列出了一些复杂的技术：

*   **贝叶斯光学分类器：**在理论上，有一种叫做贝叶斯最优分类器的东西永远是最好的，但它是不切实际的，因为这样做在计算上是不可行的。
*   **贝叶斯参数平均：**许多人尝试对贝叶斯最优分类器进行变体，以使其更实用，如贝叶斯参数平均变体。 但它仍然容易过度拟合，而且它的表现往往优于袋装，这与随机森林背后的想法是相同的；你只需对数据进行多次重新采样，运行不同的模型，然后让它们都对最终结果进行投票即可。 事实证明，这也同样有效，而且要简单得多！
*   **贝叶斯模型组合：**最后，有一种叫做贝叶斯模型组合的东西，它试图解决贝叶斯最优分类器和贝叶斯参数平均的所有缺点。 但是，归根结底，它并不比仅仅针对模型组合进行交叉验证好到哪里去。

同样，这些都是非常复杂的技术，很难使用。 在实践中，我们会更好地使用我们详细讨论过的更简单的方法。 但是，如果你想听起来很聪明，并且经常使用贝叶斯这个词，至少熟悉这些技巧，并知道它们是什么，这是很好的。

这就是整体学习。 再说一次，简单的技术，如自举聚合，或装袋，或提升，或堆叠，或模型桶，通常是正确的选择。 有一些更花哨的技术，但它们主要是理论上的。 但是，至少你现在知道他们了。

尝试合奏学习总是一个好主意。 它已经一次又一次地被证明，它将产生比任何单一模型更好的结果，所以一定要考虑一下！

# 支持向量机概述

最后，我们将讨论**支持向量机**(**SVM**)，这是一种非常先进的对高维数据进行聚类或分类的方法。

那么，如果您有多个想要预测的特性，该怎么办呢？ 支持向量机可以是一个非常强大的工具，而且结果可能好得吓人！ 这在幕后非常复杂，但重要的是理解什么时候使用它，以及它在更高层次上是如何工作的。 现在，让我们来讲讲支持向量机。

支持向量机实际上是一个奇特的概念，这是一个奇特的名字。 但幸运的是，它非常容易使用。 重要的是知道它能做什么，有什么好处。 因此，支持向量机很好地用于高维数据的分类，我指的是许多不同的特征。 所以，可以很容易地使用像 k-Means 聚类这样的东西来对具有两个维度的数据进行聚类，你知道，可能年龄在一个轴上，收入在另一个轴上。 但是，如果我有很多很多不同的特征，我试图从这些特征中预测出来，那该怎么办呢？ 嗯，支持向量机可能是一个很好的方法。

支持向量机查找用于划分数据的高维支持向量(从数学上讲，这些支持向量定义超平面)。 也就是说，从数学上讲，支持向量机可以做的是找到高维支持向量(这就是它得名的原因)，这些支持向量定义了将数据分成不同簇的高维平面。

显然，这一切很快就会让数学变得非常奇怪。 幸运的是，`scikit-learn`包将为您完成所有工作，而您不必真正投入其中。 在幕后，你需要理解它使用了一种叫做内核的技巧来实际找到那些在低维中可能不明显的支持向量或超平面。 您可以使用不同的内核，以不同的方式完成此任务。 主要的一点是，如果您有具有许多不同特征的高维数据，并且您可以使用具有不同计算成本的不同内核，并且可能更适合于手头的问题，那么 SVM 是一个很好的选择。

The important point is that SVMs employ some advanced mathematical trickery to cluster data, and it can handle data sets with lots of features. It's also fairly expensive - the "kernel trick" is the only thing that makes it possible.

我想指出的是，支持向量机是一种有监督的学习技术。 所以，我们实际上要在一组训练数据上训练它，我们可以用它来预测未来看不见的数据或测试数据。 这与 k-Means 聚类略有不同，k-Means 完全是无人监督的；相比之下，对于支持向量机，它是基于实际的训练数据进行训练的，在那里你可以对某些数据集有正确的分类答案，它可以从这些数据中学习。 所以，如果你愿意的话，支持向量机对分类和聚类很有用--但它是一种有监督的技术！

在支持向量机中，您经常看到的一个例子是使用一种称为支持向量分类的方法。 典型的例子使用 Iris 数据集，它是 SCRICIT-LEARN 附带的样例数据集之一。 这一套是对不同的花进行分类，对不同的虹膜花和它们的种类进行不同的观察。 这个想法是利用每朵花上花瓣的长度和宽度以及每朵花的萼片的长度和宽度的信息来对它们进行分类。 (很明显，萼片是花瓣下面的一个小支撑结构。 直到现在我也不知道这一点。)。 你有四个维度的属性；你有花瓣的长度和宽度，以及萼片的长度和宽度。 根据这些信息，你可以用它来预测虹膜的种类。

这是一个使用 SVC 实现这一点的例子：基本上，我们将萼片宽度和萼片长度投影到两个维度，因此我们可以真正地将其可视化：

![](img/11af3028-7b21-40ac-94ae-b9db94d97655.jpg)

使用不同的内核可能会得到不同的结果。 具有线性内核的 SVC 将产生与您在上图中看到的非常相似的结果。 您可以使用多项式核或更花哨的核，这些核可能向下投影到二维曲线，如图所示。 你可以这样做一些非常奇特的分类。

这些都会增加计算成本，并且会产生更复杂的关系。 但是，在这种情况下，过于复杂可能会产生误导性的结果，因此您需要小心，并在适当的时候实际使用培训/测试。 由于我们正在进行有监督的学习，您实际上可以进行训练/测试并找到正确的有效模型，或者可能使用集成方法。

您需要为手头的任务找到合适的内核。 对于像多项式 SVC 这样的东西，正确的次数多项式是什么？ 即使像线性 SVC 这样的东西也会有不同的参数与之关联，您可能需要针对这些参数进行优化。 对于一个真实的示例，这会更有意义，所以让我们深入研究一些实际的 Python 代码，看看它是如何工作的！

# 基于 SCRICIT-LEARN 的支持向量机聚类

让我们在这里尝试一些支持向量机。 幸运的是，它使用起来比理解起来容易得多。 我们将回到我在 k-Means 聚类中使用的同一个例子，在那里我将创建一些关于 100 个随机人的年龄和收入的捏造的聚类数据。

如果您想回到 k-Means 聚类部分，您可以了解更多关于生成假数据的代码背后的想法。 如果您准备好了，请考虑以下代码：

```py
import numpy as np 

#Create fake income/age clusters for N people in k clusters 
def createClusteredData(N, k): 
    pointsPerCluster = float(N)/k 
    X = [] 
    y = [] 
    for i in range (k): 
        incomeCentroid = np.random.uniform(20000.0, 200000.0) 
        ageCentroid = np.random.uniform(20.0, 70.0) 
        for j in range(int(pointsPerCluster)): 
            X.append([np.random.normal(incomeCentroid, 10000.0),              np.random.normal(ageCentroid, 2.0)]) 
            y.append(i) 
    X = np.array(X) 
    y = np.array(y) 
    return X, y 

```

请注意，因为我们在这里使用的是有监督学习，所以我们不仅需要再次获得特征数据，还需要我们的训练数据集的实际答案。

这里的`createClusteredData()`函数是根据年龄和收入，为聚集在`k`点周围的人创建一组随机数据，它返回两个数组。 第一个数组是特征数组，我们称之为`X`，然后我们有我们试图预测的东西的数组，我们称之为`y`。 很多时候，在 SCRICIT 中学习-当你创建一个你可以预测的模型时，这是它需要的两个输入，一个是特征向量的列表，另一个是你试图预测的东西，它可以从中学习。 所以，我们继续运行它。

因此，现在我们将使用`createClusteredData()`函数创建具有 5 个不同簇的 100 个随机人。 我们将创建一个散点图来说明这些问题，并看看它们最终会出现在什么地方：

```py
%matplotlib inline 
from pylab import * 

(X, y) = createClusteredData(100, 5) 

plt.figure(figsize=(8, 6)) 
plt.scatter(X[:,0], X[:,1], c=y.astype(np.float)) 
plt.show() 

```

下图显示了我们正在处理的数据。 每次运行此命令时，您都会得到一组不同的群集。 所以，你知道，我实际上并没有使用随机的种子...。 让生活变得有趣。

这里有几个新东西--我在`plt.figure()`上使用`figsize`参数来实际绘制更大的图。 因此，如果您需要调整`matplotlib`中的大小，就可以这样做。 我使用的是与最终得到的分类号相同的颜色。 所以我开始时使用的簇的编号被绘制为这些数据点的颜色。 你可以看到，这是一个非常具有挑战性的问题，这里肯定有一些交织在一起的集群：

![](img/18ecc82e-4a00-47b9-b93e-7b7de259f4ab.jpg)

现在我们可以使用线性 SVC(SVC 是支持向量机的一种形式)，将其实际划分为群集。 我们将使用线性核的支持向量机，其 C 值为`1.0`。 C 只是一个可以调整的错误惩罚项；默认情况下是`1`。 通常情况下，你不会想搞砸这一点，但如果你正在使用整体学习或训练/测试在正确的模型上进行某种收敛，这是你可以尝试的事情之一。 然后，我们将该模型与我们的特征数据以及我们为我们的训练数据集拥有的实际分类相匹配。

```py
from sklearn import svm, datasets 

C = 1.0 
svc = svm.SVC(kernel='linear', C=C).fit(X, y) 

```

所以，让我们继续运行它。 我不想过多地讨论我们将如何在这里实际可视化结果，只要相信`plotPredictions()`是一个可以绘制分类范围和 SVC 的函数。

它帮助我们直观地看到不同的分类出现在哪里。 基本上，它是在整个网格上创建一个网格，它会将 SVC 模型中的不同分类绘制为网格上的不同颜色，然后我们将在此基础上绘制原始数据：

```py
def plotPredictions(clf): 
    xx, yy = np.meshgrid(np.arange(0, 250000, 10), 
                     np.arange(10, 70, 0.5)) 
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) 

    plt.figure(figsize=(8, 6)) 
    Z = Z.reshape(xx.shape) 
    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8) 
    plt.scatter(X[:,0], X[:,1], c=y.astype(np.float)) 
    plt.show() 

plotPredictions(svc) 

```

那么，让我们看看这是如何实现的。 SVC 的计算成本很高，因此需要很长时间才能运行：

![](img/a1342ff6-9d4a-4546-a76a-ed1092efdbb0.jpg)

你可以在这里看到它已经尽了最大努力。 考虑到它必须绘制直线和多边形，它在拟合我们所拥有的数据方面做得很不错。 所以，你知道，它确实错过了一些-但总的来说，结果相当不错。

SVC 实际上是一种非常强大的技术；它的真正优势在于更高维的特征数据。 去玩吧。 顺便说一句，如果您不仅想要可视化结果，您可以在 SVC 模型上使用`predict()`函数，就像在 SCISKIT-LEARN 中的几乎任何模型上一样，来传递您感兴趣的功能数组。 如果我想要预测年收入 20 万美元的 40 岁人的分类，我会使用以下代码：

```py
svc.predict([[200000, 40]])

```

在我们的例子中，这会将此人放入 1 号群集：

![](img/8a7954d2-6d7d-482b-a3dc-05b75deeab61.jpg)

如果我这里有一个收入 5 万美元的 65 岁的人，我会使用以下代码：

```py
svc.predict([[50000, 65]])

```

下面是您的输出现在应该是什么样子：

![](img/30e1d81a-b05c-48a1-9f17-ac864d6e9cd6.jpg)

无论本例中表示的是什么，该人最终都会出现在 2 号集群中。 所以，去玩吧。

# 状况 / 活动 / 活性 / 行为

现在，线性只是你可以使用的众多内核中的一个，就像我说的，你可以使用很多不同的内核。 其中之一是多项式模型，所以你可能想试一试。 请务必继续查看文档。 看看病历对你来说是很好的练习。 如果你打算使用 SCRICKIT-深入学习，有很多不同的功能和选项可供你选择。 所以，去上网查一下 SCRICIT-LISTING，找出 SVC 方法的其他内核是什么，然后试一试，看看你是否真的得到了更好的结果。

这是一个小小的练习，不仅是在使用支持向量机和不同类型的 SVC，也是为了熟悉如何自己学习更多关于 SVC 的知识。 老实说，任何数据科学家或工程师都有一个非常重要的特质，那就是当你不知道答案的时候，能够自己去查找信息。

所以，你知道，我不是懒惰地不告诉你其他内核是什么，我想让你习惯于不得不自己去查这些东西的想法，因为如果你必须一直问别人关于这些东西的问题，你会在工作场所变得非常恼人，很快就会变得非常恼人。 所以，去查一查，绕过它，看看你能想出什么。

这就是 SVM/SVC，这是一种非常强大的技术，你可以用它来对数据进行分类，在有监督的学习中。 现在你知道了它的工作原理和使用方法，所以把它放在你的小把戏包里吧！

# 简略的 / 概括的 / 简易判罪的 / 简易的

在本章中，我们看到了一些有趣的机器学习技术。 我们介绍了机器学习背后的一个基本概念，称为训练/测试。 我们了解了如何使用训练/测试来尝试找到合适的次数多项式来拟合给定的数据集。 然后分析了有监督和无监督机器学习的区别。

我们了解了如何实现垃圾邮件分类器，并使用朴素贝叶斯技术使其能够确定电子邮件是否为垃圾邮件。 我们谈到了 k-均值聚类，这是一种无监督的学习技术，可以帮助将数据分组到集群中。 我们还看了一个使用 SCRICKIT 的例子--学习根据人们的收入和年龄对他们进行分组。

然后我们继续研究熵的概念以及如何测量它。 我们介绍了决策树的概念，以及如何在给定一组训练数据的情况下，实际让 Python 生成流程图，以便您实际做出决策。 我们还建立了一个系统，可以根据简历中的信息自动过滤出简历，并预测一个人的招聘决定。

我们一路上学习了集成学习的概念，最后我们讨论了支持向量机，这是一种非常先进的对高维数据进行聚类或分类的方法。 然后，我们继续使用支持向量机(SVM)通过 SCRKIT-LEARN 对人群进行聚类。 在下一章中，我们将讨论推荐系统。