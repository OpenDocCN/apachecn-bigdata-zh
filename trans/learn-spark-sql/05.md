# 在流应用程序中使用火花 SQL

在本章中，我们将介绍在流应用程序中使用 Spark SQL 的典型用例。我们的重点将是使用 Spark 2.0 中引入的数据集/数据框架 API 的结构化流。此外，我们将介绍 Apache Kafka 并与之合作，因为它是许多网络级流应用程序体系结构的组成部分。流式应用程序通常涉及对传入数据或消息的实时、上下文感知响应。我们将使用几个例子来说明构建此类应用程序的关键概念和技术。

在本章中，我们将学习以下主题:

*   什么是流数据应用？
*   典型的流用例
*   使用 Spark SQL 数据框架/数据集应用编程接口构建流式应用程序
*   在结构化流应用程序中使用卡夫卡
*   为自定义数据源创建接收器

# 介绍流式数据应用

传统的批处理应用程序通常运行数小时，处理存储在关系数据库中的所有或大部分数据。最近，基于 Hadoop 的系统被用来支持基于 MapReduce 的批处理作业，以处理非常大的分布式数据量。相反，流处理发生在连续生成的流数据上。这种处理用于各种各样的分析应用程序，这些应用程序计算事件、聚合值、样本输入数据等之间的相关性。

流处理通常会吸收一系列数据，并在逐个记录/逐个事件的基础上或在滑动时间窗口内动态地递增计算统计数据和其他函数。

流数据应用越来越多地应用机器学习算法和**复杂事件处理** ( **CEP** )算法，以提供战略洞察力和快速智能地对快速变化的业务条件做出反应的能力。此类应用程序可以扩展以处理非常大量的流数据，并实时做出适当的响应。此外，许多组织正在实现包含实时层和批处理层的体系结构。在这样的实现中，尽可能地为这两层维护一个单一的代码库是非常重要的(关于这样的架构的例子，参考[第 12 章](12.html#6DVPG0-e9cbc07f866e437b8aa14e841622275c)、*大规模应用架构中的火花 SQL*)。Spark 结构化流应用编程接口帮助我们以可扩展、可靠和容错的方式实现这些目标。

流媒体应用的一些实际使用案例包括物联网应用中的传感器数据处理、股票市场应用，如风险管理和算法交易、网络监控、监控应用、电子商务应用中的即时客户参与、欺诈检测等。

因此，出现了许多提供构建流数据应用程序所需的基础架构的平台，包括 Apache Kafka、Apache Spark Streaming、Apache Storm、Amazon Kinesis Streams 等。

在本章中，我们将探索使用 Apache Spark 和 Apache Kafka 的流处理。在接下来的几节中，我们将使用 Spark SQL 数据框架/数据集 API 详细探讨 Spark 结构化流。

# 构建 Spark 流应用程序

在本节中，我们将主要关注新引入的结构化流功能(在 Spark 2.0 中)。结构化流 API 是带有 Spark 2.2 的 GA，使用它们是构建流 Spark 应用程序的首选方法。基于卡夫卡的处理组件的几个更新，包括性能改进，也已经在 Spark 2.2 中发布。我们在[第 1 章](01.html#KVCC0-e9cbc07f866e437b8aa14e841622275c)、*中介绍了结构化流，开始使用 Spark SQL* 。在本章中，我们将深入探讨这个主题，并给出几个代码示例来展示它的功能。

简单回顾一下，结构化流提供了一种快速、可扩展、容错、端到端的一次性流处理，开发人员无需对底层流机制进行推理。

它建立在 Spark SQL 引擎上，流式计算可以用在静态数据上表示批处理计算的相同方式来表示。它提供了几个数据抽象，包括流查询、流源和流接收器，以简化流应用程序，而不涉及数据流的底层复杂性。Scala、Java 和 Python 中都有编程 API，您可以使用熟悉的数据集/数据框架 API 来实现您的应用程序。

在[第 1 章](01.html#KVCC0-e9cbc07f866e437b8aa14e841622275c)、*开始使用 Spark SQL* 中，我们使用 IPinYou 数据集创建了一个流数据框架，然后在其上定义了一个流查询。我们展示了在每个时间间隔内更新的结果。在这里，我们重新创建我们的流数据帧，然后在其上执行各种功能，以展示流输入数据上可能的计算类型。

首先，我们启动 Spark shell 并导入本章动手部分所需的必要类。在大多数示例中，我们将使用文件源来模拟传入的数据:

```scala
scala> import org.apache.spark.sql.types._scala> import org.apache.spark.sql.functions._scala> import scala.concurrent.duration._scala> import org.apache.spark.sql.streaming.ProcessingTimescala> import org.apache.spark.sql.streaming.OutputMode.Completescala> import spark.implicits._
```

接下来，我们为源文件中的投标记录定义模式，如下所示:

```scala
scala> val bidSchema = new StructType().add("bidid", StringType).add("timestamp", StringType).add("ipinyouid", StringType).add("useragent", StringType).add("IP", StringType).add("region", IntegerType).add("cityID", IntegerType).add("adexchange", StringType).add("domain", StringType).add("turl", StringType).add("urlid", StringType).add("slotid", StringType).add("slotwidth", StringType).add("slotheight", StringType).add("slotvisibility", StringType).add("slotformat", StringType).add("slotprice", StringType).add("creative", StringType).add("bidprice", StringType)
```

在下一步中，我们将基于输入的 CSV 文件定义一个流数据源。我们指定上一步中定义的模式和其他必需的参数(使用选项)。我们还将每批处理的文件数量限制为一个:

```scala
scala> val streamingInputDF = spark.readStream.format("csv").schema(bidSchema).option("header", false).option("inferSchema", true).option("sep", "\t").option("maxFilesPerTrigger", 1).load("file:///Users/aurobindosarkar/Downloads/make-ipinyou-data-master/original-data/ipinyou.contest.dataset/bidfiles")
```

您可以像打印静态数据帧一样打印流式数据帧的模式:

```scala
scala> streamingInputDF.printSchema()root|-- bidid: string (nullable = true)|-- timestamp: string (nullable = true)|-- ipinyouid: string (nullable = true)|-- useragent: string (nullable = true)|-- IP: string (nullable = true)|-- region: integer (nullable = true)|-- cityID: integer (nullable = true)|-- adexchange: string (nullable = true)|-- domain: string (nullable = true)|-- turl: string (nullable = true)|-- urlid: string (nullable = true)|-- slotid: string (nullable = true)|-- slotwidth: string (nullable = true)|-- slotheight: string (nullable = true)|-- slotvisibility: string (nullable = true)|-- slotformat: string (nullable = true)|-- slotprice: string (nullable = true)|-- creative: string (nullable = true)|-- bidprice: string (nullable = true)
```

# 实现基于滑动窗口的功能

在本小节中，我们将介绍对流式数据的滑动窗口操作。

由于时间戳数据的格式不正确，我们将定义一个新的列，并将输入的时间戳字符串转换为正确的格式和类型，以便进行处理:

```scala
scala> val ts = unix_timestamp($"timestamp", "yyyyMMddHHmmssSSS").cast("timestamp")scala> val streamingCityTimeDF = streamingInputDF.withColumn("ts", ts).select($"cityID", $"ts")
```

接下来，我们将定义一个流查询，将输出写入标准输出。我们将在滑动窗口上定义聚合，在该窗口中，我们按窗口和城市标识对数据进行分组，并计算每个组的计数。

关于结构化流式编程的更详细描述，请参考[http://spark . Apache . org/docs/latest/structured-streaming-programming-guide . html .](http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)

在这里，我们统计每五分钟更新一次的 10 分钟窗口内的出价数量，即每五分钟滑动一次的 10 分钟窗口内收到的出价。使用窗口的流式查询如下所示:

```scala
scala> val windowedCounts = streamingCityTimeDF.groupBy(window($"ts", "10 minutes", "5 minutes"), $"cityID").count().writeStream.outputMode("complete").format("console").start()
```

输出被写入标准输出，因为我们在格式参数中使用了`console`关键字指定的`Console Sink`。输出包含窗口、城市标识和计算计数的列，如下所示。我们看到两个批次，因为我们在输入目录中放置了两个文件:

![](../images/00170.jpeg)

![](../images/00171.jpeg)

# 将流式数据集与静态数据集结合

在本小节中，我们将给出一个将流数据集与静态数据集结合的示例。我们将基于`cityID`加入数据集，以实现包含城市名称而不是`cityID`的用户友好输出。首先，我们为我们的城市记录定义一个模式，并从包含城市标识及其对应城市名称的 CSV 文件中创建静态数据帧:

```scala
scala> val citySchema = new StructType().add("cityID", StringType).add("cityName", StringType)scala> val staticDF = spark.read.format("csv").schema(citySchema).option("header", false).option("inferSchema", true).option("sep", "\t").load("file:///Users/aurobindosarkar/Downloads/make-ipinyou-data-master/original-data/ipinyou.contest.dataset/city.en.txt")
```

接下来，我们将加入流和静态数据帧，如图所示:

```scala
scala> val joinedDF = streamingCityTimeDF.join(staticDF, "cityID")
```

我们将执行前面的流查询，在连接的数据框中指定城市名称的列，而不是城市标识:

```scala
scala> val windowedCityCounts = joinedDF.groupBy(window($"ts", "10 minutes", "5 minutes"), $"cityName").count().writeStream.outputMode("complete").format("console").start()
```

结果如下。在这里，我们看到了一批输出数据，因为我们已经从源目录中删除了一个输入文件。在本章的剩余部分，我们将把处理限制在单个输入文件，以节省空间:

![](../images/00172.jpeg)

接下来，我们创建一个新的数据帧，其中包含时间戳列和先前创建的数据帧中的一些选定列:

```scala
scala> val streamingCityNameBidsTimeDF = streamingInputDF.withColumn("ts", ts).select($"ts", $"bidid", $"cityID", $"bidprice", $"slotprice").join(staticDF, "cityID") 
```

由于我们不计算聚合，只是希望将流式投标附加到结果中，因此我们使用`outputMode`“附加”而不是“完成”，如下所示:

```scala
scala> val cityBids = streamingCityNameBidsTimeDF.select($"ts", $"bidid", $"bidprice", $"slotprice", $"cityName").writeStream.outputMode("append").format("console").start()
```

![](../images/00173.jpeg)

# 在结构化流中使用数据集应用编程接口

到目前为止，我们已经对数据帧使用了非类型化的 API。为了使用类型化的 API，我们可以从使用数据框架切换到数据集。数据帧/数据集应用编程接口支持大多数流操作；但是，还不支持一些操作，如多个流聚合和不同的操作。其他如外部连接和排序是有条件支持的。

For a complete list of unsupported and conditionally supported operations, refer to [http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html).

在这里，我们给出了几个使用类型化 API 的例子。

首先，我们将定义一个名为`Bid`的`case`类:

```scala
scala> case class Bid(bidid: String, timestamp: String, ipinyouid: String, useragent: String, IP: String, region: Integer, cityID: Integer, adexchange: String, domain: String, turl: String, urlid: String, slotid: String, slotwidth: String, slotheight: String, slotvisibility: String, slotformat: String, slotprice: String, creative: String, bidprice: String)
```

我们可以使用上一步中定义的`case`类从流式数据帧定义流式数据集:

```scala
scala> val ds = streamingInputDF.as[Bid]
```

# 使用输出接收器

您可以将流式输出数据定向到各种输出接收器，包括文件、Foreach、控制台和内存接收器。通常，控制台和内存接收器用于调试目的。因为我们已经在前面的章节中使用了控制台接收器；这里我们将更详细地讨论其他接收器的使用。

# 使用 Foreach 接收器对输出进行任意计算

如果你想对输出执行任意计算，那么你可以使用`Foreach`接收器。为此，您需要实现`ForeachWriter`界面，如图所示。在我们的示例中，我们只需打印记录，但您也可以根据自己的要求执行其他计算:

```scala
import org.apache.spark.sql.ForeachWriterval writer = new ForeachWriter[String] {override def open(partitionId: Long, version: Long) = trueoverride def process(value: String) = println(value)override def close(errorOrNull: Throwable) = {}}
```

在下一步中，我们将使用上一步中定义的`Foreach`接收器实现一个示例。指定上一步中实现的`ForeachWriter`，如图所示:

```scala
scala> val dsForeach = ds.filter(_.adexchange == "3").map(_.useragent).writeStream.foreach(writer).start()
```

As a result, the user-agent information is displayed as shown:

![](../images/00174.gif)

# 使用内存接收器将输出保存到表中

如果要将输出数据保存为表格，可以使用内存接收器；这对于交互式查询非常有用。我们像以前一样定义流式数据帧。但是，我们将格式参数指定为`memory`和表名。最后，我们在表中执行一个 SQL 查询，如图所示:

```scala
scala> val aggAdexchangeDF = streamingInputDF.groupBy($"adexchange").count()scala> val aggQuery = aggAdexchangeDF.writeStream.queryName("aggregateTable").outputMode("complete").format("memory").start()scala> spark.sql("select * from aggregateTable").show()   
```

![](../images/00175.jpeg)

# 使用文件接收器将输出保存到分区表

我们还可以将输出保存为分区表。例如，我们可以按时间对输出进行分区，并将它们存储为 HDFS 的拼花文件。这里，我们展示了一个使用文件接收器将输出存储到拼花文件的例子。必须在给定命令中指定检查点目录位置:

```scala
scala> val cityBidsParquet = streamingCityNameBidsTimeDF.select($"bidid", $"bidprice", $"slotprice", $"cityName").writeStream.outputMode("append").format("parquet").option("path", "hdfs://localhost:9000/pout").option("checkpointLocation", "hdfs://localhost:9000/poutcp").start()
```

您可以检查 HDFS 文件系统的输出拼花文件和检查点文件，如图所示:

```scala
Aurobindos-MacBook-Pro-2:~ aurobindosarkar$ hdfs dfs -ls /pout
```

![](../images/00176.jpeg)

```scala
Aurobindos-MacBook-Pro-2:~ aurobindosarkar$ hdfs dfs -ls /poutcp
```

![](../images/00177.jpeg)

在下一节中，我们将探讨一些用于管理和监控流查询的有用特性。

# 监控流式查询

在此阶段，如果您列出系统中的活动流查询，您应该会看到以下输出:

```scala
scala> spark.streams.active.foreach(x => println("ID:"+ x.id + "             Run ID:"+ x.runId + "               Status: "+ x.status))ID:0ebe31f5-6b76-46ea-a328-cd0c637be49c             Run ID:6f203d14-2a3a-4c9f-9ea0-8a6783d97873               Status: {"message" : "Waiting for data to arrive","isDataAvailable" : false,"isTriggerActive" : false}ID:519cac9a-9d2f-4a01-9d67-afc15a6b03d2             Run ID:558590a7-cbd3-42b8-886b-cdc32bb4f6d7               Status: {"message" : "Waiting for data to arrive","isDataAvailable" : false,"isTriggerActive" : false}ID:1068bc38-8ba9-4d5e-8762-bbd2abffdd51             Run ID:bf875a27-c4d8-4631-9ea2-d51a0e7cb232               Status: {"message" : "Waiting for data to arrive","isDataAvailable" : false,"isTriggerActive" : false}ID:d69c4005-21f1-487a-9fe5-d804ca86f0ff             Run ID:a6969c1b-51da-4986-b5f3-a10cd2397784               Status: {"message" : "Waiting for data to arrive","isDataAvailable" : false,"isTriggerActive" : false}ID:1fa9e48d-091a-4888-9e69-126a2f1c081a             Run ID:34dc2c60-eebc-4ed6-bf25-decd6b0ad6c3               Status: {"message" : "Waiting for data to arrive","isDataAvailable" : false,  "isTriggerActive" : false}ID:a7ff2807-dc23-4a14-9a9c-9f8f1fa6a6b0             Run ID:6c8f1a83-bb1c-4dd7-897483042a286bae               Status: {"message" : "Waiting for data to arrive","isDataAvailable" : false,"isTriggerActive" : false}
```

我们还可以监控和管理特定的流查询，例如`windowedCounts`查询(一个`StreamingQuery`对象)，如图所示:

```scala
scala> // get the unique identifier of the running query that persists across restarts from checkpoint datascala> windowedCounts.id          res6: java.util.UUID = 0ebe31f5-6b76-46ea-a328-cd0c637be49cscala> // get the unique id of this run of the query, which will be generated at every start/restartscala> windowedCounts.runId       res7: java.util.UUID = 6f203d14-2a3a-4c9f-9ea0-8a6783d97873scala> // the exception if the query has been terminated with errorscala> windowedCounts.exception       res8: Option[org.apache.spark.sql.streaming.StreamingQueryException] = Nonescala> // the most recent progress update of this streaming queryscala> windowedCounts.lastProgress res9: org.apache.spark.sql.streaming.StreamingQueryProgress =
```

![](../images/00178.jpeg)

要停止流查询执行，可以执行`stop()`命令，如下所示:

```scala
scala> windowedCounts.stop()
```

在下一节中，我们将把重点转移到使用卡夫卡作为结构化流应用程序中输入数据流的来源。

# 将卡夫卡与火花结构化流结合使用

Apache Kafka 是一个分布式流媒体平台。它使您能够发布和订阅数据流，并在它们产生时处理和存储它们。Kafka 被业界广泛采用于网络规模的应用程序是因为其高吞吐量、低延迟、高可扩展性、高并发性、可靠性和容错特性。

# 介绍卡夫卡的概念

Kafka 通常用于构建实时流数据管道，以在系统之间可靠地移动数据，并对数据流进行转换和反应。Kafka 作为一个集群在一个或多个服务器上运行。

这里描述了卡夫卡的一些关键概念:

*   **主题**:消息发布到的类别或流名称的高级抽象。一个话题可以有`0`、`1`，也可以有很多订阅发布给它的消息的消费者。用户为每种新的邮件类别定义一个新的主题。

*   **生产者**:向某个主题发布消息的客户端。

*   **消费者**:消费来自某个主题的消息的客户端。

*   **代理**:一台或多台复制并保存消息数据的服务器。

此外，生产者和消费者可以同时读写多个主题。每个卡夫卡主题都是分区的，写入每个分区的消息都是连续的。分区中的消息具有唯一标识分区中每个消息的偏移量。

The reference site for Apache Kafka installation, tutorials, and examples is [https://kafka.apache.org/](https://kafka.apache.org/).

主题的分区是分布式的，每个代理处理共享分区的请求。每个分区通过可配置数量的代理进行复制。Kafka 集群在一段可配置的时间内保留所有发布的消息。Apache Kafka 使用 Apache ZooKeeper 作为其分布式流程的协调服务。

# 介绍动物园管理员的概念

ZooKeeper 是一个分布式、开源的分布式应用协调服务。它使开发人员不必从头开始实施协调服务。它使用共享的分层命名空间来允许分布式进程相互协调，并避免与竞争条件和死锁相关的错误。

The reference site for Apache ZooKeeper installation and tutorials is [https://zookeeper.apache.org/](https://zookeeper.apache.org/).

ZooKeeper 数据保存在内存中，因此具有非常高的吞吐量和低延迟。它在一组主机上复制，以提供高可用性。ZooKeeper 提供了一组保证，包括顺序一致性和原子性。

# 引入卡夫卡-火花集成

我们在这里展示一个简单的例子，让您熟悉卡夫卡-火花集成。本节使用的环境:Apache Spark 2.1.0 和 Apache Kafka 0.10.1.0(下载文件:`kafka_2.11-0.10.1.0.tgz)`)。

首先，我们使用 Apache Kafka 发行版提供的脚本启动一个单节点 ZooKeeper，如图所示:

```scala
bin/zookeeper-server-start.sh config/zookeeper.properties
```

在 Zookeeper 节点启动并运行之后，我们使用 Apache Kafka 发行版中可用的脚本启动 Kafka 服务器，如下所示:

```scala
bin/kafka-server-start.sh config/server.properties
```

接下来，我们创建一个名为`test`的主题，我们将向该主题发送消息供 Spark streaming 使用。对于我们的简单示例，我们将复制因子和分区数量都指定为`1`。我们可以使用可用于此目的的实用程序脚本，如图所示:

```scala
bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test
```

我们可以使用这个脚本查看主题列表(包括“测试”):

```scala
bin/kafka-topics.sh --list --zookeeper localhost:2181
```

接下来，我们启动一个基于命令行的生成器，向卡夫卡发送消息，如下所示。这里，每一行都作为单独的消息发送。当您键入每一行并按 enter(如图所示)时，您应该会看到每一行都出现在您的 Spark 流查询(在不同的窗口中运行)中:

```scala
bin/kafka-console-producer.sh --broker-list localhost:9092 --topic testThis is the first message.This is another message.
```

在一个单独的窗口中，用命令行中指定的合适的卡夫卡包启动 Spark shell，如图所示:

```scala
Aurobindos-MacBook-Pro-2:spark-2.1.0-bin-hadoop2.7 aurobindosarkar$ ./bin/spark-shell --packages org.apache.spark:spark-streaming-kafka-0-10_2.11:2.1.0,org.apache.spark:spark-sql-kafka-0-10_2.11:2.1.0
```

Spark shell 启动后，我们将创建一个流数据集，格式指定为“kafka”。此外，我们还将指定 Kafka 服务器及其运行的端口，并显式订阅我们之前创建的主题，如下所示。键和值字段被转换为字符串类型，以使输出可读:

```scala
scala> val ds1 = spark.readStream.format("kafka").option("kafka.bootstrap.servers", "localhost:9092").option("subscribe", "test").load().selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)").as[(String, String)]
```

接下来，我们将启动一个流查询，将流数据集输出到标准输出，如图所示:

```scala
scala> val query = ds1.writeStream.outputMode("append").format("console").start()
```

在卡夫卡制作人窗口中键入句子时，您应该会看到以下输出:

![](../images/00179.jpeg)

# 卡夫卡介绍——火花结构化流

然后，我们将提供卡夫卡-火花结构化流的另一个例子，其中我们将 iPinYou 投标文件的内容定向到生产者，如所示:

```scala
Aurobindos-MacBook-Pro-2:kafka_2.11-0.10.1.0 aurobindosarkar$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic connect-test < /Users/aurobindosarkar/Downloads/make-ipinyou-data-master/original-data/ipinyou.contest.dataset/bidfiles/bid.20130311.txt
```

我们还将创建一个名为`connect-test`的新主题，一个包含文件记录的新流数据集，以及一个在屏幕上列出它们的新流查询，如图所示:

```scala
scala> val ds2 = spark.readStream.format("kafka").option("kafka.bootstrap.servers", "localhost:9092").option("subscribe", "connect-test").load().selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)").as[(String, String)]scala> val query = ds2.writeStream.outputMode("append").format("console").start()
```

这里给出了截断的输出。当记录流入时，它们分布在多个批次中:

![](../images/00180.gif)

在下一节中，我们将创建一个用于访问任意流数据源的接收器。

# 为自定义数据源编写接收器

到目前为止，我们已经使用了在 Spark 中提供内置支持的数据源。但是，Spark Streaming 可以从任何任意来源接收数据，但是我们需要实现一个接收器来从自定义数据源接收数据。

在本节中，我们将为从**伦敦运输** ( **TfL** )站点提供的公共 API 定义一个自定义数据源。该网站为伦敦的每种交通方式提供统一的 API。这些应用编程接口提供对实时数据的访问，例如，铁路到达。输出以 XML 和 JSON 格式提供。我们将使用 APIs 对伦敦地铁特定线路的当前到达进行预测。

The reference site for TfL is [https://tfl.gov.uk](https://tfl.gov.uk); register on this site to generate an application key for accessing the APIs.

我们将从扩展抽象类`Receiver`和实现`onStart()`和`onStop()`方法开始。在`onStart()`方法中，我们启动负责接收数据的线程，在`onStop()`中，我们停止这些线程。`receive`方法使用 HTTP 客户端接收数据流，如图所示:

```scala
import org.apache.spark.storage.StorageLevelimport org.apache.spark.streaming.receiver.Receiverimport org.jfarcand.wcs.{TextListener, WebSocket}import scala.util.parsing.json.JSONimport scalaj.http.Httpimport java.io.BufferedReader;import java.io.IOException;import java.io.InputStreamReader;import org.apache.http.HttpResponse;import org.apache.http.client.ClientProtocolException;import org.apache.http.client.methods.HttpGet;import org.apache.http.impl.client.DefaultHttpClient;/*** Spark Streaming Example TfL Receiver*/class TFLArrivalPredictionsByLine() extends Receiver[String](StorageLevel.MEMORY_ONLY) with Runnable {//Replace the app_key parameter with your own keyprivate val tflUrl = "https://api.tfl.gov.uk/Line/circle/Arrivals?stopPointId=940GZZLUERC&app_id=a73727f3&app_key=xxx"@transientprivate var thread: Thread = _override def onStart(): Unit = {thread = new Thread(this)thread.start()}override def onStop(): Unit = {thread.interrupt()}override def run(): Unit = {while (true){receive();Thread.sleep(60*1000);}}private def receive(): Unit = {val httpClient = new DefaultHttpClient();val getRequest = new HttpGet(tflUrl);getRequest.addHeader("accept", "application/json");val response = httpClient.execute(getRequest);if (response.getStatusLine().getStatusCode() != 200) {throw new RuntimeException("Failed : HTTP error code : "+ response.getStatusLine().getStatusCode());}val br = new BufferedReader(new InputStreamReader((response.getEntity().getContent())));var output=br.readLine();while(output!=null){        println(output)output=br.readLine()} }}
```

以下对象创建`StreamingContext`并启动应用程序。`awaitTermination()`方法确保应用程序持续运行。

您可以使用 *Ctrl* + *C* 来终止应用程序:

```scala
import org.apache.spark.SparkConfimport org.apache.spark.streaming.{Seconds, StreamingContext}/*** Spark Streaming Example App*/object TFLStreamingApp {def main(args: Array[String]) {val conf = new SparkConf().setAppName("TFLStreaming")val ssc = new StreamingContext(conf, Seconds(300))val stream = ssc.receiverStream(new TFLArrivalPredictionsByLine())stream.print()if (args.length > 2) {stream.saveAsTextFiles(args(2))}ssc.start()ssc.awaitTermination()}}
```

这里列出了用于编译和打包应用程序的`sbt`文件:

```scala
name := "spark-streaming-example"version := "1.0"scalaVersion := "2.11.7"resolvers += "jitpack" at "https://jitpack.io"libraryDependencies ++= Seq("org.apache.spark" %% "spark-core" % "2.0.0",       "org.apache.spark" %% "spark-streaming" % "2.0.0","org.apache.httpcomponents" % "httpclient" % "4.5.2","org.scalaj" %% "scalaj-http" % "2.2.1","org.jfarcand" % "wcs" % "1.5")
```

我们使用`spark-submit`命令来执行我们的应用程序，如下所示:

```scala
Aurobindos-MacBook-Pro-2:scala-2.11 aurobindosarkar$ /Users/aurobindosarkar/Downloads/spark-2.2.0-bin-hadoop2.7/bin/spark-submit --class TFLStreamingApp --master local[*] spark-streaming-example_2.11-1.0.jar
```

流式节目的输出如下:

![](../images/00181.jpeg)

# 摘要

在本章中，我们介绍了流数据应用。我们提供了几个使用 Spark SQL 数据框架/数据集 API 构建流应用程序的例子。此外，我们展示了卡夫卡在结构化流应用程序中的使用。最后，我们展示了一个为自定义数据源创建接收器的示例。

在下一章中，我们将把重点转移到在机器学习应用程序中使用 Spark SQL。具体来说，我们将探索特征工程和机器学习管道的关键概念。