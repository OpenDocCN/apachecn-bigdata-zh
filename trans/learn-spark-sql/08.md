# 使用火花 SQL 和火花

许多数据科学家使用 R 来执行探索性数据分析、数据可视化、数据管理、数据处理和机器学习任务。SparkR 是一个 R 包，通过利用 Apache Spark 的分布式处理能力，使从业者能够处理数据。在本章中，我们将介绍 SparkR(一个 R 前端包)，它利用 Spark 的引擎来执行大规模的数据分析。我们还将描述 SparkR 的设计和实现的关键元素。

更具体地说，在本章中，您将学习以下主题:

*   什么是 SparkR？
*   了解 SparkR 架构
*   理解迷你图数据框
*   使用 SparkR 进行**探索性数据分析** ( **EDA** )和数据收集任务
*   使用 SparkR 进行数据可视化
*   使用 SparkR 进行机器学习

# 介绍 SparkR

r 是一种用于统计计算和数据可视化的语言和环境。它是统计学家和数据科学家最常用的工具之一。r 是开源的，提供了一个动态的交互环境，具有丰富的包和强大的可视化功能。这是一种解释语言，包括对数值计算的广泛支持，以及用于执行数值运算的向量、矩阵、数组和库的数据类型。

r 为使用数据帧的结构化数据处理提供支持。r 数据帧使数据操作更简单、更方便。然而，R 的动态设计限制了可能的优化程度。此外，交互式数据分析能力和整体可扩展性也受到限制，因为 R 运行时是单线程的，只能处理适合单台机器内存的数据集。

For more details on R, refer to the *R project website* at [https://www.r-project.org/about.html](https://www.r-project.org/about.html).

SparkR 解决了这些缺点，使数据科学家能够在分布式环境中大规模处理数据。SparkR 是一个 R 包，它提供了一个轻量级的前端，因此您可以使用来自 R 的 Apache Spark。它将 Spark 的分布式处理功能、与各种数据源的轻松连接以及内存外数据结构与 R 的动态环境、交互性、包和可视化功能相结合。

传统上，数据科学家一直将 R 与其他框架一起使用，如 Hadoop MapReduce、Hive、Pig 等。然而，有了 SparkR，他们可以避免使用多个大数据工具和平台，并使用多种不同的语言来完成他们的目标。SparkR 允许他们在 R 中工作，并使用 Spark 的分布式计算模型。

SparkR 接口类似于 R and R 包，而不是我们目前遇到的 Python/Scala/Java 接口。SparkR 实现了一个分布式数据框架，支持在大型数据集上的操作，如统计计算、列选择、SQL 执行、过滤行、执行聚合等。

SparkR 支持到/从本地 R 数据帧的转换。SparkR 与 Spark 项目的紧密集成使 SparkR 能够重用其他 Spark 模块，包括 Spark SQL、MLlib 等。此外，火花 SQL 数据源应用编程接口支持从各种来源读取输入，如 HDFS、HBase、卡珊德拉和文件格式，如 CSV、JSON、Parquet、Avro 等。

在下一节中，我们将简要介绍 SparkR 体系结构。

# 了解 SparkR 架构

SparkR 的分布式数据框架支持 R 用户熟悉的编程语法。高级数据框架应用编程接口将 R 应用编程接口与火花中优化的 SQL 执行引擎集成在一起。

SparkR 的体系结构主要由两个组件组成:驱动程序上的 R 到 JVM 绑定，使 R 程序能够向 Spark 集群提交作业，以及支持在 Spark 执行器上运行 R。

![](../images/00223.jpeg)

SparkR 的设计包括支持在 Spark 执行器机器上启动 R 进程。但是，在计算结果之后，序列化查询和反序列化结果会产生开销。随着 R 和 JVM 之间传输的数据量的增加，这些开销也会变得更加显著。但是，缓存可以在 SparkR 中实现高效的交互式查询处理。

For a detailed description of SparkR design and implementation, refer: "SparkR: Scaling R Programs with Spark" by Shivaram Venkataraman1, Zongheng Yang, *et al,* available at: [https://cs.stanford.edu/~matei/papers/2016/sigmod_sparkr.pdf.](https://cs.stanford.edu/~matei/papers/2016/sigmod_sparkr.pdf)

在下一节中，我们将概述 SparkR 的分布式数据框架组件 Spark 数据框架。

# 理解迷你图数据框

SparkR 的主要组件是一个名为 **SparkR 数据帧**的分布式数据帧。Spark 数据框应用编程接口类似于本地 R 数据框，但使用 Spark 的执行引擎和关系查询优化器可扩展到大型数据集。它是数据的分布式集合，组织成类似于关系数据库表或 R DataFrame 的列。

Spark 数据框可以从许多不同的数据源创建，如数据文件、数据库、R 数据框等。加载数据后，开发人员可以使用熟悉的 R 语法来执行各种操作，如筛选、聚合和合并。SparkR 对 DataFrame 操作执行惰性评估。

此外，SparkR 支持数据帧上的许多功能，包括统计功能。我们也可以使用 magrittr 之类的库来链接命令。开发人员可以使用 SQL 命令在 SparkR 数据帧上执行 SQL 查询。最后，可以使用 collect 操作符将 SparkR 数据帧转换为本地 R 数据帧。

在下一节中，我们将介绍 EDA 和数据管理任务中使用的典型 SparkR 编程操作。

# 将 SparkR 用于电子设计自动化和数据管理任务

在本节中，我们将使用 Spark SQL 和 SparkR 对我们的数据集进行初步探索。本章中的示例使用几个公开可用的数据集来说明操作，并且可以在 SparkR shell 中运行。

迷你图的入口点是迷你图会话。它将 R 程序连接到一个 Spark 集群。如果您在迷你图外壳中工作，迷你图会话已经为您创建。

这时，启动 SparkR shell，如图所示:

```scala
Aurobindos-MacBook-Pro-2:spark-2.2.0-bin-hadoop2.7 aurobindosarkar$./bin/SparkR
```

您可以在 SparkR shell 中安装所需的库，如 ggplot2，如图所示:

```scala
> install.packages('ggplot2', dep = TRUE)
```

# 读写火花数据帧

SparkR 支持通过 Spark DataFrames 界面对各种数据源进行操作。SparkR 的数据帧支持多种方法来读取输入、执行结构化数据分析以及将数据帧写入分布式存储。

`read.df`方法可用于从各种数据源创建火花数据帧。我们需要指定输入数据文件的路径和数据源的类型。数据源应用编程接口本身支持格式，如 CSV、JSON 和 Parquet。

完整的功能列表可以在 http://spark.apache.org/docs/latest/api/R/的应用编程接口文档中找到。对于最初的代码示例，我们将使用来自[第 3 章](03.html#1AT9A0-e9cbc07f866e437b8aa14e841622275c)、*使用 Spark SQL 进行数据探索、*的数据集，其中包含与葡萄牙银行机构的直接营销活动(电话)相关的数据。

输入文件为**逗号分隔值** ( **CSV** )格式，包含一个标题，字段由分号分隔。输入文件可以是任何一个 Spark 数据源；例如，如果是 JSON 或 Parquet 格式，那么我们只需要将源参数分别更改为`json`或`parquet`。

我们可以通过使用`read.df`加载我们的输入 CSV 文件来创建`SparkDataFrame`，如图所示:

```scala
> csvPath <- "file:///Users/aurobindosarkar/Downloads/bank-additional/bank-additional-full.csv"> df <- read.df(csvPath, "csv", header = "true", inferSchema = "true", na.strings = "NA", delimiter= ";")
```

同样，我们可以使用`write.df`将数据帧写入分布式存储。我们在源参数中指定输出数据帧的名称和格式(如`read.df`函数)。

数据源应用编程接口可用于将火花数据帧保存为多种不同的文件格式。例如，我们可以使用`write.df`将上一步创建的火花数据帧保存到拼花文件中:

```scala
write.df(df, path = "hdfs://localhost:9000/Users/aurobindosarkar/Downloads/df.parquet", source = "parquet", mode = "overwrite")
```

`read.df`和`write.df`功能分别用于将数据从存储区带到工作区和将数据从工作区写入存储区。它不会将这些数据带入 R 过程。

# 探索火花数据帧的结构和内容

在本节中，我们将探讨 Spark 数据框架中包含的维度、模式和数据。

首先，我们使用缓存或持久函数来缓存火花数据帧以获得性能。我们还可以指定存储级别选项，如`DISK_ONLY`、`MEMORY_ONLY`、`MEMORY_AND_DISK`等，如下图所示:

```scala
> persist(df, "MEMORY_ONLY")
```

通过键入数据框的名称，我们可以列出 Spark 数据框的列和相关数据类型，如下所示:

```scala
> df
```

spark data frames[年龄:`int`、工作:`string`、婚姻:`string`、教育:`string`、违约:`string`、住房:`string`、贷款:`string`、联系人:`string`、月份:`string`、周 _ 日:`string`、持续时间:`int`、活动:`int`、pday:`int`、先前:`int`、poutcome: `string`、emp.var.rate: `double`、cons.price.idx::

SparkR 可以从输入文件的标题行自动推断模式。我们可以打印数据框模式，如下所示:

```scala
> printSchema(df)
```

![](../images/00224.gif)

我们还可以使用`names`函数显示数据框中的列名，如下所示:

```scala
> names(df)
```

![](../images/00225.gif)

接下来，我们显示一些示例值(来自每个列)和来自 Spark 数据框的记录，如下所示:

```scala
> str(df)
```

![](../images/00226.gif)

```scala
> head(df, 2)
```

![](../images/00227.gif)

我们可以显示数据框的尺寸，如图所示。使用`MEMORY_ONLY`选项在缓存或持久功能后执行 dim 是确保数据帧被加载并保存在内存中以便更快操作的好方法:

```scala
> dim(df)[1] 41188 21
```

我们还可以使用计数或`nrow`函数来计算数据框中的行数:

```scala
> count(df)[1] 41188> nrow(df)[1] 41188
```

此外，我们可以使用`distinct`函数获取指定列中包含的不同值的数量:

```scala
> count(distinct(select(df, df$age)))[1] 78
```

# 在火花数据帧上运行基本操作

在本节中，我们使用 SparkR 对 Spark 数据帧执行一些基本操作，包括聚合、拆分和采样。例如，我们可以从数据框中选择感兴趣的列。这里，我们只从数据框中选择`education`列:

```scala
> head(select(df, df$education))education1 basic.4y2 high.school3 high.school4 basic.6y5 high.school6 basic.9y
```

或者，我们也可以指定列名，如下所示:

```scala
> head(select(df, "education"))
```

我们可以使用子集函数选择满足一定条件的行，例如状态为`marital``married`的行，如下图所示:

```scala
> subsetMarried <- subset(df, df$marital == "married")> head(subsetMarried, 2)
```

![](../images/00228.gif)

我们可以使用过滤功能只保留`education`级`basic.4y`的行，如下图所示:

```scala
> head(filter(df, df$education == "basic.4y"), 2)
```

![](../images/00229.gif)

SparkR 数据框支持对分组后的数据进行多种常见聚合。例如，我们可以计算数据集中`marital`状态值的直方图，如下所示。在这里，我们使用`n`运算符来统计每个婚姻状态出现的次数:

```scala
> maritaldf <- agg(groupBy(df, df$marital), count = n(df$marital))
```

```scala
> head(maritaldf)marital count1 unknown 802 divorced 46123 married 249284 single 11568
```

我们还可以对聚合的输出进行排序，以获得最常见的一组婚姻状态，如下所示:

```scala
> maritalCounts <- summarize(groupBy(df, df$marital), count = n(df$marital))> nMarriedCategories <- count(maritalCounts)> head(arrange(maritalCounts, desc(maritalCounts$count)), num = nMarriedCategories)marital count1 married 249282 single 115683 divorced 46124 unknown 80
```

接下来，我们使用`magrittr`包来流水线化函数，而不是嵌套它们，如下所示。

首先，使用`install.packages`命令安装`magrittr`软件包，如果软件包尚未安装:

```scala
> install.packages("magrittr")
```

请注意，在 R 中加载和附加新包时，可能会有名称冲突，其中一个函数正在屏蔽另一个函数。根据两个包的加载顺序，先加载的包中的一些函数被后加载的包中的函数屏蔽。在这种情况下，我们需要在此类呼叫前加上包名:

```scala
> library(magrittr)
```

我们将`filter`、`groupBy`和`summarize`功能进行管道化，如下例所示:

```scala
> educationdf <- filter(df, df$education == "basic.4y") %>% groupBy(df$marital) %>% summarize(count = n(df$marital))> head(educationdf)
```

![](../images/00230.gif)

接下来，我们从目前为止使用的分布式 Spark 版本创建一个本地数据帧。我们使用`collect`功能将火花数据帧移动到火花驱动器上的本地/R 数据帧，如图所示。通常，您会在将数据移动到本地数据框之前对其进行汇总或取样:

```scala
> collect(summarize(df,avg_age = mean(df$age)))avg_age1 40.02406
```

我们可以从我们的数据帧创建一个`sample`，并将其移动到本地数据帧，如图所示。这里，我们获取 10%的输入记录，并从中创建一个本地数据帧:

```scala
> ls1df <- collect(sample(df, FALSE, 0.1, 11L))> nrow(df)[1] 41188> nrow(ls1df)[1] 4157
```

SparkR 还提供了许多可以直接应用于列的函数，用于数据处理和聚合。

例如，我们可以在 DataFrame 中添加一个新列，其中包含一个新列，调用持续时间从秒转换为分钟，如下所示:

```scala
> df$durationMins <- round(df$duration / 60)> head(df, 2)
```

以下是获得的输出:

![](../images/00231.gif)

# 在火花数据帧上执行 SQL 语句

火花数据帧也可以注册为火花 SQL 中的临时视图，允许我们对其数据运行 SQL 查询。`sql`函数使应用程序能够以编程方式运行 SQL 查询，并将结果作为火花数据帧返回。

首先，我们将火花数据框注册为临时视图:

```scala
> createOrReplaceTempView(df, "customer")
```

接下来，我们使用`sql`函数执行 SQL 语句。例如，我们为 13 至 19 岁的客户选择`education`、`age`、`marital`、`housing`和`loan`列，如下图所示:

```scala
> sqldf <- sql("SELECT education, age, marital, housing, loan FROM customer WHERE age >= 13 AND age <= 19")> head(sqldf)
```

![](../images/00232.gif)

# 合并迷你图数据框

我们可以使用操作参数`by`和`by.x` / `by.y`明确指定 SparkR 应该合并数据帧的列。合并操作根据值`all.x`和`all.y`确定 SparkR 应该如何合并数据框，这两个值分别表示`x`和`y`中的哪些行应该包含在连接中。例如，我们可以通过显式指定`all.x = FALSE`、`all.y = FALSE`来指定一个`inner join`(默认)，或者用`all.x = TRUE`、`all.y = FALSE`来指定一个左边的`outer join`。

For more details on join and merge operations, refer to [https://github.com/UrbanInstitute/sparkr-tutorials/blob/master/merging.md.](https://github.com/UrbanInstitute/sparkr-tutorials/blob/master/merging.md)

或者，我们也可以使用`join`操作逐行合并数据帧。

对于以下示例，我们使用了可从以下网址获得的犯罪数据集:[https://archive . ics . UCI . edu/ml/Datasets/Communities+和+Crime+Unnormalized](https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized) 。

像以前一样，我们读入输入数据集，如下所示:

```scala
> library(magrittr)> csvPath <- "file:///Users/aurobindosarkar/Downloads/CommViolPredUnnormalizedData.csv"> df <- read.df(csvPath, "csv", header = "false", inferSchema = "false", na.strings = "NA", delimiter= ",")
```

接下来，我们选择与犯罪类型相关的特定列，并将默认列名重命名为更有意义的名称，如下所示:

```scala
> crimesStatesSubset = subset(df, select = c(1,2, 130, 132, 134, 136, 138, 140, 142, 144))
```

![](../images/00233.gif)

```scala
> head(crimesStatesdf, 2)
```

![](../images/00234.gif)

接下来，我们读入一个包含美国各州名称的数据集，如下所示:

```scala
> state_names <- read.df("file:///Users/aurobindosarkar/downloads/csv_hus/states.csv", "csv", header = "true", inferSchema = "true", na.strings = "NA", delimiter= ",")
```

我们使用 names 函数列出了两个数据帧的列。两个数据框之间的公共列是“代码”列(包含状态代码):

```scala
> names(crimesStatesdf)[1] "comm" "code" "nmurders" "nrapes" "nrobberies"[6] "nassaults" "nburglaries" "nlarcenies" "nautothefts" "narsons"> names(state_names)[1] "st" "name" "code"
```

接下来，我们使用公共列执行内部连接:

```scala
> m1df <- merge(crimesStatesdf, state_names)> head(m1df, 2)
```

![](../images/00235.gif)

这里，我们基于显式指定表达式来执行内部连接:

```scala
> m2df <- merge(crimesStatesdf, state_names, by = "code")> head(m2df, 2)
```

![](../images/00236.gif)

For the following example, we use the tennis tournament match statistics Dataset available at: [http://archive.ics.uci.edu/ml/Datasets/Tennis+Major+Tournament+Match+Statistics.](http://archive.ics.uci.edu/ml/datasets/Tennis+Major+Tournament+Match+Statistics)

以下是获得的输出:

![](../images/00237.jpeg)

# 使用用户定义函数

在 SparkR 中，支持多种类型的**用户自定义功能** ( **UFDs** )。例如，我们可以使用`dapply`或`dapplyCollect`在大数据集上运行给定的函数。`dapply`函数对火花数据帧的每个分区应用一个函数。函数的输出应该是 data.frame。

该模式指定生成的火花数据帧的行格式:

```scala
> df1 <- select(df, df$duration)> schema <- structType(structField("duration", "integer"),+ structField("durMins", "double"))> df2 <- dapply(df1, function(x) { x <- cbind(x, x$duration / 60) }, schema)> head(collect(df2))
```

![](../images/00238.gif)

与 dapply 类似，`dapplyCollect`函数将该函数应用于 Spark 数据帧的每个分区，并将结果收集回来。函数的输出应该是一个`data.frame`，模式参数不是必需的。请注意，`dapplyCollect`如果 UDF 的输出不能传输给驾驶员或放入驾驶员的记忆中，则可能会失败。

我们可以使用`gapply`或`gapplyCollect`在按输入列分组的大数据集上运行给定的函数。在以下示例中，我们确定了一组最大持续时间值:

```scala
> df1 <- select(df, df$duration, df$age)> schema <- structType(structField("age", "integer"), structField("maxDuration", "integer"))> result <- gapply(+ df1,+ "age",+ function(key, x) {+ y <- data.frame(key, max(x$duration))+ },+ schema)> head(collect(arrange(result, "maxDuration", decreasing = TRUE)))
```

![](../images/00239.gif)

`gapplyCollect`类似地对火花数据帧的每个分区应用一个函数，但也将结果收集回`R data.frame`。

在下一节中，我们将介绍 SparkR 函数来计算示例数据集的汇总统计信息。

# 使用 SparkR 计算汇总统计信息

描述(或汇总)操作会创建一个新的数据框，其中包含指定数据框或数值列列表的计数、平均值、最大值、平均值和标准偏差值:

```scala
> sumstatsdf <- describe(df, "duration", "campaign", "previous", "age")> showDF(sumstatsdf)
```

![](../images/00240.gif)

在大型数据集上计算这些值的计算成本很高。因此，我们在此呈现这些统计度量的单独计算:

```scala
> avgagedf <- agg(df, mean = mean(df$age))> showDF(avgagedf) # Print this DF+-----------------+| mean            |+-----------------+|40.02406040594348|+-----------------+
```

接下来，我们创建一个列出最小值、最大值和范围宽度的数据框:

```scala
> agerangedf <- agg(df, minimum = min(df$age), maximum = max(df$age), range_width = abs(max(df$age) - min(df$age)))> showDF(agerangedf)
```

接下来，我们计算样本方差和标准偏差，如下所示:

```scala
> agevardf <- agg(df, variance = var(df$age))> showDF(agevardf)+------------------+| variance         |+------------------+|108.60245116511807|+------------------+> agesddf <- agg(df, std_dev = sd(df$age))> showDF(agesddf)+------------------+| std_dev          |+------------------+|10.421249980934057|+------------------+
```

操作`approxQuantile`返回数据帧列的近似分位数。我们使用概率参数和`relativeError`参数的可接受误差来指定要近似的分位数。我们定义了一个新的数据框`df1`，它删除了`age`的缺失值，然后计算近似的`Q1`、`Q2`和`Q3`值，如下所示:

```scala
> df1 <- dropna(df, cols = "age")> quartilesdf <- approxQuantile(x = df1, col = "age", probabilities = c(0.25, 0.5, 0.75), relativeError = 0.001)> quartilesdf[[1]][1] 32[[2]][1] 38[[3]][1] 47
```

我们可以使用`skewness`操作来测量列分布中偏斜的大小和方向。在以下示例中，我们测量了`age`列的偏斜度:

```scala
> ageskdf <- agg(df, skewness = skewness(df$age))> showDF(ageskdf)+------------------+| skewness         |+------------------+|0.7846682380932389|+------------------+
```

同样，我们可以测量一个柱的峰度。这里，我们测量`age`列的峰度:

```scala
> agekrdf <- agg(df, kurtosis = kurtosis(df$age))> showDF(agekrdf)+------------------+| kurtosis         |+------------------+|0.7910698035274022|+------------------+
```

接下来，我们计算两个数据帧列之间的样本协方差和相关性。这里，我们计算`age`和`duration`列之间的协方差和相关性:

```scala
> covagedurdf <- cov(df, "age", "duration")> corragedurdf <- corr(df, "age", "duration", method = "pearson")> covagedurdf[1] -2.339147> corragedurdf[1] -0.000865705
```

接下来，我们为作业列创建一个相对频率表。每个不同职务类别值的相对频率显示在百分比列中:

```scala
> n <- nrow(df)> jobrelfreqdf <- agg(groupBy(df, df$job), Count = n(df$job), Percentage = n(df$job) * (100/n))> showDF(jobrelfreqdf)
```

![](../images/00241.gif)

最后，我们使用操作`crosstab`在两个分类列之间创建一个列联表。在以下示例中，我们为职务和婚姻列创建了一个应急表:

```scala
> contabdf <- crosstab(df, "job", "marital")> contabdf
```

![](../images/00242.gif)

在下一节中，我们使用 SparkR 来执行各种数据可视化任务。

# 使用 SparkR 进行数据可视化

ggplot2 包的 SparkR 扩展`ggplot2.SparkR`，允许 SparkR 用户构建强大的可视化效果。

在本节中，我们使用各种图来可视化我们的数据。此外，我们还展示了在地图上绘制数据和可视化图形的示例:

```scala
> csvPath <- "file:///Users/aurobindosarkar/Downloads/bank-additional/bank-additional-full.csv"> df <- read.df(csvPath, "csv", header = "true", inferSchema = "true", na.strings = "NA", delimiter= ";")> persist(df, "MEMORY_ONLY")> require(ggplot2)
```

Refer to the ggplot website for different options available to improve the displays of each of your plots at [http://docs.ggplot2.org](http://docs.ggplot2.org).

在下一步中，我们绘制一个基本条形图，给出数据中不同婚姻状态的频率计数:

```scala
> ldf <- collect(select(df, df$age, df$duration, df$education, df$marital, df$job))> g1 <- ggplot(ldf, aes(x = marital))> g1 + geom_bar()
```

![](../images/00243.jpeg)

在以下示例中，我们绘制了年龄列的直方图和几个条形图，给出了教育、婚姻状况和工作值的频率计数:

```scala
> library(MASS)> par(mfrow=c(2,2))> truehist(ldf$"age", h = 5, col="slategray3", xlab="Age Groups(5 years)")> barplot((table(ldf$education)), names.arg=c("1", "2", "3", "4", "5", "6", "7", "8"), col=c("slateblue", "slateblue2", "slateblue3", "slateblue4", "slategray", "slategray2", "slategray3", "slategray4"), main="Education")> barplot((table(ldf$marital)), names.arg=c("Divorce", "Married", "Single", "Unknown"), col=c("slategray", "slategray1", "slategray2", "slategray3"), main="Marital Status")> barplot((table(ldf$job)), , names.arg=c("1", "2", "3", "4", "5", "6", "7", "8", "9", "a", "b", "c"), main="Job")
```

![](../images/00244.jpeg)

以下表达式创建了一个条形图，描述了按婚姻类型分组的教育程度的比例频率:

```scala
> g2 <- ggplot(ldf, aes(x = marital, fill = education))> g2 + geom_bar(position = "fill")
```

![](../images/00245.jpeg)

以下表达式绘制了一个直方图，给出了数据中入库年龄值的频率计数:

```scala
> g3 <- ggplot(ldf, aes(age))> g3 + geom_histogram(binwidth=5)
```

![](../images/00246.jpeg)

以下表达式返回一个频率多边形，相当于之前绘制的直方图:

```scala
> g3 + geom_freqpoly(binwidth=5)
```

![](../images/00247.jpeg)

以下表达式给出了不同婚姻状况类型的呼叫持续时间值的箱线图:

```scala
> g4 <- ggplot(ldf, aes(x = marital, y = duration))> g4 + geom_boxplot()
```

![](../images/00248.jpeg)

以下表达式反映了不同教育水平下的年龄直方图:

```scala
> g3 + geom_histogram() + facet_wrap(~education)
```

![](../images/00249.jpeg)

在以下示例中，我们同时显示了不同列的几个箱线图:

```scala
> par(mfrow=c(1,2))> boxplot(ldf$age, col="slategray2", pch=19, main="Age")> boxplot(ldf$duration, col="slategray2", pch=19, main="Duration")
```

![](../images/00250.jpeg)

在 SparkR 中构建表达式或函数时，我们应该避免计算量大的操作。例如，即使 SparkR 中的收集操作允许我们利用 ggplot2 特性，我们也应该尽可能少地收集数据，因为我们需要确保操作结果适合单个节点的可用内存。

以下散点图的问题在于过度绘制。这些点一个接一个地绘制，扭曲了绘图的视觉外观。我们可以调整 alpha 参数的值以使用透明点:

```scala
> ggplot(ldf, aes(age, duration)) + geom_point(alpha = 0.3) + stat_smooth()I
```

![](../images/00251.jpeg)

要显示为一组二维面板或将面板包装成多行，我们使用`facet_wrap`:

```scala
> ageAndDurationValuesByMarital <- ggplot(ldf, aes(age, duration)) + geom_point(alpha = "0.2") + facet_wrap(~marital)> ageAndDurationValuesByMarital
```

![](../images/00252.jpeg)

调整后的α值改善了散点图的可视化；但是，我们可以将这些点总结为平均值，并绘制它们以获得更清晰的可视化效果，如下例所示:

```scala
> createOrReplaceTempView(df, "customer")> localAvgDurationEducationAgeDF <- collect(sql("select education, avg(age) as avgAge, avg(duration) as avgDuration from customer group by education"))> avgAgeAndDurationValuesByEducation <- ggplot(localAvgDurationEducationAgeDF, aes(group=education, x=avgAge, y=avgDuration)) + geom_point() + geom_text(data=localAvgDurationEducationAgeDF, mapping=aes(x=avgAge, y=avgDuration, label=education), size=2, vjust=2, hjust=0.75)> avgAgeAndDurationValuesByEducation
```

![](../images/00253.jpeg)

在下一个例子中，我们创建一个密度图，并覆盖一条通过平均值的线。密度图是查看变量分布的好方法；例如，在我们的示例中，我们绘制了呼叫持续时间值:

```scala
> plot(density(ldf$duration), main = "Density Plot", xlab = "Duration", yaxt = 'n')> abline(v = mean(ldf$duration), col = 'green', lwd = 2)> legend('topright', legend = c("Actual Data", "Mean"), fill = c('black', 'green'))
```

![](../images/00254.jpeg)

在下一节中，我们将展示一个在地图上绘制值的示例。

# 在地图上可视化数据

在本节中，我们将介绍如何合并两个数据集并将结果绘制在地图上:

```scala
> csvPath <- "file:///Users/aurobindosarkar/Downloads/CommViolPredUnnormalizedData.csv"> df <- read.df(csvPath, "csv", header = "false", inferSchema = "false", na.strings = "NA", delimiter= ",")> persist(df, "MEMORY_ONLY")> xdf = select(df, "_c1","_c143")> newDF <- withColumnRenamed(xdf, "_c1", "state")> arsonsstatesdf <- withColumnRenamed(newDF, "_c143", "narsons")
```

我们要可视化的数据集是按状态划分的平均 arsons 数，计算如下:

```scala
> avgArsons <- collect(agg(groupBy(arsonsstatesdf, "state"), AVG_ARSONS=avg(arsonsstatesdf$narsons)))
```

接下来，我们将`states.csv`数据集读入一个 R 数据帧:

```scala
> state_names <- read.csv("file:///Users/aurobindosarkar/downloads/csv_hus/states.csv")
```

接下来，我们使用一个`factor`变量将状态代码替换为状态名称:

```scala
> avgArsons$region <- factor(avgArsons$state, levels=state_names$code, labels=tolower(state_names$name))
```

要创建一张美国地图，其中各州根据每个州的平均纵火数量进行着色，我们可以使用`ggplot2's map_data`功能:

```scala
> states_map <- map_data("state")
```

最后，我们将数据集与地图合并，使用`ggplot`显示地图，如下图所示:

```scala
> merged_data <- merge(states_map, avgArsons, by="region")> ggplot(merged_data, aes(x = long, y = lat, group = group, fill = AVG_ARSONS)) + geom_polygon(color = "white") + theme_bw()
```

![](../images/00255.jpeg)

For more on plotting on geographical maps, refer to the *Exploring geographical data using SparkR and ggplot2* by Jose A. Dianes at [https://www.codementor.io/spark/tutorial/exploratory-geographical-data-using-sparkr-and-ggplot2](https://www.codementor.io/spark/tutorial/exploratory-geographical-data-using-sparkr-and-ggplot2).

在下一节中，我们将展示一个图形可视化的例子。

# 可视化图形节点和边

将图形可视化以获得整体结构属性的感觉是关键。在本节中，我们将在 SparkR shell 中绘制几个图表。

For more details, refer to *Static and dynamic network visualization with R* by Katherine Ognynova at [http://kateto.net/network-visualization](http://kateto.net/network-visualization). For the following example, we use a Dataset containing the network of interactions on the stack exchange website Ask Ubuntu available at: [https://snap.stanford.edu/data/sx-askubuntu.html](https://snap.stanford.edu/data/sx-askubuntu.html).

我们从百分之十的数据样本创建一个本地数据帧，并创建一个图表，如下所示:

```scala
> library(igraph)> library(magrittr)> inDF <- read.df("file:///Users/aurobindosarkar/Downloads/sx-askubuntu.txt", "csv", header="false", delimiter=" ")> linksDF <- subset(inDF, select = c(1, 2)) %>% withColumnRenamed("_c0", "src") %>% withColumnRenamed("_c1", "dst")> llinksDF <- collect(sample(linksDF, FALSE, 0.01, 1L))> g1 <- graph_from_data_frame(llinksDF, directed = TRUE, vertices = NULL)> plot(g1, edge.arrow.size=.001, vertex.label=NA, vertex.size=0.1)
```

![](../images/00256.jpeg)

在本例中，我们可以通过进一步减小样本大小并移除某些边缘(如循环)来获得更清晰的可视化效果，如下所示:

```scala
> inDF <- read.df("file:///Users/aurobindosarkar/Downloads/sx-askubuntu.txt", "csv", header="false", delimiter=" ")> linksDF <- subset(inDF, select = c(1, 2)) %>% withColumnRenamed("_c0", "src") %>% withColumnRenamed("_c1", "dst")> llinksDF <- collect(sample(linksDF, FALSE, 0.0005, 1L))> g1 <- graph_from_data_frame(llinksDF, directed = FALSE)> g1 <- simplify(g1, remove.multiple = F, remove.loops = T)> plot(g1, edge.color="black", vertex.color="red", vertex.label=NA, vertex.size=2)
```

![](../images/00257.jpeg)

在下一节中，我们将探讨如何将 SparkR 用于机器学习任务。

# 使用 SparkR 进行机器学习

SparkR 支持越来越多的机器学习算法，如**广义线性模型** ( **glm** )、朴素贝叶斯模型、K-Means 模型、Logistic 回归模型、**潜在狄利克雷分配** ( **LDA** )模型、多层感知器分类模型、用于回归分类的梯度提升树模型、用于回归分类的随机森林模型、**交替最小二乘** ( **ALS** )矩阵分解模型等。

SparkR 使用 Spark MLlib 来训练模型。汇总和预测功能分别用于打印拟合模型的汇总和对新数据进行预测。`write.ml` / `read.ml`操作可用于保存/加载装配好的模型。SparkR 还支持模型拟合可用 R 公式运算符的子集，如`~`、`.`、`:`、`+`和`-`。

对于以下示例，我们使用了 https://archive.ics.uci.edu/ml/Datasets/Wine+Quality 的葡萄酒质量数据集:

```scala
> library(magrittr)> csvPath <- "file:///Users/aurobindosarkar/Downloads/winequality/winequality-white.csv"
```

![](../images/00258.gif)

```scala
> winedf <- mutate(indf, label = ifelse(indf$quality >= 6, 1, 0))> winedf <- drop(winedf, "quality")> seed <- 12345
```

我们使用示例函数创建训练和测试数据帧，如下所示:

```scala
> trainingdf <- sample(winedf, withReplacement=FALSE, fraction=0.9, seed=seed)> testdf <- except(winedf, trainingdf)
```

接下来，我们针对 SparkDataFrame 拟合一个逻辑回归模型，如下所示:

```scala
> model <- spark.logit(trainingdf, label ~ ., maxIter = 10, regParam = 0.1, elasticNetParam = 0.8)
```

接下来，我们使用 summary 函数打印已装配模型的概要:

```scala
> summary(model)
```

![](../images/00259.gif)

接下来，我们使用预测函数对测试数据帧进行预测:

```scala
> predictions <- predict(model, testdf)> showDF(select(predictions, "label", "rawPrediction", "probability", "prediction"), 5)
```

![](../images/00260.gif)

only showing top 5 rows

接下来，我们计算标签和预测值之间的不匹配数量:

```scala
> nrow(filter(predictions, predictions$label != predictions$prediction))[1] 111
```

在下面的示例中，我们在 Spark 数据帧上拟合了一个随机森林分类模型。然后，我们使用`summary`函数获得拟合的随机森林模型的摘要，并使用`predict`函数对测试数据进行预测，如下所示:

```scala
> model <- spark.randomForest(trainingdf, label ~ ., type="classification", maxDepth = 5, numTrees = 10)> summary(model)
```

![](../images/00261.gif)

```scala
> predictions <- predict(model, testdf)> showDF(select(predictions, "label", "rawPrediction", "probability", "prediction"), 5)
```

![](../images/00262.gif)

```scala
> nrow(filter(predictions, predictions$label != predictions$prediction))[1] 79
```

类似于前面的例子，我们在下面的例子中拟合了一个广义线性模型:

```scala
> csvPath <- "file:///Users/aurobindosarkar/Downloads/winequality/winequality-white.csv"
```

![](../images/00263.gif)

```scala
> trainingdf <- sample(indf, withReplacement=FALSE, fraction=0.9, seed=seed)> testdf <- except(indf, trainingdf)> model <- spark.glm(indf, quality ~ ., family = gaussian, tol = 1e-06, maxIter = 25, weightCol = NULL, regParam = 0.1)> summary(model)
```

![](../images/00264.gif)

```scala
> predictions <- predict(model, testdf)> showDF(select(predictions, "quality", "prediction"), 5)
```

![](../images/00265.gif)

接下来，我们给出一个聚类的例子，其中我们针对 Spark 数据帧拟合一个多元高斯混合模型:

```scala
> winedf <- mutate(indf, label = ifelse(indf$quality >= 6, 1, 0))> winedf <- drop(winedf, "quality")> trainingdf <- sample(winedf, withReplacement=FALSE, fraction=0.9, seed=seed)> testdf <- except(winedf, trainingdf)> testdf <- except(winedf, trainingdf)> model <- spark.gaussianMixture(trainingdf, ~ sulphates + citric_acid + fixed_acidity + total_sulfur_dioxide + chlorides + free_sulfur_dioxide + density + volatile_acidity + alcohol + pH + residual_sugar, k = 2)> summary(model)> predictions <- predict(model, testdf)> showDF(select(predictions, "label", "prediction"), 5)
```

![](../images/00266.gif)

接下来，我们对从连续分布中采样的数据执行双边**科尔莫戈罗夫-斯米尔诺夫** ( **KS** )测试。我们比较数据的经验累积分布和理论分布之间的最大差异，以检验样本数据来自理论分布的零假设。在下面的例子中，我们用正态分布来说明`fixed_acidity`列上的测试:

```scala
> test <- spark.kstest(indf, "fixed_acidity", "norm", c(0, 1))> testSummary <- summary(test)> testSummary
```

科尔莫戈罗夫-斯米尔诺夫试验总结:

```scala
degrees of freedom = 0statistic = 0.9999276519560749pValue = 0.0#Very strong presumption against null hypothesis: Sample follows theoretical distribution.
```

最后，在下面的例子中，我们使用`spark.lapply`对多个模型进行分布式训练。所有计算的结果必须存储在一台机器的内存中:

```scala
> library(magrittr)> csvPath <- "file:///Users/aurobindosarkar/Downloads/winequality/winequality-white.csv"> indf <- read.df(csvPath, "csv", header = "true", inferSchema = "true", na.strings = "NA", delimiter= ";") %>% withColumnRenamed("fixed acidity", "fixed_acidity") %>% withColumnRenamed("volatile acidity", "volatile_acidity") %>% withColumnRenamed("citric acid", "citric_acid") %>% withColumnRenamed("residual sugar", "residual_sugar") %>% withColumnRenamed("free sulfur dioxide", "free_sulfur_dioxide") %>% withColumnRenamed("total sulfur dioxide", "total_sulfur_dioxide")> lindf <- collect(indf)
```

我们为广义线性模型的族参数传递一个只读的参数列表，如下所示:

```scala
> families <- c("gaussian", "poisson")> train <- function(family) {+ model <- glm(quality ~ ., lindf, family = family)+ summary(model)+ }
```

以下语句返回模型摘要列表:

```scala
> model.summaries <- spark.lapply(families, train)
```

最后，我们可以打印这两个模型的摘要，如下所示:

```scala
> print(model.summaries)
```

模型 1 的总结是:

![](../images/00267.gif)

模型 2 的总结是:

![](../images/00268.gif)

# 摘要

在本章中，我们介绍了 SparkR。我们介绍了 SparkR 架构和 SparkR 数据框架应用编程接口。此外，我们还提供了将 SparkR 用于 EDA 和数据管理任务、数据可视化和机器学习的代码示例。

在下一章中，我们将使用 Spark 模块的混合来构建 Spark 应用程序。我们将展示将火花 SQL 与火花流、火花机器学习等相结合的应用程序示例。