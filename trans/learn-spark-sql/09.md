# 用火花 SQL 开发应用程序

在本章中，我们将展示几个使用 Spark SQL 开发应用程序的例子。我们将主要关注基于文本分析的应用，包括预处理管道、单词包技术、计算金融文档的可读性度量、识别文档库中的主题以及使用朴素贝叶斯分类器。此外，我们将描述机器学习示例的实现。

更具体地说，您将在本章中了解以下内容:

*   Spark 基于 SQL 的应用程序开发
*   预处理文本数据
*   构建预处理数据管道
*   确定文档库中的主题
*   使用朴素贝叶斯分类器
*   开发机器学习应用程序

# 介绍火花 SQL 应用程序

机器学习、预测分析和相关的数据科学主题正变得越来越流行，用于解决跨业务领域的现实问题。这些应用程序正在推动许多组织的关键业务决策。这种应用的例子包括推荐引擎、定向广告、语音识别、欺诈检测、图像识别和分类等等。Spark(以及 Spark SQL)正日益成为这些大规模分布式应用的首选平台。

随着金融新闻、电话会议、监管文件、社交媒体等在线数据源的出现，对文本和其他各种格式(包括文本、音频和视频)的非结构化数据的自动化和智能分析的兴趣激增。这些应用包括来自监管文件的情绪分析、新闻文章和故事的大规模自动化分析、推特分析、股价预测应用等。

在本章中，我们将介绍一些处理文本数据的方法和技术。此外，我们将展示一些在文本数据上应用机器学习模型来对文档进行分类、从文档语料库中获得见解以及处理文本信息以进行情感分析的示例。

在下一节中，我们将从几个有助于将监管文件转化为词汇集合的方法开始介绍。这一步允许使用特定于领域的字典来对文档的语气进行分类，训练算法来识别文档特征，或者将隐藏结构识别为文档集合中的公共主题。

For a more detailed survey of textual analysis methods in accounting and finance, refer to *Textual Analysis in Accounting and Finance: A Survey* by Tim Loughran and Bill McDonald, at [https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2504147](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2504147).

我们还将研究在实现文本分析应用程序中存在的典型问题、挑战和限制，例如，将标记转换为单词、消除句子歧义以及清理嵌入的标签、文档和财务披露文档中存在的其他嘈杂元素。此外，请注意，在解析文档时，HTML 格式的使用是导致错误的主要原因。这种解析依赖于文本结构和相关标记语言的一致性，通常会导致明显的错误。此外，重要的是要理解我们通常对文本传达的有意和无意的信息都感兴趣。

# 理解文本分析应用

语言和写作的内在本质导致了分析文档时的高维问题。因此，一些最广泛使用的文本方法依赖于独立性的关键假设，其中一个单词的顺序和直接上下文并不重要。忽略单词序列的方法通常被标记为“单词包”技术。

与定量分析相比，文本分析要不精确得多。文本数据需要一个额外的步骤，将文本转化为量化的度量，然后将其用作各种基于文本的分析或 ML 方法的输入。这些方法中的许多都是基于将文档解构为由单词行和单词列组成的术语文档矩阵。

在使用单词包的应用程序中，标准化单词计数的方法很重要，因为原始计数直接取决于文档长度。简单地使用比例就可以解决这个问题，但是，我们也可能想要调整一个单词的权重。通常，这些方法基于文档中给定术语的稀有性，例如，**术语频率-逆文档频率** ( **tf-idf** )。

在下一节中，我们将探索使用 Spark SQL 对财务文档进行文本分析。

# 使用火花 SQL 进行文本分析

在本节中，我们将给出一个为文本分析准备数据所需的典型预处理的详细示例(来自会计和金融领域)。我们还将计算几个可读性指标(衡量信息接收者是否能准确重构预期信息的指标)。

# 预处理文本数据

在本节中，我们将开发一组用于预处理`10-K`语句的函数。在我们的示例中，我们将使用 EDGAR 网站上的`10-K`归档的“完整提交文本文件”作为输入文本。

For more details on the `Regex` expressions used to preprocess `10-K` filings refer to *The Annual Report Algorithm: Retrieval of Financial Statements and Extraction of Textual Information*, by Jorg Hering at [http://airccj.org/CSCP/vol7/csit76615.pdf](http://airccj.org/CSCP/vol7/csit76615.pdf).

首先，我们导入本章要求的所有包:

```scala
scala> import spark.implicits._ 
scala> import org.apache.spark.sql._ 
scala> import org.apache.spark.sql.types._ 
scala> import scala.util.matching.Regex 
scala> import org.apache.spark.ml.{Pipeline, PipelineModel} 
scala> import org.apache.spark.rdd.RDD 
scala> import scala.math scala> import org.apache.spark.ml.feature.{HashingTF, IDF, RegexTokenizer, Tokenizer, NGram, StopWordsRemover, CountVectorizer} 
scala> import org.apache.spark.sql.{Row, DataFrame} 
scala> import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, IndexToString} scala> import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier, LogisticRegression, NaiveBayes, NaiveBayesModel} 
scala> import org.apache.spark.ml.Pipeline 
scala> import org.apache.spark.ml.evaluation.{RegressionEvaluator, MulticlassClassificationEvaluator} 
scala> import org.apache.spark.ml.linalg.Vector 
scala> import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder, TrainValidationSplit} 
scala> import org.apache.spark.ml.clustering.{LDA} 
scala> import scala.collection.mutable.WrappedArray 
scala> import org.apache.spark.ml._ 

//The following package will be created later in this Chapter. 
scala> import org.chap9.edgar10k._ 
```

接下来，我们读取输入文件，并将输入行转换为单个字符串进行处理。可以从[https://www . sec . gov/Archives/Edgar/data/320193/000119312514383437/0001193125-14-383437-index . html](https://www.sec.gov/Archives/edgar/data/320193/000119312514383437/0001193125-14-383437-index.html)下载以下示例的输入文件:

```scala
scala> val inputLines = sc.textFile("file:///Users/aurobindosarkar/Downloads/edgardata/0001193125-14-383437.txt") 

scala> val linesToString = inputLines.toLocalIterator.mkString  
```

随着预处理功能的执行，输入字符串的长度会逐渐减少，因为在每一步都会删除大量无关的文本/标签。我们计算原始字符串的起始长度，以跟踪在每个处理步骤中应用特定函数的影响:

```scala
scala> linesToString.length 
res0: Int = 11917240 
```

通常，作为第一步，我们从输入文本中删除缩写、标题和数字(带小数)。下一个代码片段展示了一个用于移除缩写的函数。处理标题和十进制数字的其他规则也可以包含在这里。您可以取消对`println`语句的注释，以显示输入和输出字符串长度(表示预处理和后处理长度):

```scala
scala> def deleteAbbrev(instr: String): String = { 
     |       //println("Input string length="+ instr.length()) 
     |       val pattern = new Regex("[A-Z]\\.([A-Z]\\.)+") 
     |       val str = pattern.replaceAllIn(instr, " ") 
     |       //println("Output string length ="+ str.length()) 
     |       //println("String length reduced by="+ (instr.length - str.length())) 
     |       str 
     | } 

scala> val lineRemAbbrev = deleteAbbrev(linesToString) 
```

除此之外，`10-K`文件由几个展品组成——XBRL、图形、财务报表中嵌入的其他文档(文件)类型；其中包括微软 Excel 文件(文件扩展名`*.xlsx`)、ZIP 文件(文件扩展名`*.zip`)和编码 PDF 文件(文件扩展名`*.pdf`)。

在下一步中，我们将应用其他规则来删除这些嵌入的文档:

```scala
scala> def deleteDocTypes(instr: String): String = { 
     |       //println("Input string length="+ instr.length()) 
     |       val pattern = new Regex("(?s)<TYPE>(GRAPHIC|EXCEL|PDF|ZIP|COVER|CORRESP|EX-10[01].INS|EX-99.SDR [KL].INS|EX-10[01].SCH|EX-99.SDR [KL].SCH|EX-10[01].CAL|EX-99.SDR [KL].CAL|EX-10[01].DEF|EX-99.SDR [KL].LAB|EX-10[01].LAB|EX-99.SDR [KL].LAB|EX-10[01].PRE|EX-99.SDR [KL].PRE|EX-10[01].PRE|EX-99.SDR [KL].PRE).*?</TEXT>")    
     |       val str = pattern.replaceAllIn(instr, " ") 
     |       //println("Output string length ="+ str.length()) 
     |       //println("String length reduced by="+ (instr.length - str.length())) 
     |       str 
     | } scala> val lineRemDocTypes = deleteDocTypes(lineRemAbbrev)
```

接下来，我们删除核心文档和附件中包含的所有元数据，如下所示:

```scala
scala> def deleteMetaData(instr: String): String = { |       val pattern1 = new Regex("<HEAD>.*?</HEAD>") |       val str1 = pattern1.replaceAllIn(instr, " ") |       val pattern2 = new Regex("(?s)<TYPE>.*?<SEQUENCE>.*?<FILENAME>.*?<DESCRIPTION>.*?") |       val str2 = pattern2.replaceAllIn(str1, " ") |       str2 | } scala> val lineRemMetaData = deleteMetaData(lineRemDocTypes)
```

在删除所有 HTML 元素及其对应的属性之前，我们删除文档中的表，因为它们通常包含非文本(定量)信息。

以下函数使用一组正则表达式来删除财务报表中嵌入的表和 HTML 元素:

```scala
scala> def deleteTablesNHTMLElem(instr: String): String = { 
     |       val pattern1 = new Regex("(?s)(?i)<Table.*?</Table>") 
     |       val str1 = pattern1.replaceAllIn(instr, " ") 
     |       val pattern2 = new Regex("(?s)<[^>]*>") 
     |       val str2 = pattern2.replaceAllIn(str1, " ") 
     |       str2 
     | } 

scala> val lineRemTabNHTML = deleteTablesNHTMLElem(lineRemMetaData) 
```

接下来，我们提取每个 HTML 格式文档正文部分的文本。由于 EDGAR 系统接受具有扩展字符集的提交，如`&nbsp; &amp;`、`&reg;`等，它们需要被解码和/或适当替换以进行文本分析。

我们在这个函数中展示了一些这样的例子:

```scala
scala> def deleteExtCharset(instr: String): String = { 
     |       val pattern1 = new Regex("(?s)( |&nbsp;|&#x(A|a)0;)") 
     |       val str1 = pattern1.replaceAllIn(instr, " ") 
     |       val pattern2 = new Regex("(’|’)") 
     |       val str2 = pattern2.replaceAllIn(str1, "'") 
     |       val pattern3 = new Regex("x") 
     |       val str3 = pattern3.replaceAllIn(str2, " ") 
     |       val pattern4 = new Regex("(¨|§|&reg;|™|&copy;)") 
     |       val str4 = pattern4.replaceAllIn(str3, " ") 
     |       val pattern5 = new Regex("(“|”|“|”)") 
     |       val str5 = pattern5.replaceAllIn(str4, "\"") 
     |       val pattern6 = new Regex("&amp;") 
     |       val str6 = pattern6.replaceAllIn(str5, "&") 
     |       val pattern7 = new Regex("(–|—|–)") 
     |       val str7 = pattern7.replaceAllIn(str6, "-") 
     |       val pattern8 = new Regex("⁄") 
     |       val str8 = pattern8.replaceAllIn(str7, "/") 
     |       str8 
     | } 

scala> val lineRemExtChrst = deleteExtCharset(lineRemTabNHTML) 
```

接下来，我们定义一个函数来清除多余的空白、换行和回车:

```scala
scala> def deleteExcessLFCRWS(instr: String): String = { 
     |       val pattern1 = new Regex("[\n\r]+") 
     |       val str1 = pattern1.replaceAllIn(instr, "\n") 
     |       val pattern2 = new Regex("[\t]+") 
     |       val str2 = pattern2.replaceAllIn(str1, " ") 
     |       val pattern3 = new Regex("\\s+") 
     |       val str3 = pattern3.replaceAllIn(str2, " ") 
     |       str3 
     | } 

scala> val lineRemExcessLFCRWS = deleteExcessLFCRWS(lineRemExtChrst) 
```

在下一个代码块中，我们定义了一个函数来说明如何移除用户指定的一组字符串。这些字符串可以从输入文件或数据库中读取。如果您可以对通常出现在整个文档中(并且因文档而异)但在文本分析中没有任何附加价值的无关文本执行`Regex`，则不需要此步骤:

```scala
scala> def deleteStrings(str: String): String = { 
     |       val strings = Array("IDEA: XBRL DOCUMENT", "\\/\\* Do Not Remove This Comment \\*\\/", "v2.4.0.8") 
     |       //println("str="+ str.length()) 
     |       var str1 = str 
     |       for(myString <- strings) { 
     |          var pattern1 = new Regex(myString) 
     |          str1 = pattern1.replaceAllIn(str1, " ") 
     |       } 
     |       str1 
     | } 

scala> val lineRemStrings = deleteStrings(lineRemExcessLFCRWS) 
```

在下一步中，我们从文档字符串中删除所有的网址、文件名、数字和标点符号(除了句点)。在此阶段保留句点，用于计算文本中的句点数量(表示句子数量)(如下节所示):

```scala
scala> def deleteAllURLsFileNamesDigitsPunctuationExceptPeriod(instr: String): String = { 
     |       val pattern1 = new Regex("\\b(https?|ftp|file)://[-a-zA-Z0-9+&@#/%?=~_|!:,.;]*[-a-zA-Z0-9+&@#/%=~_|]") 
     |       val str1 = pattern1.replaceAllIn(instr, "") 
     |       val pattern2 = new Regex("[_a-zA-Z0-9\\-\\.]+.(txt|sgml|xml|xsd|htm|html)") 
     |       val str2 = pattern2.replaceAllIn(str1, " ") 
     |       val pattern3 = new Regex("[^a-zA-Z|^.]") 
     |       val str3 = pattern3.replaceAllIn(str2, " ") 
     |       str3 
     | } 

scala> val lineRemAllUrlsFileNamesDigitsPuncXPeriod = deleteAllURLsFileNamesDigitsPunctuationExceptPeriod(lineRemStrings) 
```

在下一节中，我们将讨论一些通常用于衡量可读性的指标，即用户是否可以访问包含在`10-K`文件中的文本信息。

# 计算可读性

雾指数和年度报告中包含的字数已被广泛用作年度报告可读性的衡量标准(即`Form 10-Ks`)。Fog Index 是两个变量的函数:平均句子长度(以单词为单位)和复杂单词(定义为具有两个以上音节的单词的百分比):

*雾指数= 0.4 *(平均每句话字数+复合词百分比)*

雾指数方程估计了一读理解课文所需的教育年限。因此，雾指数值`16`意味着读者需要十六年的教育——基本上是大学学位——才能在一读时理解课文。一般来说，雾度指数超过 18 的文件被认为是不可读的，因为需要超过硕士学位才能理解文本。

解析`10-K`以计算每句话的平均字数通常是一个困难且容易出错的过程，因为这些文档包含各种缩写，并使用句点来描述部分标识符或作为分隔符。此外，现实世界的系统还需要识别这些文件中包含的许多列表(基于标点和行距)。例如，这种应用程序需要避免计算部分标题、省略号或其他情况下的句点，因为句点可能不会终止句子，然后假设剩余的句点是句子终止。

平均每句话字数是由单词数除以句子终止数决定的。这通常是通过删除缩写和其他虚假的句点源，然后计算句子结束符的数量和单词的数量来完成的。

我们计算文本中剩余的周期数，如下所示:

```scala
scala> val countPeriods = lineRemAllUrlsFileNamesDigitsPuncXPeriod.count(_ == '.')   
countPeriods: Int = 2538 
```

接下来，我们删除所有句点(以及文本中剩余的任何其他非字母字符)，以得到包含在原始文档中的初始单词集。请注意，所有这些单词可能仍然不是合法的单词:

```scala
scala> def keepOnlyAlphas(instr: String): String = { 
     |       val pattern1 = new Regex("[^a-zA-Z|]") 
     |       val str1 = pattern1.replaceAllIn(instr, " ") 
     |       val str2 = str1.replaceAll("[\\s]+", " ") 
     |       str2 
     | } 

scala> val lineWords = keepOnlyAlphas(lineRemAllUrlsFileNamesDigitsPuncXPeriod) 
```

![](../images/00269.jpeg)

在接下来的步骤中，我们将字符串转换为数据帧，并使用`explode()`函数为每个单词创建一行:

```scala
scala> val wordsStringDF = sc.parallelize(List(lineWords)).toDF() 

scala> val wordsDF = wordsStringDF.withColumn("words10k", explode(split($"value", "[\\s]"))).drop("value") 
```

接下来，我们阅读字典(最好是特定领域的字典)。我们将把我们的单词列表与本词典进行匹配，以得出我们的最终单词列表(在此阶段之后，它们都应该是合法的单词)。

```scala
scala> val dictDF = spark.read.format("csv").option("header", "true").load("file:///Users/aurobindosarkar/Downloads/edgardata/LoughranMcDonald_MasterDictionary_2014.csv") 
```

For our purposes, we have used the Loughran & McDonold's Master Dictionary as it contains words typically found in 10-K statements. You can download the `LoughranMcDonald_MasterDictionary_2014.csv` file and the associated documentation from [https://www3.nd.edu/~mcdonald/Word_Lists.html](https://www3.nd.edu/~mcdonald/Word_Lists.html).

在下一步中，我们将单词列表数据框与字典连接起来，并计算最终列表中的单词数:

```scala
scala> val joinWordsDict = wordsDF.join(dictDF, lower(wordsDF("words10k")) === lower(dictDF("Word"))) 

scala> val numWords = joinWordsDict.count() 
numWords: Long = 54701 

```

每个句子的平均单词数是通过将单词数除以前面计算的周期数来计算的:

```scala
scala> val avgWordsPerSentence = numWords / countPeriods 
avgWordsPerSentence: Long = 21 
```

我们使用字典中的`Syllables`列来计算单词列表中有两个以上音节的单词数量，如下所示:

```scala
scala> val numPolySylb = joinWordsDict.select("words10k", "Syllables").where(joinWordsDict("Syllables") > 2) 

scala> val polySCount = numPolySylb.count() 
polySCount: Long = 14093 

```

最后，我们将参数插入到我们的等式中来计算雾指数，如图所示:

```scala
scala> val fogIndex = 0.4*(avgWordsPerSentence+((polySCount/numWords)*100)) 
fogIndex: Double = 8.4 
```

反对在金融文件中使用可读性指标(如雾指数)的论点是，根据所使用的写作风格，这些文件中的大多数是无法区分的。此外，即使这些文档中复杂单词的百分比可能很高，这些单词或行业术语也很容易被这些文档的受众(例如，投资者群体)理解。

作为年度报告可读性的简单代理，拉夫兰和麦克唐纳建议使用总`10-K`文件大小的自然日志(完整的提交文本文件)。与雾索引相比，这种方法更容易获得，并且不涉及复杂的`10-K`文档解析。

在接下来的步骤中，我们将提供一个函数来计算文件(或者更具体地说，RDD)的大小:

```scala
scala> def calcFileSize(rdd: RDD[String]): Long = { 
     |   rdd.map(_.getBytes("UTF-8").length.toLong) 
     |      .reduce(_+_) //add the sizes together 
     | } 

scala> val lines = sc.textFile("file:///Users/aurobindosarkar/Downloads/edgardata/0001193125-14-383437.txt") 
```

文件大小(以兆字节为单位)和文件大小日志可以计算如下:

```scala
scala> val fileSize = calcFileSize(lines)/1000000.0 
fileSize: Double = 11.91724 

scala> math.log(fileSize) 
res1: Double = 2.477986091202679 
```

尽管文件大小很好地代表了文件的可读性，例如`10-K`文件，但它可能不太适合新闻稿、新闻报道和盈利电话会议的文本。在这种情况下，由于文本的长度变化不大，其他更注重内容的方法可能更合适。

在下一节中，我们将讨论在文本分析中使用单词列表。

# 使用单词列表

在测量金融文档的语气或情绪时，从业者通常计算与特定情绪相关联的词的数量，该数量由文档中的词的总数来缩放。因此，例如，文档中否定词的比例越高，表明语气越悲观。

使用字典来测量声调有几个重要的优点。除了在规模上计算情感的便利性之外，这类词典的使用通过消除个人主观性促进了标准化。单词分类的一个重要组成部分是识别每个分类中出现频率最高的单词。

在接下来的步骤中，我们使用字典中包含的单词-情感指标来获得对`10-K`文件的情感或语气的感觉。这可以根据同一组织过去提交的文件或同一部门或不同部门的其他组织计算得出:

```scala
scala> val negWordCount = joinWordsDict.select("words10k", "negative").where(joinWordsDict("negative") > 0).count() 
negWordCount: Long = 1004 

scala> val sentiment = negWordCount / (numWords.toDouble) 
sentiment: Double = 0.01835432624632091 
```

通常，在这种分析中，语气词的使用也很重要。例如，使用较弱的模式词(例如，可能、可能和可能)可能表明公司存在问题:

```scala
scala> val modalWordCount = joinWordsDict.select("words10k", "modal").where(joinWordsDict("modal") > 0).groupBy("modal").count() 
```

在下面的代码中，我们统计了每一类情态词的字数。根据本词典的参考文献，a `1`表示“强情态”(如“总是”、“肯定”、“从不”等词)，a `2`表示“中等情态”(如“可以”、“一般”、“通常”等词)，a `3`表示“弱情态”(如“几乎”、“可能”、“可能”、“暗示”等词):

```scala
scala> modalWordCount.show() 
+-----+-----+ 
|modal|count| 
+-----+-----+ 
|    3|  386| 
|    1|  115| 
|    2|  221| 
+-----+-----+ 

```

在下一节中，我们将使用本节中定义的一些函数为`10-K`文件创建一个数据预处理管道。

# 创建数据预处理管道

在本节中，我们将把前面几节中的一些数据处理功能转换成定制的转换器。这些 Transformer 对象将输入数据帧映射到输出数据帧，通常用于为机器学习应用程序准备数据帧。

我们创建以下类作为`UnaryTransformer`对象，这些对象对一个输入数据帧列应用转换，并通过向其附加一个新列(包含应用函数的处理结果)来生成另一个。这些自定义的 Transformer 对象可以成为处理管道的一部分。

首先，我们创建四个自定义`UnaryTransformer`类，我们将在示例中使用，如下所示:

**tablesnhtml elemcleaner . scale**

```scala
package org.chap9.edgar10kimport org.apache.spark.ml.UnaryTransformerimport org.apache.spark.sql.types.{DataType, DataTypes, StringType}import scala.util.matching.Regeximport org.apache.spark.ml.util.Identifiableclass TablesNHTMLElemCleaner(override val uid: String) extends UnaryTransformer[String, String, TablesNHTMLElemCleaner] {def this() = this(Identifiable.randomUID("cleaner"))def deleteTablesNHTMLElem(instr: String): String = {val pattern1 = new Regex("(?s)(?i)<Table.*?</Table>")val str1 = pattern1.replaceAllIn(instr, " ")val pattern2 = new Regex("(?s)<[^>]*>")val str2 = pattern2.replaceAllIn(str1, " ")str2}override protected def createTransformFunc: String => String = {deleteTablesNHTMLElem _}override protected def validateInputType(inputType: DataType): Unit = {require(inputType == StringType)}override protected def outputDataType: DataType = DataTypes.StringType}
```

**allurlfilenamesdisplancatunacionexception period cleaner . scale**

```scala
package org.chap9.edgar10kimport org.apache.spark.ml.UnaryTransformerimport org.apache.spark.sql.types.{DataType, DataTypes, StringType}import scala.util.matching.Regeximport org.apache.spark.ml.util.Identifiableclass AllURLsFileNamesDigitsPunctuationExceptPeriodCleaner(override val uid: String) extends UnaryTransformer[String, String, AllURLsFileNamesDigitsPunctuationExceptPeriodCleaner] {def this() = this(Identifiable.randomUID("cleaner"))def deleteAllURLsFileNamesDigitsPunctuationExceptPeriod(instr: String): String = {val pattern1 = new Regex("\\b(https?|ftp|file)://[-a-zA-Z0-9+&@#/%?=~_|!:,.;]*[-a-zA-Z0-9+&@#/%=~_|]")val str1 = pattern1.replaceAllIn(instr, "")val pattern2 = new Regex("[_a-zA-Z0-9\\-\\.]+.(txt|sgml|xml|xsd|htm|html)")val str2 = pattern2.replaceAllIn(str1, " ")val pattern3 = new Regex("[^a-zA-Z|^.]")val str3 = pattern3.replaceAllIn(str2, " ")str3}override protected def createTransformFunc: String => String = {deleteAllURLsFileNamesDigitsPunctuationExceptPeriod _}override protected def validateInputType(inputType: DataType): Unit = {require(inputType == StringType)}override protected def outputDataType: DataType = DataTypes.StringType}
```

**onlyaphascleaner . scale**

```scala
package org.chap9.edgar10kimport org.apache.spark.ml.UnaryTransformerimport org.apache.spark.sql.types.{DataType, DataTypes, StringType}import scala.util.matching.Regeximport org.apache.spark.ml.util.Identifiableclass OnlyAlphasCleaner(override val uid: String) extends UnaryTransformer[String, String, OnlyAlphasCleaner] {def this() = this(Identifiable.randomUID("cleaner"))def keepOnlyAlphas(instr: String): String = {val pattern1 = new Regex("[^a-zA-Z|]")val str1 = pattern1.replaceAllIn(instr, " ")val str2 = str1.replaceAll("[\\s]+", " ")str2}override protected def createTransformFunc: String => String = {keepOnlyAlphas _}override protected def validateInputType(inputType: DataType): Unit = {require(inputType == StringType)}override protected def outputDataType: DataType = DataTypes.StringType}
```

**超限 FCRWSCleaner .量表**

```scala
package org.chap9.edgar10kimport org.apache.spark.ml.UnaryTransformerimport org.apache.spark.sql.types.{DataType, DataTypes, StringType}import scala.util.matching.Regeximport org.apache.spark.ml.util.Identifiableclass ExcessLFCRWSCleaner(override val uid: String) extends UnaryTransformer[String, String, ExcessLFCRWSCleaner] {def this() = this(Identifiable.randomUID("cleaner"))def deleteExcessLFCRWS(instr: String): String = {val pattern1 = new Regex("[\n\r]+")val str1 = pattern1.replaceAllIn(instr, "\n")val pattern2 = new Regex("[\t]+")val str2 = pattern2.replaceAllIn(str1, " ")val pattern3 = new Regex("\\s+")val str3 = pattern3.replaceAllIn(str2, " ")str3}override protected def createTransformFunc: String => String = {deleteExcessLFCRWS _}override protected def validateInputType(inputType: DataType): Unit = {require(inputType == StringType)}override protected def outputDataType: DataType = DataTypes.StringType}
```

创建以下`build.sbt`文件，用于编译和打包目标类:

```scala
name := "Chapter9"version := "2.0"scalaVersion := "2.11.8"libraryDependencies ++= Seq(("org.apache.spark" % "spark-core_2.11" % "2.2.0" % "provided"),("org.apache.spark" % "spark-sql_2.11" % "2.2.0" % "provided"),("org.apache.spark" % "spark-mllib_2.11" % "2.2.0" % "provided"))libraryDependencies += "com.github.scopt" %% "scopt" % "3.4.0"libraryDependencies += "com.typesafe" % "config" % "1.3.0"libraryDependencies += "com.typesafe.scala-logging" %% "scala-logging-api" % "2.1.2"libraryDependencies += "com.typesafe.scala-logging" %% "scala-logging-slf4j" % "2.1.2"libraryDependencies += "org.scalatest" % "scalatest_2.11" % "3.0.1" % "test"
```

使用以下`SBT`命令将类编译并打包到一个 JAR 文件中:

```scala
Aurobindos-MacBook-Pro-2:Chapter9 aurobindosarkar$ sbt package
```

最后，用会话中包含的前面的 JAR 文件重新启动 Spark shell:

```scala
Aurobindos-MacBook-Pro-2:spark-2.2.1-SNAPSHOT-bin-hadoop2.7 aurobindosarkar$ bin/spark-shell --driver-memory 12g --conf spark.driver.maxResultSize=12g --conf spark.sql.shuffle.partitions=800 --jars /Users/aurobindosarkar/Downloads/Chapter9/target/scala-2.11/chapter9_2.11-2.0.jar
```

以下示例的数据集，Reuters-21578，Distribution 1.0，可从[https://archive . ics . UCI . edu/ml/datasets/Reuters-21578+text+category+collection](https://archive.ics.uci.edu/ml/datasets/reuters-21578+text+categorization+collection)下载。

在这里，我们将在下载的 SGML 文件中取一个由`<Reuters>...</Reuters>`标记划分的条目，创建一个包含单个故事的新输入文件。这大致模拟了一个新的故事进入我们的管道。更具体地说，这个故事可以通过卡夫卡队列进入，我们可以创建一个连续的火花 SQL 应用程序来处理进入的故事文本。

首先，我们将新创建的文件读入一个数据帧，如图所示:

```scala
scala> val linesDF1 = sc.textFile("file:///Users/aurobindosarkar/Downloads/reuters21578/reut2-020-1.sgm").toDF()
```

接下来，我们使用本节前面定义的类创建变形金刚的实例。通过指定每个转换器的输出列作为链中下一个转换器的输入列，转换器在管道中被链接在一起:

```scala
scala> val tablesNHTMLElemCleaner = new TablesNHTMLElemCleaner().setInputCol("value").setOutputCol("tablesNHTMLElemCleaned")scala> val allURLsFileNamesDigitsPunctuationExceptPeriodCleaner = new AllURLsFileNamesDigitsPunctuationExceptPeriodCleaner().setInputCol("tablesNHTMLElemCleaned").setOutputCol("allURLsFileNamesDigitsPunctuationExceptPeriodCleaned")scala> val onlyAlphasCleaner = new OnlyAlphasCleaner().setInputCol("allURLsFileNamesDigitsPunctuationExceptPeriodCleaned").setOutputCol("text")scala> val excessLFCRWSCleaner = new ExcessLFCRWSCleaner().setInputCol("text").setOutputCol("cleaned")
```

在通过我们的清理组件处理完文本之后，我们再添加两个阶段，以便对文本进行标记化并从文本中移除停止词，如图所示。我们使用在[https://www3.nd.edu/~mcdonald/Word_Lists.html](https://www3.nd.edu/~mcdonald/Word_Lists.html)可用的通用停止词列表文件:

```scala
scala> val tokenizer = new RegexTokenizer().setInputCol("cleaned").setOutputCol("words").setPattern("\\W")scala> val stopwords: Array[String] = sc.textFile("file:///Users/aurobindosarkar/Downloads/StopWords_GenericLong.txt").flatMap(_.stripMargin.split("\\s+")).collect
```

![](../images/00270.jpeg)

```scala
scala> val remover = new StopWordsRemover().setStopWords(stopwords).setCaseSensitive(false).setInputCol("words").setOutputCol("filtered")
```

在这一阶段，我们所有处理阶段的组件都已准备好组装成流水线。

For more details on Spark pipelines, refer to [https://spark.apache.org/docs/latest/ml-pipeline.html](https://spark.apache.org/docs/latest/ml-pipeline.html).

我们创建一个管道并链接所有的转换器来指定管道阶段，如下所示:

```scala
scala> val pipeline = new Pipeline().setStages(Array(tablesNHTMLElemCleaner, allURLsFileNamesDigitsPunctuationExceptPeriodCleaner, onlyAlphasCleaner, excessLFCRWSCleaner, tokenizer, remover))
```

对包含原始文本文档的原始数据帧调用`pipeline.fit()`方法:

```scala
scala> val model = pipeline.fit(linesDF1)
```

我们可以使用上一步中的管道模型，将原始数据集转换为其他下游文本应用程序所需的形式。我们还删除了中间处理步骤中的列，以清理我们的数据框:

```scala
scala> val cleanedDF = model.transform(linesDF1).drop("value").drop("tablesNHTMLElemCleaned").drop("excessLFCRWSCleaned").drop("allURLsFileNamesDigitsPunctuationExceptPeriodCleaned").drop("text").drop("word")
```

此外，我们可以通过删除任何包含空字符串或空格的行来清理最终的输出列，如下所示:

```scala
scala> val finalDF = cleanedDF.filter(($"cleaned" =!= "") && ($"cleaned" =!= " "))scala> cleanedDF.count()res3: Long = 62
```

其余的处理与我们前面介绍的相似。以下步骤将包含我们的单词的列分解成单独的行，将我们的最终单词列表与字典连接起来，然后计算情感和模态单词的用法:

```scala
scala> val wordsInStoryDF = finalDF.withColumn("wordsInStory", explode(split($"cleaned", "[\\s]"))).drop("cleaned")scala> val joinWordsDict = wordsInStoryDF.join(dictDF, lower(wordsInStoryDF("wordsInStory")) === lower(dictDF("Word")))scala> wordsInStoryDF.count()res4: Long = 457scala> val numWords = joinWordsDict.count().toDoublenumWords: Double = 334.0scala> joinWordsDict.select("wordsInStory").show()
```

![](../images/00271.jpeg)

```scala
scala> val negWordCount = joinWordsDict.select("wordsInStory", "negative").where(joinWordsDict("negative") > 0).count()negWordCount: Long = 8scala> val sentiment = negWordCount / (numWords.toDouble)sentiment: Double = 0.023952095808383235scala> val modalWordCount = joinWordsDict.select("wordsInStory", "modal").where(joinWordsDict("modal") > 0).groupBy("modal").count()scala> modalWordCount.show()+-----+-----+|modal|count|+-----+-----+|    3|    2||    1|    5||    2|    4|+-----+-----+
```

下一组步骤说明了使用前面的管道来处理我们语料库中的另一个故事。然后，我们可以比较这些结果，从而在故事中获得相对的悲观感(通过衡量负面情绪来反映):

```scala
scala> val linesDF2 = sc.textFile("file:///Users/aurobindosarkar/Downloads/reuters21578/reut2-008-1.sgm").toDF()scala> val cleanedDF = model.transform(linesDF2).drop("value").drop("tablesNHTMLElemCleaned").drop("excessLFCRWSCleaned").drop("allURLsFileNamesDigitsPunctuationExceptPeriodCleaned").drop("text").drop("word")cleanedDF: org.apache.spark.sql.DataFrame = [cleaned: string,scala> val finalDF = cleanedDF.filter(($"cleaned" =!= "") && ($"cleaned" =!= " "))scala> cleanedDF.count()res7: Long = 84scala> val wordsInStoryDF = finalDF.withColumn("wordsInStory", explode(split($"cleaned", "[\\s]"))).drop("cleaned")scala> val joinWordsDict = wordsInStoryDF.join(dictDF, lower(wordsInStoryDF("wordsInStory")) === lower(dictDF("Word")))scala> wordsInStoryDF.count()res8: Long = 598scala> val numWords = joinWordsDict.count().toDoublenumWords: Double = 483.0scala> joinWordsDict.select("wordsInStory").show()
```

![](../images/00272.jpeg)

```scala
scala> val negWordCount = joinWordsDict.select("wordsInStory", "negative").where(joinWordsDict("negative") > 0).count()negWordCount: Long = 15
```

基于以下负面情绪计算，我们可以得出结论，相对而言，这个故事比前一个分析的更悲观:

```scala
scala> val sentiment = negWordCount / (numWords.toDouble)sentiment: Double = 0.031055900621118012scala> val modalWordCount = joinWordsDict.select("wordsInStory", "modal").where(joinWordsDict("modal") > 0).groupBy("modal").count()scala> modalWordCount.show()+-----+-----+|modal|count|+-----+-----+|    3|    1||    1|    3||    2|    4|+-----+-----+
```

在下一节中，我们将把重点转移到确定文档语料库中的主要主题。

# 理解文档库中的主题

基于单词包的技术也可以用于对文档中的常见主题进行分类，或者识别文档语料库中的主题。一般来说，像大多数技术一样，这些技术试图根据每个单词与潜在变量的关系来降低术语文档矩阵的维数。

这种分类的最早方法之一是**潜在语义分析** ( **LSA** )。LSA 可以避免基于计数的方法对同义词和具有多重含义的术语的限制。多年来，LSA 的概念已经演变成另一种被称为**潜在狄利克雷分配** ( **LDA** )的模式。

LDA 允许我们识别一组文档中潜在的主题结构。LSA 和线性判别分析都使用术语文档矩阵来降低术语空间的维数和产生主题权重。LSA 和线性判别分析技术的一个限制是，它们在应用于大型文档时效果最好。

For more detailed explanation of LDA, refer to *Latent Dirichlet Allocation*,by David M. Blei, Andrew Y. Ng, and Michael I. Jordan, at [http://ai.stanford.edu/~ang/papers/jair03-lda.pdf](http://ai.stanford.edu/~ang/papers/jair03-lda.pdf).

现在，我们给出一个在 XML 文档语料库上使用线性判别分析的例子。

从用于阅读 XML 文档的包开始启动 Spark shell，因为我们将在本节中阅读一个基于 XML 的语料库:

```scala
Aurobindos-MacBook-Pro-2:spark-2.2.1-SNAPSHOT-bin-hadoop2.7 aurobindosarkar$ bin/spark-shell --driver-memory 12g --conf spark.driver.maxResultSize=12g --conf spark.sql.shuffle.partitions=800 --packages com.databricks:spark-xml_2.11:0.4.1
```

接下来，我们为主题数量、最大迭代次数和词汇大小定义几个常量，如图所示:

```scala
scala> val numTopics: Int = 10scala> val maxIterations: Int = 100scala> val vocabSize: Int = 10000
```

如下使用的`PERMISSIVE`模式允许我们继续创建数据帧，即使在解析过程中遇到损坏的记录。`rowTag`参数指定要读取的 XML 节点。在这里，我们对文档中使用 LDA 进行主题分析的句子感兴趣。

本示例的数据集包含来自澳大利亚联邦法院的 4000 个澳大利亚法律案例，可从 https://archive.ics.uci.edu/ml/datasets/Legal+Case+Reports 下载。

我们阅读了所有的案例档案，如下所示:

```scala
scala> val df = spark.read.format("com.databricks.spark.xml").option("rowTag", "sentences").option("mode", "PERMISSIVE").load("file:///Users/aurobindosarkar/Downloads/corpus/fulltext/*.xml")
```

接下来，我们为每个法律案例生成文档标识，如下所示:

```scala
scala> val docDF = df.select("sentence._VALUE").withColumn("docId", monotonically_increasing_id()).withColumn("sentences", concat_ws(",", $"_VALUE")).drop("_VALUE")scala> // Split each document into wordsscala> val tokens = new RegexTokenizer().setGaps(false).setPattern("\\p{L}+").setInputCol("sentences").setOutputCol("words").transform(docDF)scala> //Remove stop words using the default stop word list provided with the Spark distribution.scala> val filteredTokens = new StopWordsRemover().setCaseSensitive(false).setInputCol("words").setOutputCol("filtered").transform(tokens)
```

我们使用`CountVectorizer`(和`CountVectorizerModel`)将我们的法律文件集合转换为令牌计数的向量。这里，我们没有先验字典可用，所以`CountVectorizer`被用作提取词汇并生成`CountVectorizerModel`的估计器。该模型在词汇表上产生文档的稀疏表示，然后将其传递给线性判别分析算法。在拟合过程中，`CountVectorizer`将选择语料库中按词频排序的前`vocabSize`个词:

```scala
scala> val cvModel = new CountVectorizer().setInputCol("filtered").setOutputCol("features").setVocabSize(vocabSize).fit(filteredTokens)scala> val termVectors = cvModel.transform(filteredTokens).select("docId", "features")scala> val lda = new LDA().setK(numTopics).setMaxIter(maxIterations)scala> val ldaModel = lda.fit(termVectors)scala> println("Model was fit using parameters: " + ldaModel.parent.extractParamMap)Model was fit using parameters: {lda_8b00356ca964-checkpointInterval: 10,lda_8b00356ca964-featuresCol: features,lda_8b00356ca964-k: 10,lda_8b00356ca964-keepLastCheckpoint: true,lda_8b00356ca964-learningDecay: 0.51,lda_8b00356ca964-learningOffset: 1024.0,lda_8b00356ca964-maxIter: 100,lda_8b00356ca964-optimizeDocConcentration: true,lda_8b00356ca964-optimizer: online,lda_8b00356ca964-seed: 1435876747,lda_8b00356ca964-subsamplingRate: 0.05,lda_8b00356ca964-topicDistributionCol: topicDistribution}
```

我们计算对数似然和对数困惑的线性判别分析模型，如图所示。更高可能性的模型意味着更好的模型。同样，较低的困惑代表较好的模型:

```scala
scala> val ll = ldaModel.logLikelihood(termVectors)ll: Double = -6.912755229181568E7scala> val lp = ldaModel.logPerplexity(termVectors)lp: Double = 7.558777992719632scala> println(s"The lower bound on the log likelihood of the entire corpus: $ll")The lower bound on the log likelihood of the entire corpus: -6.912755229181568E7scala> println(s"The upper bound on perplexity: $lp")The upper bound on perplexity: 7.558777992719632
```

接下来，我们使用`describeTopics()`功能来显示由它们的顶部加权项描述的主题，如图所示:

```scala
scala> val topicsDF = ldaModel.describeTopics(3)scala> println("The topics described by their top-weighted terms:")The topics described by their top-weighted terms are the following:scala> topicsDF.show(false)
```

![](../images/00273.jpeg)

```scala
scala> val transformed = ldaModel.transform(termVectors)scala> transformed.select("docId", "topicDistribution").take(3).foreach(println)[0,[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]][8589934592,[8.963966883240337E-5,7.477786237947913E-5,1.1695214724007773E-4,7.651092413869693E-5,5.878144972523343E-5,1.533289774455994E-4,8.250794034920294E-5,6.472049126896475E-5,7.008103300313653E-5,0.9992126995056172]][17179869184,[9.665344356333612E-6,8.06287932260242E-6,0.13933607311582796,8.249745717562721E-6,6.338075472527743E-6,1.6528598250017008E-5,8.89637068587104E-6,6.978449157294409E-6,0.029630980885952427,0.8309682265352574]]scala> val vocab = cvModel.vocabulary
```

![](../images/00274.jpeg)

我们可以显示包含对应于术语索引的实际术语的结果，如下所示。从这里显示的文字可以明显看出，我们正在处理一个法律主体:

```scala
scala> for ((row) <- topicsDF) {| var i = 0| var termsString = ""| var topicTermIndicesString = ""| val topicNumber = row.get(0)| val topicTerms:WrappedArray[Int] = row.get(1).asInstanceOf[WrappedArray[Int]]|| for (i <- 0 to topicTerms.length-1){| topicTermIndicesString += topicTerms(i) +", "| termsString += vocab(topicTerms(i)) +", "| }|| println ("Topic: "+ topicNumber+ "|["+topicTermIndicesString + "]|[" + termsString +"]")| }Topic: 1|[3, 231, 292, ]|[act, title, native, ]Topic: 5|[4, 12, 1, ]|[tribunal, appellant, applicant, ]Topic: 6|[0, 6, 13, ]|[mr, evidence, said, ]Topic: 0|[40, 168, 0, ]|[company, scheme, mr, ]Topic: 2|[0, 32, 3, ]|[mr, agreement, act, ]Topic: 7|[124, 198, 218, ]|[commissioner, income, tax, ]Topic: 3|[3, 44, 211, ]|[act, conduct, price, ]Topic: 8|[197, 6, 447, ]|[trade, evidence, mark, ]Topic: 4|[559, 130, 678, ]|[patent, dr, university, ]Topic: 9|[2, 1, 9, ]|[court, applicant, respondent, ]Using
```

语篇分析的下一个主题是搭配词。对于一些单词来说，它们的大部分意思来自于与其他单词的搭配。基于搭配预测词义通常是超越简单的单词包方法的最常见的扩展之一。

在下一节中，我们将研究朴素贝叶斯分类器在 n-gram 上的使用。

# 使用朴素贝叶斯分类器

朴素贝叶斯分类器是基于贝叶斯条件概率定理的一族概率分类器。这些分类器假设特征之间是独立的。朴素贝叶斯通常是以词频为特征集的文本分类的基准方法。尽管有很强的独立性假设，朴素贝叶斯分类器是快速和容易实现的；因此，它们在实践中非常常用。

虽然朴素贝叶斯非常流行，但它也有可能导致偏向某一类而不是另一类的错误。例如，倾斜的数据会导致分类器偏向一个类而不是另一个类。类似地，独立性假设可能会导致错误的分类权重，使一个类别优先于另一个类别。

For specific heuristics for dealing with problems associated with Naive Bayes classifers, refer to *Tackling the Poor Assumptions of Naive Bayes Text Classifiers*, by Rennie, Shih, et al at [https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf](https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf).

朴素贝叶斯方法的主要优点之一是，它不需要大的训练数据集来估计分类所需的参数。在使用监督机器学习的各种单词分类方法中，朴素贝叶斯方法非常流行；例如，年度报告中哪些句子可以归类为“负面”、“正面”或“中性”。朴素贝叶斯方法也最常用于 n-grams 和**支持向量机** ( **支持向量机**)。

n-gram 用于各种不同的任务。例如，n-grams 可用于开发监督机器学习模型的特征，如支持向量机、最大熵模型和朴素贝叶斯。当`N`的值为`1`时，n-grams 被称为 unigrams(本质上是一个句子中的单个单词；当`N`的值为`2`时，它们被称为二元模型，当`N`为`3`时，它们被称为三元模型，以此类推。这里的主要思想是在特征空间中使用诸如二元模型之类的标记，而不是单个单词或单幅图。

本例的数据集包含约 169 万条亚马逊电子产品评论，可从[http://jmcauley.ucsd.edu/data/amazon/](http://jmcauley.ucsd.edu/data/amazon/)下载。

For a more detailed explanation of steps used in this example, check out Natural Language Processing with Apache Spark ML and Amazon Reviews (Parts 1 & 2), Mike Seddon, at [http://mike.seddon.ca/natural-language-processing-with-apache-spark-ml-and-amazon-reviews-part-1/](http://mike.seddon.ca/natural-language-processing-with-apache-spark-ml-and-amazon-reviews-part-1/).

首先，我们读取输入 JSON 文件来创建我们的输入数据帧:

```scala
scala> val inDF = spark.read.json("file:///Users/aurobindosarkar/Downloads/reviews_Electronics_5.json")scala> inDF.show()
```

![](../images/00275.jpeg)

您可以打印模式，如图所示:

```scala
scala> inDF.printSchema()root|-- asin: string (nullable = true)|-- helpful: array (nullable = true)| |-- element: long (containsNull = true)|-- overall: double (nullable = true)|-- reviewText: string (nullable = true)|-- reviewTime: string (nullable = true)|-- reviewerID: string (nullable = true)|-- reviewerName: string (nullable = true)|-- summary: string (nullable = true)|-- unixReviewTime: long (nullable = true)
```

接下来，我们打印出每个评级值的记录数，如下所示。请注意，记录的数量非常偏向于评级 5。这种偏差会影响我们的结果，有利于评级 5 而不是其他评级:

```scala
scala> inDF.groupBy("overall").count().orderBy("overall").show()+-------+-------+|overall| count|+-------+-------+|    1.0| 108725||    2.0|  82139||    3.0| 142257||    4.0| 347041||    5.0|1009026|+-------+-------+
```

我们从数据框创建一个视图，如下所示。这一步可以方便地帮助我们创建一个更加平衡的培训数据框架，其中包含来自每个评级类别的相同数量的记录:

```scala
scala> inDF.createOrReplaceTempView("reviewsTable")scala> val reviewsDF = spark.sql(| """| SELECT text, label, rowNumber FROM (| SELECT| overall AS label, reviewText AS text, row_number() OVER (PARTITION BY overall ORDER BY rand()) AS rowNumber FROM reviewsTable| ) reviewsTable| WHERE rowNumber <= 60000| """| )scala> reviewsDF.groupBy("label").count().orderBy("label").show()+-----+-----+|label|count|+-----+-----+|  1.0|60000||  2.0|60000||  3.0|60000||  4.0|60000|| 5.0|60000|+-----+-----+
```

现在，我们使用行号创建训练和测试数据集:

```scala
scala> val trainingData = reviewsDF.filter(reviewsDF("rowNumber") <= 50000).select("text","label")scala> val testData = reviewsDF.filter(reviewsDF("rowNumber") > 10000).select("text","label")
```

在给定的步骤中，我们标记我们的文本，删除停止词，并创建二元模型和三元模型:

```scala
scala> val regexTokenizer = new RegexTokenizer().setPattern("[a-zA-Z']+").setGaps(false).setInputCol("text")scala> val remover = new StopWordsRemover().setInputCol(regexTokenizer.getOutputCol)scala> val bigrams = new NGram().setN(2).setInputCol(remover.getOutputCol)scala> val trigrams = new NGram().setN(3).setInputCol(remover.getOutputCol)
```

在接下来的步骤中，我们为单项式、二元式和三元式定义`HashingTF`实例:

```scala
scala> val removerHashingTF = new HashingTF().setInputCol(remover.getOutputCol)scala> val ngram2HashingTF = new HashingTF().setInputCol(bigrams.getOutputCol)scala> val ngram3HashingTF = new HashingTF().setInputCol(trigrams.getOutputCol)scala> val assembler = new VectorAssembler().setInputCols(Array(removerHashingTF.getOutputCol, ngram2HashingTF.getOutputCol, ngram3HashingTF.getOutputCol))scala> val labelIndexer = new StringIndexer().setInputCol("label").setOutputCol("indexedLabel").fit(reviewsDF)scala> val labelConverter = new IndexToString().setInputCol("prediction").setOutputCol("predictedLabel").setLabels(labelIndexer.labels)
```

然后，我们创建一个朴素贝叶斯分类器的实例:

```scala
scala> val nb = new NaiveBayes().setLabelCol(labelIndexer.getOutputCol).setFeaturesCol(assembler.getOutputCol).setPredictionCol("prediction").setModelType("multinomial")
```

我们组装加工流水线，如图所示:

```scala
scala> val pipeline = new Pipeline().setStages(Array(regexTokenizer, remover, bigrams, trigrams, removerHashingTF, ngram2HashingTF, ngram3HashingTF, assembler, labelIndexer, nb, labelConverter))
```

我们创建了一个用于交叉验证的参数网格，以获得模型的最佳参数集，如下所示:

```scala
scala> val paramGrid = new ParamGridBuilder().addGrid(removerHashingTF.numFeatures, Array(1000,10000)).addGrid(ngram2HashingTF.numFeatures, Array(1000,10000)).addGrid(ngram3HashingTF.numFeatures, Array(1000,10000)).build()paramGrid: Array[org.apache.spark.ml.param.ParamMap] =Array({hashingTF_4b2023cfcec8-numFeatures: 1000,hashingTF_7bd4dd537583-numFeatures: 1000,hashingTF_7cd2d166ac2c-numFeatures: 1000}, {hashingTF_4b2023cfcec8-numFeatures: 10000,hashingTF_7bd4dd537583-numFeatures: 1000,hashingTF_7cd2d166ac2c-numFeatures: 1000}, {hashingTF_4b2023cfcec8-numFeatures: 1000,hashingTF_7bd4dd537583-numFeatures: 10000,hashingTF_7cd2d166ac2c-numFeatures: 1000}, {hashingTF_4b2023cfcec8-numFeatures: 10000,hashingTF_7bd4dd537583-numFeatures: 10000,hashingTF_7cd2d166ac2c-numFeatures: 1000}, {hashingTF_4b2023cfcec8-numFeatures: 1000,hashingTF_7bd4dd537583-numFeatures: 1000,hashingTF_7cd2d166ac2c-numFeatures: 10000}, {hashingTF_4b2023cfcec8-numFeatures: 10000,hashingTF_7bd4dd537...
```

在下一步中，请注意，k 折叠交叉验证通过将数据集拆分为一组不重叠的随机分区折叠来执行模型选择，这些折叠用作单独的训练和测试数据集；例如，使用`k=3`折叠，K 折叠交叉验证将生成三个(训练、测试)数据集对，每个数据集对使用`2/3`的数据进行训练，使用`1/3`进行测试。

每个折叠恰好用作测试集一次。为了评估特定的`ParamMap`，`CrossValidator`计算通过在两个不同的(训练、测试)数据集对上拟合估计器而产生的三个模型的平均评估度量。在确定最佳`ParamMap`之后，`CrossValidator`最终使用整个数据集上的最佳`ParamMap`重新调整估计器:

```scala
scala> val cv = new CrossValidator().setEstimator(pipeline).setEvaluator(new MulticlassClassificationEvaluator().setLabelCol("indexedLabel").setPredictionCol("prediction").setMetricName("accuracy")).setEstimatorParamMaps(paramGrid).setNumFolds(5)scala> val cvModel = cv.fit(trainingData)scala> val predictions = cvModel.transform(testData)scala> val evaluator = new MulticlassClassificationEvaluator().setLabelCol("indexedLabel").setPredictionCol("prediction").setMetricName("accuracy")scala> val accuracy = evaluator.evaluate(predictions)accuracy: Double = 0.481472scala> println("Test Error = " + (1.0 - accuracy))Test Error = 0.518528
```

执行前一组步骤后，获得的预测结果并不好。让我们检查是否可以通过减少评审类别的数量和增加训练集中的记录数量来提高结果，如图所示:

```scala
scala> def udfReviewBins() = udf[Double, Double] { a => val x = a match { case 1.0 => 1.0; case 2.0 => 1.0; case 3.0 => 2.0; case 4.0 => 3.0; case 5.0 => 3.0;}; x;}scala> val modifiedInDF = inDF.withColumn("rating", udfReviewBins()($"overall")).drop("overall")scala> modifiedInDF.show()
```

![](../images/00276.jpeg)

```scala
scala> modifiedInDF.groupBy("rating").count().orderBy("rating").show()+------+-------+|rating| count|+------+-------+|   1.0| 190864||   2.0| 142257||   3.0|1356067|+------+-------+scala> modifiedInDF.createOrReplaceTempView("modReviewsTable")scala> val reviewsDF = spark.sql(| """| SELECT text, label, rowNumber FROM (| SELECT| rating AS label, reviewText AS text, row_number() OVER (PARTITION BY rating ORDER BY rand()) AS rowNumber FROM modReviewsTable| ) modReviewsTable| WHERE rowNumber <= 120000| """| )reviewsDF: org.apache.spark.sql.DataFrame = [text: string,scala> reviewsDF.groupBy("label").count().orderBy("label").show()+-----+------+|label| count|+-----+------+|  1.0|120000||  2.0|120000||  3.0|120000|+-----+------+scala> val trainingData = reviewsDF.filter(reviewsDF("rowNumber") <= 100000).select("text","label")scala> val testData = reviewsDF.filter(reviewsDF("rowNumber") > 20000).select("text","label")scala> val regexTokenizer = new RegexTokenizer().setPattern("[a-zA-Z']+").setGaps(false).setInputCol("text")scala> val remover = new StopWordsRemover().setInputCol(regexTokenizer.getOutputCol)scala> val bigrams = new NGram().setN(2).setInputCol(remover.getOutputCol)scala> val trigrams = new NGram().setN(3).setInputCol(remover.getOutputCol)scala> val removerHashingTF = new HashingTF().setInputCol(remover.getOutputCol)scala> val ngram2HashingTF = new HashingTF().setInputCol(bigrams.getOutputCol)scala> val ngram3HashingTF = new HashingTF().setInputCol(trigrams.getOutputCol)scala> val assembler = new VectorAssembler().setInputCols(Array(removerHashingTF.getOutputCol, ngram2HashingTF.getOutputCol, ngram3HashingTF.getOutputCol))scala> val labelIndexer = new StringIndexer().setInputCol("label").setOutputCol("indexedLabel").fit(reviewsDF)
```

标签转换器可用于恢复原始标签的文本，以提高文本标签的可读性:

```scala
scala> val labelConverter = new IndexToString().setInputCol("prediction").setOutputCol("predictedLabel").setLabels(labelIndexer.labels)
```

接下来，我们创建一个朴素贝叶斯分类器的实例:

```scala
scala> val nb = new NaiveBayes().setLabelCol(labelIndexer.getOutputCol).setFeaturesCol(assembler.getOutputCol).setPredictionCol("prediction").setModelType("multinomial")
```

我们用所有的变形金刚和朴素贝叶斯估计器组装我们的管道，如下所示:

```scala
scala> val pipeline = new Pipeline().setStages(Array(regexTokenizer, remover, bigrams, trigrams, removerHashingTF, ngram2HashingTF, ngram3HashingTF, assembler, labelIndexer, nb, labelConverter))
```

我们使用交叉验证为模型选择最佳参数，如图所示:

```scala
scala> val paramGrid = new ParamGridBuilder().addGrid(removerHashingTF.numFeatures, Array(1000,10000)).addGrid(ngram2HashingTF.numFeatures, Array(1000,10000)).addGrid(ngram3HashingTF.numFeatures, Array(1000,10000)).build()paramGrid: Array[org.apache.spark.ml.param.ParamMap] =Array({hashingTF_2f3a479f07ef-numFeatures: 1000,hashingTF_0dc7c74af716-numFeatures: 1000,hashingTF_17632a08c82c-numFeatures: 1000}, {hashingTF_2f3a479f07ef-numFeatures: 10000,hashingTF_0dc7c74af716-numFeatures: 1000,hashingTF_17632a08c82c-numFeatures: 1000}, {hashingTF_2f3a479f07ef-numFeatures: 1000,hashingTF_0dc7c74af716-numFeatures: 10000,hashingTF_17632a08c82c-numFeatures: 1000}, {hashingTF_2f3a479f07ef-numFeatures: 10000,hashingTF_0dc7c74af716-numFeatures: 10000,hashingTF_17632a08c82c-numFeatures: 1000}, {hashingTF_2f3a479f07ef-numFeatures: 1000,hashingTF_0dc7c74af716-numFeatures: 1000,hashingTF_17632a08c82c-numFeatures: 10000}, {hashingTF_2f3a479f07ef-numFeatures: 10000,hashingTF_0dc7c74af...scala> val cv = new CrossValidator().setEstimator(pipeline).setEvaluator(new MulticlassClassificationEvaluator().setLabelCol("indexedLabel").setPredictionCol("prediction").setMetricName("accuracy")).setEstimatorParamMaps(paramGrid).setNumFolds(5)scala> val cvModel = cv.fit(trainingData)scala> val predictions = cvModel.transform(testData)scala> val evaluator = new MulticlassClassificationEvaluator().setLabelCol("indexedLabel").setPredictionCol("prediction").setMetricName("accuracy")scala> val accuracy = evaluator.evaluate(predictions)accuracy: Double = 0.63663scala> println("Test Error = " + (1.0 - accuracy))Test Error = 0.36336999999999997
```

请注意，我们的预测结果有了显著的改进，这是因为将分级分成了更少的类别，并增加了用于训练我们模型的记录数量。

在下一节中，我们将展示一个基于文本数据的机器学习的例子。

# 开发机器学习应用程序

在本节中，我们将展示一个用于文本分析的机器学习示例。请参考[第 6 章](06.html#3279U0-e9cbc07f866e437b8aa14e841622275c)、*在机器学习应用程序*中使用 Spark SQL，了解本节介绍的机器学习代码的更多详细信息。

以下示例中使用的数据集包含 1，080 份巴西公司的自由文本业务描述文档，分为九个类别的子集。您可以从[https://archive.ics.uci.edu/ml/datasets/CNAE-9](https://archive.ics.uci.edu/ml/datasets/CNAE-9)下载该数据集。

```scala
scala> val inRDD = spark.sparkContext.textFile("file:///Users/aurobindosarkar/Downloads/CNAE-9.data")scala> val rowRDD = inRDD.map(_.split(",")).map(attributes => Row(attributes(0).toDouble, attributes(1).toDouble, attributes(2).toDouble, attributes(3).toDouble, attributes(4).toDouble, attributes(5).toDouble,...attributes(852).toDouble, attributes(853).toDouble, attributes(854).toDouble, attributes(855).toDouble, attributes(856).toDouble))
```

接下来，我们为输入记录定义一个模式:

```scala
scala> val schemaString = "label _c715 _c195 _c480 _c856 _c136 _c53 _c429 _c732 _c271 _c742 _c172 _c45 _c374 _c233 _c720..._c408 _c604 _c766 _c676 _c52 _c755 _c728 _c693 _c119 _c160 _c141 _c516 _c419 _c69 _c621 _c423 _c137 _c549 _c636 _c772 _c799 _c336 _c841 _c82 _c123 _c474 _c470 _c286 _c555 _c36 _c299 _c829 _c361 _c263 _c522 _c495 _c135"scala> val fields = schemaString.split(" ").map(fieldName => StructField(fieldName, DoubleType, nullable = false))scala> val schema = StructType(fields)
```

然后，我们使用模式将 RDD 转换为数据帧，如图所示:

```scala
scala> val inDF = spark.createDataFrame(rowRDD, schema)scala> inDF.take(1).foreach(println)
```

![](../images/00277.jpeg)

接下来，我们使用`monotonically_increasing_id()`函数向数据框添加一个索引列，如图所示:

```scala
scala> val indexedDF= inDF.withColumn("id", monotonically_increasing_id())scala> indexedDF.select("label", "id").show()
```

![](../images/00278.jpeg)

在接下来的步骤中，我们组装特征向量:

```scala
scala> val columnNames = Array("_c715","_c195","_c480","_c856","_c136","_c53","_c429","_c732","_c271","_c742","_c172","_c45","_c374","_c233","_c720","_c294","_c461","_c87","_c599","_c84","_c28","_c79","_c615","_c243","_c603","_c531","_c503","_c630","_c33","_c428","_c385","_c751","_c664","_c540","_c626","_c730","_c9","_c699","_c117","...c693","_c119","_c160","_c141","_c516","_c419","_c69","_c621","_c423","_c137","_c549","_c636","_c772","_c799","_c336","_c841","_c82","_c123","id","_c474","_c470","_c286","_c555","_c36","_c299","_c829","_c361","_c263","_c522","_c495","_c135")scala> val assembler = new VectorAssembler().setInputCols(columnNames).setOutputCol("features")scala> val output = assembler.transform(indexedDF)scala> output.select("id", "label", "features").take(5).foreach(println)[0,1.0,(857,[333,606,829],[1.0,1.0,1.0])][1,2.0,(857,[725,730,740,844],[1.0,1.0,1.0,1.0])][2,3.0,(857,[72,277,844],[1.0,1.0,2.0])][3,4.0,(857,[72,606,813,822,844],[1.0,1.0,1.0,1.0,3.0])][4,5.0,(857,[215,275,339,386,475,489,630,844],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0])]
```

我们将输入数据集分为训练数据集(90%的记录)和测试数据集(10%的记录):

```scala
scala> val Array(trainingData, testData) = output.randomSplit(Array(0.9, 0.1), seed = 12345)scala> val copyTestData = testData.drop("id").drop("features")copyTestData: org.apache.spark.sql.DataFrame = [label: double, _c715: double ... 855 more fields]scala> copyTestData.coalesce(1).write.format("csv").option("header", "false").mode("overwrite").save("file:///Users/aurobindosarkar/Downloads/CNAE-9/input")
```

在下一步中，我们在训练数据集上创建并拟合逻辑回归模型:

```scala
scala> val lr = new LogisticRegression().setMaxIter(20).setRegParam(0.3).setElasticNetParam(0.8)scala> // Fit the modelscala> val lrModel = lr.fit(trainingData)
```

我们可以列出模型的参数，如图所示:

```scala
scala> println("Model was fit using parameters: " + lrModel.parent.extractParamMap)Model was fit using parameters: {logreg_801d78ffc37d-aggregationDepth: 2,logreg_801d78ffc37d-elasticNetParam: 0.8,logreg_801d78ffc37d-family: auto,logreg_801d78ffc37d-featuresCol: features,logreg_801d78ffc37d-fitIntercept: true,logreg_801d78ffc37d-labelCol: label,logreg_801d78ffc37d-maxIter: 20,logreg_801d78ffc37d-predictionCol: prediction,logreg_801d78ffc37d-probabilityCol: probability,logreg_801d78ffc37d-rawPredictionCol: rawPrediction,logreg_801d78ffc37d-regParam: 0.3,logreg_801d78ffc37d-standardization: true,logreg_801d78ffc37d-threshold: 0.5,logreg_801d78ffc37d-tol: 1.0E-6}
```

接下来，我们显示逻辑回归模型的系数和截距值:

```scala
scala> println(s"Coefficients: \n${lrModel.coefficientMatrix}")Coefficients:10 x 857 CSCMatrix(1,206) 0.33562831098750884(5,386) 0.2803498729889301(7,545) 0.525713129850472scala> println(s"Intercepts: ${lrModel.interceptVector}")Intercepts: [-5.399655915806082,0.6130722758222028,0.6011509547631415,0.6381333836655702,0.6011509547630515,0.5542027670693254,0.6325214445680327,0.5332703316733128,0.6325214445681948,0.5936323589132501]
```

接下来，我们使用该模型对测试数据集进行预测，如下所示:

```scala
scala> val predictions = lrModel.transform(testData)
```

选择示例行以显示预测数据框中的关键列，如图所示:

```scala
scala> predictions.select("prediction", "label", "features").show(5)
```

![](../images/00279.jpeg)

我们使用评估器计算测试误差，如下所示:

```scala
scala> val evaluator = new MulticlassClassificationEvaluator().setLabelCol("label").setPredictionCol("prediction").setMetricName("accuracy")scala> val accuracy = evaluator.evaluate(predictions)accuracy: Double = 0.2807017543859649scala> println("Test Error = " + (1.0 - accuracy))Test Error = 0.7192982456140351
```

将逻辑回归模型应用于我们的数据集的结果不是特别好。现在，我们定义参数网格来探索一组更好的参数是否可以改善我们模型的整体预测结果:

```scala
scala> val lr = new LogisticRegression()lr: org.apache.spark.ml.classification.LogisticRegression = logreg_3fa32a4b5c6dscala> val paramGrid = new ParamGridBuilder().addGrid(lr.regParam, Array(0.1, 0.01)).addGrid(lr.elasticNetParam, Array(0.0, 0.5, 1.0)).build()paramGrid: Array[org.apache.spark.ml.param.ParamMap] =Array({logreg_3fa32a4b5c6d-elasticNetParam: 0.0,logreg_3fa32a4b5c6d-regParam: 0.1}, {logreg_3fa32a4b5c6d-elasticNetParam: 0.0,logreg_3fa32a4b5c6d-regParam: 0.01}, {logreg_3fa32a4b5c6d-elasticNetParam: 0.5,logreg_3fa32a4b5c6d-regParam: 0.1}, {logreg_3fa32a4b5c6d-elasticNetParam: 0.5,logreg_3fa32a4b5c6d-regParam: 0.01}, {logreg_3fa32a4b5c6d-elasticNetParam: 1.0,logreg_3fa32a4b5c6d-regParam: 0.1}, {logreg_3fa32a4b5c6d-elasticNetParam: 1.0,logreg_3fa32a4b5c6d-regParam: 0.01})
```

在这里，我们展示了使用`CrossValidator`为我们的模型选择更好的参数:

```scala
scala> val cv = new CrossValidator().setEstimator(lr).setEvaluator(new MulticlassClassificationEvaluator).setEstimatorParamMaps(paramGrid).setNumFolds(5)
```

接下来，我们运行交叉验证来选择最佳参数集:

```scala
scala> val cvModel = cv.fit(trainingData)scala> println("Model was fit using parameters: " + cvModel.parent.extractParamMap)Model was fit using parameters: {cv_00543dadc091-estimator: logreg_41377555b425,cv_00543dadc091-estimatorParamMaps: [Lorg.apache.spark.ml.param.ParamMap;@14ca1e23,cv_00543dadc091-evaluator: mcEval_0435b4f19e2a,cv_00543dadc091-numFolds: 5,cv_00543dadc091-seed: -1191137437}
```

我们对测试数据集进行预测。这里，`cvModel`使用找到的最佳模型(`lrModel`):

```scala
scala> cvModel.transform(testData).select("id", "label", "probability", "prediction").collect().foreach { case Row(id: Long, label: Double, prob: Vector, prediction: Double) =>| println(s"($id, $label) --> prob=$prob, prediction=$prediction")| }
```

![](../images/00280.jpeg)

```scala
scala> val cvPredictions = cvModel.transform(testData)cvPredictions: org.apache.spark.sql.DataFrame = [label: double, _c715: double ... 860 more fields]scala> cvPredictions.select("prediction", "label", "features").show(5)
```

![](../images/00281.jpeg)

```scala
scala> val evaluator = new MulticlassClassificationEvaluator().setLabelCol("label").setPredictionCol("prediction").setMetricName("accuracy")
```

请注意，由于交叉验证，预测精度显著提高:

```scala
scala> val accuracy = evaluator.evaluate(cvPredictions)accuracy: Double = 0.9736842105263158scala> println("Test Error = " + (1.0 - accuracy))Test Error = 0.02631578947368418
```

最后，将模型保存到文件系统中。我们将在本节后面从程序的文件系统中检索它:

```scala
scala> cvModel.write.overwrite.save("file:///Users/aurobindosarkar/Downloads/CNAE-9/model")
```

接下来，我们构建一个应用程序，使用代码和保存的模型以及来自 spark-shell 会话的测试数据进行编译、打包和执行，如图所示:

**偏斜。刻度**

```scala
package org.chap9.mlimport scala.collection.mutableimport scopt.OptionParserimport org.apache.spark.sql._import org.apache.spark.sql.types._import org.apache.spark.ml._import org.apache.spark.examples.mllib.AbstractParamsimport org.apache.spark.ml.feature.{VectorAssembler}import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel}import org.apache.spark.ml.tuning.{CrossValidator, CrossValidatorModel}import org.apache.spark.ml.evaluation.{MulticlassClassificationEvaluator}import org.apache.spark.sql.{DataFrame, SparkSession}object LRExample {case class Params(inputModelPath: String = null,testInput: String = "") extends AbstractParams[Params]def main(args: Array[String]) {val defaultParams = Params()val parser = new OptionParser[Params]("LRExample") {head("LRExample: an example Logistic Regression.")arg[String]("inputModelPath").text(s"input path to saved model.").required().action((x, c) => c.copy(inputModelPath = x))arg[String]("<testInput>").text("test input path to new data").required().action((x, c) => c.copy(testInput = x))checkConfig { params =>if ((params.testInput == null) || (params.inputModelPath == null)) {failure(s"Both Test Input File && input model path values need to be provided.")} else {success}}}parser.parse(args, defaultParams) match {case Some(params) => run(params)case _ => sys.exit(1)}}def run(params: Params): Unit = {val spark = SparkSession.builder.appName(s"LogisticRegressionExample with $params").getOrCreate()println(s"LogisticRegressionExample with parameters:\n$params")val inRDD = spark.sparkContext.textFile("file://" + params.testInput)val rowRDD = inRDD.map(_.split(",")).map(attributes => Row(attributes(0).toDouble, attributes(1).toDouble, attributes(2).toDouble, attributes(3).toDouble, attributes(4).toDouble, attributes(5).toDouble, attributes(6).toDouble,...attributes(850).toDouble, attributes(851).toDouble, attributes(852).toDouble, attributes(853).toDouble, attributes(854).toDouble, attributes(855).toDouble, attributes(856).toDouble))val schemaString = "label _c715 _c195 _c480 _c856 _c136 _c53 _c429 _c732 _c271 _c742 _c172 _c45 _c374 _c233 _c720 _c294 _c461 _c87 _c599 _c84 _c28 _c79 _c615 _c243..._c336 _c841 _c82 _c123 _c474 _c470 _c286 _c555 _c36 _c299 _c829 _c361 _c263 _c522 _c495 _c135"val fields = schemaString.split(" ").map(fieldName => StructField(fieldName, DoubleType, nullable = false))val schema = StructType(fields)val inDF = spark.createDataFrame(rowRDD, schema)val indexedDF= inDF.withColumn("id",org.apache.spark.sql.functions.monotonically_increasing_id())val columnNames = Array("_c715","_c195","_c480","_c856","_c136","_c53","_c429","_c732","_c271","_c742","_c172","_c45","_c374","_c233","_c720","_c294","_c461","_c87","_c599","_c84","_c28","_c...141","_c516","_c419","_c69","_c621","_c423","_c137","_c549","_c636","_c772","_c799","_c336","_c841","_c82","_c123","id","_c474","_c470","_c286","_c555","_c36","_c299","_c829","_c361","_c263","_c522","_c495","_c135")val assembler = new VectorAssembler().setInputCols(columnNames).setOutputCol("features")val output = assembler.transform(indexedDF)val cvModel = CrossValidatorModel.load("file://"+ params.inputModelPath)val cvPredictions = cvModel.transform(output)val evaluator = new MulticlassClassificationEvaluator().setLabelCol("label").setPredictionCol("prediction").setMetricName("accuracy")val accuracy = evaluator.evaluate(cvPredictions)println("Test Error = " + (1.0 - accuracy))spark.stop()}}
```

在根`SBT`目录中创建一个`lib`文件夹(与包含`build.sbt`文件的目录相同)，并将`scopt_2.11-3.3.0.jar`和`spark-examples_2.11-2.2.1-SNAPSHOT.jar`从 spark distribution 的`examples`目录复制到其中。

接下来，使用前面章节中使用的相同`build.sbt`文件编译并打包源代码，如下所示:

```scala
Aurobindos-MacBook-Pro-2:Chapter9 aurobindosarkar$ sbt package
```

最后，使用 spark-submit 执行您的 Spark Scala 程序，如图所示:

```scala
Aurobindos-MacBook-Pro-2:scala-2.11 aurobindosarkar$ /Users/aurobindosarkar/Downloads/spark-2.2.1-SNAPSHOT-bin-hadoop2.7/bin/spark-submit --jars /Users/aurobindosarkar/Downloads/Chapter9/lib/spark-examples_2.11-2.2.1-SNAPSHOT.jar,/Users/aurobindosarkar/Downloads/Chapter9/lib/scopt_2.11-3.3.0.jar --class org.chap9.ml.LRExample --master local[*] chapter9_2.11-2.0.jar /Users/aurobindosarkar/Downloads/CNAE-9/model /Users/aurobindosarkar/Downloads/CNAE-9/input/part-00000-61f03111-53bb-4404-bef7-0dd4ac1be950-c000.csvLogisticRegressionExample with parameters:{inputModelPath: /Users/aurobindosarkar/Downloads/CNAE-9/model,testInput: /Users/aurobindosarkar/Downloads/CNAE-9/input/part-00000-61f03111-53bb-4404-bef7-0dd4ac1be950-c000.csv}Test Error = 0.02631578947368418
```

# 摘要

在本章中，我们介绍了文本分析空间中的一些 Spark SQL 应用程序。此外，我们还提供了详细的代码示例，包括构建数据预处理管道、实现情感分析、使用带有 n-gram 的朴素贝叶斯分类器以及实现 LDA 应用程序来识别文档语料库中的主题。此外，我们研究了实现机器学习示例的细节。

在下一章中，我们将关注在深度学习应用程序中使用 Spark SQL 的用例。我们将探索一些新兴的深度学习库，并展示实现深度学习相关应用的示例。