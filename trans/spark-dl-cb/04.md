# 递归神经网络的痛点

在本章中，我们将介绍以下食谱:

*   前馈网络介绍
*   RNNs 的顺序工作
*   绘制点# 1–消失渐变问题
*   痛点 2–爆炸式梯度问题
*   LSTMs 的顺序工作

# 介绍

递归神经网络已被证明在涉及顺序数据的学习和预测的任务中非常有效。然而，当涉及到自然语言时，长期依赖的问题就起了作用，这基本上是记住特定对话、段落或句子的上下文，以便在未来做出更好的预测。例如，考虑这样一个句子:

*去年，我碰巧去了中国。中国菜不仅不同于世界上其他地方的中国菜，而且人们也非常热情好客。在这个美丽的国家呆了三年，我学会了说一口流利的英语....*

如果将前面的句子输入递归神经网络来预测句子中的下一个单词(例如中文)，网络会发现很困难，因为它没有句子上下文的记忆。这就是我们所说的长期依赖。为了正确预测中文这个词，网络需要知道句子的上下文，以及记住我去年碰巧访问中国的事实。因此，递归神经网络在执行此类任务时变得效率低下。然而，这个问题被**长短期记忆单元** ( **LSTMs** )所克服，该单元能够记忆长期依赖性并以单元状态存储信息。LSTMs 将在后面讨论，但本章的大部分内容将集中在神经网络、激活函数、递归网络的基本介绍、递归网络的一些主要痛点或缺点，以及如何通过使用 LSTMs 来克服这些缺点。

# 前馈网络介绍

要理解递归网络，首先你必须了解前馈网络的基础知识。这两种网络都是以它们通过在网络节点上执行的一系列数学运算来移动信息的方式命名的。一个只在一个方向上通过每个节点提供信息(从不接触给定的节点两次)，而另一个通过一个循环将信息反馈给同一个节点(有点像反馈循环)。很容易理解第一种如何被称为**前馈网络，**而后者是递归的。

# 准备好了

理解任何神经网络图时最重要的概念是计算图的概念。计算图只不过是相互连接的神经网络节点，每个节点都执行特定的数学功能。

# 怎么做...

前馈神经网络通过一组计算节点引导输入(到输入层)，这些节点只不过是按层排列的数学算子和激活函数，以计算网络输出。输出层是神经网络的最后一层，通常包含线性函数。输入层和输出层之间的层称为**隐藏层**，通常包含非线性元素或函数:

1.  下图(a)显示了多层前馈神经网络中的节点是如何相互连接的:

![](../images/00095.jpeg)

FeedForward Neural Network

1.  前馈神经网络主要在隐藏层节点中使用的函数类型(激活函数)上有所不同。它们的不同之处还在于用于在训练期间优化网络其他参数的算法。
2.  上图所示的节点之间的关系不需要为每个节点都完全填充；优化策略通常从大量隐藏节点开始，随着训练的进行，通过消除连接(可能还有节点)来调整网络。在训练过程中可能没有必要使用每个节点。

# 它是如何工作的...

神经元是任何神经网络的基本结构元件。神经元可以被认为是一个简单的数学函数或算子，它对流经它的输入进行运算，以产生从它流出的输出。神经元的输入乘以节点的权重矩阵，对所有输入求和，转换，并通过激活函数。这些基本上是数学中的矩阵运算，如下所述:

1.  神经元的计算图形表示如前图(b)所示。
2.  单个神经元或节点的传递函数如下:

![](../images/00096.jpeg)

这里， *x* <sub xmlns:epub="http://www.idpf.org/2007/ops" class="calibre39">*i*</sub> 是对 ith 节点的输入， *w* <sub xmlns:epub="http://www.idpf.org/2007/ops" class="calibre39">*i*</sub> 是与 *i* <sup xmlns:epub="http://www.idpf.org/2007/ops" class="calibre40">th</sup> 节点相关联的权重项， *b* 是通常为防止过度拟合而添加的偏差， *f* (⋅)是对流入节点的输入进行操作的激活函数，

 *3.  具有乙状结肠激活函数的神经元通常用于神经网络的隐藏层，而身份函数通常用于输出层。
4.  激活函数通常以确保节点输出严格递增、平滑(连续一阶导数)或渐近的方式选择。
5.  以下逻辑函数用作乙状结肠激活函数:

![](../images/00097.jpeg)

6.  如果激活函数是反对称的，则使用反向传播算法训练的神经可以学习得更快，也就是说，*f*(-*x*)=-*f*(*x*)与乙状结肠激活函数的情况相同。反向传播算法将在本章后面的章节中详细讨论。

7.  然而，逻辑函数并不是反对称的，而是可以通过简单的缩放和移位使其反对称，从而得到双曲正切函数，该函数具有由*f*(*x*)= 1-*f*T6】2(*x*描述的一阶导数，如下数学函数所示:

![](../images/00098.jpeg)

8.  乙状线函数及其导数的简单形式允许快速准确地计算梯度，以优化权重和偏差的选择，并进行二阶误差分析。

# 还有更多...

在神经网络层中的每个神经元/节点上，执行一系列矩阵运算。下图给出了可视化前馈网络的更数学的方法，这将有助于您更好地理解每个节点/神经元的操作:

![](../images/00099.jpeg)

1.  直觉上，我们可以看到输入(是向量或矩阵)首先乘以权重矩阵。这个项加上一个偏置，然后使用激活函数(如 ReLU、tanh、sigmoid、threshold 等)激活，产生输出。激活函数是确保网络能够学习线性和非线性函数的关键。
2.  然后，该输出流入下一个神经元作为其输入，并再次执行相同的操作集。许多这样的神经元组合在一起形成一个层(它执行某个功能或学习输入向量的某个特征)，许多这样的层组合在一起形成一个前馈神经网络，它可以学习完全识别输入，如下图所示:

![](../images/00100.gif)

3.  让我们假设我们的前馈网络已经被训练来对狗的图像和猫的图像进行分类。网络经过训练后，如下图所示，当出现新图像时，它将学会将图像标记为狗或猫:

![](../images/00101.jpeg)

4.  在这样的网络中，当前输出和先前或未来输出之间没有关系。
5.  这意味着前馈网络基本上可以暴露于任何随机的图像集合，并且它所暴露的第一个图像不一定会改变它如何分类第二个或第三个图像。因此，我们可以说时间步长 *t* 的输出独立于时间步长 *t - 1* 的输出。
6.  前馈网络在图像分类等数据不连续的情况下工作良好。前馈网络在温度和位置、高度和重量、车速和品牌等两个相关变量上使用时也表现良好。
7.  然而，可能存在当前输出依赖于先前时间步长的输出的情况(数据的排序很重要)。
8.  考虑一下看书的场景。你对书中句子的理解是基于你对句子中所有单词的理解。不可能使用前馈网络来预测句子中的下一个单词，因为这种情况下的输出将取决于先前的输出。

9.  类似地，在许多情况下，输出需要先前的输出或来自先前输出的一些信息(例如，股市数据、NLP、语音识别等)。如下图所示，可以修改前馈网络，以从以前的输出中获取信息:

![](../images/00102.jpeg)

10.  在时间步骤 *t* ，在 *t* 的输入以及来自 *t-1* 的信息都被提供给网络以获得在时间 *t* 的输出。
11.  类似地，来自 *t* 的信息以及新的输入在时间步长 *t+1* 被馈送到网络，以在 *t+1* 产生输出。上图的右侧是表示这种网络的通用方式，其中网络的输出作为未来时间步长的输入流回。这样的网络称为**递归神经网络** ( **RNN** )。

# 请参见

**激活函数**:在人工神经网络中，给定一个输入或一组输入，节点的激活函数决定了节点产生的输出类型。输出*y<sub class="calibre39">k</sub>T5】由输入 *u <sub class="calibre39">k</sub>* 和偏置 *b <sub class="calibre39">k</sub>* 给出，通过激活功能 *φ(。)*如下式所示:*

![](../images/00103.jpeg)

有各种类型的激活功能。以下是常用的几种:

1.  **阈值功能**:

![](../images/00104.gif)

![](../images/00105.jpeg)

从上图可以清楚地看出，这种函数将神经元的输出值限制在 0 到 1 之间。这在许多情况下可能是有用的。然而，该函数是不可微的，这意味着它不能用于学习非线性，这在使用反向传播算法时至关重要。

2.  **Sigmoid 功能**:

![](../images/00106.jpeg)

![](../images/00107.jpeg)

sigmoid 函数是一个逻辑函数，与阈值函数一样，下限为 0，上限为 1。这个激活函数是连续的，因此也是可微的。在 sigmoid 函数中，前一个函数的斜率参数由α给出。该函数本质上是非线性的，这对于提高性能至关重要，因为与常规线性函数不同，它能够适应输入数据中的非线性。具有非线性能力可确保权重和偏差的微小变化会导致神经元输出的显著变化。

3.  **双曲正切函数(tanh)** :

![](../images/00108.jpeg)

该函数使激活函数的范围从-1 到+1，而不是像前面的情况那样在 0 和 1 之间。

4.  **整流线性单位(ReLU)函数**:ReLU 是许多逻辑单位之和的平滑逼近，产生稀疏的活动向量。以下是该函数的方程式:

![](../images/00109.jpeg)

![](../images/00110.jpeg)

ReLU function graph

在上图中，soft plus![](../images/00111.jpeg)(x)= log(1+e<sup class="calibre40">x</sup>)是整流器的平滑近似。

5.  **Maxout 函数**:该函数利用了一种被称为**“脱扣”**的技术，并提高了脱扣技术快速近似模型平均的准确性，以便于优化。

Maxout 网络不仅学习隐藏单元之间的关系，还学习每个隐藏单元的激活功能。通过主动退出隐藏单元，网络被迫在训练过程中从给定的输入中找到其他路径来获得输出。下图是其工作原理的图形描述:

![](../images/00112.jpeg)

Maxout Network

上图显示了 Maxout 网络，它有五个可见单元，三个隐藏单元，每个隐藏单元有两个神经元。Maxout 函数由以下等式给出:

![](../images/00113.jpeg)

![](../images/00114.jpeg)

这里是 W.. <sub class="calibre39">ij</sub> 是通过访问第二坐标 *i* 和第三坐标 *j* 处的矩阵 W∑![](../images/00115.jpeg)获得的输入大小的平均向量。中间单元的数量( *k)* 称为 Maxout 网使用的件数。下图显示了 Maxout 功能与 ReLU 和**参数整流线性单元** ( **PReLU** )功能的比较:

![](../images/00116.jpeg)

Graphical comparison of Maxout, ReLU and PReLU function

# RNNs 的顺序工作

递归神经网络是一种人工神经网络，旨在识别和学习数据序列中的模式。这种顺序数据的一些例子有:

*   书法
*   客户评论、书籍、源代码等文本
*   口语/自然语言
*   数字时间序列/传感器数据
*   股价变动数据

# 准备好

在递归神经网络中，前一时间步的隐藏状态在下一时间步反馈到网络中，如下图所示:

![](../images/00117.jpeg)

基本上，进入网络的向上箭头代表每个时间步长 RNN 的输入(矩阵/向量)，而从网络出来的向上箭头代表每个 RNN 单元的输出。水平箭头表示在特定时间步长(由特定神经元)学习的信息转移到下一个时间步长。

More information about using RNNs can be found at :
[https://deeplearning4j.org/usingrnns](https://deeplearning4j.org/usingrnns)

# 怎么做...

在递归网络的每个节点/神经元上，执行一系列矩阵乘法步骤。首先将输入向量/矩阵乘以权重向量/矩阵，将偏差加到该项上，最后通过激活函数产生输出(就像前馈网络的情况一样):

1.  下图以计算图的形式展示了可视化神经网络的直观数学方式:

![](../images/00118.jpeg)

2.  在第一时间步(即 *t=0* )， *h* <sub class="calibre39">*0*</sub> 使用上图右侧的第一个公式计算。由于 *h* <sup class="calibre40">*-1*</sup> 不存在，中项变为 0。
3.  输入矩阵 *x* <sub class="calibre39">*0*</sub> 乘以权重矩阵 *w <sub class="calibre39">i</sub>* ，并在该术语中添加一个偏差 *b <sub class="calibre39">h</sub>* 。
4.  将前面两个矩阵相加，然后通过激活函数*g<sub class="calibre39">h</sub>T3】得到*h<sub class="calibre39">0</sub>T7】。**
5.  类似地， *y <sub class="calibre39">0</sub>* 使用上图右侧的第二个等式计算，通过将 *h <sub class="calibre39">0</sub>* 与权重矩阵 *w <sub class="calibre39">y</sub>* 相乘，向其添加一个偏差 *b <sub class="calibre39">y</sub>* ，并通过激活函数 *g <sub class="calibre39">y</sub>* 。

6.  在下一时间步(即 *t=1* )， *h <sup class="calibre40">(t-1)</sup>* 确实存在。无非是*h<sub class="calibre39">0</sub>T9】。该术语与权重矩阵 *w <sub class="calibre39">R</sub>* 相乘，也与新的输入矩阵 *x <sub class="calibre39">1</sub>* 一起作为网络的输入。*
7.  这个过程在多个时间步长上重复，权重、矩阵和偏差在不同的时间步长上流经整个网络。
8.  整个过程在一次迭代中执行，这构成了网络的前向通路。

# 它是如何工作的...

为了训练前馈神经网络，最常用的技术是时间反向传播。它是一种监督学习方法，用于通过在每个时间步长后更新网络中的权重和偏差来减少损失函数。执行多个训练周期(也称为时期)，其中由损失函数确定的误差通过称为梯度下降的技术向后传播。在每个训练周期结束时，网络更新其权重和偏差，以产生更接近期望输出的输出，直到实现足够小的误差:

1.  反向传播算法在每次迭代期间基本上实现以下三个基本步骤:
    *   输入数据的正向传递和损失函数的计算
    *   梯度和误差的计算
    *   通过时间反向传播，并相应地调整权重和偏差
2.  在输入的加权和(在加上偏差后通过激活函数)被输入网络并获得输出后，网络立即比较预测输出与实际情况(正确输出)的差异。
3.  接下来，由网络计算误差。这只是从实际/正确输出中减去的网络输出。

4.  下一步包括根据计算出的误差在整个网络中反向传播。然后更新权重和偏差，以注意误差是增加还是减少。
5.  网络还记得误差是通过增加权重和偏差还是通过减少权重和偏差而增加的。
6.  基于前面的推论，网络在每次迭代期间以误差变得最小的方式继续更新权重和偏差。下面的例子将使事情变得更清楚。
7.  考虑一个教机器如何将数字翻倍的简单案例，如下表所示:

![](../images/00119.jpeg)

8.  如您所见，通过随机初始化权重( *W = 3* ，我们获得了 0、3、6 和 9 的输出。
9.  通过从模型输出列中减去正确输出列来计算误差。平方误差只不过是每个误差项自身的乘积。使用平方误差通常是更好的做法，因为它消除了误差项的负值。
10.  然后，模型意识到，为了最小化误差，需要更新权重。
11.  假设模型在下一次迭代中将其权重更新为 *W = 4* 。这将导致以下输出:

![](../images/00120.jpeg)

12.  模型现在意识到，通过将权重增加到 *W = 4* ，误差实际上增加了。因此，在下一次迭代中，模型通过将其减少到 *W = 2* 来更新其权重，从而产生实际/正确的输出。
13.  注意，在这个简单的情况下，当重量增加时误差增加，当重量减少时误差减小，如下所示:

![](../images/00121.jpeg)

14.  在实际的神经网络中，在每次迭代期间执行许多这样的权重更新，直到模型收敛于实际/正确的输出。

# 还有更多...

如前所述，当重量增加时误差增加，但当重量减少时误差减小。但情况可能并不总是如此。网络使用下图来确定如何更新权重以及何时停止更新权重:

![](../images/00122.jpeg)

*   让权重在第一次迭代开始时初始化为零。随着网络通过从 A 点到 B 点增加权重来更新权重，错误率开始降低。
*   一旦权重达到 B 点，错误率就变得最小。网络不断跟踪错误率。
*   当从点 B 向点 C 进一步增加权重时，网络意识到错误率再次开始增加。因此，网络停止更新其权重，并恢复到 B 点的权重，因为它们是最优的。
*   在下一个场景中，考虑权重被随机初始化为某个值(比如 C 点)的情况，如下图所示:

![](../images/00123.jpeg)

*   在进一步增加这些随机权重时，误差也会增加(从 C 点开始，远离 B 点，如图中的小箭头所示)。
*   网络意识到误差增加，并从点 C 开始降低权重，从而误差减小(由图中从点 C 向点 B 移动的长箭头指示)。这种权重的降低会一直持续到误差达到最小值(图上的 B 点)。
*   网络甚至在到达点 B(由远离点 B 并向图中的点 A 移动的箭头指示)之后继续进一步更新其权重。然后它意识到误差再次增加。因此，它停止权重更新，并恢复到给出最小误差值的权重(即 B 点的权重)。
*   这就是神经网络在反向传播后执行权重更新的方式。这种权重更新是基于动量的。它依赖于每次迭代期间网络每个神经元的计算梯度，如下图所示:

![](../images/00124.jpeg)

基本上，每次输入流入神经元时，相对于输出，计算每个输入的梯度。链式法则用于计算反向传播的反向过程中的梯度。

# 请参见

反向传播背后的数学原理的详细解释可以在以下链接中找到:

*   [https://mattmazur . com/2015/03/17/a 逐步反向传播-示例/](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)
*   [https://beinghuanman . ai/反向传播非常简单-谁让它变得复杂-97b794c97e5](https://becominghuman.ai/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c)

Andrej Karpathy 的博客有大量关于递归神经网络的有用信息。这里有一个链接解释了它们不合理的有效性:

*   [http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)

# 痛点 1–消失的梯度问题

递归神经网络非常适合涉及顺序数据的任务。然而，它们也有缺点。本节将突出并讨论一个这样的缺点，称为**消失渐变问题**。

# 准备好

名称消失梯度问题源于这样一个事实，即在反向传播步骤中，一些梯度消失或变为零。从技术上讲，这意味着在网络的反向传递过程中，没有误差项被反向传播。当网络变得更深、更复杂时，这就成了一个问题。

# 怎么做...

本节将描述消失梯度问题如何在递归神经网络中出现:

*   当使用反向传播时，网络首先计算误差，它只不过是从实际输出平方中减去模型输出(如平方误差)。
*   利用该误差，模型随后计算相对于权重变化(de/dw)的误差变化。
*   计算出的导数乘以学习率![](../images/00125.jpeg)得到![](../images/00126.jpeg) w，这只是权重的变化。术语![](../images/00127.jpeg) w 被添加到原始权重中，以将其更新为新权重。
*   假设 de/dw(相对于权重的误差梯度或变化率)的值远小于 1，那么该项乘以学习率![](../images/00128.jpeg)(总是远小于 1)给出了一个非常小的、可以忽略的、可以忽略的数字。
*   发生这种情况是因为反向传播期间的权重更新仅对最近的时间步长是准确的，并且这种准确性在反向传播之前的时间步长时降低，并且当权重更新流过许多时间步长时变得几乎不重要。
*   在某些情况下，句子可能非常长，神经网络试图预测句子中的下一个单词。它这样做是基于句子的上下文，为此它需要来自许多先前时间步骤的信息(这些被称为**长期依赖关系**)。网络需要反向传播的先前步骤的次数随着句子长度的增加而增加。在这种情况下，循环网络变得无法记住过去许多时间步长的信息，因此无法做出准确的预测。
*   当这种情况发生时，网络需要许多更复杂的计算，其结果是迭代次数显著增加，并且在此期间误差项的变化消失(通过随时间减少)并且权重的变化(![](../images/00129.jpeg) w)变得可忽略不计。因此，新的或更新的权重几乎等于先前的权重。
*   由于没有权重更新发生，网络停止学习或能够更新其权重，这是一个问题，因为这将导致模型过度拟合数据。
*   下图说明了整个过程:

![](../images/00130.jpeg)

# 它是如何工作的...

本节将描述渐变消失问题的一些影响:

1.  当我们使用某种基于梯度的优化技术来训练神经网络模型时，就会出现这个问题。
2.  一般来说，增加更多的隐藏层往往会使网络能够学习更复杂的任意函数，从而在预测未来结果方面做得更好。深度学习有很大的不同，因为它有大量的隐藏层，从 10 到 200 不等。现在可以理解复杂的顺序数据，并执行诸如语音识别、图像分类、图像字幕等任务。
3.  前面的步骤导致的问题是，在某些情况下，梯度变得如此之小，以至于它们几乎消失，这反过来又阻止权重在未来的时间步长中更新它们的值。
4.  在最坏的情况下，它可能导致网络的训练过程被停止，这意味着网络停止学习它打算通过训练步骤学习的不同特征。
5.  反向传播背后的主要思想是，作为研究人员，它允许我们监控和理解机器学习算法是如何处理和学习各种特征的。当梯度消失时，就无法解释网络的运行情况，因此识别和调试错误就变得更加困难。

# 还有更多...

以下是解决渐变消失问题的一些方法:

*   一种在一定程度上克服这个问题的方法是使用 ReLU 激活函数。它计算函数 *f(x)=max(0，x)(即，t* 激活函数简单地将输出的低电平阈值设置为零)，并防止网络产生负梯度。
*   克服这一问题的另一种方法是分别对每一层进行无监督训练，然后通过反向传播对整个网络进行微调，就像于尔根·施密德胡伯在他对神经网络多层次结构的研究中所做的那样。本文的链接将在下一节中提供。
*   这个问题的第三个解决方案是使用 **LSTM** ( **长短期记忆**)单位或 **GRUs(门控循环单位)**，它们是 rnn 的特殊类型。

# 请参见

以下链接提供了对消失渐变问题的更深入描述，以及解决该问题的一些方法:

*   [https://ayaro fai . com/rohan-4-消失的渐变问题-ec68f76ffb9b](https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b)
*   [http://www . cs . Toronto . edu/~ rgrosse/courses/CSC 321 _ 2017/readings/L15 % 20 爆炸% 20 和% 20 消失% 20 radients . pdf](http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf)
*   [http://people.idsia.ch/~juergen/cvpr2012.pdf](http://people.idsia.ch/~juergen/cvpr2012.pdf)

# 痛点 2–爆炸式梯度问题

递归神经网络的另一个缺点是梯度爆炸的问题。这类似于消失梯度问题，但恰恰相反。有时，在反向传播过程中，梯度会爆炸到非常大的值。如同消失梯度问题一样，当网络体系结构变得更深时，会出现爆炸梯度的问题。

# 准备好

名称爆炸梯度问题源于这样一个事实，即在反向传播步骤中，一些梯度消失或变为零。从技术上讲，这意味着在网络的反向传递过程中，没有误差项被反向传播。当网络变得更深、更复杂时，这就成了一个问题。

# 怎么做...

本节将描述递归神经网络中的爆炸梯度问题:

*   爆炸梯度问题与消失梯度问题非常相似，但恰恰相反。
*   当递归神经网络中出现长期依赖性时，误差项通过网络向后传播，有时会爆炸或变得非常大。
*   这个误差项乘以学习率得到一个非常大的![](../images/00131.jpeg) w。这产生了新的权重，看起来与以前的权重非常不同。它被称为爆炸梯度问题，因为梯度的值变得太大。
*   下图以算法的方式说明了渐变爆炸的问题:

![](../images/00132.jpeg)

# 它是如何工作的...

由于神经网络使用基于梯度的优化技术来学习数据中存在的特征，因此必须保留这些梯度，以便网络基于梯度的变化来计算误差。本节将描述在递归神经网络中如何出现爆炸梯度问题:

*   当使用反向传播时，网络首先计算误差，它只不过是从实际输出平方中减去模型输出(如平方误差)。
*   利用该误差，模型随后计算相对于权重变化(de/dw)的误差变化。
*   计算出的导数乘以学习率![](../images/00133.jpeg)得到![](../images/00134.jpeg) w，这只是权重的变化。术语![](../images/00135.jpeg) w 被添加到原始权重中，以将其更新为新权重。
*   假设 de/dw(相对于权重的误差梯度或变化率)的值大于 1，那么该项乘以学习率![](../images/00136.jpeg)给出了一个非常非常大的数字，该数字在试图进一步优化权重时对网络没有用，因为权重不再在相同的范围内。
*   发生这种情况是因为反向传播期间的权重更新仅对最近的时间步长是准确的，并且这种准确性在反向传播之前的时间步长时降低，并且当权重更新流过许多时间步长时变得几乎不重要。
*   随着输入数据中序列数量的增加，网络需要反向传播的前一步的次数也会增加。在这种情况下，循环网络变得无法记住过去许多时间步长的信息，因此无法对未来的时间步长做出准确的预测。
*   当这种情况发生时，网络需要许多更复杂的计算，其结果是迭代次数显著增加，并且在此期间误差项的变化增加超过 1，并且权重的变化(![](../images/00137.jpeg) w)爆炸。因此，与以前的重量相比，新的或更新的重量完全超出范围。
*   由于没有权重更新发生，网络停止学习或能够在指定范围内更新其权重，这是一个问题，因为这将导致模型过度拟合数据。

# 还有更多...

以下是解决渐变爆炸问题的一些方法:

*   某些渐变裁剪技术可以用来解决渐变爆炸的问题。
*   防止这种情况的另一种方法是使用截断的时间反向传播，我们可以选择一个更小的时间步长(比如 15)来开始反向传播，而不是在最后一个时间步长(或输出层)开始反向传播。这意味着网络将在一个实例中仅反向传播最后 15 个时间步骤，并且仅学习与这 15 个时间步骤相关的信息。这类似于将小批量数据馈送到网络，因为在非常大的数据集的情况下，计算数据集的每个元素的梯度会变得非常昂贵。
*   防止梯度爆炸的最后一个选择是通过监控它们并相应地调整学习速率。

# 请参见

在以下链接中可以找到对消失和爆炸渐变问题的更详细解释:

*   [http://neuralnetworksanddeeplearning.com/chap5.html](http://neuralnetworksanddeeplearning.com/chap5.html)
*   [https://www . dlology . com/blog/如何处理-vanishingexploding-gradients-in-keras/](https://www.dlology.com/blog/how-to-deal-with-vanishingexploding-gradients-in-keras/)
*   [https://machinelearning master . com/爆炸式神经网络梯度/](https://machinelearningmastery.com/exploding-gradients-in-neural-networks/)

# LSTMs 的顺序工作

**长短期记忆单元**(**【LSTM】**)单元只不过是比递归网络稍高级的架构。LSTMs 可以被认为是一种特殊的递归神经网络，具有学习序列数据中存在的长期依赖关系的能力。这背后的主要原因是，与递归神经网络不同，LSTMs 包含内存，能够在细胞内存储和更新信息。

# 准备好了

长短期记忆单元的主要组成部分如下:

*   输入门
*   遗忘之门
*   更新门

每个门都由一个 sigmoid 层和一个逐点乘法运算组成。sigmoid 层输出 0 到 1 之间的数字。这些值描述了允许每个组件的多少信息通过各自的门。值为 0 意味着门不允许任何东西通过，而值为 1 意味着门允许所有信息通过。

理解 LSTM 细胞的最好方法是通过计算图，就像递归神经网络一样。

LSTMs 最初是由 Sepp Hochreiter 和 Jurgen Schmidhuber 在 1997 年开发的。以下是他们发表论文的链接:

*   [http://www.bioinf.jku.at/publications/older/2604.pdf](http://www.bioinf.jku.at/publications/older/2604.pdf)

# 怎么做...

本节将描述单个 LSTM 电池的内部组件，主要是电池内部存在的三个不同的门。许多这样的细胞堆叠在一起形成一个 LSTM 网络:

1.  LSTMs 也有类似 RNNs 的链状结构。标准 rnn 基本上是重复单元的模块，就像一个简单的函数(例如 tanh)。

2.  由于每个单元内存的存在，与网络节点相比，网络节点具有长时间保留信息的能力。这使他们能够在输入序列的早期阶段学习重要信息，并使其能够对模型在每个时间步骤结束时做出的决策产生重大影响。
3.  通过能够从输入序列的早期阶段就存储信息，LSTMs 能够主动地保存可以通过时间和层反向传播的错误，而不是让错误消失或爆炸。
4.  LSTMs 能够在许多时间步长上学习信息，因此通过保留通过这些层反向传播的误差，具有更密集的层结构。
5.  被称为“门”的细胞结构赋予 LSTM 保留信息、添加信息或从“T2”细胞状态“T3”中移除信息的能力。
6.  下图说明了 LSTM 的结构。试图理解 LSTMs 的关键特征在于理解 LSTM 网络体系结构和小区状态，这可以在这里可视化:

![](../images/00138.gif)

7.  在上图中， *x <sub class="calibre39">t</sub>* 和 *h <sub class="calibre39">t-1</sub>* 是细胞的两个输入。 *x* <sub class="calibre39">*t*</sub> 为当前时间步长的输入，h <sub class="calibre39">t-1</sub> 为前一时间步长的输入(为前一时间步长内前一单元格的输出)。除了这两个输入之外，我们还有 *h* <sub class="calibre39">，</sub>，这是 LSTM 单元通过其门对两个输入执行操作后的电流输出(即时间步长 t)。
8.  在上图中，r <sub xmlns:epub="http://www.idpf.org/2007/ops" class="calibre39">t</sub> 代表从输入门出现的输出，输入门接收输入*h*<sub xmlns:epub="http://www.idpf.org/2007/ops" class="calibre39">T5】t-1</sub>和 *x <sub class="calibre39">t</sub>* ，将这些输入与其权重矩阵 *W <sub class="calibre39">z</sub>* 相乘，并通过 sigmoid 激活函数。
9.  类似地，术语 *z <sub class="calibre39">t</sub>* 代表从遗忘门出现的输出。这个门有一组权重矩阵(由 *W <sub class="calibre39">r</sub>* 表示)，这些矩阵是特定于这个门的，并且控制门如何工作。
10.  最后是![](../images/00139.jpeg) <sub class="calibre39">t</sub> ，这是更新门出现的输出。在这种情况下，有两个部分。第一部分是 sigmoid 层，也称为**输入门层**，其主要功能是决定更新哪些值。下一层是 tanh 层。该层的主要功能是创建一个矢量或数组，其中包含可以添加到单元状态的新值。

# 它是如何工作的...

多个 LSTM 小区/单元的组合形成了 LSTM 网络。下图显示了这种网络的体系结构:

![](../images/00140.jpeg)

1.  在上图中，完整的 LSTM 单元由***【A】***表示。单元格取一系列输入的当前输入( *x* *<sub class="calibre39">i</sub>* )，并产生( *h* *<sub class="calibre39">i</sub>* )，这只是当前隐藏状态的输出。该输出随后被发送到下一个 LSTM 单元作为其输入。
2.  LSTM 细胞比 RNN 细胞稍微复杂一些。RNN 单元只有一个作用于电流输入的功能/层，而 LSTM 单元有三个层，这三个层是在任何给定时刻控制流经单元的信息的三个门。
3.  这种电池的性能很像计算机中的硬盘存储器。因此，该单元具有允许在其单元状态内写入、读取和存储信息的能力。该单元还决定存储什么信息，以及何时允许读取、写入和擦除信息。这是由相应打开或关闭的门促成的。
4.  与今天计算机中的数字存储系统相比，LSTM 电池中的栅极是模拟的。这意味着只能通过 sigmoids 的逐元素乘法来控制门，从而产生 0 到 1 之间的概率值。高值将导致闸门保持打开，而低值将导致闸门保持关闭。
5.  当涉及到神经网络操作时，模拟系统比数字系统有优势，因为它们是可微分的。这使得模拟系统更适合像反向传播这样主要依赖梯度的任务。
6.  根据信息的强度和重要性，这些门传递信息或者阻挡信息，或者只让一部分信息通过它们。信息在每个时间步长通过特定于每个门的权重矩阵集进行过滤。因此，每一个门都完全控制如何对它接收到的信息采取行动。
7.  与每个门相关联的权重矩阵，像调整输入和隐藏状态的权重一样，是基于递归网络的学习过程和梯度下降来调整的。

8.  第一个门被称为**忘记门**，它控制着从前一个状态保持的信息。该门将前一个单元格输出(*h**<sub class="calibre39">t</sub>**-1*)与当前输入( *x* *<sub class="calibre39">t</sub>* )一起作为其输入，并应用 sigmoid 激活(![](../images/00141.jpeg))，以便为每个隐藏单元产生并输出 0 到 1 之间的值。随后是与当前状态的逐元素相乘(如前图中的第一个操作所示)。
9.  第二门称为**更新门**，其主要功能是根据当前输入更新单元状态。该门将与遗忘门的输入相同的输入(*h**T5】t-1*和 *x* *<sub class="calibre39">t</sub>* )传递到 sigmoid 激活层(![](../images/00141.jpeg))，然后是 tanh 激活层，然后在这两个结果之间执行元素乘法。接下来，对结果和当前状态执行逐元素相加(由上图中的第二个操作说明)。
10.  最后，还有一个输出门，它控制哪些信息和多少信息被传送到相邻的单元，在下一个时间步骤中作为其输入。当前单元状态通过一个 tanh 激活层，并在通过一个 sigmoid 层(![](../images/00141.jpeg))之后，与单元输入(*h*和*x*)元素相乘。
11.  更新门充当过滤器，过滤单元决定输出到下一个单元的内容。这个输出，h <sub class="calibre39">t</sub> ，然后被传递到下一个 LSTM 单元作为它的输入，并且如果许多 LSTM 单元被堆叠在彼此之上，也传递到上面的层。

# 还有更多...

与前馈网络和递归神经网络相比，LSTMs 是一个巨大的飞跃。人们可能会想，在不久的将来，下一个重大的步骤是什么，或者甚至那一步可能是什么。许多研究人员确实认为，在人工智能领域，“注意力”是下一个重大步骤。随着数据量与日俱增，处理每一个数据变得不可能。在这种情况下，注意力可能成为潜在的游戏规则改变者，导致网络只关注高优先级或高兴趣的数据或区域，而忽略无用的信息。例如，如果使用 RNN 创建图像字幕引擎，它将只选取图像的一部分来关注它输出的每个单词。

徐等人最近(2015 年)的论文正是这样做的。他们探索增加对 LSTM 细胞的关注。阅读这篇论文是开始学习神经网络中注意力使用的好地方。在将注意力用于各种任务方面已经取得了一些好的结果，目前正在对这一主题进行更多的研究。徐等人的论文可以通过以下链接找到:
[【https://arxiv.org/pdf/1502.03044v2.pdf】](https://arxiv.org/pdf/1502.03044v2.pdf)

注意力并不是 LSTMs 的唯一变体。其他一些活跃的研究是基于网格低成本模型的利用，正如 Kalchbrenner 等人的论文中所使用的，其链接位于:[https://arxiv.org/pdf/1507.01526v1.pdf](https://arxiv.org/pdf/1507.01526v1.pdf)。

# 请参见

通过访问以下链接，可以找到与生殖网络中的生殖网络网络和生殖网络管理系统相关的其他有用信息和论文:

*   [http://www.deeplearningbook.org/contents/rnn.html](http://www.deeplearningbook.org/contents/rnn.html)
*   [https://arxiv.org/pdf/1502.04623.pdf](https://arxiv.org/pdf/1502.04623.pdf)
*   [https://arxiv.org/pdf/1411.7610v3.pdf](https://arxiv.org/pdf/1411.7610v3.pdf)
*   [https://arxiv.org/pdf/1506.02216v3.pdf](https://arxiv.org/pdf/1506.02216v3.pdf)*