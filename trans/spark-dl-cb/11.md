# 使用 Word2Vec 创建和可视化单词向量

在本章中，我们将介绍以下食谱:

*   获取数据
*   导入必要的库
*   准备数据
*   构建和培训模型
*   进一步可视化
*   进一步分析

# 介绍

在对文本数据训练神经网络并使用 LSTM 单元生成文本之前，了解文本数据(如单词、句子、客户评论或故事)如何在输入神经网络之前首先转换为单词向量是很重要的。本章将描述如何将文本转换为语料库并从语料库中生成单词向量，这使得使用不同单词向量之间的欧几里德距离计算或余弦距离计算等技术对相似单词进行分组变得容易。

# 获取数据

第一步是获取一些数据。对于这一章，我们将需要大量的文本数据来将其转换为标记并可视化，以了解神经网络如何基于欧几里德和余弦距离对单词向量进行排序。这是理解不同单词如何相互关联的重要一步。这反过来又可以用来设计更好、更高效的语言和文本处理模型。

# 准备好了

请考虑以下几点:

*   模型的文本数据需要是`.txt`格式的文件，必须保证文件放在当前工作目录下。文本数据可以是来自推特订阅源、新闻订阅源、客户评论、计算机代码或工作目录中以`.txt`格式保存的整本书的任何内容。在我们的例子中，我们使用了*权力的游戏*书籍作为我们模型的输入文本。然而，任何文本都可以代替书籍，同样的模式也可以。
*   许多经典文本不再受版权保护。这意味着你可以免费下载这些书的所有文本，并在实验中使用它们，比如创建生成模型。获得不再受版权保护的免费书籍的最佳地点是古腾堡计划(https://www.gutenberg.org/)。

# 怎么做...

步骤如下:

1.  首先访问古腾堡计划网站，浏览一本你感兴趣的书。点击这本书，然后点击 UTF-8，它允许你以纯文本格式下载这本书。该链接显示在以下屏幕截图中:

![](../images/00318.jpeg)

Project Gutenberg Dataset download page

2.  点击纯文本 UTF-8 后，您应该会看到如下截图所示的页面。右键单击页面，然后单击另存为...接下来，将文件重命名为您选择的任何名称，并将其保存在您的工作目录中:

![](../images/00319.jpeg)

3.  您现在应该会在当前工作目录中看到一个具有指定文件名的`.txt`文件。
4.  古腾堡计划为每本书增加了一个标准的页眉和页脚；这不是原文的一部分。在文本编辑器中打开文件，并删除页眉和页脚。

# 它是如何工作的...

功能如下:

1.  使用以下命令检查当前工作目录:`pwd`。
2.  可以使用`cd`命令更改工作目录，如下图所示:

![](../images/00320.jpeg)

3.  请注意，在我们的例子中，文本文件包含在名为`USF`的文件夹中，因此，这被设置为工作目录。同样，您可以在工作目录中存储一个或多个`.txt`文件，用作模型的输入。
4.  UTF-8 指定文本文件中字符的编码类型。 **UTF-8** 代表 **Unicode 转换格式**。 **8** 表示它使用 **8 位**块来表示一个字符。
5.  UTF-8 是一种折衷的字符编码，可以像 ASCII 一样紧凑(如果文件只是纯英文文本)，但也可以包含任何 Unicode 字符(文件大小有所增加)。
6.  文本文件不必采用 UTF-8 格式，因为我们将在稍后阶段使用编解码器库将所有文本编码为 Latin1 编码格式。

# 还有更多...

有关 UTF 8 和拉丁 1 编码格式的更多信息，请访问以下链接:

*   [https://en . Wikipedia . org/wiki/utf-8](https://en.wikipedia.org/wiki/UTF-8)
*   [http://www . IC . unicamp . br/~ stol fi/EXport/www/ISO-8859-1-encoding . html](http://www.ic.unicamp.br/~stolfi/EXPORT/www/ISO-8859-1-Encoding.html)

# 请参见

访问以下链接，更好地了解神经网络对词向量的需求:
[https://medium . com/deep-math-machine-learning-ai/chapter-9-1-NLP-word-vectors-d51 BFF 9628 C1](https://medium.com/deep-math-machine-learning-ai/chapter-9-1-nlp-word-vectors-d51bff9628c1)

下面列出了其他一些与将单词转换为向量相关的有用文章:
[https://monkey learn . com/blog/word-embedding-transform-text-numbers/](https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/)

[https://towards data science . com/word-to-vectors-自然语言处理-b253dd0b0817](https://towardsdatascience.com/word-to-vectors-natural-language-processing-b253dd0b0817)

# 导入必要的库

在开始之前，我们需要以下库和依赖项，这些库和依赖项需要导入到我们的 Python 环境中。这些库将使我们的任务变得容易得多，因为它们有现成可用的函数和模型，可以用来代替我们自己去做。这也使得代码更加紧凑和可读。

# 准备好

创建单词向量和绘图以及在 2D 空间中可视化 n 维单词向量需要以下库和依赖项:

*   `future`
*   `codecs`
*   `glob`
*   ``multiprocessing``
*   `os`
*   ``pprint``
*   `re`
*   `nltk`
*   `Word2Vec`
*   `sklearn`
*   `numpy`
*   `matplotlib`
*   `pandas`
*   `seaborn`

# 怎么做...

步骤如下:

1.  在 Jupyter 笔记本中键入以下命令以导入所有必需的库:

```py
from __future__ import absolute_import, division, print_functionimport codecsimport globimport loggingimport multiprocessingimport osimport pprintimport reimport nltkimport gensim.models.word2vec as w2vimport sklearn.manifoldimport numpyas npimport matplotlib.pyplot as pltimport pandas as pdimport seaborn as sns%pylab inline
```

2.  您应该会看到如下截图所示的输出:

![](../images/00321.jpeg)

3.  接下来，使用以下命令导入`stopwords`和`punkt`库:

```py
nltk.download("punkt")nltk.download("stopwords")
```

4.  您看到的输出必须如下图所示:

![](../images/00322.jpeg)

# 它是如何工作的...

本节将描述用于此配方的每个库的用途。

1.  `future`库是 Python 2 和 Python 3 之间缺失的一环。它充当两个版本之间的桥梁，允许我们使用两个版本的语法。
2.  `codecs`库将用于对文本文件中出现的所有单词进行编码。这就是我们的数据集。

3.  Regex 是用来快速查找或搜索文件的库。`glob`功能允许在大型数据库中快速高效地搜索所需文件。
4.  `multiprocessing`库允许我们执行并发，这是一种运行多个线程并让每个线程运行不同进程的方式。这是一种通过并行化使程序运行得更快的方法。

5.  `os`库允许与操作系统(如 Mac、Windows 等)轻松交互，并执行读取文件等功能。
6.  `pprint`库提供了以一种可用作解释器输入的形式漂亮打印任意 Python 数据结构的能力。

7.  `re`模块提供了类似于 Perl 中的正则表达式匹配操作。
8.  NLTK 是一个自然语言工具包，能够用非常短的代码标记单词。当输入一个完整的句子时，`nltk`函数分解句子并输出每个单词的标记。基于这些标记，单词可以被组织成不同的类别。NLTK 通过将每个单词与一个巨大的预训练单词数据库进行比较来做到这一点，该数据库被称为“T2”词典。
9.  `Word2Vec`是谷歌的模型，在庞大的词向量数据集上进行训练。它将语义相似的单词组合在一起。这将是本部分最重要的库。
10.  `sklearn.manifold`允许通过采用 **t 分布随机邻域嵌入** ( **t-SNE** )技术来对数据集进行降维。由于每个单词向量都是多维的，因此我们需要某种形式的降维技术来将这些单词的维度降低到更低的维度空间，以便可以在 2D 空间中可视化。

# 还有更多...

`Numpy`是常用的`math`库。`Matplotlib`是我们将利用的`plotting`库，`pandas`通过允许对数据进行简单的整形、切片、索引、子集化和操作，在数据处理方面提供了很大的灵活性。

`Seaborn`库是我们和`matplotlib`一起需要的另一个统计数据可视化库。`Punkt`和`Stopwords`是两个简化任务的数据处理库，例如将语料库中的一段文本拆分成标记(即通过标记化)和移除`stopwords`。

# 请参见

有关使用的一些库的更多信息，请访问以下链接:

*   [https://docs.python.org/3/library/codecs.html](https://docs.python.org/3/library/codecs.html)
*   [https://docs.python.org/2/library/pprint.html](https://docs.python.org/2/library/pprint.html)
*   [https://docs.python.org/3/library/re.html](https://docs.python.org/3/library/re.html)
*   [https://www.nltk.org/](https://www.nltk.org/)
*   [https://www.tensorflow.org/tutorials/word2vec](https://www.tensorflow.org/tutorials/word2vec)
*   [http://scikit-learn.org/stable/modules/manifold.html](http://scikit-learn.org/stable/modules/manifold.html)

# 准备数据

在将数据输入模型之前，需要执行许多数据预处理步骤。本节将描述如何清理数据并准备数据，以便将其输入模型。

# 准备好

所有来自`.txt`文件的文本首先被转换成一个大的语料库。这是通过从每个文件中读取每个句子并将其添加到一个空语料库中来完成的。然后执行许多预处理步骤来消除不规则性，如空格、拼写错误、`stopwords`等。然后，必须对清理后的文本数据进行标记化，通过循环运行标记化的句子，将其添加到空数组中。

# 怎么做...

步骤如下:

1.  输入以下命令，搜索工作目录中的`.txt`文件，并打印找到的文件名称:

```py
book_names = sorted(glob.glob("./*.txt"))print("Found books:")book_names
```

在我们的案例中，工作目录中保存了五本名为`got1`、`got2`、`got3`、`got4`、`got5`的书。

2.  创建一个`corpus`，从第一个文件开始读取每个句子，对其进行编码，并使用以下命令将编码后的字符添加到一个`corpus`中:

```py
corpus = u''for book_name in book_names:print("Reading '{0}'...".format(book_name))with codecs.open(book_name,"r","Latin1") as book_file:corpus += book_file.read()print("Corpus is now {0} characters long".format(len(corpus)))print()
```

3.  执行前面步骤中的代码，其输出结果应该如下图所示:

![](../images/00323.jpeg)

4.  使用以下命令从`punkt`加载英国泡菜`tokenizer`:

```py
tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
```

5.  `Tokenize`使用以下命令将整个`corpus`转化为句子:

```py
raw_sentences = tokenizer.tokenize(corpus)
```

6.  定义将句子拆分成其组成词的功能，并以下列方式删除不必要的字符:

```py
def sentence_to_wordlist(raw):clean = re.sub("[^a-zA-Z]"," ", raw)words = clean.split()return words
```

7.  将句子中每个单词都标记化的所有原始句子添加到新的句子数组中。这是通过使用以下代码来完成的:

```py
sentences = []for raw_sentence in raw_sentences:if len(raw_sentence) > 0:sentences.append(sentence_to_wordlist(raw_sentence))
```

8.  从语料库中打印一个随机句子，直观地查看`tokenizer`如何拆分句子，并根据结果创建一个单词列表。这是使用以下命令完成的:

```py
print(raw_sentences[50])print(sentence_to_wordlist(raw_sentences[50]))
```

9.  使用以下命令计算数据集中的令牌总数:

```py
token_count = sum([len(sentence) for sentence in sentences])print("The book corpus contains {0:,} tokens".format(token_count))
```

# 它是如何工作的...

执行标记器并标记语料库中的所有句子应该会产生如下截图所示的输出:

![](../images/00324.jpeg)

接下来，删除不必要的字符，如连字符和特殊字符，以下列方式完成。使用用户定义的`sentence_to_wordlist()`函数拆分所有句子会产生如下截图所示的输出:

![](../images/00325.jpeg)

将原始句子添加到名为`sentences[]`的新数组中会产生如下截图所示的输出:
![](../images/00326.jpeg)

在打印语料库中的标记总数时，我们注意到整个语料库中有 1，110，288 个标记。下面的截图说明了这一点:

![](../images/00327.jpeg)

功能如下:

1.  来自 NLTK 的预训练`tokenizer`用于通过将每个句子计数为一个标记来标记整个语料库。每个标记化的句子都被添加到变量`raw_sentences`中，该变量存储标记化的句子。
2.  在下一步中，删除常用的停止词，并通过将每个句子拆分成单词来清理文本。
3.  一个随机的句子和它的单词表被打印出来，以理解这是如何工作的。在我们的例子中，我们选择打印`raw_sentences`数组中的第 50 个句子。
4.  句子数组中的标记(在我们的例子中是句子)的总数被计数和打印。在我们的例子中，我们看到 1，110，288 个代币由`tokenizer`创建。

# 还有更多...

有关段落和句子标记化的更多信息，请访问以下链接:

*   [https://textminingonline . com/dive-in-nltk-part-ii-句子-token ize-和-word-tokenize](https://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize)
*   [https://stackoverflow . com/questions/37605710/token ize-一段成句再成词-in-nltk](https://stackoverflow.com/questions/37605710/tokenize-a-paragraph-into-sentence-and-then-into-words-in-nltk)
*   [https://pytonspot . com/token izing-单词和句子-with-nltk/](https://pythonspot.com/tokenizing-words-and-sentences-with-nltk/)

# 请参见

有关正则表达式如何工作的更多信息，请访问以下链接:

[https://stackoverflow . com/questions/13090806/干净的标点符号行和拆分成单词 python](https://stackoverflow.com/questions/13090806/clean-line-of-punctuation-and-split-into-words-python)

# 构建和培训模型

一旦我们在一个数组中有了令牌形式的文本数据，我们就可以将它以数组格式输入到模型中。首先，我们必须为模型定义一些超参数。本节将描述如何执行以下操作:

*   声明模型超参数
*   使用`Word2Vec`建立模型
*   在准备好的数据集上训练模型
*   保存并检查训练好的模型

# 准备好

要声明的一些模型超参数包括:

*   结果词向量的维数
*   最小字数阈值
*   训练模型时要运行的并行线程数
*   上下文窗口长度
*   下采样(对于频繁出现的单词)
*   播种

一旦声明了前面提到的超参数，就可以使用`Gensim`库中的`Word2Vec`函数构建模型。

# 怎么做...

步骤如下:

1.  使用以下命令声明模型的超参数:

```py
num_features = 300min_word_count = 3num_workers = multiprocessing.cpu_count()context_size = 7downsampling = 1e-3seed = 1
```

2.  使用声明的超参数，用下面几行代码构建模型:

```py
got2vec = w2v.Word2Vec(sg=1,seed=seed,workers=num_workers,size=num_features,min_count=min_word_count,window=context_size,sample=downsampling)
```

3.  使用标记化的句子并遍历所有标记来构建模型的词汇表。这是使用`build_vocab`功能以下列方式完成的:

```py
got2vec.build_vocab(sentences,progress_per=10000, keep_raw_vocab=False, trim_rule=None)
```

4.  使用以下命令训练模型:

```py
got2vec.train(sentences, total_examples=got2vec.corpus_count, total_words=None, epochs=got2vec.iter, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False)
```

5.  创建一个名为 trained 的目录，如果它还不存在的话。使用以下命令保存并检查`trained`模型:

```py
if not os.path.exists("trained"):os.makedirs("trained")got2vec.wv.save(os.path.join("trained", "got2vec.w2v"), ignore=[])
```

6.  要在任一点加载保存的模型，请使用以下命令:

```py
got2vec = w2v.KeyedVectors.load(os.path.join("trained", "got2vec.w2v"))
```

# 它是如何工作的...

功能如下:

1.  The declaration of model parameters does not produce any output. It just makes space in the memory to store variables as model parameters. The following screenshot describes this process:

    ![](../images/00328.jpeg)

2.  The model is built using the preceding hyperparameters. In our case, we have named the model `got2vec` ,but the model may be named as per your liking. The model definition is illustrated in the following screenshot:

    ![](../images/00329.jpeg)

3.  Running the `build_vocab` command on the model should produce an output as seen in the following screenshot:

    ![](../images/00330.jpeg)

4.  Training the model is done by defining the parameters as seen in the following screenshot:

    ![](../images/00331.jpeg)

5.  上面的命令产生了如下截图所示的输出:

![](../images/00332.jpeg)

6.  保存、检查和加载模型的命令产生如下输出，如屏幕截图所示:

![](../images/00333.jpeg)

# 还有更多...

请考虑以下几点:

*   在我们的例子中，我们注意到`build_vocab`函数从 1，110，288 个单词的列表中识别出 23，960 个不同的单词类型。然而，这个数字会因不同的文本语料库而异。
*   每个单词都由一个 300 维向量表示，因为我们已经声明维度为 300。增加这个数字增加了模型的训练时间，但也确保模型容易推广到新数据。
*   1e ![](../images/00334.jpeg) 3 的下采样速率被发现是一个好的速率。这是为了让模型知道何时对频繁出现的单词进行下采样，因为它们在分析时并不重要。这类词的例子有这个、那个、那些、他们等等。
*   设置种子是为了使结果可复制。设置种子也使调试变得容易得多。
*   使用常规的中央处理器计算来训练模型需要大约 30 秒，因为模型不是很复杂。
*   勾选后，模型保存在工作目录内的`trained`文件夹下。

# 请参见

有关`Word2Vec`模型和 Gensim 库的更多信息，请访问以下链接:

[https://radimrehurek . com/genim/models/word 2vec . html](https://radimrehurek.com/gensim/models/word2vec.html)

# 进一步可视化

本节将描述如何挤压所有训练单词的维度，并将其全部放入一个巨大的矩阵中，以便可视化。因为每个单词都是一个 300 维的向量，所以需要把它降低到更低的维度，以便我们在 2D 空间中可视化它。

# 准备好了

一旦模型在训练后被保存和检查，就像你在上一节所做的那样，开始把它加载到内存中。本节将使用的库和模块有:

*   `tSNE`
*   `pandas`
*   `Seaborn`
*   `numpy`

# 怎么做...

步骤如下:

1.  使用以下命令挤压 300 维单词向量的维度:

```py
 tsne = sklearn.manifold.TSNE(n_components=2, random_state=0)
```

2.  将所有单词向量放入一个巨大的矩阵中(命名为`all_word_vectors_matrix`，使用以下命令查看:

```py
 all_word_vectors_matrix = got2vec.wv.syn0print (all_word_vectors_matrix)
```

3.  使用`tsne`技术，使用以下命令将所有学习到的表示拟合到二维空间中:

```py
 all_word_vectors_matrix_2d =  tsne.fit_transform(all_word_vectors_matrix)
```

4.  使用以下代码收集所有单词向量及其关联单词:

```py
 points = pd.DataFrame([(word, coords[0], coords[1])for word, coords in [(word, all_word_vectors_matrix_2d[got2vec.vocab[word].index])for word in got2vec.vocab]],columns=["word", "x", "y"])
```

5.  使用以下命令可以获得前十个点的`X`和`Y`坐标和相关单词:

```py
points.head(10)
```

6.  使用以下命令绘制所有点:

```py
sns.set_context("poster")points.plot.scatter("x", "y", s=10, figsize=(15, 15))
```

7.  可以放大绘制图形的选定区域，以便进行更仔细的检查。使用以下函数对原始数据进行切片:

```py
def plot_region(x_bounds, y_bounds):slice = points[(x_bounds[0] <= points.x) &(points.x <= x_bounds[1]) &(y_bounds[0] <= points.y) &(points.y <= y_bounds[1])]ax = slice.plot.scatter("x", "y", s=35, figsize=(10, 8))for i, point in slice.iterrows():ax.text(point.x + 0.005, point.y + 0.005, point.word,                                                  fontsize=11)
```

8.  使用以下命令绘制切片数据。切片数据可以被可视化为所有数据点的原始图的放大区域:

```py
plot_region(x_bounds=(20.0, 25.0), y_bounds=(15.5, 20.0))
```

# 它是如何工作的...

功能如下:

1.  t-SNE 算法是一种非线性降维技术。计算机在计算过程中很容易解释和处理许多维度。然而，人类一次只能可视化两个或三个维度。因此，当试图从数据中获取见解时，这些降维技术非常有用。

2.  在将 t-SNE 应用于 300 维向量时，我们能够将它挤压成两个维度来绘制和观察它。
3.  By specifying `n_components` as 2, we let the algorithm know that it has to squash the data into a two-dimensional space. Once this is done, we add all the squashed vectors into one giant matrix named `all_word_vectors_matrix`, which is illustrated in the following screenshot:

    ![](../images/00335.jpeg)

4.  t-SNE 算法需要在所有这些单词向量上进行训练。在常规的中央处理器上，训练大约需要五分钟。
5.  Once the t-SNE is finished training on all the word vectors, it outputs 2D vectors for each word. These vectors may be plotted as points by converting all of them into a data frame. This is done as shown in the following screenshot:

    ![](../images/00336.jpeg)

6.  我们看到前面的代码产生了许多点，每个点代表一个单词及其 X 和 Y 坐标。在检查数据框的前二十个点时，我们看到如下截图所示的输出:

![](../images/00337.jpeg)

7.  On plotting all the points using the `all_word_vectors_2D` variable, you should see an output that looks similar to the one in the following screenshot:

    ![](../images/00338.jpeg)

8.  The above command will produce a plot of all tokens or words generated from the entire text as shown in the following screenshot:

    ![](../images/00339.jpeg)

9.  We can use the `plot_region` function to zoom into a certain area of the plot so that we are able to actually see the words, along with their coordinates. This step is illustrated in the following screenshot:

    ![](../images/00340.jpeg)

10.  An enlarged or zoomed in area of the plot can be visualized by setting the `x_bounds` and `y_bounds`, values as shown in the following screenshot:

    ![](../images/00341.jpeg)

11.  A different region of the same plot can be visualized by varying the `x_bounds` and `y_bounds` values as shown in the following two screenshots:

    ![](../images/00342.jpeg)

![](../images/00343.jpeg)

# 请参见

以下几点值得注意:

*   有关 t-SNE 算法如何工作的更多信息，请访问以下链接:
*   [https://www . oreilly . com/learning/an-installed-introduction-to-t-SNE-algorithm](https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm)
*   更多关于余弦距离相似度和排名的信息可以访问以下链接:[https://code.google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/)
*   使用以下链接探索`Seaborn`库的不同功能:[https://seaborn.pydata.org/](https://seaborn.pydata.org/)

# 进一步分析

本节将描述可视化后可以对数据进行的进一步分析。例如，探索不同单词向量之间的余弦距离相似性。

# 准备好了

以下链接是一个关于余弦距离相似性如何工作的很好的博客，还讨论了一些相关的数学问题:

[http://blog . christianperone . com/2013/09/机器学习-余弦-向量空间相似度-模型-第三部分/](http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/)

# 怎么做...

请考虑以下几点:

*   Various natural-language processing tasks can be performed using the different functions of `Word2Vec`. One of them is finding the most semantically similar words given a certain word (that is, word vectors that have a high cosine similarity or a short Euclidean distance between them). This can be done by using the `most_similar` function form `Word2Vec`, as shown in the following screenshot:

    ![](../images/00344.jpeg)

    This screenshots all the closest words related to the word `Lannister`:

    ![](../images/00345.jpeg)

    This screenshot shows a list of all the words related to word `Jon`:

    ![](../images/00346.jpeg)

# 它是如何工作的...

请考虑以下几点:

*   有各种各样的方法来衡量单词之间的语义相似度。我们在这一节使用的是基于余弦相似性。我们还可以通过使用下面几行代码来探索单词之间的线性关系:

```py
 def nearest_similarity_cosmul(start1, end1, end2):similarities = got2vec.most_similar_cosmul(positive=[end2, start1],negative=[end1])start2 = similarities[0][0]print("{start1} is related to {end1}, as {start2} is related to         {end2}".format(**locals()))return start2
```

*   要查找与给定单词集最接近的单词的余弦相似度，请使用以下命令:

```py
nearest_similarity_cosmul("Stark", "Winterfell", "Riverrun")nearest_similarity_cosmul("Jaime", "sword", "wine")nearest_similarity_cosmul("Arya", "Nymeria", "dragons")
```

*   The preceding process is illustrated in the following screenshot:

    ![](../images/00347.jpeg)

*   The results are as follows:

    ![](../images/00348.jpeg)

*   如本节所见，单词向量构成了所有自然语言处理任务的基础。在深入研究更复杂的自然语言处理模型之前，了解它们以及构建这些模型的数学方法非常重要，例如**递归神经网络**和**长短期记忆** ( **LSTM** )细胞。

# 请参见

可以进行进一步的阅读，以便更好地理解余弦距离相似性、聚类和其他用于对单词向量进行排序的机器学习技术的使用。下面提供了一些关于这个主题的有用的已发表论文的链接:

*   [https://S3 . amazonaws . com/academy . edu . documents/32952068/pg 049 _ Similarity _ Measures _ for _ Text _ Document _ clustering . pdf？awsacceskeyid = akaiwowyygz2y 53 ul3a&过期=1530163881 &签名= yg6yjvjb2z 0 jjmfhzayuja2io %3D&响应-内容-处置=内联% 3B % 20 文件名% 3D 相似性 _ Measures _ for _ Text _ Document _ cl . pdf](https://s3.amazonaws.com/academia.edu.documents/32952068/pg049_Similarity_Measures_for_Text_Document_Clustering.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1530163881&Signature=YG6YjvJb2z0JjmfHzaYujA2ioIo%3D&response-content-disposition=inline%3B%20filename%3DSimilarity_Measures_for_Text_Document_Cl.pdf)
*   [http://CSIS . pace . edu/ctappet/dps/d 861-12/session 4-p2 . pdf](http://csis.pace.edu/ctappert/dps/d861-12/session4-p2.pdf)