# 第一章。大数据科学生态系统

作为一名数据科学家，您无疑非常熟悉处理文件，甚至处理大量数据。然而，我相信你会同意，除了对单一类型的数据进行简单分析之外，做任何事情都需要一种组织和编目数据的方法，以便能够有效地管理数据。的确，这是伟大数据科学家的基石。随着数据量和复杂性的增加，一个一致和稳健的方法可能是广义成功和过度拟合失败之间的区别！

本章介绍了一种利用大规模数据取得成功的方法和生态系统。它侧重于数据科学工具和技术。它介绍了环境，以及如何对其进行适当的配置，但也解释了一些与整体数据架构相关的非功能性注意事项。虽然现阶段几乎没有实际的数据科学，但它为本书其余部分的成功铺平了道路。

在本章中，我们将涵盖以下主题:

*   数据管理职责
*   数据架构
*   配套工具

# 介绍大数据生态系统

数据管理尤其重要，尤其是在数据不断变化的时候；要么不断变化，要么定期生产和更新。在这些情况下，需要一种存储、结构化和审计数据的方法，以允许模型和结果的持续处理和细化。

在这里，我们描述了如何最好地保存和组织您的数据，以便在足以满足日常需求的数据架构的上下文中与 Apache Spark 和相关工具集成。

## 数据管理

即使在中期，你只打算在家玩一点数据；然后，如果没有适当的数据管理，工作往往会升级到很容易忘记你在哪里的地步，错误就会发生。花时间思考数据的组织，尤其是数据的接收，是至关重要的。没有什么比等待长时间运行的分析完成、整理结果并生成报告更糟糕的了，结果却发现您使用了错误版本的数据，或者数据不完整、缺少字段，或者更糟糕的是您删除了结果！

坏消息是，尽管数据管理很重要，但在商业和非商业企业中，数据管理一直被忽视，可用的现成解决方案非常少。好消息是，使用本章描述的基本构件来做伟大的数据科学要容易得多。

## 数据管理职责

当我们考虑数据时，很容易忽略我们需要考虑的领域的真实范围。事实上，大多数数据“新手”都是这样考虑范围的:

1.  获取数据
2.  将数据放在某个地方(任何地方)
3.  使用数据
4.  扔掉数据

实际上，还有大量其他考虑因素，确定哪些因素适用于给定的工件是我们的共同责任。以下数据管理构建模块有助于回答或跟踪关于数据的一些重要问题:

*   File integrity
    *   数据文件是否完整？
    *   你怎么知道？
    *   它是布景的一部分吗？
    *   数据文件是否正确？
    *   在运输途中被篡改了吗？
*   Data integrity
    *   数据是否如预期？
    *   所有的字段都存在吗？
    *   是否有足够的元数据？
    *   数据质量是否足够？
    *   有没有数据漂移？
*   Scheduling
    *   数据是否定期传输？
    *   数据多久到达一次？
    *   数据是否按时收到？
    *   你能证明数据是什么时候收到的吗？
    *   需要确认吗？
*   Schema management
    *   数据是结构化的还是非结构化的？
    *   数据应该如何解读？
    *   可以推断出模式吗？
    *   数据是否随着时间而改变？
    *   这个模式可以从以前的版本进化而来吗？
*   Version Management
    *   数据的版本是什么？
    *   版本正确吗？
    *   你如何处理不同版本的数据？
    *   你怎么知道你用的是哪个版本？
*   Security
    *   数据敏感吗？
    *   它是否包含个人身份信息(PII)？
    *   是否包含个人健康信息(PHI)？
    *   是否包含支付卡信息(PCI)？
    *   我应该如何保护数据？
    *   谁有权读/写数据？
    *   它需要匿名化/净化/混淆/加密吗？
*   Disposal
    *   我们如何处置数据？
    *   我们什么时候处理数据？

如果在所有这些之后，您仍然不相信，在您继续使用`gawk`和`crontab`命令编写 bash 脚本之前，继续阅读，您将很快看到有一种更快、更灵活、更安全的方法，允许您从小规模开始，逐步创建商业级的摄取管道！

## 工作的合适工具

Apache Spark 是可扩展数据处理的新兴事实标准。在撰写本书时，它是最活跃的 **Apache 软件基金会** ( **ASF** )项目，并且有丰富多样的配套工具可用。每天都有新的项目出现，其中许多在功能上重叠。因此，需要时间来了解它们的用途，并决定它们是否适合使用。不幸的是，没有快速解决这个问题的方法。通常，必须在个案的基础上做出具体的权衡；很少有一刀切的解决方案。因此，鼓励读者探索可用的工具并明智地选择！

本书介绍了各种技术，希望它们能让读者对一些更有用、更实用的技术有所了解，从而开始在自己的项目中使用它们。此外，我们希望表明，如果代码写得仔细，即使一个决定被证明是不正确的，也可以通过巧妙使用**应用程序接口** ( **应用程序接口**)(或 Spark Scala 中的高阶函数)来交换技术。

# 整体架构

让我们从数据架构的高级介绍开始:它们做什么，为什么有用，什么时候应该使用，以及 Apache Spark 如何适应。

![Overall architecture](img/image_01_001.jpg)

一般来说，现代数据架构有四个基本特征:

*   数据摄取
*   数据湖
*   数据科学
*   数据存取

让我们现在介绍其中的每一个，以便我们可以在后面的章节中更详细地讨论。

## 数据摄取

传统上，数据是在严格的规则下获取的，并根据预定的模式进行格式化。这个过程被称为**提取、转换、加载** ( **ETL** )，并且仍然是由大量商业工具以及一些开源产品支持的非常常见的做法。

![Data Ingestion](img/image_01_002.jpg)

ETL 方法倾向于执行预先检查，确保数据质量和模式一致性，以便简化后续的在线分析处理。它特别适合处理具有一组特定特征的数据，即那些与经典实体关系模型相关的数据。然而，它并不适合所有的场景。

在大数据革命期间，对结构化、半结构化和非结构化数据的需求出现了隐喻性的爆发，导致创建了需要处理具有不同特征集的数据的系统。这些由短语 *4 Vs:体积、变化、速度和准确性*[http://www.ibmbigdatahub.com/infographic/four-vs-big-data](http://www.ibmbigdatahub.com/infographic/four-vs-big-data)来定义。当传统的 ETL 方法在这种新的负担下举步维艰时——因为它们需要太多的时间来处理大量的数据，或者在面对变化时过于死板，一种不同的方法出现了。进入**阅读图式**范式。在这里，数据以其原始形式(或至少非常接近)被摄取，规范化、验证等细节在分析处理时完成。

这通常被称为**提取负载变换** ( **ELT** ，参考传统方法:

![Data Ingestion](img/image_01_003.jpg)

这种方法重视及时交付数据，将详细的处理延迟到绝对需要的时候。通过这种方式，数据科学家可以立即访问数据，使用传统方法无法获得的一系列技术来搜索洞察力。

虽然我们在这里只提供了一个高级概述，但是这种方法是如此重要，以至于在整本书中，我们将通过实现各种阅读时模式算法来进一步探索。我们将假设数据摄取的 ELT 方法，也就是说，我们鼓励在用户方便的时候加载数据。这可以是每隔 *n* 分钟，过夜或在低用量期间。然后，可以通过离线运行批处理作业来检查数据的完整性、质量等，同样由用户决定。

## 数据湖

数据湖是一种方便的、无处不在的数据存储。它非常有用，因为它提供了许多关键优势，主要是:

*   可靠的存储
*   可扩展的数据处理能力

让我们简单看一下其中的每一个。

### 可靠储存

数据湖的底层存储实现有很好的选择，包括 **Hadoop 分布式文件系统** ( **HDFS** )、 **MapR-FS** 和**亚马逊 AWS S3** 。

在整本书中，HDFS 将是假定的存储实施。此外，在本书中，作者使用了分布式 Spark 设置，部署在运行在霍顿工程 HDP 环境中的另一个资源协商器上。因此，除非另有说明，HDFS 是所使用的技术。如果您不熟悉这些技术，本章将进一步讨论它们。

无论如何，值得了解的是，Spark 本地引用 HDFS 位置，通过前缀`file://`访问本地文件位置，并通过前缀`s3a://`引用 S3 位置。

### 可扩展的数据处理能力

显然，Apache Spark 将是我们首选的数据处理平台。此外，您可能还记得，Spark 允许用户通过配置适当的集群管理器，在他们喜欢的环境中执行代码，无论是本地的、独立的、纱或 Mesos 在`masterURL`中。顺便说一下，这可以在以下三个位置中的任何一个位置完成:

*   发出`spark-submit`命令时使用`--master`选项
*   在`conf/spark-defaults.conf`文件中添加`spark.master`属性
*   在`SparkConf`对象上调用`setMaster`方法

如果您不熟悉 HDFS，或者您没有访问集群的权限，那么您可以使用本地文件系统运行本地 Spark 实例，这对测试很有用。但是，请注意，通常只有在集群上执行时才会出现不良行为。所以，如果你对 Spark 是认真的，那么投资一个分布式集群管理器是值得的，为什么不试试 Spark 独立集群模式，或者亚马逊 AWS EMR 呢？例如，亚马逊提供了许多价格合理的云计算途径，你可以在[https://aws.amazon.com/ec2/spot/](https://aws.amazon.com/ec2/spot/)探索现场实例的想法。

## 数据科学平台

数据科学平台提供服务和应用编程接口，使有效的数据科学得以实现，包括探索性数据分析、机器学习模型创建和完善、图像和音频处理、自然语言处理和文本情感分析。

这是 Spark 真正擅长的领域，也是本书剩余部分的主要焦点，它利用了一组强大的本机机器学习库、无与伦比的并行图形处理能力和强大的社区。Spark 为数据科学提供了真正可扩展的机会。

其余章节将提供对这些领域的洞察，包括[第 6 章](06.html "Chapter 6. Scraping Link-Based External Data")、*基于刮链的外部数据*、[第 7 章](07.html "Chapter 7. Building Communities")、*构建社区*和[第 8 章](08.html "Chapter 8. Building a Recommendation System")、*构建推荐系统*。

## 数据访问

数据湖中的数据最常被使用 Hadoop 生态系统工具的数据工程师和科学家访问，如 Apache Spark、Pig、Hive、Impala 或 Drill。然而，有时其他用户甚至其他系统需要访问数据，而正常的工具要么过于技术性，要么无法满足用户对真实延迟的苛刻期望。

在这些情况下，通常需要将数据复制到数据集市或索引存储中，以便可以使用更传统的方法，如报表或仪表板。这个过程通常包括为低延迟访问创建索引和重组数据，称为数据输出。

幸运的是，Apache Spark 在传统数据库、商业智能工具以及可视化和报告软件中有各种各样的适配器和连接器。其中许多将在整本书中介绍。

# 数据技术

当 Hadoop 第一次启动时，Hadoop 这个词指的是 HDFS 和 MapReduce 处理范式的结合，因为这是原始论文[http://research.google.com/archive/mapreduce.html](http://research.google.com/archive/mapreduce.html)的大纲。从那时起，出现了大量的技术来补充 Hadoop，随着 Apache 纱的发展，我们现在看到了其他处理范例的出现，例如 Spark。

Hadoop 现在经常被用作整个大数据软件堆栈的口头禅，因此在这一点上，为这本书定义该堆栈的范围是谨慎的。我们将在整本书中介绍的典型数据架构和一系列技术详述如下:

![Data technologies](img/B05261_01_04-1.jpg)

这些技术之间的关系是一个密集的话题，因为存在复杂的相互依赖关系，例如，Spark 依赖于 GeoMesa，而 GeoMesa 依赖于 Accumulo，而 Accumulo 依赖于 Zookeeper 和 HDFS！因此，为了管理这些关系，有可用的平台，如 Cloudera 或 Hortonworks HDP[http://hortonworks.com/products/sandbox/](http://hortonworks.com/products/sandbox/)。这些提供了整合的用户界面和集中配置。平台的选择是读者的选择，但是，不建议最初安装一些技术，然后转移到托管平台，因为遇到的版本问题将非常复杂。因此，通常更容易从一台干净的机器开始，并提前决定采取哪个方向。

我们在本书中使用的所有软件都与平台无关，因此符合前面描述的通用架构。它可以独立安装，在单个或多个服务器环境中使用相对简单，无需使用托管产品。

## 阿帕奇Spark的作用

在许多方面，Apache Spark 是将这些组件结合在一起的粘合剂。它越来越代表软件栈的中心。它集成了各种各样的组件，但没有一个是硬连线的。事实上，甚至底层的存储机制也可以换出。将这一特性与利用不同处理框架的能力相结合，意味着最初的 Hadoop 技术实际上变成了组件，而不是一个令人印象深刻的框架。我们架构的逻辑图如下所示:

![The role of Apache Spark](img/image_01_005.jpg)

随着 Spark 获得发展势头和广泛的行业认可，针对各种组件的许多原始 Hadoop 实现都已针对 Spark 进行了重构。因此，为了进一步增加图片的复杂性，通常有几种可能的方法来以编程方式利用任何特定的组件；尤其是命令式和声明式版本，这取决于应用编程接口是否从最初的 Hadoop Java 实现中移植而来。在余下的章节中，我们试图尽可能保持Spark精神的真实性。

# 配套工具

既然我们已经建立了要使用的技术堆栈，让我们描述每个组件，并解释它们为什么在 Spark 环境中有用。这本书的这一部分是作为参考而不是直接阅读的。如果你熟悉大部分技术，那么你可以刷新你的知识，继续下一节，[第 2 章](02.html "Chapter 2. Data Acquisition")，*数据采集*。

## 阿帕奇 HDFS

**Hadoop 分布式文件系统** ( **HDFS** )是一个内置冗余的分布式文件系统。默认情况下，它被优化为在三个或更多节点上工作(尽管一个节点可以正常工作，并且限制可以增加)，这提供了在复制块中存储数据的能力。因此，不仅一个文件被分割成许多块，而且这些块的三个副本同时存在。这巧妙地提供了数据冗余(如果一个丢失了，另外两个仍然存在)，但也提供了*数据局部性*。当针对 HDFS 运行分布式作业时，系统不仅会尝试收集该作业的数据输入所需的所有块，还会尝试仅使用物理上靠近运行该作业的服务器的块；因此，它能够仅使用本地存储上的数据块或靠近自身的节点上的数据块来减少网络带宽。这在实践中是通过将 HDFS 物理磁盘分配给节点，将节点分配给机架来实现的；块以节点本地、机架本地和集群本地的方式写入。所有到 HDFS 的指令都通过名为**命名节点**的中央服务器传递，因此这提供了一个可能的中央故障点；有多种方法可以提供 NameNode 冗余。

此外，在多租户的 HDFS 场景中，许多进程同时访问同一个文件，负载平衡也可以通过使用多个块来实现；例如，如果一个文件占用一个数据块，该数据块将被复制三次，因此，有可能同时从三个不同的物理位置读取。虽然这看起来不是一个大的胜利，但在数百或数千个节点的集群上，网络 IO 通常是运行作业的唯一最大限制因素——作者当然经历过在数千个节点的集群上，作业不得不等待数小时才能完成的情况，这纯粹是因为大量其他线程调用数据导致网络带宽被最大化。

如果您正在运行笔记本，要求数据存储在本地，或者希望使用您已经拥有的硬件，那么 HDFS 是一个不错的选择。

### 优势

以下是使用 HDFS 的优势:

*   **冗余**:块的可配置复制提供了节点和磁盘故障的容限
*   **负载平衡**:数据块复制意味着可以从不同的物理位置访问相同的数据
*   **数据局部性**:分析尝试访问最近的相关物理块，减少网络 IO。
*   **数据平衡**:当数据块变得过于聚集或碎片化时，有一种算法可以重新平衡数据块。
*   **灵活存储**:如果需要更多的空间，可以增加更多的磁盘和节点；虽然这不是一个热过程，但是集群将需要停机来添加这些资源
*   **额外费用**:不涉及第三方费用
*   **数据加密**:隐式加密(开启时)

### 缺点

以下是缺点:

*   名称节点提供了一个中心故障点；为了缓解这种情况，有二级和高可用性选项可用
*   集群需要基本的管理和一些潜在的硬件工作

### 安装

要使用 HDFS，我们应该决定是以本地、伪分布式还是全分布式的方式运行 Hadoop 对于单个服务器，伪分布式很有用，因为分析应该直接从这台机器转换到任何 Hadoop 集群。无论如何，我们应该安装至少包含以下组件的 Hadoop:

*   名称节点
*   辅助名称节点(或高可用性名称节点)
*   数据节点

Hadoop 可以通过[http://hadoop.apache.org/releases.html](http://hadoop.apache.org/releases.html)安装。

Spark 需要知道 Hadoop 配置的位置，具体如下文件:`hdfs-site.xml`、`core-site.xml`。然后在Spark配置的配置参数`HADOOP_CONF_DIR`中进行设置。

HDFS 将在本地可用，因此文件`hdfs://user/local/dir/text.txt`可以在 Spark 中简单地使用`/user/local/dir/text.txt`进行寻址。

## 亚马逊 S3

S3 抽象出了所有与并行性、存储限制和安全性相关的问题，允许非常大的并行读/写操作，并以非常低的成本提供了一个很好的**服务级别协议** ( **服务级别协议**)。如果您需要快速启动并运行，不能在本地存储数据，或者不知道您未来的存储需求是什么，这是完美的。应该认识到`s3n`和`S3a`利用的是对象存储模型，而不是文件存储，因此存在一些折衷:

*   最终一致性是指一个应用程序所做的更改(创建、更新和删除)直到某个未定义的时间才可见，尽管大多数 AWS 区域现在都支持写后读一致性。
*   `s3n`和`s3a`利用非原子重命名和删除操作；因此，重命名或删除大型目录所需的时间与条目数量成正比。但是，在此期间，目标文件可以对其他进程保持可见，事实上，直到最终的一致性得到解决。

可以通过网页和大多数流行语言的应用编程接口，通过命令行工具(`s3cmd`)访问 S3；它通过基本配置与 Hadoop 和 Spark 进行了本机集成。

### 优势

以下是优点:

*   无限存储容量
*   没有硬件考虑
*   可用加密(用户存储的密钥)
*   99.9%的可用性
*   裁员

### 缺点

以下是缺点:

*   存储和传输数据的成本
*   没有数据局部性
*   最终一致性
*   相对较高的延迟

### 安装

您可以创建一个 AWS 帐户:[https://aws.amazon.com/free/](https://aws.amazon.com/free/)。通过该帐户，您将可以访问 S3，并且只需创建一些凭据。

目前的 S3 标准是`s3a`；要通过 Spark 使用它，需要对 Spark 配置进行一些更改:

```scala
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem 
spark.hadoop.fs.s3a.access.key=MyAccessKeyID 
spark.hadoop.fs.s3a.secret.key=MySecretKey

```

如果使用 HDP，您可能还需要:

```scala
spark.driver.extraClassPath=${HADOOP_HOME}/extlib/hadoop-aws-currentversion.jar:${HADOOP_HOME}/ext/aws-java-sdk-1.7.4.jar

```

然后，所有 S3 文件都可以在 Spark 中使用 S3 对象引用的前缀`s3a://`进行访问:

```scala
val rdd = spark.sparkContext.textFile("s3a://user/dir/text.txt") 

```

假设我们已经设置了`spark.hadoop.fs.s3a.impl`，我们也可以在线使用 AWS 凭证:

```scala
spark.sparkContext.textFile("s3a://AccessID:SecretKey@user/dir/file") 

```

但是，该方法不接受任一键中的正斜杠字符`/`。这通常通过从 AWS 获取另一个密钥来解决(不断生成一个新密钥，直到没有正斜杠出现)。

我们还可以通过位于您的 AWS 帐户的 S3 选项卡下的网络界面浏览这些对象。

## 阿帕奇卡夫卡

Apache Kafka 是一个用 Scala 编写的分布式消息代理，可在 Apache 软件基金会许可证下获得。该项目旨在为处理实时数据馈送提供一个统一、高吞吐量、低延迟的平台。其结果基本上是一个可大规模扩展的发布-订阅消息队列，这使得它对企业基础设施处理流数据非常有价值。

### 优势

以下是优点:

*   发布-订阅消息
*   容错的
*   保证交货
*   失败时重播消息
*   高度可扩展的无共享架构
*   支持背压
*   低延迟
*   良好的Spark流集成
*   易于客户端实现

### 缺点

以下是缺点:

*   至少一次语义-由于缺少事务管理器(到目前为止)，不能提供一次消息传递
*   需要动物园管理员进行操作

### 安装

由于 Kafka 是一个 pub-sub 工具，它的目的是管理消息(发布者)并将它们导向相关的端点(订阅者)。这是使用代理完成的，它是在实现卡夫卡时安装的。Kafka 可通过 Hortonworks HDP 平台获得，也可以从该链接[http://kafka.apache.org/downloads.html](http://kafka.apache.org/downloads.html)独立安装。

Kafka 使用 Zookeeper 来管理领导层选举(因为 Kafka 可以是分布式的，因此允许冗余)，在前面的链接中找到的快速入门指南可以用来设置单个节点 Zookeeper 实例，还提供了一个客户端和消费者来发布和订阅主题，这提供了消息处理的机制。

## Apache Parquet

自从 Hadoop 诞生以来，基于列的格式(相对于基于行的格式)的想法得到了越来越多的支持。Parquet 的开发是为了利用压缩、高效的列数据表示，并且在设计时考虑了复杂的嵌套数据结构；率先采用 Apache Dremel 论文[http://research.google.com/pubs/pub36632.html](http://research.google.com/pubs/pub36632.html)中讨论的算法。Parquet 允许在每列级别上指定压缩方案，并且可以在实现时添加更多编码。它还被设计为在整个 Hadoop 生态系统中提供兼容性，并且像 Avro 一样，将数据模式与数据本身一起存储。

### 优势

以下是优点:

*   柱状存储
*   高效存储
*   每列压缩
*   支持谓词下推
*   支持列修剪
*   与其他格式兼容，例如 Avro
*   高效阅读，专为部分数据检索而设计

### 缺点

以下是缺点:

*   不适合随机访问
*   潜在的计算密集型写入

### 安装

拼花地板在 Spark 中有售，可通过以下方式直接访问:

```scala
val ds = Seq(1, 2, 3, 4, 5).toDS 
ds.write.parquet("/data/numbers.parquet") 
val fromParquet = spark.read.parquet("/data/numbers.parquet")
```

## Apache Avro

Apache Avro 是最初为 Hadoop 开发的数据序列化框架。它使用 JSON 来定义数据类型和协议(尽管有一个替代的 IDL)，并以紧凑的二进制格式序列化数据。Avro 为持久数据提供了序列化格式，为 Hadoop 节点之间以及从客户端程序到 Hadoop 服务的通信提供了有线格式。另一个有用的特性是它能够存储数据模式和数据本身，因此任何 Avro 文件都可以被读取，而不需要引用外部源。此外，Avro 支持模式演化，因此用旧模式版本编写的 Avro 文件与用新模式版本读取的 Avro 文件之间向后兼容。

### 优势

以下是优点:

*   图式进化
*   节省磁盘空间
*   支持 JSON 和 IDL 中的模式
*   支持多种语言
*   支持压缩

### 缺点

以下是缺点:

*   需要模式来读写数据
*   序列化计算量大

### 安装

由于我们在本书中使用了 Scala、Spark 和 Maven 环境，Avro 可以如下导入:

```scala
<dependency>   
   <groupId>org.apache.avro</groupId>   
   <artifactId>avro</artifactId>   
   <version>1.7.7</version> 
</dependency> 

```

然后，创建一个模式，并生成 Scala 代码，使用该模式将数据写入 Avro。这在[第 3 章](03.html "Chapter 3. Input Formats and Schema")、*输入格式和模式*中有详细解释。

## Apache NiFi

Apache NiFi 起源于美国国家安全局 ( **国家安全局**)，作为他们技术转让计划的一部分，于 2014 年开放源代码。NiFi 支持在一个简单的用户界面中生成数据路由和转换的可伸缩有向图。它还支持数据来源、广泛的预构建处理器以及快速高效地构建新处理器的能力。它具有优先级、可调交付容差和背压功能，允许用户根据特定需求调整处理器和管道，甚至允许在运行时修改流量。所有这些加起来就是一个非常灵活的工具，可以构建从一次性文件下载数据流到企业级 ETL 管道的所有内容。与编写快速 bash 脚本相比，使用 NiFi 构建管道和下载文件通常更快，添加了用于此目的的功能丰富的处理器，这是一个引人注目的提议。

### 优势

以下是优点:

*   处理器种类繁多
*   轮辐式架构
*   **图形用户界面** ( **图形用户界面**)
*   可攀登的
*   简化并行处理
*   简化线程处理
*   允许运行时修改
*   通过集群实现冗余

### 缺点

以下是缺点:

*   没有交叉错误处理程序
*   表达式语言只是部分实现
*   缺少流文件版本管理

### 安装

Apache NiFi 可以与 Hortonworks 一起安装，被称为 Hortonworks 数据流。它也可以从阿帕奇、[https://nifi.apache.org/](https://nifi.apache.org/)独立安装。[第二章](02.html "Chapter 2. Data Acquisition")、*数据采集*有关于倪飞的介绍。

## 阿帕奇纱

纱线是 Hadoop 2.0 的主要组件，它本质上允许 Hadoop 插入处理范例，而不仅仅局限于最初的 MapReduce。纱线由三个主要组件组成:资源管理器、节点管理器和应用程序管理器。钻研纱不在本书的范围之内；需要理解的主要事情是，如果我们运行的是 Hadoop 集群，那么我们的 Spark 作业可以在客户端模式下使用 SHART 执行，如下所示:

```scala
spark-submit --class package.Class /  
             --master yarn / 
             --deploy-mode client [options] <app jar> [app options] 

```

### 优势

以下是优点:

*   支持Spark
*   支持优先调度
*   支持数据局部性
*   作业历史档案
*   与 HDP 合作

### 缺点

以下是缺点:

*   没有中央处理器资源控制
*   不支持数据沿袭

### 安装

纱是作为 Hadoop 的一部分安装的；这可能是 HDP 霍顿工程公司、Apache Hadoop 或其他供应商之一。无论如何，我们应该安装至少包含以下组件的 Hadoop:

*   资源管理器
*   节点管理器(1 个或更多)

为了确保 Spark 可以使用纱线，它只需要知道`yarn-site.xml`的位置，这是使用您的 Spark 配置中的`YARN_CONF_DIR`参数设置的。

## 阿帕奇·卢塞恩

Lucene 是一个索引和搜索库工具，最初是用 Java 构建的，但现在移植到了其他几种语言，包括 Python。Lucene 在它的时代已经产生了许多子项目，包括 Mahout、Nutch 和 Tika。这些项目现在已经成为顶级的 Apache 项目，而 Solr 最近加入了一个子项目。Lucene 具有全面的能力，但尤其以其在问答搜索引擎和信息检索系统中的使用而闻名。

### 优势

以下是优点:

*   高效的全文搜索
*   可攀登的
*   多语言支持
*   出色的开箱即用功能

### 缺点

缺点是数据库通常更适合关系操作。

### 安装

Lucene 可以从[https://lucene.apache.org/](https://lucene.apache.org/)下载，如果你想了解更多，可以直接和图书馆互动。

在利用 Lucene 的时候，我们只需要在我们的项目中包含`lucene-core-<version>.jar`。例如，使用 Maven 时:

```scala
<dependency> 
    <groupId>org.apache.lucene</groupId> 
    <artifactId>lucene-core</artifactId> 
    <version>6.1.0</version> 
</dependency> 

```

## 木叶树

Kibana 是一个分析和可视化平台，也提供图表和流数据汇总。它使用 Elasticsearch 作为数据源(反过来使用 Lucene)，因此可以大规模利用非常强大的搜索和索引功能。Kibana 可用于以多种不同方式可视化数据，包括条形图、直方图和地图。在这一章的最后，我们已经简单地提到了基巴纳，它将在本书中被广泛使用。

### 优势

以下是优点:

*   大规模可视化数据
*   快速开发仪表板的直观界面

### 缺点

以下是缺点:

*   仅与弹性搜索集成
*   Kibana 版本与特定的弹性搜索版本相关联

### 安装

Kibana 可以很容易地作为一个独立的部分安装，因为它有自己的网络服务器。可以从[https://www.elastic.co/downloads/kibana](https://www.elastic.co/downloads/kibana)下载。由于 Kibana 需要 Elasticsearch，这也需要安装；有关更多信息，请参见前面的链接。Kibana 配置在`config/kibana.yml`中处理，如果你已经安装了弹性搜索的独立版本，那么不需要修改，它将开箱即用！

## 弹性搜索

Elasticsearch 是一个基于 Lucene 的网络搜索引擎(见前文)。它提供了一个分布式的、支持多租户的全文搜索引擎，其中包含无模式的 JSON 文档。它是用 Java 构建的，但是由于它的 HTTP web 接口，可以从任何语言中使用。这使得它对于要通过网页显示的事务和/或数据密集型指令特别有用。

### 优势

优点如下:

*   分布的
*   无模式
*   HTTP 接口

### 缺点

缺点如下

*   无法执行分布式事务
*   缺少前端工具

### 安装

弹性搜索可以从[https://www.elastic.co/downloads/elasticsearch](https://www.elastic.co/downloads/elasticsearch)安装。为了提供对 Rest API 的访问，我们可以导入 Maven 依赖项:

```scala
<dependency> 
    <groupId>org.elasticsearch</groupId> 
    <artifactId>elasticsearch-spark_2.10</artifactId> 
    <version>2.2.0-m1</version> 
</dependency> 

```

还有一个很好的工具可以帮助管理弹性搜索内容。在[https://chrome.google.com/webstore/category/extensions](https://chrome.google.com/webstore/category/extensions)搜索 Chrome 扩展，Sense。进一步的解释见:[https://www . elastic . co/blog/found-sense-a-cool-JSON-aware-interface-to-elastic search](https://www.elastic.co/blog/found-sense-a-cool-json-aware-interface-to-elasticsearch)。或者，它也可以在[网站上获得。](https://www.elastic.co/guide/en/sense/current/installing.html)

## 积累

Accumulo 是一个基于谷歌 Bigtable 设计的无 sql 数据库，最初由美国国家安全局开发，随后于 2011 年发布给 Apache 社区。Accumulo 为我们提供了通常的大数据优势，如批量加载和并行读取，但也有一些额外的功能；迭代器，用于高效的服务器端和客户端预计算、数据聚合，以及最重要的单元级安全性。Accumulo 的安全特性使其对企业应用非常有用，因为它可以在多租户环境中实现灵活的安全性。与卡夫卡一样，Accumulo 由 Apache Zookeeper 提供动力，并利用 Apache 节俭、[https://thrift.apache.org/](https://thrift.apache.org/)，这启用了跨语言**远程过程调用** ( **RPC** )功能。

### 优势

优点如下:

*   谷歌 Bigtable 的纯实现
*   小区级安全
*   可攀登的
*   裁员
*   为服务器端计算提供迭代器

### 缺点

缺点如下:

*   动物园管理员并不普遍受到 DevOps 的欢迎
*   并不总是批量关系操作最有效的选择

### 安装

Accumulo 可以作为 Hortonworks HDP 版本的一部分安装，也可以从[https://accumulo.apache.org/](https://accumulo.apache.org/)作为独立实例安装。然后，在编写[https://accumulo . Apache . org/1.7/accumulo _ user _ manual # _ installation](https://accumulo.apache.org/1.7/accumulo_user_manual#_installation)时，应该使用安装文档来配置实例。

在[第 7 章](07.html "Chapter 7. Building Communities")、*构建社区*中，我们演示了带有 Spark 的 Accumulo 的使用，以及一些更高级的功能，如`Iterators`和`InputFormats`。我们还展示了如何在弹性搜索和累积之间处理数据。

# 总结

在这一章中，我们介绍了数据架构的概念，并解释了如何将职责分组到有助于在数据的整个生命周期中管理数据的功能中。我们解释说，所有数据处理都需要一定程度的尽职调查，无论这是否由公司规则强制执行，如果没有这一点，分析及其结果可能会很快失效。

在确定了我们的数据架构的范围后，我们浏览了各个组件及其各自的优缺点，解释了我们的选择是基于集体经验的。事实上，在选择组件时总是有选择的，在做出任何承诺之前，应该始终仔细考虑它们各自的特性。

在下一章中，我们将深入探讨如何获取数据。我们将建议如何将数据带到平台上，并讨论与通过管道处理和处理数据相关的方面。