# 第十章。配置和使用外部库

本章指导您使用外部库来扩展数据分析，以使 Spark 更加通用。将给出使用 Spark core 和 ML/MLlib 为机器学习应用程序部署第三方开发的包或库的示例。我们还将讨论如何用 Spark 的核心库编译和使用外部库来处理时间序列。正如承诺的那样，我们还将讨论如何配置 SparkR 来增加探索性数据操作和操作。简而言之，本章将涵盖以下主题:

*   带有 Spark 的第三方 ML 库
*   在集群上部署 Spark ML 时使用外部库
*   使用 Cloudera 的 Spark-TS 包进行时间序列分析
*   使用 RStudio 配置迷你图
*   在 Windows 上配置 Hadoop 运行时

为了给开发人员提供一个用户友好的环境，还可以将第三方 API 和库与 Spark Core 以及其他 API(如 Spark MLlib/ML、Spark Streaming、GraphX 等)结合起来。感兴趣的读者可以参考以下在星火网站上被列为**第三方套餐**的网站:[https://spark-packages.org/](https://spark-packages.org/)。

该网站是 Apache Spark 第三方软件包的社区索引。截至目前，本网站共注册包裹 252 件，如*表 1* 所示:

<colgroup class="calibre11"><col class="calibre12"></colgroup> 
| **域** | **包装数量** | **URL** |
| 火花芯 | nine | [https://spark-packages.org/?q=tags%3A%22Core%22](https://spark-packages.org/?q=tags%3A%22Core%22) |
| 数据源 | Thirty-nine | [https://spark-packages.org/?q=tags%3A%22Data%20Sources%22](https://spark-packages.org/?q=tags%3A%22Data%20Sources%22) |
| 机器学习 | Fifty-five | [https://spark-packages.org/?q =标签% 3A %机器% 20 学习%22](https://spark-packages.org/?q=tags%3A%22Machine%20Learning%22) |
| 流动 | Thirty-six | [https://spark-packages.org/?q=tags%3A%22Streaming%22](https://spark-packages.org/?q=tags%3A%22Streaming%22) |
| 图形处理 | Thirteen | [https://spark-packages.org/?q=tags%3A%22Graph%22](https://spark-packages.org/?q=tags%3A%22Graph%22) |
| 与 Python 的火花 | five | [https://spark-packages.org/?q=tags%3A%22PySpark%22](https://spark-packages.org/?q=tags%3A%22PySpark%22) |
| 集群部署 | Ten | [https://spark-packages.org/?q=tags%3A%22Deployment%22](https://spark-packages.org/?q=tags%3A%22Deployment%22) |
| 数据处理示例 | Eighteen | [https://spark-packages.org/?q=tags%3A%22Examples%22](https://spark-packages.org/?q=tags%3A%22Examples%22) |
| 应用程序 | Ten | [https://spark-packages.org/?q=tags%3A%22Applications%22](https://spark-packages.org/?q=tags%3A%22Applications%22) |
| 工具 | Twenty-four | [https://spark-packages.org/?q=tags%3A%22Tools%22](https://spark-packages.org/?q=tags%3A%22Tools%22) |
| 包装总数:252 |   |   |

表 1:基于应用领域的 Spark 第三方库

# 带 Spark 的第三方 ML 库

55 个第三方机器学习库包括用于神经数据分析、广义聚类、流、主题建模、特征选择、矩阵分解、分布式 ML 的分布式 DataFrame、模型矩阵、用于 Spark 的 Stanford Core NLP 包装器、社交网络分析、深度学习模块运行、基础统计的汇编、二进制分类器校准和 DataFrame 的标记器的库。

*表 2* 根据机器学习的用例和应用领域，提供了最有用的包的概要。感兴趣的读者应访问各自的网站，了解更多信息:

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"></colgroup> 
| **火花第三方 ML 库** | **用例** |
| 雷标量网络 | 神经网络使用 Spark 进行大规模神经数据分析，其中神经网络实现使用 Scala 完成。 |
| 广义 kmeans 聚类混杂物二分法火花 knn | 使聚集这个项目推广了 Spark MLLIB K-means 集群，以支持任意距离函数。Spark MLlib 的高扩展性网格密度聚类算法。这是 Spark 上平分 K-Means 聚类的一个原型实现。Spark 上的 k 近邻算法。 |
| 火花毫升流流式矩阵分解推特流 | 流动在 Spark 中可视化流式机器学习。基于用户和产品偏好的矩阵分解的流媒体推荐引擎。推特上的机器学习。使用 Apache Spark，Web 服务器和闪电图服务器。 |
| 管道 | 基于码头工人的流水线使用 Spark、Spark SQL、Spark Streaming、ML、MLlib、GraphX、Kafka、Cassandra、Redis、Apache Zeppelin、Spark-Notebook、iPython/Jupyter Notebook、Tableau、H2O 流和超光速粒子的端到端、实时、高级分析大数据参考管道。 |
| dllib咖啡厅公园(T0) | 深度学习dllib 是一个运行在 Apache Spark 上的深度学习工具。用户需要下载工具作为。jar，然后可以与 Spark 集成，开发基于深度学习的应用程序。CaffeOnSpark 是一个可扩展的深度学习，由 Spark 执行者运行。它基于对等(P2P)通信。`dl4j-spark-ml`通过与 Spark ML 集成，可以用来开发基于深度学习的 ML 应用。 |
| kNN_ISsparkbootspark-校准 | 分类kNN-IS:基于迭代火花的大数据 k 近邻分类器设计。AdaBoost 的分布式实现。使用阿帕奇火花的 MH 和 MP-Boost。评估 Spark 中的二进制分类器校准(即分类器输出与观察到的类别比例的匹配程度)。 |
| 禅 | 回归Zen 在 Spark 之上提供了一个大规模高效机器学习的平台。例如，逻辑回归、线性回归、潜在狄利克雷分配(LDA)、因子分解机器和深度神经网络(DNN)在当前版本中实现。 |
| 模型矩阵火花信息理论特征选择 | 特征工程spark-infotheority-feature-selection 工具为开发大规模机器学习应用程序提供了 Spark 的替代方案。他们通过包括特征提取器、特征选择器在内的流水线技术提供了强大的特征工程。它专注于构建基于稀疏特征向量的管道。另一方面，它可以作为基于信息论的特征选择框架。基于信息论的算法包括 mRMR、InfoGain、JMI 和其他常用的 FS 过滤器。 |
| 火花 knn 图 | 图形处理构造和处理 k-nn 图的 Spark 算法 |
| 主题描述 | 主题建模基于 Apache Spark 的分布式主题建模 |
| 火花统计 | 统计数字除了 SparkR，Spark.statistics 还是一个基于 Spark 内核的基本统计实现的汇编器 |

表 2:基于使用 Spark 的机器学习的用例和应用领域的最有用的第三方包的概要

# 使用带火花核心的外部库

为了使用这些外部库，一个简单的解决方法是使用以下参数启动`pyspark` shell 或 spark-shell，而不是将 jars 放在任何特定的文件夹中:

```scala
bin/pyspark --packages com.databricks:spark-csv_2.10:1.0.3
bin/spark-shell --packages com.databricks:spark-csv_2.10:1.0.3

```

这将自动装载所需的`spark-csv`罐子。但是，这两个 jar 文件必须使用 Ubuntu 中的以下命令下载到 Spark 发行版:

```scala
wget http://search.maven.org/remotecontent?filepath=org/apache/commons/commons-csv/1.1/commons-csv-1.1.jar
wget http://search.maven.org/remotecontent?filepath=com/databricks/spark-csv_2.10/1.0.0/spark-csv_2.10-1.0.0.jar

```

然后，要创建活动的火花会话，请使用以下代码行:

```scala
static SparkSession spark = SparkSession 
        .builder() 
        .appName("JavaLDAExample") 
          .master("local[*]") 
          .config("spark.sql.warehouse.dir", "C:/Exp/")               
          .getOrCreate(); 

```

实例化活动的 Spark 会话后，使用以下代码行读取 csv 输入文件:

```scala
String input = "input/letterdata.data"; 
Dataset<Row> df = spark.read().format("com.databricks.spark.csv").option("header", "true").load(input);  
df.show();   

```

请注意，这里我们使用`format()`方法定义`com.databricks.spark.csv`输入格式，该方法由 Databricks 专门开发，用于更快的 CSV 文件读取和解析，并使用`option()`方法将标题的辅助选项设置为 true。最后，例如，`load()`方法从`input/letterdata.data`位置加载输入数据。

作为延续，在下一节中，我们将讨论为时间序列数据分析配置 Spark-TS 库。

### 类型

感兴趣的读者请访问位于[https://spark-packages.org/?的 Spark 第三方 ML 包网页 q =标签% 3A %机器% 20 学习%22](https://spark-packages.org/?q=tags%3A%22Machine%20Learning%22) 了解特定于软件包的讨论、更新和配置程序。

# 使用 Cloudera Spark-TS 包进行时间序列分析

如 *[第九章](09.html#25JP22-0b803698e2de424b8aa3c56ad52b005d "Chapter 9.  Advanced Machine Learning with Streaming and Graph Data")**带流和图形数据的高级机器学习*所述，我们将看到如何配置 Cloudera 开发的 Spark-TS 包。在这一部分，我们将主要讨论时间体验。

## 时间序列数据

时间序列数据由测量序列组成，每个测量序列都发生在一个时间点。各种各样的术语被用来描述时间序列数据，其中许多适用于冲突或重叠的概念。为了清晰起见，在 Spark-TS 中，Cloudera 坚持使用特定的词汇。时间序列数据分析中有三个重要对象:时间序列、即时和观察:

*   时间序列是实数(即浮点)值的序列，每个值都链接到特定的时间戳。特别是，这坚持时间序列的意思是一元时间序列。在 Scala 中，时间序列通常由出现在[https://github.com/scalanlp/breeze](https://github.com/scalanlp/breeze)向量上的微风表示，而在 Python 中，时间序列由一维 NumPy 数组表示(更多信息请参考[http://www.numpy.org/](http://www.numpy.org/)，时间序列的`DateTimeIndex`如[所示，https://github . com/sryza/spark-time series/blob/master/src/main/Scala/com/cloudera/sparkts/datetime index . Scala](https://github.com/sryza/spark-timeseries/blob/master/src/main/scala/com/cloudera/sparkts/DateTimeIndex.scala)。
*   另一方面，瞬间是对应于单个时间点的时间序列集合中的值的向量。在 Spark-TS 库中，每个时间序列通常都标有一个键，使其能够在时间序列集合中被识别。
*   最后，观察是(时间戳、键、值)的元组，即时间序列或瞬间中的单个值。

但是，并非所有带有时间戳的数据都是时间序列数据。例如，日志不直接适合时间序列，因为它们由离散的事件组成，而不是以时间间隔进行的标量测量。然而，每小时日志消息的测量将构成一个时间序列。

## 配置火花开关

从 Scala 访问 Spark-TS 最直接的方法是在 Maven 项目中依赖它。通过在`pom.xml`中包含以下回购来实现:

```scala
<repositories> 
    <repository> 
      <id>cloudera-repos</id> 
      <name>Cloudera Repos</name> 
      <url>https://repository.cloudera.com/artifactory/cloudera-repos/</url> 
    </repository> 
</repositories> 
 And including the following dependency in the pom.xml:  
<dependency> 
      <groupId>com.cloudera.sparkts</groupId> 
      <artifactId>sparkts</artifactId> 
      <version>0.1.0</version> 
</dependency> 

```

要获取原始`pom.xml`文件，感兴趣的读者应访问以下网址:

[https://github . com/sryza/spark-time series/blob/master/POM . XML](https://github.com/sryza/spark-timeseries/blob/master/pom.xml)

或者，要在 spark-shell 中访问它，请从[https://repository . cloudera . com/cloudera/libs-release-local/com/cloudera/sparkts/sparkts/0 . 1 . 0/sparkts-0 . 1 . 0-JAR-with-dependencies . JAR](https://repository.cloudera.com/cloudera/libs-release-local/com/cloudera/sparkts/sparkts/0.1.0/sparkts-0.1.0-jar-with-dependencies.jar)中下载该 JAR，然后使用本章*使用带有 Spark Core 的外部库*一节中讨论的以下命令启动 shell:

```scala
spark-shell \
 --jars sparkts-0.1.0-jar-with-dependencies.jar \
 --driver-class-path sparkts-0.1.0-jar-with-dependencies.jar

```

## 时间系列 RDD

根据在 Cloudera 网站[http://blog . Cloudera . com/blog/2015/12/spark-ts-a-new-library-for-analysis-time-series-data-with-Apache-Spark/](http://blog.cloudera.com/blog/2015/12/spark-ts-a-new-library-for-analyzing-time-series-data-with-apache-spark/)上撰写的 Spark-TS 工程博客，TimeSeriesRDD 是 Spark-TS 的核心，RDD 的每个对象都存储了完整的单变量序列。倾向于只适用于时间序列的操作要高效得多。例如，如果您想从原始时间序列集合中生成一组滞后时间序列，只需查看输入 RDD 中的单个记录，就可以计算出每个滞后序列。

类似地，根据周围的值输入缺失的值，或者将时间序列模型拟合到每个序列，所有需要的数据都呈现在一个数组中。因此，Spark-TS 库的核心抽象是 TimeSeriesRDD，它只是一个时间序列的集合，您可以在其上以分布式方式进行操作。这种方法允许您避免存储每个序列的时间戳，而是存储所有序列向量都符合的单个`DateTimeIndex`。`TimeSeriesRDD[K]`扩展`RDD[(K, Vector[Double])]`，其中 K 是键类型(通常是字符串)，元组中的第二个元素是代表时间序列的 Breeze 向量。

更技术性的讨论可以在 GitHub 网址中找到:[https://github.com/sryza/spark-timeseries](https://github.com/sryza/spark-timeseries)。由于这是一个第三方包，我们认为详细的讨论超出了本书的范围。

# 用 RStudio 配置迷你图

让我们假设您的机器上安装了 RStudio。遵循这里提到的步骤:

1.  现在打开 RStudio，创建一个新的 R 脚本；然后写下以下代码:

    ```scala
          SPARK_HOME = "/home/spark-2.0.0-bin-hadoop2.7/R/lib" 
          Sys.setenv(SPARK_MEM="8g") 
          Sys.setenv(SPARK_HOME = "/home/spark-2.0.0-bin-hadoop2.7") 
          .libPaths(c(file.path(Sys.getenv("SPARK_HOME"), "R",
          "lib"),.libPaths())) 

    ```

2.  使用以下代码为 SparkR 加载必要的包:

    ```scala
          library(SparkR, lib.loc = SPARK_HOME)
          ibrary(SparkR) 

    ```

3.  按如下方式配置 SparkR 环境:

    ```scala
          sc <- sparkR.init(appName = "SparkR-DataFrame-example", master =
          "local")
          sqlContext <- sparkRSQL.init(sc) 

    ```

4.  现在让我们创建第一个数据框并打印前几行，如下所示:

    ```scala
          df <- createDataFrame(sqlContext, faithful) 
          head(df) 

    ```

5.  您可能需要安装以下软件包才能使`devtools`软件包工作:

    ```scala
          install.packages("xml2", dependencies = TRUE) 
          install.packages("Rcpp", dependencies = TRUE) 
          install.packages("plyr", dependencies = TRUE) 
          install.packages("devtools", dependencies = TRUE) 
          install.packages("MatrixModels", dependencies = TRUE) 
          install.packages("quantreg", dependencies = TRUE)  
          install.packages("moments", dependencies = TRUE) 
          install.packages("xml2") 
          install.packages(c("digest", "gtable", "scales", "rversions",
          "lintr")) 

    ```

6.  此外，您可能需要为 RCurl 安装`libcurl`，这是开发工具所依赖的。为此，只需运行以下命令:

    ```scala
     sudo apt-get -y build-dep libcurl4-gnutls-dev 
          sudo apt-get install libcurl4-gnutls-dev 
          sudo apt-get install r-cran-plyr 
          sudo apt-get install r-cran-reshape2

    ```

7.  现在使用以下代码从 GitHub 配置`ggplot2.SparkR`包:

    ```scala
          library(devtools) 
          devtools::install_github("SKKU-SKT/ggplot2.SparkR") 

    ```

8.  现在，让我们计算刚刚创建的样本数据帧的偏斜度和峰度。在此之前，加载必要的包:

    ```scala
          library(moments) 
          library(ggplot2) 

    ```

9.  让我们为每日练习示例创建数据框架，如 *[第 4 章](04.html#147LC2-5afe140a04e845e0842b44be7971e11a "Chapter 4. Extracting Knowledge through Feature Engineering")**中*特征工程和数据探索*部分所示，通过特征工程*提取知识，并使用`head`命令显示前几行:

    ```scala
          time_taken <- c (15, 16, 18, 17.16, 16.5, 18.6, 19.0, 20.4, 20.6, 
          25.15, 27.27, 25.24, 21.05, 21.65, 20.92, 22.61, 23.71, 35, 39, 50) 
          df_new <- data.frame(time_taken)  
          head(df_new)  
          df<- createDataFrame(sqlContext, data = df_new)  
          head(df) 

    ```

10.  Now calculate the skewness and kurtosis, as follows:

    ```scala
          skewness(df) 
          kurtosis(df_new) 

    ```

    你可能知道我们在[第四章](04.html#147LC2-5afe140a04e845e0842b44be7971e11a "Chapter 4. Extracting Knowledge through Feature Engineering")、*中使用了`skewness`和`kurtosis`两个术语，通过特征工程*提取知识。如果你不熟悉这两个术语，这里是它们的一点定义。嗯，从统计学的角度来看，`skewness`是对称性的一种度量。或者更准确地说，它表示数据集的分布缺乏对称性。

    现在你可能想知道什么是对称。如果数据集在中心点左右看起来相同，那么它的分布是对称的。

    另一方面，峰度是数据相对于正态分布是重尾还是轻尾的一种度量:

11.  最后，我们通过调用`ggplot2.SparkR`包的`ggplot()`方法来绘制密度图:

    ```scala
          ggplot(df, aes(x = time_taken)) + stat_density(geom="line",
          col= "green", size = 1, bw = 4) + theme_bw() 

    ```

如果你不熟悉`ggplot2` R 包，注意`ggplot2`是一个基于基图和点阵图形的图形语法的 R 绘图系统。它提供了许多复杂的图形细节，使绘图变得很麻烦，例如，在图形中放置或绘制图例，以及提供强大的图形模型。这将使你的生活更容易，以便产生简单的以及复杂的多层图形。

### 类型

更多关于`ggplot2`及其文档的信息可以在以下网站找到:[http://docs.ggplot2.org/current/](http://docs.ggplot2.org/current/)。

# 在 Windows 上配置 Hadoop 运行时

如果您正在使用 Eclipse 在 windows 上开发您的机器学习应用程序(当然是作为 Maven 项目)，您可能会面临一个问题，因为 Spark 希望在 Windows 上也有一个用于 Hadoop 的运行时环境。

更具体地说，假设您正在运行一个用 Java 编写的 Spark 项目，主类为`JavaNaiveBayes_ML.java`，那么您将会遇到一个 IO 异常，它说:

```scala
16/10/04 11:59:52 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
```

![Configuring Hadoop run-time on Windows](../images/00125.jpeg)

图 1:由于缺少 Hadoop 运行时导致的 IO 异常

原因是，默认情况下，Hadoop 是为 Linux 环境开发的，如果您在 windows 平台上开发您的 Spark 应用程序，则需要一个桥，为 Hadoop 运行时提供 Hadoop 环境，以便 Spark 正确执行。

那么，如何解决这个问题呢？解决办法是直截了当的。正如错误信息所说，我们需要一个可执行文件，即`winutils.exe`。现在从本章的 Packt 的代码目录下载`winutils.exe`文件，复制粘贴到 Spark 发行目录，配置 Eclipse。

更具体地说，假设你的包含 Hadoop 的 Spark 发行版位于`C:/Users/spark-2.0.0-bin-hadoop2.7`。在 Spark 发行版中有一个名为`bin.`的目录，现在，将可执行文件粘贴到那里(即`path = C:/Users/spark-2.0.0-binhadoop2.7/` `bin/`)。

解决方案的第二阶段是去 Eclipse，选择主类(这里是`JavaNaiveBayes_ML.java`，然后去**运行**菜单。从**运行**菜单转到**运行配置**选项，并从该选项中选择**环境**选项卡。如果您选择该选项卡，您将可以选择为使用 JVM 的 Eclipse 创建一个新的环境变量。

现在创建一个新的环境变量，并将该值设为`C:/Users/spark-2.0.0-bin-hadoop2.7/`。现在按下**应用**并重新运行您的应用程序，您的问题应该会得到解决。

从技术上讲，IO 异常的细节可以在图 1 中描述如下:

# 总结

在本章中，我们展示了如何使用带有 Spark 的外部库来扩展数据分析。

开源贡献者正在开发越来越多的 Spark 以及第三方包。读者应该在星火网站上了解最新的新闻和发布。他们也应该被告知最新的机器学习 API，因为 Spark 的开发是持续的和创新的，当然，有时在某个包变得过时或被弃用之后。

在本书中，我们试图指导您如何使用 Spark 开发的最流行和最广泛使用的机器学习算法。然而，还有其他算法我们无法讨论，越来越多的算法将被添加到 Spark ML 和 MLlib 包中。

这或多或少是我们与火花的小旅程的结束。现在从我们这边给各位读者一个一般性的建议，或者如果你对机器学习、Java、或者 Spark 比较陌生的话，先试着去了解一个问题是否真的是机器学习问题。如果是机器学习问题，试着猜测什么类型的学习算法应该是最适合的，即分类、聚类、回归、推荐或频繁模式挖掘。

然后定义和表述问题。之后，您应该根据我们已经讨论过的 Spark 的特性工程概念生成或下载适当的数据。然后，您可以选择一个 ML 模型，它将在准确性方面提供更好的结果。然而，如前所述，模型选择实际上取决于您的数据和问题类型。

现在，您已经准备好训练模型的数据，直接训练模型进行预测分析。

当你的模型被训练时，评估它，看看它如何运行并满足你的预测期望。嗯，如果你对性能不满意，试着换成其他 ML 算法来选择模型。正如[第 7 章](07.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d "Chapter 7. Tuning Machine Learning Models")、*调整机器学习模型*中所讨论的，由于您所拥有的数据的性质，即使是正确的模型选择有时也不能提供最佳结果。

那该怎么办呢？很简单。使用可用的调整算法调整您的 ML 模型，以正确设置超参数。您可能还需要使您的模型适应新的数据类型，尤其是如果您正在为动态环境(如时间序列分析或流分析)开发一个 ML 应用程序。

最后，部署您的模型，您就拥有了一个健壮的 ML 应用程序。

我们给读者的最终建议是定期浏览 Spark 网站(位于[http://spark.apache.org/](http://spark.apache.org/))以获取更新，并尝试将 Spark 提供的常规 API 与其他第三方应用程序结合起来，以获得最佳的协作效果。

***本电子书由 AlenMiler 在 AvaxHome 上发布！**T3】*

***我博客里有很多新电子书:***[http://avxhome.in/blogs/AlenMiler](https://tr.im/fgrfegtr)

***镜像:***[https://avxhome . unblock . tw/blogs/alemiler](https://tr.im/geresttre)