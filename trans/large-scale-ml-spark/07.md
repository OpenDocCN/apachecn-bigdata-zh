# 第七章。调整机器学习模型

调整算法或机器学习应用程序只是一个过程，当优化影响模型的参数时，为了使算法能够以最佳方式运行(在运行时和内存使用方面)，需要经历这个过程。本章旨在指导读者进行模型调整。它将涵盖优化最大似然算法性能的主要技术。技术将从 MLlib 和 Spark ML 的角度进行解释。在[第 5 章](05.html#190862-0b803698e2de424b8aa3c56ad52b005d "Chapter 5.  Supervised and Unsupervised Learning by Examples")、*示例监督和非监督学习*和[第 6 章](06.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d "Chapter 6.  Building Scalable Machine Learning Pipelines")、*构建可扩展机器学习管道*中，我们描述了如何开发一些完整的机器学习应用程序和管道，从数据收集到模型评估。在本章中，我们将尝试重用其中的一些应用程序，通过调整几个参数来提高性能，例如超参数调整、使用 MLlib 和 Spark ML 的网格搜索参数调整、随机搜索参数调整和交叉验证。这个假设，也是一个重要的统计检验，将被讨论。总之，本章将涵盖以下主题:

*   关于机器学习模型调整的详细信息
*   模型调整中的典型挑战
*   评估机器学习模型
*   验证和评估技术
*   机器学习的参数调整
*   假设检验
*   机器学习模型选择

# 关于机器学习模型调整的详细信息

从根本上说，人们可以认为**机器学习** ( **ML** )的最终目标是制造一台能够根据数据自动构建模型的机器，而不需要繁琐耗时的人工参与。您将看到，ML 中的一个困难是，学习算法就像决策树、随机森林和聚类技术，需要您在将模型用于实际目的之前设置参数。或者，您需要对这些参数设置一些约束。

如何设置这些参数取决于一组因素和规格。您在这方面的目标通常是将这些参数设置为最佳值，使您能够以最佳方式完成学习任务。因此，调整算法或最大似然技术可以被简单地认为是一个过程，在这个过程中，人们经历了一系列步骤，在这些步骤中，他们优化影响模型性能的参数，以便使算法以最佳方式运行。

在[第 3 章](03.html#TI1E2-0b803698e2de424b8aa3c56ad52b005d "Chapter 3. Understanding the Problem by Understanding the Data")、*通过理解数据来理解问题*、[第 5 章](05.html#190862-0b803698e2de424b8aa3c56ad52b005d "Chapter 5.  Supervised and Unsupervised Learning by Examples")、*通过示例进行监督和非监督学习*中，我们讨论了一些根据您的数据选择最佳算法的技术，并讨论了使用最广泛的算法。为了从你的模型中得到最好的结果，你必须首先定义什么是*最好的*。我们将以抽象和具体的方式讨论调优。

在机器学习的抽象意义上，调优涉及使用变量或基于已被识别为影响系统性能的参数，如通过某种适当的度量所评估的。因此，性能的提高揭示了哪些参数设置更有利(即已调优)或不太有利(即未调优)。从常识来说，调优本质上是在给定硬件工作环境、特定工作负载等情况下，为算法选择最佳参数以优化其性能。机器学习中的调优是实现这一点的自动化过程。

好了，我们言归正传，通过一些例子让讨论更加具体。如果你采用像 KNN 或 K-Means 这样的 ML 算法进行聚类，作为开发人员/数据科学家/数据工程师，你需要在你的模型或质心中指定 K 的数量。

所以问题是‘你怎么能这么做？’从技术上讲，没有捷径可以绕过调整模型的需要。在计算上，一种天真的方法是尝试用不同的 K 值作为模型，当然，当你改变模型中的 K 值时，观察它如何走向组内和组间误差。

第二个例子是使用**支持向量机** ( **SVM** )进行分类任务。如您所知，SVM 分类需要一个初始学习阶段，在该阶段中，训练数据用于调整分类参数。这实际上意味着一个初始参数调整阶段，在这个阶段，您可能会尝试调整模型以获得高质量的结果。

第三个实际例子表明，对于 Apache 网络服务器的所有部署来说，没有完美的优化集。可以说，系统管理员从工作中的数据中学习，并根据其特定环境优化自己的 Apache 网络服务器配置。现在想象一个自动化的过程来完成这三件事，也就是说，一个系统可以自己从数据中学习；机器学习的定义。以这种基于数据的方式调整自身参数的系统将是机器学习中的一个调整实例。现在，让我们总结一下为什么我们要评估模型的预测性能的要点:

*   我们希望估计泛化误差，即我们的模型对未来(看不见的)数据的预测性能。
*   我们希望通过调整学习算法并从给定的假设空间中选择性能最佳的模型来提高预测性能。
*   我们想找出最适合手头问题的机器学习算法；因此，我们希望比较不同的算法，从算法的假设空间中选择性能最好的算法以及性能最好的模型。

简而言之，在寻找最佳参数集的过程中有四个步骤:

*   **定义参数空间**:首先，我们需要决定我们想要为算法考虑的确切参数值。
*   **定义交叉验证设置**:其次我们需要决定如何为数据选择最佳的交叉验证折叠(将在本章后面讨论)。
*   **定义度量**:第三，我们需要决定使用哪个度量来确定参数的最佳组合。例如，精确度、均方根误差、精确度、召回率或 f 分数等等。
*   **训练、评估和比较**:第四，对于参数值的每个唯一组合，进行交叉验证，根据用户在第三步中定义的误差度量，可以选择性能最好的模型。

有许多技术和算法可用于模型调整，如超参数优化或模型选择、超参数调整、网格搜索参数调整、随机搜索参数调整和**交叉验证** ( **CV** )。不幸的是，Spark 的当前实现只开发了其中的几个，包括交叉验证器和 TrainValidationSplit。

因此，我们将尝试使用这两个超参数来调整不同的模型，包括随机森林、线性回归和逻辑回归。来自[第 5 章](05.html#190862-0b803698e2de424b8aa3c56ad52b005d "Chapter 5.  Supervised and Unsupervised Learning by Examples")、*示例监督和非监督学习*和[第 6 章](06.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d "Chapter 6.  Building Scalable Machine Learning Pipelines")、*构建可扩展机器学习管道*的一些应用程序将被重新使用，而无需再次提供许多细节来使模型调整更容易。

# 模型调整中的典型挑战

经过下面的讨论，你可能会认为这个过程很困难，你是对的。事实上，由于很难确定什么是最佳模型参数，在用更好调整的参数有效地试验更简单的选项之前，通常使用一些更复杂的学习算法。正如我们已经讨论过的，机器学习需要大量的实验。学习算法(通常称为超参数)内部旋钮的调整在从模型构建到预测以及部署之前都同样重要。

从技术上讲，在具有不同超参数设置的训练数据集上运行学习算法将导致不同的模型，当然还有不同的性能参数。根据甲骨文开发人员的说法，不建议在没有建立明确目标的情况下就开始调优，因为如果有任何成功的定义，你就不可能成功。

随后，我们通常对从训练数据集中选择表现最好的模型感兴趣；我们需要找到一种方法来评估他们各自的表现，以便对他们进行排名。超越单纯的算法微调一步，我们通常不仅仅是在实验我们认为在给定环境下最佳的单一算法。通常，我们希望比较不同的算法，通常是在预测和计算性能方面。

通常有一个关于使用网格搜索和随机搜索进行参数调整的非常基本的问题。典型地，一些机器学习方法具有需要使用它们中的任何一个来调整的参数。比如按照卫青的说法。等，(*支持向量机的通用公式，第九届国际神经信息处理会议录(ICONIP，02)，V-5，2002 年 11 月 18-22 日*)支持向量机的标准公式如下:

![Typical challenges in model tuning](../images/00086.jpeg)

图 1:SVM 的标准公式

现在，假设我们需要调整模型参数 C，我们需要轻松地完成。从方程中可以明显看出，调音 C 还涉及到其他参数如 *xi* 、 *i* 、*w*；其中，正则化参数 C > 0 是 w 的范数。在 RHS 中，它是稳定器，![Typical challenges in model tuning](../images/00100.jpeg)是依赖于目标函数 f(xi)的经验损失项。在标准支持向量机(线性 SVM 或其他变种)中，正则化函数可以通过求解凸二次优化问题进一步最小化。一旦问题得到解决，它保证了唯一的全局最小解，以获得最佳的预测性能。因此，整个过程或多或少是一个优化问题。

总之，*图 2，模型调优过程、考虑和工作流*将调优过程及其考虑显示为一个工作流:

![Typical challenges in model tuning](../images/00061.jpeg)

图 2:模型调优过程、考虑事项和工作流程

现在，如果给你原始数据集，你很可能会进行预处理，并将数据集分割成训练集和测试集。因此，要调优超参数 C，首先需要将训练集拆分为验证训练集和验证测试集。

之后，您可以尝试使用验证训练集和验证测试集来调整参数。然后使用你所获得的最佳参数，在完整的训练集上重新训练模型。现在，作为最后一步，您可以在测试集上执行测试。

到目前为止，你的方法似乎还可以，但是平均来说，你认为以下两个选项中哪一个更好？

*   使用来自验证的最终模型是否更好，该模型是在最终测试的验证训练集上训练的？
*   还是使用整个训练集并用网格或随机搜索中的最佳参数重新训练模型更好？虽然参数没有针对这个集合进行优化，但我们在这种情况下有最终的训练数据。

您是否打算选择选项 1，因为参数已经在此培训(即验证培训集)集上进行了优化？或者您打算选择选项 2，因为尽管参数没有针对训练集进行优化，但您在这种情况下拥有最终的训练数据？我们建议您选择选项 2，但前提是您信任选项 2 中的验证设置。原因是您已经执行了**交叉验证** ( **CV** )来确定最通用的参数设置或模型选择或您试图优化的任何内容。这些发现应该应用于整个训练集，并在测试集上测试一次。

假设你选择了选项 2；现在第二个挑战正在演变。我们如何估计机器学习模型的性能？从技术上讲，你可能会说，我们应该将训练数据滋养到我们的学习算法中，以学习最优模型。然后我们可以根据测试标签来预测标签。第三，我们统计测试数据集中错误预测的数量来计算模型的错误率。

就这样？别这么快，我的朋友！视我们的目标而定，不幸的是，猜测该模型的性能并不是微不足道的。也许我们应该从另一个角度来解决前面的问题:我们为什么要关心绩效评估？嗯，理想情况下，模型的估计性能告诉我们它在未观察到的数据上表现如何——对未来数据进行预测通常是我们在机器学习应用或新算法开发中想要解决的主要问题。

最后，根据数据结构、问题类型、问题领域和需要解决的适当用例，还有其他几个挑战，当您开始大量练习时，您会遇到这些挑战。

# 评估机器学习模型

在本节中，我们将讨论如何评估机器学习模型，因为您应该始终评估模型，以确定它是否准备好一致地表现良好，预测新数据和未来数据的目标。显然，未来的数据可能有许多未知的目标值。因此，您需要检查与性能相关的度量，例如数据上 ML 模型的准确性度量。在这方面，您需要提供一个数据集，该数据集包含从训练好的模型生成的分数，然后评估该模型以计算一组行业标准评估指标。

为了适当地评估模型，您需要呈现一个已经用目标标记的数据样本，并且该数据将被用作来自训练数据源的基础事实或事实数据集。正如我们已经讨论过的，评估具有相同训练数据集的最大似然模型的预测准确性可能没有用。

原因是模型本身可以根据收到的奖励记住训练数据，而不是从中进行归纳。因此，当最大似然模型训练完成时，您可以从模型中呈现的观测值中知道要预测的目标值。之后，您可以将您训练的 ML 模型返回的预测值与已知的目标值进行比较。

最后，您可能会对计算摘要度量感兴趣，该度量显示了性能度量，以指示预测值和真实值与精度参数(如精度、召回率、加权真实正值、加权真实负值、提升等)的匹配程度。然而，在本节中，我们将首先特别讨论如何评估回归、分类(即二元分类、多类分类)和聚类模型。

## 评估回归模型

假设你是一个在线货币交易者，从事外汇或外贸工作。现在你有两种货币对可以买卖，例如英镑/美元和美元/日元对。如果你仔细看这两对，你会发现美元在两对中都很常见。现在，如果你观察美元、英镑或日元的历史价格，你可以预测你应该在买入还是卖出中打开交易的未来结果。

这类问题可以视为典型的回归问题。这里，目标变量(在这种情况下是价格)是基于市场开放时间随时间变化的连续数值。因此，为了对价格进行预测，基于给定的某一货币的特征值(即本例中的美元、英镑或日元)，我们可以拟合一个简单的线性回归或逻辑回归模型。

在这种情况下，特征值可能是历史价格和某些外部因素，这些因素会使某一货币或货币对的价值发生漂移。这样，训练好的构建模型就可以预测某种货币的价格。

回归模型(即线性、逻辑或广义线性回归模型)可用于查找或计算我们训练的同一数据集的分数，现在所有货币的预测价格或这三种货币的历史价格都可用。我们可以通过平均分析预测价格与实际价格之间的偏差来进一步评估模型的性能。这样，人们就可以猜测预测的价格是涨还是跌，并可以从外汇或外贸等在线货币网站上赚钱。

## 评估二元分类模型

正如我们已经在二元分类场景中讨论过的，目标变量只有两种可能的结果。例如:{0，1}、{spam，hap}、{B，N}、{false，true}和{负值，正值}等。现在假设给你一个数据集，该数据集由世界各地的研究人员组成，具有人口统计、社会经济和就业变量，并且你希望将博士奖学金金额(即工资水平)预测为二进制变量，例如，{ <=1.5K$, > =4.5K$}。

在这个特殊的例子中，负类代表的是月薪低于或等于 1500 美元的研究员。因此，从另一方面来说，正类代表所有其他工资超过或等于 4500 美元的研究人员。

现在从问题场景来看，很明显也是一个回归问题。因此，您将训练一个模型，对数据进行评分，并评估结果，看看它与实际标签的偏差有多大。因此，在这种类型的问题中，您将执行一个实验来评估两类(即二进制类)逻辑回归模型的性能，这是 ML 领域中最常用的二进制分类器之一。

## 评估多类分类模型

在[第 6 章](06.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d "Chapter 6.  Building Scalable Machine Learning Pipelines")、*构建可扩展机器学习管道*中，我们开发了几个应用程序和管道，您可能还记得，我们还使用逻辑回归为光学字符识别数据集开发了一个多类分类问题，并使用多类度量显示了结果。在该示例中，有 26 个类，对应 26 个字符(即从 A 到 Z)。我们必须预测某个角色的类别或标签，以及它是否真的属于正确的类别或标签。这种回归问题可以用多类分类方法解决。

因此，在这种类型的问题中，您将执行一个实验来评估多类(也就是说，两个以上的类)逻辑回归模型的性能。

## 评估聚类模型

由于聚类模型在许多不同方面与分类和回归模型有明显的不同，如果您评估一个聚类模型，您会发现一组不同的统计数据和聚类模型的性能相关指标。在群集模型评估技术中返回的性能指标描述了分配给每个群集的数据点数量、群集之间的分隔量以及数据点在每个群集内的聚集程度。

例如，如果您回忆一下，在[第 5 章](05.html#190862-0b803698e2de424b8aa3c56ad52b005d "Chapter 5.  Supervised and Unsupervised Learning by Examples")、*通过示例进行监督和无监督学习*中，您发现了一个聚类问题，我们在*通过 Spark 进行无监督学习:示例部分*中使用 Spark ML 和 MLlib 讨论并解决了该问题。在那个特殊的例子中，我们使用萨拉托加纽约住宅数据集展示了住宅区的 K-Means 聚类，并展示了基于价格和地段大小特征的探索性分析，用于位于同一区域的房屋的可能住宅区。这种问题可以通过使用聚类模型来解决和评估。然而，Spark 的当前实现还没有提供任何用于模型评估的开发算法。

# 验证和评估技术

在机器学习应用程序开发中，有几个广泛使用的术语可能会有点棘手和混乱，所以让我们仔细讨论并整理它们。这些术语包括模型、目标函数、假设、混淆矩阵、模型部署、归纳算法、分类器、学习算法、交叉验证和参数:

*   **目标函数**:在强化学习或预测建模中，假设我们专注于对一个对象进行建模。最终目标是学习或逼近一个特定的未知但有目标的函数。目标函数表示为*f(x)= y*；其中 *x* 和 *y* 都是可变的， *f(x)* 是我们想要建模的真实函数，这也意味着我们试图最大化或实现目标值 *y* 。
*   **假设**:统计假设(不要把这个和研究者提出的研究假设混为一谈)是在观察一个过程的基础上可以检验的广义函数。该过程类似于通过训练数据集中的一组随机变量建模的真实函数。假设检验背后的目标是提出一个假设，用于测量示例训练集和测试集的两个数据集之间的统计关系，其中，两个数据集都必须是来自理想化(记住不是随机化或标准化模型)模型的统计显著性。
*   **学习算法**:如前所述，机器学习应用的最终目标是找到或逼近目标函数。在这个连续的过程中，学习算法是一组使用训练数据集对目标函数建模的指令。从技术上讲，一个学习算法往往会附带一个假设空间，并制定最终的假设。
*   **模型**:统计模型是一个数学模型，它举例说明了一组假设，同时从更大的人群中产生样本和类似的数据。最后，模型通常代表了数据生成过程的一种非常理想化的形式。此外，在机器学习领域，术语假设和模型经常互换使用。
*   **归纳算法**:一个归纳算法将输入的特定实例进行归纳，产生一个超越这些输入实例的广义模型。
*   **模型部署**:模型部署通常是指将已经构建和开发的模型应用于真实数据，以便对示例进行预测。
*   **交叉验证**:这是一种根据机器学习模型中的误差来估计精度的方法，方法是将数据分成 K 个大小大致相等的互斥子集或折叠。然后，模型在迭代中被训练和测试 K 次，每次都在可用的数据集上，不包括一个折叠，然后在那个折叠上被测试。
*   **分类器**:分类器是假设或离散值函数的特例，用于将最具分类性的类标签分配给特定的数据点，如标签点。
*   **回归器**:回归器也是一种假设的特殊情况，它将未标记的特征映射到预定义度量空间内的值。
*   **超参数**:超参数是机器学习算法的调谐参数，学习算法将训练数据拟合到该参数。

根据 Pedro d . et al .*在[http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf](http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)学习机器学习*需要了解的一些有用的事情，我们还需要概述一些事情，如下所示:

*   **表示**:分类器或回归器必须用某种形式语言表示，计算机可以通过创建适当的假设空间来处理。
*   **评价**:需要一个评价函数(即目标函数或评分函数)来区分好与坏的分类器回归器，该回归器由构建或训练模型的算法内部使用。
*   **优化**:我们还需要有一个在分类器或者回归器之间进行搜索的方法，针对得分最高的一个。因此，优化是学习者效率的关键，有助于确定最佳参数。

简而言之，在最大似然算法中学习的关键公式是:

> *学习=表征+评价+优化*

因此，为了验证和评估训练好的模型，您需要非常清楚地理解上面的术语，这样您就可以将 ML 问题概念化，并正确使用 Spark ML 和 MLlib APIs。

# 机器学习模型的参数调整

在本节中，我们将讨论机器学习模型的调整参数和技术，例如超参数调整、随机搜索参数调整以及网格搜索参数调整和交叉验证。

## 超参数调谐

超参数调整是一种基于所呈现数据的性能来选择正确参数组合的技术。从机器学习算法中获得有意义和准确的结果是实践中的基本要求之一。例如，假设我们有两个超参数要为*图 3* 中显示的管道进行调整，即使用逻辑回归估计器的 Spark ML 管道模型(虚线仅在管道拟合期间出现)。

我们可以看到，我们已经为每个输入了三个候选值。因此，总共有九种组合。但是图中只显示了四个，分别是**记号化器**、**哈希函数**、**变压器**和**逻辑回归** ( **LR** )。现在我们想找到一个最终会导致最佳评估结果的模型。正如我们在[第 6 章](06.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d "Chapter 6.  Building Scalable Machine Learning Pipelines")、*构建可扩展的机器学习管道中已经讨论过的那样，*拟合模型由标记器、哈希 TF 特征提取器和拟合逻辑回归模型组成:

![Hyperparameter tuning](../images/00148.jpeg)

图 3:使用逻辑回归估计器的 Spark ML 管道模型(虚线仅在管道拟合期间出现)

*图 3* 展示了前面提到的管道的典型工作流程。然而，虚线仅在管道安装期间出现。

如前所述，拟合的管道模型是一个变压器。变压器可用于预测、模型验证和模型检查。此外，我们还认为，最大似然算法的一个命运多舛的显著特征是，通常它们有许多需要调整以获得更好性能的超参数。

例如，这些超参数的正则化程度不同于由 Spark MLlib 优化的模型参数。因此，如果没有数据和算法的专家知识，就很难猜测或测量超参数的最佳组合。由于复杂数据集基于 ML 问题类型，管道的大小和超参数的数量可能呈指数(或线性)增长，即使对于 ML 专家来说，超参数的调整也变得麻烦，更不用说调整参数的结果可能变得不可靠

根据在[http://spark.apache.org/docs/latest/ml-guide.html](http://spark.apache.org/docs/latest/ml-guide.html)提供的火花应用编程接口文件，一个独特和统一的应用编程接口用于指定火花 ML 估算器和变压器。A `ParamMap`是一组(参数、值)对，其中`Param`作为命名参数，由 Spark 提供独立的文档。从技术上讲，有两种方法可以将参数传递给算法，如以下选项中所指定的:

*   设置参数。例如，如果一个 LR 是`LogisticRegression`(即估计器)的一个实例，你可以调用`setMaxIter()`方法如下:`LR.setMaxIter(5)`。它基本上符合指向回归实例的模型，如下所示:`LR.fit()`。在这个特定的例子中，最多有五次迭代。
*   第二种选择是将`ParamMaps`传递给`fit()`或`transform()`(详见*图 1* )。在这种情况下，任何参数都将被`ParamMaps`覆盖，该`ParamMaps`先前是通过 ML 应用特定代码或算法中的设置方法指定的。

### 类型

为数据集创建一系列性能良好的算法的一个很好的方法是使用 R 中的 Caret 包，因为在 Spark 中进行调优并不那么健壮。脱字符号是辉瑞公司的马克斯·库恩创建和维护的一个 R 包。开发始于 2005 年，后来被开源并上传到 CRAN，实际上是一个首字母缩略词，代表**分类和回归训练** ( **CARET** )。它最初是出于对给定问题运行多种不同算法的需要而开发的。感兴趣的读者可以通过访问:[http://topepo.github.io/caret/index.html](http://topepo.github.io/caret/index.html)来查看该软件包，以获得理论和实际考虑。

## 网格搜索参数调整

假设您已经选择了您的超参数，并且通过应用调优，您现在还需要找到特征。在这方面，超参数和特征空间的全网格搜索计算量太大。因此，您需要执行 K 折叠交叉验证的折叠，而不是完整的网格搜索:

*   使用所有可用的特征，在文件夹的训练集上使用交叉验证来调整所需的超参数
*   使用这些超参数选择所需的特征
*   对 K 中的每个折叠重复计算
*   最终的模型是使用从 CV 的每个折叠中选择的 N 个最普遍的特征在所有数据上构建的

有趣的是，超参数也将使用交叉验证循环中的所有数据再次进行调整。与全网格搜索相比，这种方法会有很大的缺点吗？本质上，我是在自由参数的每个维度中进行线性搜索(在一个维度中找到最佳值，保持该常数，然后在下一个维度中找到最佳值)，而不是参数设置的每个单独组合。

沿着单个参数搜索而不是完全优化它们的最大缺点是忽略了交互。例如，很常见的是，不止一个参数会影响模型的复杂性。在这种情况下，您需要查看它们的相互作用，以便成功优化超参数。根据您的数据集有多大以及您比较了多少个模型，返回最大观察性能的优化策略可能会遇到麻烦(网格搜索和您的策略都是如此)。

原因是，在大量性能估计中搜索最大值会忽略性能估计的差异:您可能最终得到一个偶然看起来不错的模型和训练/测试分割组合。更糟糕的是，你可能会得到几个看起来完美的组合，然后优化无法知道选择哪个模型，从而变得不稳定。

## 随机搜索参数调整

优化列车调谐参数的默认方法是使用网格搜索。这种方法通常是有效的，但是在有许多调整参数的情况下，它可能是低效的。在许多模型中，这有利于在相对较短的时间内找到合理的调谐参数值。然而，在一些模型中，小搜索域中的效率可以抵消其他优化。不幸的是，Spark 中用于超参数调整的当前实现没有提供任何用于随机搜索调整的技术。

### 类型

相比之下，例如，CARET 中的许多模型利用*子模型技巧*，其中评估了 M 个调谐参数组合；潜在地需要比 M 模型少得多的拟合。当使用简单的网格搜索时，这种方法得到了最好的利用。因此，使用随机搜索可能效率不高。最后，火车包裹的很多模型参数很少。参数的平均数量为 1.7。要使用随机搜索，另一个选项在**列车控制**中提供，称为搜索。这个参数的可能值是`grid`和`random`。CARET 中包含的内置模型包含生成随机调优参数组合的代码。唯一组合的总数由要训练的**调谐长度**选项指定。

## 交叉验证

交叉验证(也称为**旋转估计** ( **RE** )是一种模型验证技术，用于评估统计分析和结果的质量。目标是使模型向独立的测试集推广。

交叉验证技术的一个完美应用是从机器学习模型中进行预测。从技术上讲，如果您希望在将预测模型部署为 ML 应用程序时，估计它在实践中的准确表现，这将有所帮助。

在交叉验证过程中，通常使用已知类型的数据集来训练模型。相反，它使用未知类型的数据集进行测试。在这方面，交叉验证帮助描述了在训练阶段使用验证集测试模型的数据集。

然而，为了最小化机器学习模型中的缺陷，如过拟合和欠拟合，交叉验证技术提供了关于模型如何推广到独立集的见解。

有两种类型的交叉验证可以键入如下:

*   **穷举交叉验证**:包括留 p-out 交叉验证和留 1-out 交叉验证
*   **非穷举交叉验证**:包括 K 重交叉验证和重复随机子抽样验证交叉验证

由于篇幅限制，本书将不对此进行详细讨论。此外，使用 Spark ML 和 Spark MLlib，读者将能够按照我们下一节中的示例执行交叉验证。

除了时间序列数据，在大多数情况下，研究人员/数据科学家/数据工程师使用 10 倍交叉验证，而不是在验证集上进行测试(其中 K = 10)。这是跨用例和问题类型最广泛使用的交叉验证技术。此外，为了减少可变性，使用不同的分区执行交叉验证的多次迭代；最后，验证结果在各轮之间取平均值。

使用交叉验证代替传统验证有两个主要优点，概述如下:

*   首先，如果没有足够的数据来划分不同的训练集和测试集，就有可能失去重要的建模或测试能力。
*   第二，K 倍交叉验证估计量比单个保持集估计量具有更低的方差。这种低方差限制了可变性，如果可用数据量有限，这也非常重要。

在这些情况下，正确估计模型预测和相关性能的一个公平方法是使用交叉验证作为模型选择和验证的一种强大的通用技术。

更技术性的例子将在*机器学习模型选择*部分显示。让我们画一个具体的例子来说明这一点。假设，我们需要为模型调整执行手动功能和参数选择，然后对整个数据集执行具有 10 倍交叉验证的模型评估。最好的策略是什么？我们建议您采用如下提供乐观得分的策略:

*   将数据集划分为训练集，比如 80%，测试集 20%或你选择的任何内容
*   在训练集上使用 K 折叠交叉验证来调整模型
*   重复简历，直到你发现你的模型优化，因此调整
*   现在使用您的模型在测试集上进行预测，以获得模型外误差的估计

# 假设检验

假设检验是一种统计工具，用于确定结果是否具有统计学意义。此外，它还可以用来证明您收到的结果是偶然发生的，还是实际结果。

此外，在这方面，根据甲骨文开发人员在[https://docs . Oracle . com/CD/a 57673 _ 01/DOC/server/DOC/a 48506/method . htm](https://docs.oracle.com/cd/A57673_01/DOC/server/doc/A48506/method.htm)的说法，某个工作流将提供更好的性能调整。他们建议的典型步骤如下:

*   设定明确的调优目标
*   创建最少的可重复测试
*   检验假设
*   保留所有记录
*   避免常见错误
*   当目标达到时，停止调整

通常，首先计算检验统计量的观测值 tobs。之后，在零假设下计算概率，也称为 p 值。最后，当且仅当 p 值小于显著性水平(所选概率)阈值时，零假设被拒绝，取而代之的是替代假设。

要了解更多信息，请参考以下出版物: *R.A. Fisher 等人，《生物农业和医学研究统计表》，第 6 版。表四，爱丁堡奥利弗&博伊德有限公司*。

这里有两个经验法则(尽管根据数据质量和类型的不同，这些法则可能会有所不同):

*   如果 p 值是 p > 0.05，接受你的假设。请注意，如果偏差足够小，则概率可能是接受水平。例如，p 值为 0.6 意味着任何偏离预期结果的概率为 60%。然而，这在可接受的偏差范围内。
*   如果 p 值是 p < 0.05，拒绝你的假设，得出结论，除了偶然因素外，还有其他因素在使偏差完美。类似地，p 值为 0.01 意味着只有 1%的概率这种偏离仅仅是由于偶然，这意味着必须涉及其他因素，这些都需要解决。

然而，这两条规则可能并不适用于所有假设检验。在下一小节中，我们将展示一个使用 Spark MLlib 进行假设检验的例子。

## 使用 Spark MLlib 的 ChiSqTestResult 进行假设检验

根据 Apache 在[http://Spark . Apache . org/docs/latest/MLlib-statistics . html #假设检验](http://spark.apache.org/docs/latest/mllib-statistics.html#hypothesis-testing)提供的 API 文档，Spark MLlib 的当前实现支持 Pearson 的卡方( *χ2* 检验拟合优度和独立性:

*   拟合的好坏，或者是否正在进行独立性测试，由输入数据类型决定
*   拟合优度测试需要类型向量的输入(大部分是密集向量，尽管它适用于稀疏向量)。另一方面，独立性测试需要一个矩阵作为输入格式

除此之外，Spark MLlib 还支持输入类型 RDD [LabeledPoint]，以通过卡方独立性测试实现特征选择，尤其是基于 SVM 或回归的测试，其中统计类提供了运行皮尔逊卡方测试的必要方法。

此外，Spark MLlib 为概率分布相等性提供了**Kolmogorov-Smirnov**(**KS**)测试的单样本、双面实现。Spark MLlib 提供了一些测试的在线实现，以支持像 A/B 测试这样的用例。这些测试可以在火花流数据流[(布尔值，双精度)]上进行，其中每个元组的第一个元素表示对照组(假)或治疗组(真)，第二个元素是观察值。

然而，由于简洁和页面限制，这两种测试技术将不再讨论。下面的例子演示了如何通过`ChiSqTestResult`运行和解释假设检验。在该示例中，我们将展示三个测试:对从乳腺癌诊断数据集创建的密集向量的结果的拟合优度，对随机创建的矩阵的独立性测试，以及最后，对来自癌症数据集本身的列联表的独立性测试。

**第一步:装载所需包裹**

下面是加载所需包的代码:

```scala
import java.io.BufferedReader; 
import java.io.FileReader; 
import java.io.IOException; 
import org.apache.spark.api.java.JavaRDD; 
import org.apache.spark.api.java.function.Function; 
import org.apache.spark.mllib.linalg.DenseVector; 
import org.apache.spark.mllib.linalg.Matrices; 
import org.apache.spark.mllib.linalg.Matrix; 
import org.apache.spark.mllib.linalg.Vector; 
import org.apache.spark.mllib.regression.LabeledPoint; 
import org.apache.spark.mllib.stat.Statistics; 
import org.apache.spark.mllib.stat.test.ChiSqTestResult; 
import org.apache.spark.rdd.RDD; 
import org.apache.spark.sql.SparkSession; 
import com.example.SparkSession.UtilityForSparkSession; 

```

**步骤 2:创建火花会话**

以下代码帮助我们创建火花会话:

```scala
static SparkSession spark = UtilityForSparkSession.mySession(); 

```

`UtilityForSparkSession`类的实现如下:

```scala
public class UtilityForSparkSession { 
  public static SparkSession mySession() { 
  SparkSession spark = SparkSession 
               .builder() 
               .appName("JavaHypothesisTestingOnBreastCancerData ") 
               .master("local[*]") 
              .config("spark.sql.warehouse.dir", "E:/Exp/") 
              .getOrCreate(); 
    return spark; 
  } 
} 

```

**第三步:进行拟合优度测试**

首先，我们需要从分类数据集(如威斯康星乳腺癌诊断数据集)中准备一个密集向量。由于我们已经提供了关于这个数据集的许多示例，因此在本节中我们不再讨论数据探索。下面一行代码收集了我们使用`myVector()`方法创建的向量:

```scala
Vector v = myVector(); 

```

其中，`myVector()`方法的实现如下:

```scala
public static Vector myVector() throws NumberFormatException, IOException {     
BufferedReader br = new BufferedReader(new FileReader(path)); 
    String line = nulNow let's compute the goodness of the fit. Note, if a second vector tol; 
    Vector v = null; 
    while ((line = br.readLine()) != null) { 
      String[] tokens = line.split(","); 
      double[] features = new double[30]; 
      for (int i = 2; i < features.length; i++) { 
        features[i-2] =     
                       Double.parseDouble(tokens[i]); 
      } 
      v = new DenseVector(features); 
    } 
    return v; 
  } 

```

现在让我们计算拟合优度。注意，如果没有提供第二个要测试的向量作为参数，测试运行将自动针对均匀分布进行:

```scala
ChiSqTestResult goodnessOfFitTestResult = Statistics.chiSqTest(v); 

```

现在，让我们使用以下内容打印好的结果:

```scala
System.out.println(goodnessOfFitTestResult + "\n"); 

```

注意测试的总结包括 p 值、自由度、测试统计、使用的方法和零假设。我们得到了以下输出:

```scala
Chi squared test summary: 
method: pearson 
degrees of freedom = 29  
statistic = 4528.611649568829  
pValue = 0.0  

```

有一个非常强的反对无效假设的假设:观察到的遵循与预期相同的分布。

由于 p 值低到微不足道，因此我们不能接受基于数据的假设。

**第四步:对列联矩阵进行独立性测试**

首先让我们随机创建一个应急 4x3 矩阵。这里，矩阵如下所示:

```scala
 ((1.0, 3.0, 5.0, 2.0), (4.0, 6.0, 1.0, 3.5), (6.9, 8.9, 10.5, 12.6)) 
Matrix mat = Matrices.dense(4, 3, new double[] { 1.0, 3.0, 5.0, 2.0, 4.0, 6.0, 1.0, 3.5, 6.9, 8.9, 10.5, 12.6});     

```

现在让我们对输入列联矩阵进行皮尔逊独立性检验:

```scala
ChiSqTestResult independenceTestResult = Statistics.chiSqTest(mat); 

```

现在让我们评估测试结果，并给出测试总结，包括 p 值和自由度:

```scala
System.out.println(independenceTestResult + "\n"); 

```

我们得到了如下统计数据:

```scala
Chi squared test summary: 
method: pearson 
degrees of freedom = 6  
statistic = 6.911459343085576  
pValue = 0.3291131185252161  
No presumption against null hypothesis: the occurrence of the outcomes is statistically independent.  

```

**第五步:列联表上的独立性测试**

首先，让我们通过癌症数据集中的 RDDs 创建一个应急表，如下所示:

```scala
static String path = "breastcancer/input/wdbc.data"; 
RDD<String> lines = spark.sparkContext().textFile(path, 2);     
JavaRDD<LabeledPoint> linesRDD = lines.toJavaRDD().map(new Function<String, LabeledPoint>() { 
    public LabeledPoint call(String lines) { 
    String[] tokens = lines.split(","); 
    double[] features = new double[30]; 
    for (int i = 2; i < features.length; i++) { 
    features[i - 2] = Double.parseDouble(tokens[i]); 
            } 
    Vector v = new DenseVector(features); 
    if (tokens[1].equals("B")) { 
    return new LabeledPoint(1.0, v); // benign 
      } else { 
    return new LabeledPoint(0.0, v); // malignant 
        } 
      } 
    }); 

```

我们从原始(特征、标签)对构建了一个列联表，并使用它来进行独立性测试。现在让我们对标签上的每个特征进行`ChiSquaredTestResult`测试:

```scala
ChiSqTestResult[] featureTestResults = Statistics.chiSqTest(linesRDD.rdd()); 

```

现在，让我们使用下面的代码段来观察针对每一列(即每 30 个特征点)的测试结果:

```scala
int i = 1; 
for (ChiSqTestResult result : featureTestResults) { 
System.out.println("Column " + i + ":"); 
System.out.println(result + "\n");  
i++; 
} 

Column 1: 
Chi-squared test summary: 
method: Pearson 
degrees of freedom = 455  
statistic = 513.7450859274513  
pValue = 0.02929608473276224  
Strong presumption against null hypothesis: the occurrence of the outcomes is statistically independent. 

Column 2: 
Chi-squared test summary: 
method: Pearson 
degrees of freedom = 478  
statistic = 498.41630331377735  
pValue = 0.2505929829141742  
No presumption against null hypothesis: the occurrence of the outcomes is statistically independent. 

Column 3: 
Chi-squared test summary: 
method: Pearson 
degrees of freedom = 521  
statistic = 553.3147340697276  
pValue = 0.1582572931194156  
No presumption against null hypothesis: the occurrence of the outcomes is statistically independent. 
. 
. 
Column 30: 
Chi-squared test summary: 
method: Pearson 
degrees of freedom = 0  
statistic = 0.0  
pValue = 1.0  
No presumption against null hypothesis: the occurrence of the outcomes is statistically independent.  

```

从这个结果中，我们可以看到，对于某些特征点(即列)，与其他特征点相比，我们具有较大的 p 值。因此，建议读者在应用超参数调整之前选择合适的数据集并进行假设检验。在这方面没有具体的例子，因为结果可能会因数据集而异。

## 使用 Spark MLlib 的 Kolmogorov-Smirnov 检验进行假设检验

自 Spark 1 . 1 . 0 发布以来，Spark 还通过 Kolmogorov-Smirnov 测试为实时流数据提供了进行假设测试的工具。其中，获得实际观察到的测试统计结果(至少与该结果一样极端)的概率。它实际上假设零假设总是正确的。

### 类型

更多详细信息，感兴趣的读者可以参考下面目录下 Spark 发行版中的 Java 类(`JavaHypothesisTestingKolmogorovSmirnovTestExample.java`):`spark-2.0.0-bin-hadoop2.7\examples\src\main\java\org\apache\spark\examples\mllib`。

## 火花 MLlib 的流显著性测试

除了 Kolmogorov-Smirnov 检验，Spark 还支持 Streaming 显著性检验，这是假设检验的在线实现，就像 A/B 检验一样？这些测试可以使用数据流在 Spark 流上执行(关于这个主题的更多技术讨论将在[第 9 章](09.html#25JP22-0b803698e2de424b8aa3c56ad52b005d "Chapter 9.  Advanced Machine Learning with Streaming and Graph Data")、*流和图形数据高级机器学习*中进行)。

基于 MLlib 的流重要性测试支持以下两个参数:

*   **peacePeriod** :这是从流中忽略的初始数据点的数量。这实际上是用来减轻新奇的影响和你将收到的流的质量。
*   **windowSize** :这是过去进行假设检验的批次数。如果将其值设置为 0，它将使用接收和处理的所有先前批次执行累积处理。

感兴趣的读者可以参考[上的 Spark API 文档。](http://spark.apache.org/docs/latest/mllib-statistics.html#hypothesis-testing)

# 机器学习模型选择

大多数机器学习算法依赖于各种参数。当我们训练一个模型时，我们需要提供这些参数的值。训练模型的有效性取决于我们选择的模型参数。找出最佳参数集的过程称为模型选择。

## 通过交叉验证技术选择模型

当使用 Python 的 scikit-learn 库或 R 执行机器学习时，您通常可以通过为模型使用现成的设置来获得合理的预测性能。然而，如果您投入一些时间来调整模型以适应您的特定问题和数据集，回报可能是巨大的。

然而，我们还需要考虑其他问题，如过度拟合、交叉验证和偏差方差权衡。这些想法是做好优化算法超参数工作的核心。

在本节中，我们将探索超参数优化背后的概念，并演示为著名的垃圾邮件过滤数据集调整和训练逻辑回归分类器的过程。目标是调整并应用逻辑回归到这些特征，以预测给定的电子邮件/短信是否是垃圾邮件:

![Model selection via the cross-validation technique](../images/00057.jpeg)

图 4:通过交叉验证选择模型

### 交叉验证和火花

管道通过一次调整整个管道来实现模型选择，而不是无连接地调整管道中的每个元素。参见[http://spark.apache.org/docs/latest/ml-guide.html](http://spark.apache.org/docs/latest/ml-guide.html)的原料药文件。

Spark ML 的当前实现支持使用`CrossValidator`类进行模型选择。它需要一个评估器、一组`ParamMaps`和一个评估器。模型选择任务从分割数据集开始(即，将其分割成一组折叠)，然后将这些折叠用作单独的训练和测试数据集。

例如，K=10 倍，`CrossValidator`将生成 10 个(训练、测试)数据集对。其中每一个都使用三分之二(2/3)的数据进行训练，另三分之一(1/3)用于测试。之后，`CrossValidator`遍历`ParamMaps`的集合。对于每个`ParamMap`，它训练给定的评估器，并使用可用的评估器对其进行评估。评估器可以是相关的 ML 任务，例如:

*   `RegressionEvaluator`为回归相关问题
*   `BinaryClassificationEvaluator`为二进制数据及其相关问题
*   `MultiClassClassificationEvaluator`对于多类问题

当选择最佳`ParamMap`时，使用默认的度量。请注意，`ParamMap`也可以被每个评估器中的`setMetric()`方法覆盖。相比之下，在选择最佳型号时:

*   `ParamMap`产生最佳评估指标
*   然后在 K 个折叠上平均评估度量
*   最后，选择最佳模型

一旦选择了最佳的`ParamMap`和模型，`CrossValidator`将使用它们来拟合整个数据集的估计值。

为了更清楚地了解`CrossValidator`并从参数网格中进行选择，Spark 使用`ParamGridBuilder`实用程序来构建参数网格。例如，假设参数网格的`hashingTF.numFeatures`值为 4，LR 值为 3。`regParam`。另外，假设`CrossValidator`使用 10 倍。

一旦这些值将结果乘以 120(即 4*3 *10 = 120)，这意味着大量不同的模型(即 120)正在被训练。因此，使用`CrossValidator`有时会非常昂贵。尽管如此，它也是一种成熟的方法，用于选择相关的性能和超参数，与基于启发式的手动调谐相比，这些参数在统计上更合理。

### 类型

感兴趣的读者可以参考以下三本书了解更多信息:

埃文·r·斯帕克斯等，*大规模机器学习的自动化模型搜索*，ACM，978-1-4503-3651-2/15/08，[http://dx.doi.org/10.1145/2806777.2806945](http://dx.doi.org/10.1145/2806777.2806945)。

考利，G. C. & Talbot，N. L .关于模型选择中的过度拟合和性能评估中的后续选择偏差，*机器学习研究杂志*，JMLR。org，2010，11，2079-2107。

名词（noun 的缩写）Japkowicz 和 M. Shah，*评估学习算法:分类视角*，剑桥大学出版社，2011。

在下一小节中，我们将展示如何使用 Spark ML API 对数据集执行交叉验证以进行模型选择。

### 使用 Spark ML 对数据集进行垃圾邮件过滤的交叉验证

在本小节中，我们将向您展示如何对垃圾邮件数据集执行交叉验证，以进行模型选择。我们将首先使用逻辑回归，然后我们将继续使用其他模型。最后，我们将推荐最适合垃圾邮件分类的模型。

**第一步:导入必要的包/库/API**

以下是导入必要的包/库/应用编程接口的代码:

```scala
import java.io.Serializable; 
import java.util.Arrays; 
import java.util.logging.Level; 
import java.util.logging.Logger; 
import org.apache.spark.api.java.JavaPairRDD; 
import org.apache.spark.api.java.JavaRDD; 
import org.apache.spark.api.java.function.Function; 
import org.apache.spark.ml.Pipeline; 
import org.apache.spark.ml.PipelineStage; 
import org.apache.spark.ml.classification.LogisticRegression; 
import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator; 
import org.apache.spark.ml.feature.HashingTF; 
import org.apache.spark.ml.feature.Tokenizer; 
import org.apache.spark.ml.param.ParamMap; 
import org.apache.spark.ml.tuning.CrossValidator; 
import org.apache.spark.ml.tuning.CrossValidatorModel; 
import org.apache.spark.ml.tuning.ParamGridBuilder; 
import org.apache.spark.sql.Dataset; 
import org.apache.spark.sql.Row; 
import org.apache.spark.sql.SparkSession; 
import scala.Tuple2; 

```

**步骤 2:初始化必要的火花环境**

以下代码帮助我们初始化必要的 Spark 环境:

```scala
  static SparkSession spark = SparkSession 
        .builder() 
        .appName("CrossValidationforSpamFiltering") 
        .master("local[*]") 
        .config("spark.sql.warehouse.dir", "E:/Exp/") 
        .getOrCreate(); 

```

这里我们将应用名称设置为交叉验证，将主 URL 设置为`local[*]`，将 Spark 会话设置为程序的入口点。请相应地设置这些参数。最重要的是，将仓库目录设置为`E:/Exp/`，并用合适的路径替换。

**步骤 3:从垃圾短信数据集中准备一个数据集**

将垃圾邮件数据作为输入，从数据中准备一个数据集，将其用作原始文本，并通过调用`show()`方法检查数据是否被正确读取:

```scala
Dataset<Row> df = spark.read().text("input/SMSSpamCollection.txt"); 
df.show(); 

```

![Cross-validation using Spark ML for SPAM filtering a dataset](../images/00046.jpeg)

图 5:前 20 行

有关数据的更多信息，请参考[第 4 章](04.html#147LC2-5afe140a04e845e0842b44be7971e11a "Chapter 4. Extracting Knowledge through Feature Engineering")、*通过特征工程提取知识*一节*管道-Spark ML*示例，以及数据集探索的描述。在*图 5* 中可以看到，前 20 行，只有两个标签——垃圾邮件或者 ham(也就是二进制分类问题)和文本值(也就是每行)关联在一起。

但是，没有数字标签或标识。因此，我们需要准备数据集(训练集)，使得数据框也包含 ID、标签和文本(即值)，以便我们可以准备测试集，并使用任何分类算法(即逻辑回归)预测相应的标签，并可以决定我们的模型选择是否合适。

但是，为此，我们需要首先准备训练数据集。如您所见，上面显示的数据框只有一列，如前所述，我们确实需要有三列。如果我们从前一个数据集(即`df`)准备一个 RDD，我们会更容易进行转换。

**第 4 步:创建 Java 对 rdd 来存储行和索引**

通过将数据帧(即`df`)转换(转换)为 Java RDD，并通过压缩索引，创建一对 Java RDD:

```scala
JavaPairRDD<Row, Long> rowRDD = df.toJavaRDD().zipWithIndex(); 

```

**第 5 步:创建带标签的文档关系数据库**

通过基于两个标签分割数据集并将文本标签转换为数字标签(即，如果 ham 为 0.0，则为`1.0`)来创建`LabeledDocument` Java RDDs。注意`LabeledDocument`是用户定义的类，在*步骤 6* 中讨论:

```scala
JavaRDD<LabeledDocument> splitedRDD = rowRDD.map(new Function<Tuple2<Row, Long>, LabeledDocument>() { 
@Override 
public LabeledDocument call(Tuple2<Row, Long> v1) throws Exception {   
  Row r = v1._1; 
  long index = v1._2; 
  String[] split = r.getString(0).split("\t"); 
  if(split[0].equals("ham")) 
    return new LabeledDocument(index,split[1], 1.0); 
  else 
    return new LabeledDocument(index,split[1], 0.0); 
      } 
    });  

```

**第六步:准备训练数据集**

使用`createDataFrame()`方法并通过指定类别，从`LabeledDocument`关系数据库准备训练数据集。最后，使用`show()`方法查看数据帧结构，如下所示:

```scala
Dataset<Row> training = spark.createDataFrame(splitedRDD, LabeledDocument.class); 
training.show(false); 

```

![Cross-validation using Spark ML for SPAM filtering a dataset](../images/00147.jpeg)

图 6:从数据集中新创建的标签和标识；其中标识是行数。

从*图 6* 、*数据集新建的标签和标识；其中 ID 是行数*，我们可以看到新的训练数据集有三列:ID、test 和 label。这实际上可以通过针对原始文档的每一行(即每一行)添加一个新的标识来实现。让我们创建一个名为`Document`的类，它应该为每一行文本设置一个唯一的标识。该类的结构可以如下所示:

```scala
public class Document implements Serializable { 
  private long id; 
  private String text; 
  //Initialise the constructor that should take two parameters: id and text// 
  Set the id 
  Get the id 
  Set the text 
  Get the text   
  } 

```

现在让我们看看类构造函数的结构:

```scala
  public LabeledDocument (long id, String text) {   
      this.id = id; 
           this.text = text; 
  } 

```

标识的设置者和获取者可以是这样的:

```scala
  public void setId(long id) { 
    this.id = id; 
  } 
  public long getId() { 
    return this.id; 
  } 

```

类似地，文本的 setter 和 getter 方法可以是这样的:

```scala
  public String getText() { 
    return this.text; 
  } 

  public void setText(String text) { 
    this.text = text; 
  } 

```

因此，如果我们总结一下，`Document`类可能是这样的:

```scala
import java.io.Serializable; 
public class Document implements Serializable { 
  private long id; 
  private String text; 
  public Document(long id, String text) { 
    this.id = id; 
    this.text = text; 
  } 
  public long getId() { 
    return this.id; 
  } 
  public void setId(long id) { 
    this.id = id; 
  } 
  public String getText() { 
    return this.text; 
  } 
  public void setText(String text) { 
    this.text = text; 
  } 
} 

```

另一方面，`LabeledDocument`类的结构可以如下，并且可以从 Document 类扩展(稍后讨论):

现在让我们看看类构造函数的结构:

```scala
  public LabeledDocument(long id, String text, double label) {   
    this.label = label; 
  } 

```

然而，我们还没有完成，因为我们将扩展`Document`类，我们需要使用如下`super()`方法从`Document`类继承构造函数:

```scala
  public LabeledDocument(long id, String text, double label) { 
    super(id, text); 
    this.label = label; 
  }  

```

setter 方法可以是这样的:

```scala
  public void setLabel(double label) { 
    this.label = label; 
  } 

```

当然，标签的 getter 方法可以是这样的:

```scala
  public double getLabel() { 
    return this.label; 
  } 

```

因此，简单来说`LabelDocument`类如下:

```scala
import java.io.Serializable; 
public class LabeledDocument extends Document implements Serializable { 
  private double label; 
  public LabeledDocument(long id, String text, double label) { 
    super(id, text); 
    this.label = label; 
  } 
  public double getLabel() { 
    return this.label; 
  } 
  public void setLabel(double label) { 
    this.label = label; 
  } 
} 

```

**步骤 7:配置 ML 管道**

配置一个 ML 管道，由三个阶段组成:`tokenizer`、`hashingTF`和`lr`:

```scala
Tokenizer tokenizer = new Tokenizer().setInputCol("text").setOutputCol("words"); 
HashingTF hashingTF = new HashingTF().setNumFeatures(1000).setInputCol(tokenizer.getOutputCol()).setOutputCol("features"); 
LogisticRegression lr = new LogisticRegression().setMaxIter(10).setRegParam(0.01); 
Pipeline pipeline = new Pipeline().setStages(new PipelineStage[] { tokenizer, hashingTF, lr }); 

```

**第八步:构建一个参数网格来搜索**

目前，Spark 使用`ParamGridBuilder`来构建一个参数网格进行搜索。在这方面，假设我们有三个`hashingTF.numFeatures`值和两个`lr.regParam`值，这个网格将有 3×2 = 6 个参数设置供`CrossValidator`选择:

```scala
ParamMap[] paramGrid = new ParamGridBuilder() 
.addGrid(hashingTF.numFeatures(), new int[] { 10, 100, 1000 }).addGrid(lr.regParam(), new double[] { 0.1, 0.01 }) 
 .build(); 

```

我们现在将管道视为一个估计器，将其包装在一个`CrossValidator`实例中。这将允许我们共同选择所有管道阶段的参数。一个`CrossValidator`需要一个估计器，一组估计器`ParamMaps`，和一个评估器。请注意，这里的评估器是一个`BinaryClassificationEvaluator`，它的默认度量是`areaUnderROC`。

**第 9 步:创建交叉验证器实例**

下面是创建`CrossValidator`实例的代码:

```scala
    CrossValidator cv = new CrossValidator() 
        .setEstimator(pipeline) 
        .setEvaluator(new BinaryClassificationEvaluator())                  
        .setEstimatorParamMaps(paramGrid) 
        .setNumFolds(5); // 5-fold cross validation 

```

**步骤 10:运行交叉验证**

运行交叉验证并选择最佳参数集。只需使用以下代码段:

```scala
CrossValidatorModel cvModel = cv.fit(training); 

```

现在你的`CrossValidator`模型准备执行预测。然而，在此之前，我们需要一个测试集或验证集。现在让我们准备一个样本测试集。只需使用以下代码段创建数据集:

```scala
    Dataset<Row> test = spark.createDataFrame(Arrays.asList( 
      new Document(4L, "FreeMsg CALL j k"),  
      new Document(5L, "Siva  hostel"), 
      new Document(6L, "darren now"),  
     new Document(7L, "Sunshine Quiz! Win a super Sony")),Document.class); 

```

现在我们通过调用*图 7* 中的`show()`方法来看看测试集的结构:

![Cross-validation using Spark ML for SPAM filtering a dataset](../images/00072.jpeg)

图 7:测试集

**第 11 步:创建数据集收集预测参数**

下面的代码演示了如何创建数据集:

```scala
Dataset<Row> predictions = cvModel.transform(test); 

```

**步骤 12:显示测试集中各文本的预测参数**

借助以下代码，我们可以显示预测参数:

```scala
for (Row r : predictions.select("id", "text", "probability", "prediction").collect()) 
    { 
System.out.println("(" + r.get(0) + ", " + r.get(1) + ") --> prob=" + r.get(2) + ", prediction=" + r.get(3)); 
    }  

```

![Cross-validation using Spark ML for SPAM filtering a dataset](../images/00118.jpeg)

图 8:对每个文本和标识的预测

因此，如果比较*图 6* 、*中显示的结果，新创建的标签和 ID 来自数据集；其中 ID 是行数，*您会发现，测量预测精度的更复杂方法会提高预测精度，这些方法可用于根据每种类型的错误的成本来确定可以优化错误率的地方。

## 通过训练验证分割选择模型

根据 Spark 在[http://spark.apache.org/docs/latest/ml-guide.html](http://spark.apache.org/docs/latest/ml-guide.html)提供的 API 文档，Spark 还提供了一个`TrainValidationSplit`，用于与`CrossValidator`一起进行超参数调整。`TrainValidationSplit`的思想是，与迭代 k 次的交叉验证相比，它只评估参数的每个组合。因此，它的计算成本更低，并且产生结果更快。然而，结果不会像`CrossValidator`那样可靠。有一个例外:如果训练数据集足够大，那么它也可以产生可靠的结果。

`TrainValidationSplit`背后的理论是它以以下三个作为输入:

*   估计量
*   `estimatorParamMaps`参数中提供的一组`ParamMap` `s`
*   评估者

因此，它通过使用`trainRatio`参数将数据集分成两部分来开始模型选择。另一方面，`trainRatio`参数用于单独的训练和测试数据集。

例如，*训练比= 0.75* (默认值也是 0.75)`TrainValidationSplit`算法生成一个训练和测试对。在这种情况下，总数据的 75%用于训练模型。因此，剩下的 25%被用作验证集。

与`CrossValidator`类似，`TrainValidationSplit`也迭代一组参数图，如前所述。对于每个参数组合，它在每次迭代中训练给定的估计量。

因此，使用给定的评估器对模型进行评估。此后，选择最佳模型作为最佳选项，因为`ParamMap`产生最佳评估度量，从而简化模型选择。`TrainValidationSplit`最终使用最佳可用`ParamMap`和整个数据集来拟合估计器。

### 基于线性回归的光学字符识别数据集模型选择

在本小节中，我们将展示如何对光学字符识别数据执行列车验证分割调整。首先将使用逻辑回归；然后我们将继续其他模式。最后，我们将为光学字符识别数据分类推荐最合适的参数。

**第一步:导入必要的包/库/API:**

```scala
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator; 
import org.apache.spark.ml.param.ParamMap; 
import org.apache.spark.ml.regression.LinearRegression; 
import org.apache.spark.ml.tuning.ParamGridBuilder; 
import org.apache.spark.ml.tuning.TrainValidationSplit; 
import org.apache.spark.ml.tuning.TrainValidationSplitModel; 
import org.apache.spark.sql.Dataset; 
import org.apache.spark.sql.Row; 
import org.apache.spark.sql.SparkSession;
```

**步骤 2:初始化必要的火花环境**

```scala
  SparkSession spark = SparkSession 
          .builder() 
          .appName("TrainSplitOCR") 
              .master("local[*]") 
               .config("spark.sql.warehouse.dir",  
                 "E:/Exp/") 
              .getOrCreate(); 

```

这里我们将应用名称设置为`TrainValidationSplit`，主 URL 设置为`local[*]`，Spark Context 是程序的入口点。请相应地设置这些参数。

**第三步:将 OCR 数据准备成 libsvm 格式**

如果你回忆一下[第 6 章](06.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d "Chapter 6.  Building Scalable Machine Learning Pipelines")、*构建可扩展机器学习管道*中的*图 19* ，你会记得*图 9* 、*中的数据如下:原始 OCR 数据集的快照作为数据框*:

![Linear regression–based model selection for an OCR dataset](../images/00071.jpeg)

图 9:作为数据框的原始光学字符识别数据集的快照

然而，`TrainValidationSplitModel` API 的当前实现只适用于已经是`libsvm`格式的数据集。

### 类型

感兴趣的读者可以参考下面的研究文章来获得更深入的知识:张志忠和林志仁， *LIBSVM -一个支持向量机的库*。智能系统与技术学术会议，2:27:1 - 27:27，2011。该软件可在[http://www.csie.ntu.edu.tw/~cjlin/libsvm](http://www.csie.ntu.edu.tw/~cjlin/libsvm)获得。

因此，我们确实需要将数据集从当前制表符分隔的 OCR 数据转换为`libsvm`格式。

### 类型

读者应该使用 Packt 包提供的数据集，或者可以将 CSV/CSV 文件转换为相应的`libsvm`格式。感兴趣的读者可以参考我们在 https://github.com/rezacsedu/CSVtoLibSVMConverterinR的 GitHub 上提供的公共脚本，该脚本直接将 CSV 文件转换为`libsvm`格式。只需正确显示输入和输出文件路径，并在 RStudio 上运行脚本。

**第四步:准备 OCR 数据集，同时准备训练和测试集**

我们假设读者已经下载了数据，或者已经使用我们的 GitHub 脚本或使用他们自己的脚本转换了 OCR 数据。现在，以光学字符识别`libsvm`格式数据为输入，从数据中准备数据集作为原始文本，并通过调用`show()`方法检查数据是否被正确读取，如下所示:

```scala
Dataset<Row> data = spark.read().format("libsvm").load("input/Letterdata_libsvm.data"); 
data.show(false); 

```

![Linear regression–based model selection for an OCR dataset](../images/00150.jpeg)

图 10:前 20 行

```scala
// Prepare training and test data. 
Dataset<Row>[] splits = data.randomSplit(new double[] {0.9, 0.1}, 12345); 
Dataset<Row> training = splits[0]; 
Dataset<Row> test = splits[1]; 

```

要了解更多数据，请参考[第 4 章](04.html#147LC2-5afe140a04e845e0842b44be7971e11a "Chapter 4. Extracting Knowledge through Feature Engineering")、*通过特征工程提取知识*一节*管道-Spark ML*示例，以及数据集探索的描述。在*图 2* 、*使用逻辑回归估计器的 Spark ML 管道模型中可以看到(虚线只出现在管道拟合期间)*，只有两个标签(spam 或 ham)与文本值(即每行)相关联。但是，没有数字标签或标识。

因此，我们需要准备数据集(训练集)，以便数据集也包含 ID、标签和文本(即值)，以便我们可以准备一个测试集，并为任何分类算法(即逻辑回归)预测它们相应的标签，并可以决定我们的模型选择是否合适。

然而，为了做到这一点，我们首先需要准备训练数据集。如您所见，上面显示的数据框只有一列，如前所述，我们确实需要有三列。如果我们从上面的数据集(也就是说，`df`)准备一个 RDD，我们会更容易进行转换。

**步骤 5:使用线性回归配置 ML 管道**

```scala
LinearRegression lr = new LinearRegression(); 

```

**第六步:构建一个参数网格来搜索**

目前，Spark 使用`ParamGridBuilder`来构建一个参数网格进行搜索。因此，在这方面，`hashingTF.numFeatures`有三个值，`lr.regParam`有两个值，该网格将有 3×2 = 6 个参数设置供`CrossValidator`选择:

```scala
ParamMap[] paramGrid = new ParamGridBuilder() 
.addGrid(hashingTF.numFeatures(), new int[] { 10, 100, 1000 }).addGrid(lr.regParam(), new double[] { 0.1, 0.01 }) 
.build(); 

```

我们现在将管道视为一个估计器，将其包装在一个`CrossValidator`实例中。这将允许我们共同选择所有管道阶段的参数。如前所述，`CrossValidator`需要一个估计器、一组估计器`ParamMaps`和一个评估器。

### 注

请注意，这里的评估器是一个`BinaryClassificationEvaluator`，它的默认度量是`areaUnderROC`。

**第 7 步:创建一个 TrainValidationSplit 实例:**

```scala
TrainValidationSplit trainValidationSplit = new TrainValidationSplit() 
    .setEstimator(lr) 
    .setEvaluator(new MulticlassClassificationEvaluator()) 
    .setEstimatorParamMaps(paramGrid) 
    .setTrainRatio(0.7); 

```

在这种情况下，估计量只是我们在*步骤 4* 中创建的线性回归。一个`TrainValidationSplit`需要一个估计器，一组估计器`ParamMaps`和一个评估器。在这种情况下，70%的数据将用作培训，其余 30%用于验证。

**第八步:运行 TrainValidationSplit，选择参数**

运行`TrainValidationSplit`并使用训练集为您的问题选择最佳参数集。只需使用以下代码段:

```scala
TrainValidationSplitModel model = trainValidationSplit.fit(training); 

```

**第九步:对测试集进行预测**

对测试数据进行预测，其中模型是具有最佳性能参数组合的模型。最后，要显示预测，请使用以下代码段:

```scala
Dataset<Row> per_param = model.transform(test); 
per_param.show(false);   

```

![Linear regression–based model selection for an OCR dataset](../images/00036.jpeg)

图 11:针对每个特征和标签的预测

在*图 11* 中，我们展示了对实际标签的行预测。第一列是实际标签，第二列表示特征向量，第三列显示基于`TrainValidationSplitModel`创建的特征向量的原始预测。

### 基于逻辑回归的癌症数据集模型选择

在本小节中，我们将展示如何对光学字符识别数据执行列车验证分割调整。我们将首先使用逻辑回归；然后我们将继续其他模式。最后，我们将为光学字符识别数据分类推荐最合适的参数。

**第一步:导入必要的包/库/API**

```scala
import org.apache.spark.api.java.JavaRDD; 
import org.apache.spark.api.java.function.Function; 
import org.apache.spark.ml.classification.LogisticRegression; 
import org.apache.spark.ml.evaluation.RegressionEvaluator; 
import org.apache.spark.ml.feature.LabeledPoint; 
import org.apache.spark.ml.linalg.DenseVector; 
import org.apache.spark.ml.linalg.Vector; 
import org.apache.spark.ml.param.ParamMap; 
import org.apache.spark.ml.tuning.ParamGridBuilder; 
import org.apache.spark.ml.tuning.TrainValidationSplit; 
import org.apache.spark.ml.tuning.TrainValidationSplitModel; 
import org.apache.spark.rdd.RDD; 
import org.apache.spark.sql.Dataset; 
import org.apache.spark.sql.Row; 
import org.apache.spark.sql.SparkSession; 

```

**步骤 2:初始化必要的火花环境**

```scala
static SparkSession spark = SparkSession 
  .builder() 
  .appName("CrossValidationforSpamFiltering") 
  .master("local[*]") 
  .config("spark.sql.warehouse.dir", "C:/Exp/"). 
  getOrCreate(); 

```

这里我们将应用名称设置为`CancerDiagnosis`，主 URL 设置为`local[*]`，Spark Context 设置为程序的入口点。请相应地设置这些参数。

**第三步:创建爪哇 RDD**

解析癌症诊断数据并为字符串准备 Java RDDs:

```scala
String path = "breastcancer/input/wdbc.data"; 
RDD<String> lines = spark.sparkContext().textFile(path, 3); 

```

**第四步:准备癌症诊断标记点 RDDs**

正如在[第 6 章](06.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d "Chapter 6.  Building Scalable Machine Learning Pipelines")、*构建可扩展的机器学习管道*中已经讨论过的，癌症诊断数据集包含两个标签 *B* 和 *M* 用于良性和恶性。但是，我们需要将它们转换成数字标签。只需使用以下代码将它们全部从标签转换转换为`LabeledPoint` RDDs 准备:

```scala
JavaRDD<LabeledPoint> linesRDD = lines.toJavaRDD().map(new Function<String, LabeledPoint>() { 
      public LabeledPoint call(String lines) { 
      String[] tokens = lines.split(","); 
      double[] features = new double[30]; 
      for (int i = 2; i < features.length; i++) { 
          features[i - 2] =             
                 Double.parseDouble(tokens[i]); 
        } 
           Vector v = new DenseVector(features); 
           if (tokens[1].equals("B")) { 
      return new LabeledPoint(1.0, v); // benign 
    } else { 
    return new LabeledPoint(0.0, v); // malignant 
    } 
      } 
    }); 

```

![Logistic regression-based model selection for the cancer dataset](../images/00082.jpeg)

图 12:标签点 RDDs 快照

在*图 9* 、*前 20 行*中可以看到，标签 B 和 M 已经转换为 1.0 和 0.0。现在我们需要从标签点 RDDs 中创建一个数据帧。

**第五步:创建数据集并准备训练和测试集**

通过指定标签点类，从以前的 RDDs(即`linesRDD`)创建数据集:

```scala
Dataset<Row> data = spark.sqlContext().createDataFrame(linesRDD, LabeledPoint.class); 
data.show(); 

```

![Logistic regression-based model selection for the cancer dataset](../images/00136.jpeg)

图 13:创建的数据集显示了前 20 行。

```scala
Dataset<Row>[] splits=data.randomSplit(new double[] {0.8, 0.2}); 
Dataset<Row> training = splits[0]; 
Dataset<Row> test = splits[1];
```

请注意，您必须根据您的数据和问题类型相应地设置随机拆分的比例。

**步骤 6:使用逻辑回归配置 ML 管道:**

```scala
LogisticRegression lr = new LogisticRegression(); 

```

**第七步:构建一个参数网格来搜索**

目前，Spark 使用`ParamGridBuilder`来构建一个参数网格进行搜索。在这方面，假设我们有三个`hashingTF.numFeatures`值和两个`lr.regParam`值，这个网格将有 3×2 = 6 个参数设置供`CrossValidator`选择:

```scala
ParamMap[] paramGrid = new ParamGridBuilder() 
.addGrid(lr.regParam(), new double[] {0.1, 0.01}) 
.addGrid(lr.fitIntercept()) 
.addGrid(lr.elasticNetParam(), new double[] {0.0, 0.5, 1.0}) 
.build();
```

请注意，您必须根据您的数据和问题类型相应地设置上述参数的值。

**第 8 步:创建一个 TrainValidationSplit 实例:**

```scala
TrainValidationSplit trainValidationSplit = new TrainValidationSplit() 
.setEstimator(lr) 
.setEvaluator(new RegressionEvaluator()) 
.setEstimatorParamMaps(paramGrid) 
.setTrainRatio(0.8); 

```

在这种情况下，估计量只是我们在*步骤 4* 中创建的线性回归。准备癌症诊断`LabeledPoint` RDDs。一个`TrainValidationSplit`需要一个估计器、一组估计器`ParamMaps`和一个支持二进制分类的评估器，因为我们的数据集只有两个类，其中 80%用于训练，其余 20%用于验证。

**步骤 9:运行列车验证计划并选择参数**

运行`TrainValidationSplit`，使用训练集为你的问题选择最佳参数集。只需使用以下代码段:

```scala
TrainValidationSplitModel model = trainValidationSplit.fit(training); 

```

**第十步:对测试集**进行预测

对测试数据进行预测，其中模型是具有最佳性能参数组合的模型。最后，展示预测。只需使用以下代码段即可:

```scala
Dataset<Row> per_param = model.transform(test); 
per_param.show(); 

```

![Logistic regression-based model selection for the cancer dataset](../images/00038.jpeg)

图 14:针对每个特征和标签的预测

因此，如果将这些结果与*图 6 所示的结果进行比较，新创建的标签和 ID 来自数据集；其中 ID 是行数*，您会发现，对于更复杂的测量预测精度的方法，预测精度会提高，这些方法可用于根据每种类型错误的成本来确定错误率可以优化的地方。

# 总结

调整算法或机器学习应用程序可以被认为是一个简单的过程，当他们优化影响模型的参数以使算法执行得最好(在运行时和内存使用方面)时，会经历这个过程。在本章中，我们已经展示了如何使用 Spark ML 的训练验证分割和交叉验证技术来执行 ML 模型调整。

我们还想提到的是，直到当前版本的 Spark 发布之日(2016 年 10 月 14 日)，与调优相关的支持和算法仍然没有得到很好的丰富。我们鼓励感兴趣的读者访问位于[http://spark.apache.org/docs/latest/ml-tuning.html](http://spark.apache.org/docs/latest/ml-tuning.html)的 Spark 调优页面获取更多更新，因为我们相信 Spark 网站将增加更多功能，并且它们肯定会提供足够的文档。

在下一章中，我们将讨论如何使您的机器学习算法或模型适应新的数据集。本章涵盖先进的机器学习技术，使算法能够适应新数据。它将主要关注批处理/流体系结构和使用 Spark 流的在线学习算法。

最终目标是给静态机器学习模型带来活力。读者还将看到机器学习算法是如何在数据上增量学习的；也就是说，模型在每次看到新的训练实例时都会更新。