# 风暴调度器

在前面的章节中，我们介绍了 Storm 的基础知识、Storm 的安装、Storm 的开发和部署，以及 Storm 集群中的三叉戟拓扑。 在本章中，我们将重点介绍 Storm Schedulers。

在本章中，我们将介绍以下几点：

*   Storm Schedulers 简介
*   默认调度程序
*   隔离调度程序
*   资源感知调度器
*   客户感知调度程序

# Storm Scheduler 简介

如前两章所述，Nimbus 负责部署拓扑，管理程序负责执行 Storm 拓扑的喷嘴和螺栓组件中定义的计算任务。 如我们所示，我们可以根据调度程序策略为每个管理节点配置分配给拓扑的工作插槽数量，以及分配给拓扑的工作插槽数量。 简而言之，Storm Schedulers 帮助 Nimbus 决定任何给定拓扑的 Worker 分布。

# 默认调度程序

Storm Default Scheduler 在分配给给定拓扑的所有工作进程(主控引擎插槽)之间尽可能均匀地分配组件执行器。

让我们考虑一个示例拓扑，它有一个喷嘴和一个螺栓，两个组件都有两个执行器。 下图显示了如果我们已通过分配两个工作器(主管插槽)提交拓扑，则执行器的分配情况：

![](../images/00041.gif)

如上图所示，每个工作节点包含一个喷嘴执行器和一个螺栓执行器。 只有当每个组件中的执行器数量可以被分配给拓扑的工作器数量整除时，执行器才可能在工作器之间均匀分布。

# 隔离调度程序

隔离调度程序提供了一种在多个拓扑之间轻松、安全地共享 Storm 群集资源的机制。 隔离计划程序有助于为 Storm 群集内的拓扑分配/保留专用 Storm 节点集。

我们需要在 Nimbus 配置文件中定义以下属性以切换到隔离调度程序：

```
storm.scheduler: org.apache.storm.scheduler.IsolationScheduler 
```

我们可以通过指定拓扑名称和`isolation.scheduler.machines`属性内的节点数来为任何拓扑分配/保留资源，如下节所述。 我们需要在 Nimbus 配置中定义`isolation.scheduler.machines`属性，因为 Nimbus 负责在 Storm 节点之间分布拓扑工作器：

```
isolation.scheduler.machines:  
  "Topology-Test1": 2 
  "Topology-Test2": 1 
  "Topology-Test3": 4 
```

在前面的配置中，两个节点分配给`Topology-Test1`，一个节点分配给`Topology-Test2`，四个节点分配给`Topology-Test3`。

以下是隔离调度程序的关键点：

*   隔离列表中提到的拓扑优先于非隔离拓扑，这意味着如果与非隔离拓扑存在竞争，将首先将资源分配给隔离拓扑
*   无法在运行时更改拓扑的隔离设置
*   隔离拓扑通过将专用计算机分配给拓扑来解决多租户问题

# 资源感知调度器

资源感知调度程序可帮助用户指定任何组件(插口或螺栓)的单个实例所需的资源量。 我们可以通过在`storm.yaml`文件中指定以下属性来启用资源感知调度程序：

```
storm.scheduler: "org.apache.storm.scheduler.resource.ResourceAwareScheduler" 
```

# 组件级配置

您可以将内存要求分配给任何组件。 以下是可用于将内存分配给任何组件的单个实例的方法：

```
public T setMemoryLoad(Number onHeap, Number offHeap) 
```

或者，您可以使用以下选项：

```
public T setMemoryLoad(Number onHeap) 
```

以下是每个参数的定义：

*   `onHeap`：此组件的实例将消耗的堆空间量(MB)
*   `offHeap`：此组件的实例将消耗的堆外空间量(MB)

`onHeap`和`offHeap`的数据类型都是`Number`，默认值是`0.0`。

# 内存使用示例

让我们考虑一个包含两个组件的拓扑--一个喷嘴和一个螺栓：

```
SpoutDeclarer spout1 = builder.setSpout("spout1", new spoutComponent(), 4); 
spout1.setMemoryLoad(1024.0, 512.0); 
builder.setBolt("bolt1", new boltComponent(), 5).setMemoryLoad(512.0); 
```

`spout1`组件的单个实例的内存请求是 1.5 GB(堆上 1 GB，堆外 0.5 GB)，这意味着`spout1`组件的总内存请求是 4x1.5 GB=6 GB。

`bolt1`组件的单个实例的内存请求是 0.5 GB(堆上 0.5 GB，堆外 0.0 GB)，这意味着`bolt1`组件的总内存请求是 5x0.5 GB=2.5 GB。 计算这两个组件所需总内存的方法可以总结如下：

*分配给拓扑的总内存=spout1+bolt1=6+2.5=8.5 GB*

您还可以将 CPU 要求分配给任何组件。

以下是将 CPU 资源量分配给任何组件的单个实例所需的方法：

```
public T setCPULoad(Double amount) 
```

`amount`是任何给定组件的实例将消耗的 CPU 资源量。 CPU 使用率是一个很难定义的概念。 根据手头任务的不同，不同的 CPU 体系结构执行的方式也不同。 按照惯例，一个 CPU 核心通常会有 100 个点。 如果您觉得您的处理器或多或少很强大，您可以相应地进行调整。 受 CPU 限制的重任务将获得 100 分，因为它们可能会消耗整个内核。 中型任务应该得到 50 分，轻型任务应该得到 25 分，小型任务应该得到 10 分。

# CPU 使用示例

让我们考虑一个包含两个组件的拓扑--一个喷嘴和一个螺栓：

```
SpoutDeclarer spout1 = builder.setSpout("spout1", new spoutComponent(), 4); 
spout1.setCPULoad(15.0); 
builder.setBolt("bolt1", new boltComponent(), 5).setCPULoad(450.0); 
```

# 工作级别配置

您可以为每个工作进程/插槽分配堆大小。 下面是定义每个工作节点的堆大小所需的方法：

```
public void setTopologyWorkerMaxHeapSize(Number size) 
```

这里，`size`是单个工作者可用的堆空间量(MB)。

下面是一个例子：

```
Config conf = new Config(); 
conf.setTopologyWorkerMaxHeapSize(1024.0); 
```

# 节点级配置

我们可以通过在`storm.yaml`文件中设置以下属性来配置 Storm 节点可以使用的内存量和 CPU。 我们需要在每个 Storm 节点上设置以下属性：

```
supervisor.memory.capacity.mb: [amount<Double>] 
supervisor.cpu.capacity: [amount<Double>] 
```

下面是一个例子：

```
supervisor.memory.capacity.mb: 10480.0 
supervisor.cpu.capacity: 100.0 
```

这里，`100`表示整个核心，如前所述。

# 全局组件配置

如上一节所述，我们可以通过定义拓扑来定义每个组件的内存和 CPU 要求。 用户还可以在`storm.yaml`文件中设置组件的默认资源使用情况。 如果我们在代码中定义组件配置，则代码值将覆盖默认值：

```
//default value if on heap memory requirement is not specified for a component  
topology.component.resources.onheap.memory.mb: 128.0 

//default value if off heap memory requirement is not specified for a component  
topology.component.resources.offheap.memory.mb: 0.0 

//default value if CPU requirement is not specified for a component  
topology.component.cpu.pcore.percent: 10.0 

//default value for the max heap size for a worker   
topology.worker.max.heap.size.mb: 768.0 
```

# 自定义调度程序

在 Storm 中，Nimbus 使用调度程序将任务分配给主管。 默认调度程序旨在将计算资源均匀分配给拓扑。 它在拓扑之间的公平性方面运行良好，但用户无法预测拓扑组件在 Storm 群集中的位置，即拓扑的哪个组件需要分配给哪个管理节点。

让我们考虑一个例子。 假设我们有一个拓扑，它有一个管口和两个螺栓，每个组件都有一个执行器和一个任务。 下图显示了如果我们将拓扑提交到 Storm 群集，拓扑的分布情况。 假设分配给拓扑的工作进程数为 3，风暴群集中的主控引擎数为 3：

![](../images/00042.gif)

让我们假设拓扑中的最后一个螺栓**Bolt2**需要使用 GPU 而不是 CPU 来处理一些数据，并且只有一个主管使用 GPU。 我们需要编写自己的自定义调度器，以实现将任何组件分配给特定的管理节点。 以下是我们实现这一目标所需执行的步骤：

1.  在主管节点中配置更改。
2.  在组件级别配置设置。
3.  编写自定义调度程序类。
4.  注册客户计划程序类。

# 管理节点中的配置更改

Storm 在主管的配置中提供了一个字段，供用户指定自定义计划元数据。 在本例中，我们在 Supervisor 中键入`/tag`，以及它们正在运行的类型，这是通过它们的`$STORM_HOME/conf/storm.yaml`文件中的一行配置来完成的。 例如，每个管理节点在其配置中应具有以下内容：

```
supervisor.scheduler.meta: 
  type: GPU 
```

将配置更改添加到每个主管节点后，我们需要重新启动主管节点。 您需要对所有非 GPU 计算机使用 CPU 类型。

# 组件级别的配置设置

此步骤是在拓扑的 Main 方法中使用`TopologyBuilder`构建拓扑时完成的。 `ComponentConfigurationDeclarer`有一个名为`addConfiguration(String config, String value)`的方法，它允许添加自定义配置--即元数据。 在我们的示例中，我们使用以下方法添加类型信息：

```
TopologyBuilder builder = new TopologyBuilder(); 
builder.setSpout("spout", new SampleSpout(), 1); builder.setBolt("bolt1", new ExampleBolt1(), 1).shuffleGrouping("spout"); 
builder.setBolt("bolt3", new SampleBolt2(), 1).shuffleGrouping("bolt2").addConfiguration("type", "GPU"); 
```

前面的代码显示，我们键入了`bolt2`组件，并将`type`作为`GPU`。

# 编写自定义主管类

我们可以通过实现`org.apache.storm.scheduler.IScheduler`接口来编写我们的`CustomScheduler`类。 该接口包含两个重要的方法：

*   `prepare(Map conf)`：此方法仅初始化计划程序
*   `schedule(Topologies topologies, Cluster cluster)`：此方法包含负责群集管理程序插槽中的拓扑工作器的逻辑

`CustomScheduler`包含以下专用方法，该方法负责将工作进程分配给群集主控引擎插槽。

`getSupervisorsByType()`方法返回映射。 映射的键表示节点类型(例如 CPU 或 GPU)，值包含该类型的主管节点列表：

```
    private Map<String, ArrayList<SupervisorDetails>> getSupervisorsByType( 
            Collection<SupervisorDetails> supervisorDetails 
    ) { 
        // A map of type -> supervisors, to help with scheduling of components with specific types 
        Map<String, ArrayList<SupervisorDetails>> supervisorsByType = new HashMap<String, ArrayList<SupervisorDetails>>(); 

        for (SupervisorDetails supervisor : supervisorDetails) { 
            @SuppressWarnings("unchecked") 
            Map<String, String> metadata = (Map<String, String>) supervisor.getSchedulerMeta(); 

            String types; 

            if (metadata == null) { 
                types = unType; 
            } else { 
                types = metadata.get("types"); 

                if (types == null) { 
                    types = unType; 
                } 
            }
```

```
            // If the supervisor has types attached to it, handle it by populating the supervisorsByType map. 
            // Loop through each of the types to handle individually 
            for (String type : types.split(",")) { 
                type = type.trim(); 

                if (supervisorsByType.containsKey(type)) { 
                    // If we've already seen this type, then just add the supervisor to the existing ArrayList. 
                    supervisorsByType.get(type).add(supervisor); 
                } else { 
                    // If this type is new, then create a new ArrayList<SupervisorDetails>, 
                    // add the current supervisor, and populate the map's type entry with it. 
                    ArrayList<SupervisorDetails> newSupervisorList = new ArrayList<SupervisorDetails>(); 
                    newSupervisorList.add(supervisor); 
                    supervisorsByType.put(type, newSupervisorList); 
                } 
            } 
        } 

        return supervisorsByType; 
    } 
```

`populateComponentsByType()`方法还返回映射。 映射的键表示类型(CPU 或 GPU)，值包含需要分配给该类型管理节点的拓扑组件列表。 我们在这里使用非类型化类型来对没有类型的组件进行分组。 这样做的目的是以默认调度程序执行其分配的相同方式有效地处理这些非类型化组件。 这意味着，不含类型化组件的拓扑将以相同的方式在非类型化主管之间成功调度，不会出现任何问题：

```
    private <T> void populateComponentsByType( 
            Map<String, ArrayList<String>> componentsByType, 
            Map<String, T> components 
    ) { 
        // Type T can be either Bolt or SpoutSpec, so that this logic can be reused for both component types 
        JSONParser parser = new JSONParser(); 

        for (Entry<String, T> componentEntry : components.entrySet()) { 
            JSONObject conf = null; 

            String componentID = componentEntry.getKey(); 
            T component = componentEntry.getValue(); 

            try { 
                // Get the component's conf irrespective of its type (via java reflection) 
                Method getCommonComponentMethod = component.getClass().getMethod("get_common"); 
                ComponentCommon commonComponent = (ComponentCommon) getCommonComponentMethod.invoke(component); 
                conf = (JSONObject) parser.parse(commonComponent.get_json_conf()); 
            } catch (Exception ex) { 
                ex.printStackTrace(); 
            } 

            String types; 

            // If there's no config, use a fake type to group all untypeged components 
            if (conf == null) { 
                types = unType; 
            } else { 
                types = (String) conf.get("types"); 

                // If there are no types, use a fake type to group all untypeged components 
                if (types == null) { 
                    types = unType; 
                } 
            } 

            // If the component has types attached to it, handle it by populating the componentsByType map. 
            // Loop through each of the types to handle individually 
            for (String type : types.split(",")) { 
                type = type.trim(); 

                if (componentsByType.containsKey(type)) { 
                    // If we've already seen this type, then just add the component to the existing ArrayList. 
                    componentsByType.get(type).add(componentID); 
                } else { 
                    // If this type is new, then create a new ArrayList, 
                    // add the current component, and populate the map's type entry with it. 
                    ArrayList<String> newComponentList = new ArrayList<String>(); 
                    newComponentList.add(componentID); 
                    componentsByType.put(type, newComponentList); 
                } 
            } 
        } 
    } 
```

`populateComponentsByTypeWithStormInternals()`方法将 Storm 启动的内部组件的详细信息返回到拓扑的数据流：

```
    private void populateComponentsByTypeWithStormInternals( 
            Map<String, ArrayList<String>> componentsByType, 
            Set<String> components 
    ) { 
        // Storm uses some internal components, like __acker. 
        // These components are topology-agnostic and are therefore not accessible through a StormTopology object. 
        // While a bit hacky, this is a way to make sure that we schedule those components along with our topology ones: 
        // we treat these internal components as regular untypeged components and add them to the componentsByType map. 

        for (String componentID : components) { 
            if (componentID.startsWith("__")) { 
                if (componentsByType.containsKey(unType)) { 
                    // If we've already seen untypeged components, then just add the component to the existing ArrayList. 
                    componentsByType.get(unType).add(componentID); 
                } else { 
                    // If this is the first untypeged component we see, then create a new ArrayList, 
                    // add the current component, and populate the map's untypeged entry with it. 
                    ArrayList<String> newComponentList = new ArrayList<String>(); 
                    newComponentList.add(componentID); 
                    componentsByType.put(unType, newComponentList); 
                } 
            } 
        } 
    } 
```

前三种方法管理主控引擎和组件的映射。 现在，我们将编写`typeAwareScheduler()`方法，它将使用这两个映射：

```
    private void typeAwareSchedule(Topologies topologies, Cluster cluster) { 
        Collection<SupervisorDetails> supervisorDetails = cluster.getSupervisors().values(); 

        // Get the lists of typed and unreserved supervisors. 
        Map<String, ArrayList<SupervisorDetails>> supervisorsByType = getSupervisorsByType(supervisorDetails); 

        for (TopologyDetails topologyDetails : cluster.needsSchedulingTopologies(topologies)) { 
            StormTopology stormTopology = topologyDetails.getTopology(); 
            String topologyID = topologyDetails.getId(); 

            // Get components from topology 
            Map<String, Bolt> bolts = stormTopology.get_bolts(); 
            Map<String, SpoutSpec> spouts = stormTopology.get_spouts(); 

            // Get a map of component to executors 
            Map<String, List<ExecutorDetails>> executorsByComponent = cluster.getNeedsSchedulingComponentToExecutors( 
                    topologyDetails 
            ); 

            // Get a map of type to components 
            Map<String, ArrayList<String>> componentsByType = new HashMap<String, ArrayList<String>>(); 
            populateComponentsByType(componentsByType, bolts); 
            populateComponentsByType(componentsByType, spouts); 
            populateComponentsByTypeWithStormInternals(componentsByType, executorsByComponent.keySet()); 

            // Get a map of type to executors 
            Map<String, ArrayList<ExecutorDetails>> executorsToBeScheduledByType = getExecutorsToBeScheduledByType( 
                    cluster, topologyDetails, componentsByType 
            ); 

            // Initialise a map of slot -> executors 
            Map<WorkerSlot, ArrayList<ExecutorDetails>> componentExecutorsToSlotsMap = ( 
                    new HashMap<WorkerSlot, ArrayList<ExecutorDetails>>() 
            ); 

            // Time to match everything up! 
            for (Entry<String, ArrayList<ExecutorDetails>> entry : executorsToBeScheduledByType.entrySet()) { 
                String type = entry.getKey(); 

                ArrayList<ExecutorDetails> executorsForType = entry.getValue(); 
                ArrayList<SupervisorDetails> supervisorsForType = supervisorsByType.get(type); 
                ArrayList<String> componentsForType = componentsByType.get(type); 

                try { 
                    populateComponentExecutorsToSlotsMap( 
                            componentExecutorsToSlotsMap, 
                            cluster, topologyDetails, supervisorsForType, executorsForType, componentsForType, type 
                    ); 
                } catch (Exception e) { 
                    e.printStackTrace(); 

                    // Cut this scheduling short to avoid partial scheduling. 
                    return; 
                } 
            } 

            // Do the actual assigning 
            // We do this as a separate step to only perform any assigning if there have been no issues so far. 
            // That's aimed at avoiding partial scheduling from occurring, with some components already scheduled 
            // and alive, while others cannot be scheduled. 
            for (Entry<WorkerSlot, ArrayList<ExecutorDetails>> entry : componentExecutorsToSlotsMap.entrySet()) { 
                WorkerSlot slotToAssign = entry.getKey(); 
                ArrayList<ExecutorDetails> executorsToAssign = entry.getValue(); 

                cluster.assign(slotToAssign, topologyID, executorsToAssign); 
            } 

            // If we've reached this far, then scheduling must have been successful 
            cluster.setStatus(topologyID, "SCHEDULING SUCCESSFUL"); 
        } 
    } 
```

除了前面的四种方法外，我们还使用了更多的方法，这些方法可以做以下事情。

# 将组件 ID 转换为执行器

现在，让我们从组件 ID 跳到实际的执行器，因为这是 Storm 集群处理分配的级别。

这个过程相当简单：

*   从集群中获取按组件划分的执行器映射
*   根据集群检查哪些组件的执行器需要调度
*   创建类型到执行器的映射，仅填充那些正在等待调度的执行器：

```
private Set<ExecutorDetails> getAllAliveExecutors(Cluster cluster, TopologyDetails topologyDetails) { 
        // Get the existing assignment of the current topology as it's live in the cluster 
        SchedulerAssignment existingAssignment = cluster.getAssignmentById(topologyDetails.getId()); 

        // Return alive executors, if any, otherwise an empty set 
        if (existingAssignment != null) { 
            return existingAssignment.getExecutors(); 
        } else { 
            return new HashSet<ExecutorDetails>(); 
        } 
    } 

    private Map<String, ArrayList<ExecutorDetails>> getExecutorsToBeScheduledByType( 
            Cluster cluster, 
            TopologyDetails topologyDetails, 
            Map<String, ArrayList<String>> componentsPerType 
    ) { 
        // Initialise the return value 
        Map<String, ArrayList<ExecutorDetails>> executorsByType = new HashMap<String, ArrayList<ExecutorDetails>>(); 

        // Find which topology executors are already assigned 
        Set<ExecutorDetails> aliveExecutors = getAllAliveExecutors(cluster, topologyDetails); 

        // Get a map of component to executors for the topology that need scheduling 
        Map<String, List<ExecutorDetails>> executorsByComponent = cluster.getNeedsSchedulingComponentToExecutors( 
                topologyDetails 
        ); 

        // Loop through componentsPerType to populate the map 
        for (Entry<String, ArrayList<String>> entry : componentsPerType.entrySet()) { 
            String type = entry.getKey(); 
            ArrayList<String> componentIDs = entry.getValue(); 

            // Initialise the map entry for the current type 
            ArrayList<ExecutorDetails> executorsForType = new ArrayList<ExecutorDetails>(); 

            // Loop through this type's component IDs 
            for (String componentID : componentIDs) { 
                // Fetch the executors for the current component ID 
                List<ExecutorDetails> executorsForComponent = executorsByComponent.get(componentID); 

                if (executorsForComponent == null) { 
                    continue; 
                } 

                // Convert the list of executors to a set 
                Set<ExecutorDetails> executorsToAssignForComponent = new HashSet<ExecutorDetails>( 
                        executorsForComponent 
                ); 

                // Remove already assigned executors from the set of executors to assign, if any 
                executorsToAssignForComponent.removeAll(aliveExecutors); 

                // Add the component's waiting to be assigned executors to the current type executors 
                executorsForType.addAll(executorsToAssignForComponent); 
            } 

            // Populate the map of executors by type after looping through all of the type's components, 
            // if there are any executors to be scheduled 
            if (!executorsForType.isEmpty()) { 
                executorsByType.put(type, executorsForType); 
            } 
        } 

        return executorsByType; 
} 
```

# 将主管转换为插槽

现在我们必须进行最后的转换：从主管跳到老虎机。 与前面的组件及其执行器一样，我们需要这样做，因为集群在插槽级别而不是管理程序级别分配执行器。

在这一点上有几件事要做；为了保持可读性，我们已经将该过程分解成几个较小的方法。 我们需要执行的主要步骤如下：

给出一种类型的主管列表，找出我们可以分配给哪些插槽。 这只是使用一个 for 循环来收集所有主控引擎的插槽，然后根据拓扑请求返回任意数量的插槽。

将等待该类型调度的执行器跨槽均匀分组。

使用插槽中的条目填充指向执行器的映射。

这里的想法是为每个类型调用一次`populateComponentExecutorsToSlotsMap`方法，这将产生一个包含我们需要执行的所有任务的映射。

正如代码注释中所解释的那样，我们以前发现，有时我们会急切地将类型的执行器分配给一个槽，结果是一个连续的类型无法分配其执行器，从而导致部分调度。 自那以后，我们已经确保调度流确保不执行部分调度(要么全部调度，要么都不调度)，代价是额外的 for 循环，因为我们相信对于要处于的拓扑来说，这是一种更干净的状态：

```
    private void handleFailedScheduling( 
            Cluster cluster, 
            TopologyDetails topologyDetails, 
            String message 
    ) throws Exception { 
        // This is the prefix of the message displayed on Storm's UI for any unsuccessful scheduling 
        String unsuccessfulSchedulingMessage = "SCHEDULING FAILED: "; 

        cluster.setStatus(topologyDetails.getId(), unsuccessfulSchedulingMessage + message); 
        throw new Exception(message); 
    } 

    private Set<WorkerSlot> getAllAliveSlots(Cluster cluster, TopologyDetails topologyDetails) { 
        // Get the existing assignment of the current topology as it's live in the cluster 
        SchedulerAssignment existingAssignment = cluster.getAssignmentById(topologyDetails.getId()); 

        // Return alive slots, if any, otherwise an empty set 
        if (existingAssignment != null) { 
            return existingAssignment.getSlots(); 
        } else { 
            return new HashSet<WorkerSlot>(); 
        } 
    } 

    private List<WorkerSlot> getAllSlotsToAssign( 
            Cluster cluster, 
            TopologyDetails topologyDetails, 
            List<SupervisorDetails> supervisors, 
            List<String> componentsForType, 
            String type 
    ) throws Exception { 
        String topologyID = topologyDetails.getId(); 

        // Collect the available slots of each of the supervisors we were given in a list 
        List<WorkerSlot> availableSlots = new ArrayList<WorkerSlot>(); 
        for (SupervisorDetails supervisor : supervisors) { 
            availableSlots.addAll(cluster.getAvailableSlots(supervisor)); 
        } 

        if (availableSlots.isEmpty()) { 
            // This is bad, we have supervisors and executors to assign, but no available slots! 
            String message = String.format( 
                    "No slots are available for assigning executors for type %s (components: %s)", 
                    type, componentsForType 
            ); 
            handleFailedScheduling(cluster, topologyDetails, message); 
        } 

        Set<WorkerSlot> aliveSlots = getAllAliveSlots(cluster, topologyDetails); 

        int numAvailableSlots = availableSlots.size(); 
        int numSlotsNeeded = topologyDetails.getNumWorkers() - aliveSlots.size(); 

        // We want to check that we have enough available slots 
        // based on the topology's number of workers and already assigned slots. 
        if (numAvailableSlots < numSlotsNeeded) { 
            // This is bad, we don't have enough slots to assign to! 
            String message = String.format( 
                    "Not enough slots available for assigning executors for type %s (components: %s). " 
                            + "Need %s slots to schedule but found only %s", 
                    type, componentsForType, numSlotsNeeded, numAvailableSlots 
            ); 
            handleFailedScheduling(cluster, topologyDetails, message); 
        } 

        // Now we can use only as many slots as are required. 
        return availableSlots.subList(0, numSlotsNeeded); 
    } 

    private Map<WorkerSlot, ArrayList<ExecutorDetails>> getAllExecutorsBySlot( 
            List<WorkerSlot> slots, 
            List<ExecutorDetails> executors 
    ) { 
        Map<WorkerSlot, ArrayList<ExecutorDetails>> assignments = new HashMap<WorkerSlot, ArrayList<ExecutorDetails>>(); 

        int numberOfSlots = slots.size(); 

        // We want to split the executors as evenly as possible, across each slot available, 
        // so we assign each executor to a slot via round robin 
        for (int i = 0; i < executors.size(); i++) { 
            WorkerSlot slotToAssign = slots.get(i % numberOfSlots); 
            ExecutorDetails executorToAssign = executors.get(i); 

            if (assignments.containsKey(slotToAssign)) { 
                // If we've already seen this slot, then just add the executor to the existing ArrayList. 
                assignments.get(slotToAssign).add(executorToAssign); 
            } else { 
                // If this slot is new, then create a new ArrayList, 
                // add the current executor, and populate the map's slot entry with it. 
                ArrayList<ExecutorDetails> newExecutorList = new ArrayList<ExecutorDetails>(); 
                newExecutorList.add(executorToAssign); 
                assignments.put(slotToAssign, newExecutorList); 
            } 
        } 

        return assignments; 
    } 

    private void populateComponentExecutorsToSlotsMap( 
            Map<WorkerSlot, ArrayList<ExecutorDetails>> componentExecutorsToSlotsMap, 
            Cluster cluster, 
            TopologyDetails topologyDetails, 
            List<SupervisorDetails> supervisors, 
            List<ExecutorDetails> executors, 
            List<String> componentsForType, 
            String type 
    ) throws Exception { 
        String topologyID = topologyDetails.getId(); 

        if (supervisors == null) { 
            // This is bad, we don't have any supervisors but have executors to assign! 
            String message = String.format( 
                    "No supervisors given for executors %s of topology %s and type %s (components: %s)", 
                    executors, topologyID, type, componentsForType 
            ); 
            handleFailedScheduling(cluster, topologyDetails, message); 
        } 

        List<WorkerSlot> slotsToAssign = getAllSlotsToAssign( 
                cluster, topologyDetails, supervisors, componentsForType, type 
        ); 

        // Divide the executors evenly across the slots and get a map of slot to executors 
        Map<WorkerSlot, ArrayList<ExecutorDetails>> executorsBySlot = getAllExecutorsBySlot( 
                slotsToAssign, executors 
        ); 

        for (Entry<WorkerSlot, ArrayList<ExecutorDetails>> entry : executorsBySlot.entrySet()) { 
            WorkerSlot slotToAssign = entry.getKey(); 
            ArrayList<ExecutorDetails> executorsToAssign = entry.getValue(); 

            // Assign the topology's executors to slots in the cluster's supervisors 
            componentExecutorsToSlotsMap.put(slotToAssign, executorsToAssign); 
        } 
    } 
```

# 注册 CustomScheduler 类

我们需要为`CustomScheduler`类创建一个 JAR，并将其放在`$STORM_HOME/lib/`中，并通过将以下行附加到配置文件中的`$STORM_HOME/conf/storm.yaml`来告诉 Nimbus 使用新的调度器：

```
storm.scheduler: "com.stormadvance.storm_kafka_topology.CustomScheduler" 
```

重新启动 Nimbus 守护进程以反映对配置的更改。

现在，如果我们部署上图中所示的相同拓扑，则执行器的分布将如下所示(**Bolt2**被分配给 GPU 类型的管理程序)：

![](../images/00043.gif)

# 简略的 / 概括的 / 简易判罪的 / 简易的

在本章中，我们了解了内置 Storm 计划程序，并介绍了如何编写和配置自定义计划程序。

在下一章中，我们将介绍如何使用石墨和 Ganglia 来监控暴风集群。