# Storm 和 Hadoop 集成

到目前为止，我们已经了解了 Storm 如何用于开发实时流处理应用程序。 通常，这些实时应用程序很少单独使用；它们通常不与其他批处理操作结合使用。

开发批处理作业最常用的平台是 Apache Hadoop。 在本章中，我们将了解如何在 Storm-Yanson 框架的帮助下，在现有 Hadoop 群集上部署使用 Apache Storm 构建的应用程序，以优化资源的使用和管理。 我们还将介绍如何通过在 Storm 中创建 HDFS 螺栓将过程数据写入 HDFS。

在本章中，我们将介绍以下主题：

*   Apache Hadoop 及其各种组件概述
*   设置 Hadoop 群集
*   写入 Storm 拓扑以将数据持久存储到 HDFS
*   风雨纱概述
*   在 Hadoop 上部署 Storm-Year
*   在 Storm-Screen 上运行 Storm 应用程序。

# Hadoop 简介

Apache Hadoop 是一个用于开发和部署大数据应用程序的开源平台。 它最初是由雅虎开发的！ 基于 Google 发布的 MapReduce 和 Google 文件系统论文。 过去几年，Hadoop 已经成为旗舰大数据平台。

在本节中，我们将讨论 Hadoop 集群的关键组件。

# Hadoop Common

这是其他 Hadoop 模块所基于的基本库。 它为操作系统和文件系统操作提供了抽象，因此 Hadoop 可以部署在各种平台上。

# Hadoop 分布式文件系统

通常称为**HDFS**，**Hadoop 分布式文件系统**是一个可伸缩的分布式容错文件系统。 HDFS 充当 Hadoop 生态系统的存储层。 它允许在 Hadoop 集群中的各个节点之间共享和存储数据和应用程序代码。

以下是设计 HDFS 时采取的关键假设：

*   它应该可以部署在商用硬件群集上。
*   硬件故障是意料之中的，它应该能够容忍这些故障。
*   它应该可以扩展到数千个节点。
*   即使以延迟为代价，也应该针对高吞吐量对其进行优化。
*   大多数文件都会很大，因此应该针对大文件进行优化。
*   存储很便宜，因此使用复制来提高可靠性。
*   它应该是位置感知的，以便可以在数据实际驻留的物理节点上执行所请求的数据计算。 这将减少数据移动，从而降低网络拥塞。

HDFS 群集具有以下组件。

# NameNode

NameNode 是 HDFS 群集中的主节点。 它负责管理文件系统元数据和操作。 它不存储任何用户数据，而只存储集群中所有文件的文件系统树。 它还跟踪属于文件一部分的块的物理位置。

由于 NameNode 将所有数据保存在 RAM 中，因此应该将其部署在具有大量 RAM 的机器上。 此外，不应该在托管 NameNode 的计算机上托管任何其他进程，以便所有资源都专用于它。

NameNode 是 HDFS 群集中的单点故障。 如果 NameNode 失效，则无法在 HDFS 群集上执行任何操作。

![](../images/00054.jpeg)

Figure 1: HDFS Cluster

# 数据节点

DataNode 负责在 HDFS 群集中存储用户数据。 HDFS 群集中可以有多个数据节点。 DataNode 将数据存储在连接到托管该 DataNode 的系统的物理磁盘上。 不建议在 RAID 配置中将 DataNode 数据存储在磁盘上，因为 HDFS 通过跨数据节点复制数据来实现数据保护。

# HDFS 客户端

HDFS 客户端是可用于与 HDFS 群集交互的客户端库。 它通常与 NameNode 通信以执行元操作，如创建新文件等，而数据节点则服务于实际的数据读写请求。

# 辅助 NameNode

辅助 NameNode 是 HDFS 名称不佳的组件之一。 尽管名为 NameNode，但它并不是 NameNode 的备用设备。 要理解它的功能，我们需要深入研究 NameNode 是如何工作的。

NameNode 将文件系统元数据保存在主内存中。 为了持久性，它还将此元数据以图像文件的形式写入本地磁盘。 当 NameNode 启动时，它将读取此 fs 映像快照文件，以重新创建用于保存文件系统数据的内存中数据结构。 文件系统上的任何更新都会应用于内存中的数据结构，但不会应用于映像。 这些更改以单独的文件(称为编辑日志)写入磁盘。 当 NameNode 启动时，它会将这些编辑日志合并到映像中，以便下一次重新启动会更快。 在生产中，编辑日志可能会变得非常大，因为 NameNode 不会频繁重新启动。 这可能会导致 NameNode 在每次重新启动时都需要很长的引导时间。

辅助 NameNode 负责将 NameNode 的编辑日志与映像合并，以便 NameNode 下次启动更快。 它从 NameNode 获取映像快照和编辑日志，并合并它们，然后将更新后的映像快照放在 NameNode 机器上。 这减少了在重新启动时需要从 NameNode 进行的合并量，从而减少了 NameNode 的引导时间。

以下屏幕截图说明了辅助 NameNode 的工作原理：

![](../images/00055.jpeg)

Figure 2: Secondary Namenode functioning

到目前为止，我们已经看到了 Hadoop 的存储方面。 接下来，我们将研究处理组件。

# 纺纱 / 奇谈 / 闲聊

Yar 是一个集群资源管理框架，允许用户向 Hadoop 集群提交各种作业，并管理可伸缩性、容错和作业调度等。 由于 HDFS 为大量数据提供了存储层，因此纱线框架为您提供了编写大数据处理应用程序所需的管道。

以下是纱束的主要组成部分。

# 资源管理器(ResourceManager，RM)

ResourceManager 是纱线集群中应用程序的入口点。 它是集群中的主进程，负责管理集群中的所有资源。 它还负责调度提交给集群的各种作业。 此调度策略是可插拔的，并且可以由用户定制，以防他们想要支持新类型的应用程序。

# 节点管理器(NM)

集群中的每个处理节点上都部署了 NodeManager 代理。 它在节点级别相当于 ResourceManager。 它与 ResourceManager 通信以更新节点状态并接收来自它的任何作业请求。 它还负责生命周期管理和向 ResourceManager 报告各种节点指标。

# ApplicationMaster(AM)

一旦 ResourceManager 计划了作业，它就不再跟踪其状态和进度。 这使得 ResourceManager 能够支持集群中完全不同类型的应用程序，而无需担心应用程序的内部通信和逻辑。

每当提交应用程序时，ResourceManager 都会为该应用程序创建一个新的 ApplicationMaster，然后负责协商来自 ResourceManager 的资源并与 NodeMangers 就资源进行通信。 NodeManager 以资源容器的形式提供资源，资源容器是资源分配的抽象，您可以在其中知道需要多少 CPU、内存等。

一旦应用程序开始在集群中的各个节点上运行，ApplicationMaster 就会跟踪各种作业的状态，并在出现故障时重新运行这些作业。 作业完成后，它会将资源释放给 ResourceManager。

以下屏幕截图说明了纱线簇中的各种组件：

![](../images/00056.jpeg)

Figure 3: YARN components

# 安装 Hadoop

现在我们已经了解了 Hadoop 集群的存储和处理部分，让我们开始安装 Hadoop。 在本章中，我们将使用 Hadoop 2.2.0。 请注意，此版本与 Hadoop 1.X 版本不兼容。

我们将在单个节点上设置群集。 在开始之前，请确保您的系统上安装了以下软件：

*   JDK 1.7
*   `ssh-keygen`

如果您没有`wget`或`ssh-keygen`，请使用以下命令进行安装：

```scala
# yum install openssh-clients  
```

接下来，我们需要在这台机器上设置一个无密码 SSH，这是 Hadoop 所需的。

# 设置无密码 SSH

以下是设置无密码 SSH 的步骤：

1.  通过执行以下命令生成 SSH 密钥对：

```scala
    $ ssh-keygen -t rsa -P ''
    Generating public/private rsa key pair.
    Enter file in which to save the key (/home/anand/.ssh/id_rsa): 
    Your identification has been saved in /home/anand/.ssh/id_rsa.
    Your public key has been saved in /home/anand/.ssh/id_rsa.pub.
    The key fingerprint is:
    b7:06:2d:76:ed:df:f9:1d:7e:5f:ed:88:93:54:0f:24 anand@localhost.localdomain
    The key's randomart image is:
    +--[ RSA 2048]----+
    |                 |
    |            E .  |
    |             o   |
    |         . .  o  |
    |        S + .. o |
    |       . = o.   o|
    |          o... .o|
    |         .  oo.+*|
    |            ..ooX|
    +-----------------+

```

2.  接下来，我们需要将生成的公钥复制到当前用户的授权密钥列表中。 为此，请执行以下命令：

```scala
$ cp ~/.ssh/id_rsa.pub ~/.ssh/authorized_keys  
```

3.  现在，我们可以通过以下命令使用 SSH 连接到本地主机来检查无密码 SSH 是否工作正常：

```scala
$ ssh localhost
Last login: Wed Apr  2 09:12:17 2014 from localhost  
```

因为我们可以在没有密码的情况下使用 SSH 进入本地主机，所以我们的设置现在正在工作，我们现在将继续进行 Hadoop 设置。

# 获取 Hadoop 包并设置环境变量

以下是设置 Hadoop 的步骤：

1.  从 apache 站点[http://hadoop.apache.org/releases.html#Download](http://hadoop.apache.org/releases.html#Download)下载 Hadoop2.2.0。
2.  将存档解压到我们想要安装 Hadoop 的位置。 我们将此位置命名为`$HADOOP_HOME`：

```scala
$ tar xzf hadoop-2.2.0.tar.gz
$ cd hadoop-2.2.0  
```

3.  接下来，我们需要为 Hadoop 设置环境变量和路径，将以下条目添加到您的`~/.bashrc`文件中。 确保根据您的系统提供 Java 和 Hadoop 的路径：

```scala
    export JAVA_HOME=/usr/java/jdk1.7.0_45
    export HADOOP_HOME=/home/anand/opt/hadoop-2.2.0
    export HADOOP_COMMON_HOME=/home/anand/opt/hadoop-2.2.0
    export HADOOP_HDFS_HOME=$HADOOP_COMMON_HOME
    export HADOOP_MAPRED_HOME=$HADOOP_COMMON_HOME
    export HADOOP_YARN_HOME=$HADOOP_COMMON_HOME
    export HADOOP_CONF_DIR=$HADOOP_COMMON_HOME/etc/hadoop
    export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_COMMON_HOME/lib/native
    export HADOOP_OPTS="-Djava.library.path=$HADOOP_COMMON_HOME/lib"

    export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_COMMON_HOME/bin:$HADOOP_COMMON_HOME/sbin

```

4.  刷新您的`~/.bashrc`文件：

```scala
$ source ~/.bashrc  
```

5.  现在，让我们使用以下命令检查路径是否配置正确：

```scala
$ hadoop version
Hadoop 2.2.0
Subversion https://svn.apache.org/repos/asf/hadoop/common -r 1529768
Compiled by hortonmu on 2013-10-07T06:28Z
Compiled with protoc 2.5.0
From source with checksum 79e53ce7994d1628b240f09af91e1af4
This command was run using /home/anand/opt/hadoop-
2.2.0/share/hadoop/common/hadoop-common-2.2.0.jar  
```

在前面的代码片段中，我们可以看到路径设置正确。 现在，我们将在系统上设置 HDFS。

# 设置 HDFS

按照以下步骤设置 HDFS：

1.  创建用于保存 NameNode 和 DataNode 数据的目录：

```scala
$ mkdir -p ~/mydata/hdfs/namenode
$ mkdir -p ~/mydata/hdfs/datanode  
```

2.  通过在`<configuration>`标记内添加以下属性，在`$HADOOP_CONF_DIR/core-site.xml`文件中指定 NameNode 端口：

```scala
<property> 
        <name>fs.default.name</name> 
        <value>hdfs://localhost:19000</value> 
   <!-- The default port for HDFS is 9000, but we are using 19000 Storm-Yarn uses port 9000 for its application master --> 
</property> 
```

3.  通过在`<configuration>`标记内添加以下属性，在`$HADOOP_CONF_DIR/hdfs-site.xml`文件中指定 NameNode 和 DataNode 目录：

```scala
<property> 
        <name>dfs.replication</name> 
        <value>1</value> 
   <!-- Since we have only one node, we have replication factor=1 --> 
</property> 
<property> 
        <name>dfs.namenode.name.dir</name> 
        <value>file:/home/anand/hadoop-data/hdfs/namenode</value> 
   <!-- specify absolute path of the namenode directory --> 
</property> 
<property> 
        <name>dfs.datanode.data.dir</name> 
        <value>file:/home/anand/hadoop-data/hdfs/datanode</value> 
   <!-- specify absolute path of the datanode directory --> 
</property> 
```

4.  现在我们将格式化 NameNode。 这是一次性过程，只需在设置 HDFS 时完成：

```scala
    $ hdfs namenode -format
    14/04/02 09:03:06 INFO namenode.NameNode: STARTUP_MSG: 
    /*********************************************************
    STARTUP_MSG: Starting NameNode
    STARTUP_MSG:   host = localhost.localdomain/127.0.0.1
    STARTUP_MSG:   args = [-format]
    STARTUP_MSG:   version = 2.2.0
    ... ...
    14/04/02 09:03:08 INFO namenode.NameNode: SHUTDOWN_MSG: 
    /*********************************************************
    SHUTDOWN_MSG: Shutting down NameNode at localhost.localdomain/127.0.0.1
    ********************************************************/

```

5.  现在，我们完成了配置，我们将启动 HDFS：

```scala
    $ start-dfs.sh 
    14/04/02 09:27:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
    Starting namenodes on [localhost]
    localhost: starting namenode, logging to /home/anand/opt/hadoop-2.2.0/logs/hadoop-anand-namenode-localhost.localdomain.out
    localhost: starting datanode, logging to /home/anand/opt/hadoop-2.2.0/logs/hadoop-anand-datanode-localhost.localdomain.out
    Starting secondary namenodes [0.0.0.0]
    0.0.0.0: starting secondarynamenode, logging to /home/anand/opt/hadoop-2.2.0/logs/hadoop-anand-secondarynamenode-localhost.localdomain.out
    14/04/02 09:27:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable

```

6.  现在，执行`jps`命令，查看所有进程是否运行正常：

```scala
$ jps
50275 NameNode
50547 SecondaryNameNode
50394 DataNode
51091 Jps  
```

在这里，我们可以看到所有预期的进程都在运行。

7.  现在，您可以通过在浏览器中打开`http://localhost:50070`，使用 NameNode UI 检查 HDFS 的状态。 您应该会看到与以下内容类似的内容：

![](../images/00057.jpeg)

Figure 4: Namenode web UI

8.  您可以使用`hdfs dfs`命令与 HDFS 交互。 通过在控制台上运行`hdfs dfs`获取所有选项，或参阅[http://hadoop.apache.org/docs/r2.2.0/hadoop-project-dist/hadoop-common/FileSystemShell.html](http://hadoop.apache.org/docs/r2.2.0/hadoop-project-dist/hadoop-common/FileSystemShell.html)上的文档。

现在已经部署了 HDFS，接下来我们将设置纱线。

# 设置纱线

以下是设置纱线的步骤：

1.  从模板`mapred-site.xml.template`创建`mapred-site.xml`文件：

```scala
$ cp $HADOOP_CONF_DIR/mapred-site.xml.template $HADOOP_CONF_DIR/mapred-
site.xml  
```

2.  通过在`<configuration>`标记的`$HADOOP_CONF_DIR/mapred-site.xml`文件中添加以下属性，指定我们正在使用纱线框架：

```scala
<property> 
        <name>mapreduce.framework.name</name> 
        <value>yarn</value> 
</property> 
```

3.  在`$HADOOP_CONF_DIR/yarn-site.xml`文件中配置以下属性：

```scala
<property> 
        <name>yarn.nodemanager.aux-services</name> 
        <value>mapreduce_shuffle</value> 
</property> 

<property> 
        <name>yarn.scheduler.minimum-allocation-mb</name> 
        <value>1024</value> 
</property> 

<property> 
        <name>yarn.nodemanager.resource.memory-mb</name> 
        <value>4096</value> 
</property> 

<property> 
        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name> 
   <value>org.apache.hadoop.mapred.ShuffleHandler</value> 
</property> 
<property> 
        <name>yarn.nodemanager.vmem-pmem-ratio</name> 
        <value>8</value> 
</property> 
```

4.  使用以下命令启动纱线流程：

```scala
$ start-yarn.sh 
starting yarn daemons
starting resourcemanager, logging to /home/anand/opt/hadoop-2.2.0/logs/yarn-anand-resourcemanager-localhost.localdomain.out
localhost: starting nodemanager, logging to /home/anand/opt/hadoop-2.2.0/logs/yarn-anand-nodemanager-localhost.localdomain.out  
```

5.  现在，执行`jps`命令，查看所有进程是否运行正常：

```scala
$ jps
50275 NameNode
50547 SecondaryNameNode
50394 DataNode
51091 Jps
50813 NodeManager
50716 ResourceManager  
```

在这里，我们可以看到所有预期的进程都在运行。

6.  现在，您可以通过在浏览器中打开`http://localhost:8088/cluster`，使用 ResourceManager Web UI 检查纱线的状态。 您应该会看到与以下内容类似的内容：

![](../images/00058.jpeg)

Figure 5: ResourceManager web UI

7.  您可以使用`yarn`命令与纱线交互。 通过在控制台上运行`yarn`获取所有选项，或参阅[http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/YarnCommands.html](http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/YarnCommands.html)上的文档。 要使所有应用程序当前都在 YAR 上运行，请运行以下命令：

```scala
    $ yarn application -list
    14/04/02 11:41:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
    14/04/02 11:41:42 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
    Total number of applications (application-types: [] and states: [SUBMITTED, ACCEPTED, RUNNING]):0
                    Application-Id          Application-Name        Application-Type          User       Queue               State             Final-State             Progress                          Tracking-URL

```

至此，我们已经在单个节点上完成了 Hadoop 集群的部署。 接下来，我们将了解如何在此群集上运行 Storm 拓扑。

# 写入 Storm 拓扑以将数据持久存储到 HDFS

在本节中，我们将介绍如何编写 HDFS 螺栓以将数据持久化到 HDFS 中。 在本节中，我们重点介绍以下几点：

*   使用来自卡夫卡的数据
*   将数据存储到 HDFS 的逻辑
*   在预定义的时间或大小之后将文件旋转到 HDFS

执行以下步骤以创建拓扑以将数据存储到 HDFS 中：

1.  使用 groupId`com.stormadvance`和 artifactId`storm-hadoop`创建一个新的 maven 项目。
2.  在`pom.xml`文件中添加以下依赖项。 我们在`pom.xml`中添加了 Kafka Maven 依赖项来支持 Kafka Consumer。 请参考上一章的 Kafka 生成数据，这里我们将消费 Kafka 中的数据，并存储在 HDFS 中：

```scala
         <dependency> 
               <groupId>org.codehaus.jackson</groupId> 
               <artifactId>jackson-mapper-asl</artifactId> 
               <version>1.9.13</version> 
         </dependency> 

         <dependency> 
               <groupId>org.apache.hadoop</groupId> 
               <artifactId>hadoop-client</artifactId> 
               <version>2.2.0</version> 
               <exclusions> 
                     <exclusion> 
                           <groupId>org.slf4j</groupId> 
                           <artifactId>slf4j-log4j12</artifactId> 
                     </exclusion> 
               </exclusions> 
         </dependency> 
         <dependency> 
               <groupId>org.apache.hadoop</groupId> 
               <artifactId>hadoop-hdfs</artifactId> 
               <version>2.2.0</version> 
               <exclusions> 
                     <exclusion> 
                           <groupId>org.slf4j</groupId> 
                           <artifactId>slf4j-log4j12</artifactId> 
                     </exclusion> 
               </exclusions> 
         </dependency> 
         <!-- Dependency for Storm-Kafka spout --> 
         <dependency> 
               <groupId>org.apache.storm</groupId> 
               <artifactId>storm-kafka</artifactId> 
               <version>1.0.2</version> 
               <exclusions> 
                     <exclusion> 
                           <groupId>org.apache.kafka</groupId> 
                           <artifactId>kafka-clients</artifactId> 
                     </exclusion> 
               </exclusions> 
         </dependency> 

         <dependency> 
               <groupId>org.apache.kafka</groupId> 
               <artifactId>kafka_2.10</artifactId> 
               <version>0.9.0.1</version> 
               <exclusions> 
                     <exclusion> 
                           <groupId>com.sun.jdmk</groupId> 
                           <artifactId>jmxtools</artifactId> 
                     </exclusion> 
                     <exclusion> 
                           <groupId>com.sun.jmx</groupId> 
                           <artifactId>jmxri</artifactId> 
                     </exclusion> 
               </exclusions> 
         </dependency> 

         <dependency> 
               <groupId>org.apache.storm</groupId> 
               <artifactId>storm-core</artifactId> 
               <version>1.0.2</version> 
               <scope>provided</scope> 
         </dependency> 
   </dependencies> 
   <repositories> 
         <repository> 
               <id>clojars.org</id> 
               <url>http://clojars.org/repo</url> 
         </repository> 
   </repositories> 
```

3.  编写 Storm Hadoop 拓扑以使用 HDFS 中的数据并将其存储在 HDFS 中。 以下是对`com.stormadvance.storm_hadoop.topology.StormHDFSTopology`类的逐行描述：
4.  使用以下几行来使用 Kafka 中的数据：

```scala
         // zookeeper hosts for the Kafka cluster 
         BrokerHosts zkHosts = new ZkHosts("localhost:2181"); 

         // Create the KafkaReadSpout configuartion 
         // Second argument is the topic name 
         // Third argument is the zookeeper root for Kafka 
         // Fourth argument is consumer group id 
         SpoutConfig kafkaConfig = new SpoutConfig(zkHosts, "dataTopic", "", 
                     "id7"); 

         // Specify that the kafka messages are String 
         kafkaConfig.scheme = new SchemeAsMultiScheme(new StringScheme()); 

         // We want to consume all the first messages in the topic everytime 
         // we run the topology to help in debugging. In production, this 
         // property should be false 
         kafkaConfig.startOffsetTime = kafka.api.OffsetRequest.EarliestTime(); 

         // Now we create the topology 
         TopologyBuilder builder = new TopologyBuilder(); 

         // set the kafka spout class 
         builder.setSpout("KafkaReadSpout", new KafkaSpout(kafkaConfig), 1); 
```

5.  使用以下代码行定义 HDFS NameNode 详细信息和 HDFS 数据目录的名称，以将数据存储到 HDFS 中，在每 5 MB 数据块存储到 HDFS 之后创建一个新文件，并在每 1000 条记录之后将最新数据同步到该文件中：

```scala
         // use "|" instead of "," for field delimiter 
         RecordFormat format = new DelimitedRecordFormat() 
                     .withFieldDelimiter(","); 

         // sync the filesystem after every 1k tuples 
         SyncPolicy syncPolicy = new CountSyncPolicy(1000); 

         // rotate files when they reach 5MB 
         FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, 
                     Units.MB); 

         FileNameFormat fileNameFormatHDFS = new DefaultFileNameFormat() 
                     .withPath("/hdfs-bolt-output/"); 

         HdfsBolt hdfsBolt2 = new HdfsBolt().withFsUrl("hdfs://127.0.0.1:8020") 
                     .withFileNameFormat(fileNameFormatHDFS) 
                     .withRecordFormat(format).withRotationPolicy(rotationPolicy) 
                     .withSyncPolicy(syncPolicy); 
```

6.  使用以下代码将喷嘴与 HDFS 螺栓连接：

```scala
HdfsBolt hdfsBolt2 = new HdfsBolt().withFsUrl("hdfs://127.0.0.1:8020") 
                     .withFileNameFormat(fileNameFormatHDFS) 
                     .withRecordFormat(format).withRotationPolicy(rotationPolicy) 
                     .withSyncPolicy(syncPolicy); 
```

# Storm 与 Hadoop 的集成

开发和运营大数据应用程序的组织已经部署 Hadoop 群集的可能性非常高。 此外，他们很可能还部署了实时流处理应用程序，以配合在 Hadoop 上运行的批处理应用程序。

如果我们能够利用已经部署的纱线集群来运行 Storm 拓扑，那就太好了。 这将使您只需要管理一个群集，而不是两个群集，从而降低维护的运营成本。

暴风纱是由雅虎公司开发的一个项目。 这样就可以在纱线群集上部署 Storm 拓扑。 它支持在由 YAR 管理的节点上部署 Storm 流程。

下图说明了 Storm 流程如何部署在纱线上：

![](../images/00059.gif)

Figure 6: Storm processes on YARN

在下一节中，我们将了解如何设置 Storm-Screen。

# 架设风雨纱

由于 Storm-Yar 仍然是 Alpha 版本，我们将继续使用`git`存储库的基本主分支。 确保您的系统上安装了`git`。 如果不是，则运行以下命令：

```scala
# yum install git-core  
```

还要确保您的系统上安装了 Apache ZooKeeper 和 Apache Maven。 有关设置说明，请参阅前面的章节。

以下是部署 Storm-Year 的步骤：

1.  使用以下命令克隆`storm-yarn`存储库：

```scala
$ cd ~/opt
$ git clone https://github.com/yahoo/storm-yarn.git
$ cd storm-yarn  
```

2.  通过运行以下`mvn`命令构建`storm-yarn`：

```scala
    $ mvn package
    [INFO] Scanning for projects...
    [INFO] 
    [INFO] ----------------------------------------------------
    [INFO] Building storm-yarn 1.0-alpha
    [INFO] ----------------------------------------------------
    ...
    [INFO] ----------------------------------------------------
    [INFO] BUILD SUCCESS
    [INFO] ----------------------------------------------------
    [INFO] Total time: 32.049s
    [INFO] Finished at: Fri Apr 04 09:45:06 IST 2014
    [INFO] Final Memory: 14M/152M
    [INFO] ----------------------------------------------------

```

3.  使用以下命令将`storm.zip`文件从`storm-yarn/lib`复制到 HDFS：

```scala
$ hdfs dfs -mkdir -p  /lib/storm/1.0.2-wip21
$ hdfs dfs -put lib/storm.zip /lib/storm/1.0.2-wip21/storm.zip  
```

在您的情况下，确切的版本可能与`1.0.2-wip21`不同。

4.  创建一个目录来保存我们的 Storm 配置：

```scala
$ mkdir -p ~/storm-data
$ cp lib/storm.zip ~/storm-data/
$ cd ~/storm-data/
$ unzip storm.zip  
```

5.  在`~/storm-data/storm-1.0.2-wip21/conf/storm.yaml`文件中添加以下配置：

```scala
storm.zookeeper.servers: 
     - "localhost" 

nimbus.host: "localhost" 

master.initial-num-supervisors: 2 
master.container.size-mb: 128 
```

如果需要，请根据您的设置更改这些值。

6.  通过将以下内容添加到`~/.bashrc`文件，将`storm-yarn/bin`文件夹添加到您的路径中：

```scala
export PATH=$PATH:/home/anand/storm-data/storm-1.0.2-wip21/bin:/home/anand/opt/storm-yarn/bin 
```

7.  刷新`~/.bashrc`：

```scala
$ source ~/.bashrc  
```

8.  确保 ZooKeeper 在您的系统上运行。 如果没有，则通过运行以下命令启动 ZooKeeper：

```scala
$ ~/opt/zookeeper-3.4.5/bin/zkServer.sh start  
```

9.  使用以下命令启动`storm-yarn`：

```scala
    $ storm-yarn launch ~/storm-data/storm-1.0.2-wip21/conf/storm.yaml 
    14/04/15 10:14:49 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
    14/04/15 10:14:49 INFO yarn.StormOnYarn: Copy App Master jar from local filesystem and add to local environment
    ... ... 
    14/04/15 10:14:51 INFO impl.YarnClientImpl: Submitted application application_1397537047058_0001 to ResourceManager at /0.0.0.0:8032
    application_1397537047058_0001

```

暴风纱线的申请已提交，申请 ID 为`application_1397537047058_0001`。

10.  我们可以使用以下`yarn`命令检索应用程序的状态：

```scala
    $ yarn application -list
    14/04/15 10:23:13 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
    Total number of applications (application-types: [] and states: [SUBMITTED, ACCEPTED, RUNNING]):1
                    Application-Id          Application-Name        Application-Type          User       Queue               State             Final-State             Progress                          Tracking-URL
    application_1397537047058_0001             Storm-on-Yarn                    YARN         anand    default             RUNNING               UNDEFINED                  50%                                   N/A

```

11.  我们还可以看到`storm-yarn`在`http://localhost:8088/cluster/`的 ResourceManager Web 用户界面上运行。 您应该能够看到与以下内容类似的内容：

![](../images/00060.jpeg)

Figure 7: Storm-YARN on the ResourceManager web UI

您可以通过单击 UI 上的各种链接来浏览公开的各种指标。

12.  Nimbus 现在也应该正在运行，您应该能够通过位于`http://localhost:7070/`的 Nimbus web UI 查看它：

![](../images/00061.jpeg)

Figure 8: Nimbus web UI running on YARN

13.  现在，我们需要获取 Storm 配置，当在这个 Storm 集群上的 YAR 上部署拓扑时将使用该配置。 为此，请执行以下命令：

```scala
    $ mkdir ~/.storm
    $ storm-yarn getStormConfig --appId application_1397537047058_0001 --output ~/.storm/storm.yaml
    14/04/15 10:32:01 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
    14/04/15 10:32:02 INFO yarn.StormOnYarn: application report for application_1397537047058_0001 :localhost.localdomain:9000
    14/04/15 10:32:02 INFO yarn.StormOnYarn: Attaching to localhost.localdomain:9000 to talk to app master application_1397537047058_0001
    14/04/15 10:32:02 INFO yarn.StormMasterCommand: storm.yaml downloaded into /home/anand/.storm/storm.yaml  
```

确保将正确的应用程序 ID(如步骤 9 中检索到的)传递给`-appId`参数。

现在我们已经成功地部署了 Storm-Year，我们将看到如何在这个 Storm 集群上运行我们的拓扑。

# 暴风纱线上的暴风启动器拓扑

在本节中，我们将了解如何在`storm-yarn`上部署 Storm-Starter 拓扑。 Storm-Starter 是 Storm 附带的一组示例拓扑。

按照以下步骤在 Storm-Year 上运行拓扑：

1.  克隆`storm-starter`项目：

```scala
$ git clone https://github.com/nathanmarz/storm-starter
$ cd storm-starter  
```

2.  使用以下`mvn`命令打包拓扑：

```scala
$ mvn package -DskipTests  
```

3.  使用以下命令在`storm-yarn`上部署拓扑：

```scala
    $ storm jar target/storm-starter-0.0.1-SNAPSHOT.jar storm.starter.WordCountTopology word-cout-topology
    545  [main] INFO  backtype.storm.StormSubmitter - Jar not uploaded to master yet. Submitting jar...
    558  [main] INFO  backtype.storm.StormSubmitter - Uploading topology jar target/storm-starter-0.0.1-SNAPSHOT.jar to assigned location: storm-local/nimbus/inbox/stormjar-9ab704ff-29f3-4b9d-b0ac-e9e41d4399dd.jar
    609  [main] INFO  backtype.storm.StormSubmitter - Successfully uploaded topology jar to assigned location: storm-local/nimbus/inbox/stormjar-9ab704ff-29f3-4b9d-b0ac-e9e41d4399dd.jar
    609  [main] INFO  backtype.storm.StormSubmitter - Submitting topology word-cout-topology in distributed mode with conf {"topology.workers":3,"topology.debug":true}
    937  [main] INFO  backtype.storm.StormSubmitter - Finished submitting topology: word-cout-topology

```

4.  现在，我们可以在位于`http://localhost:7070/`的 Nimbus web 用户界面上看到部署的拓扑：

![](../images/00062.jpeg)

Figure 9: Nimbus web UI showing the word-count topology on YARN

5.  要查看如何与`storm-yarn`上运行的拓扑交互，请运行以下命令：

```scala
$ storm-yarn  
```

6.  它将列出与各种 Storm 流程交互并启动新主管的所有选项。

因此，在本节中，我们构建了 Storm 启动的拓扑，并在`storm-yarn`上运行它。

# 简略的 / 概括的 / 简易判罪的 / 简易的

在本章中，我们介绍了 Apache Hadoop 以及作为 Hadoop 集群一部分的各种组件，如 HDFS、YAR 等等。 我们还了解了 HDFS 和纱线集群的子组件以及它们之间的交互方式。 然后，我们介绍了如何设置单节点 Hadoop 集群。

我们还介绍了暴风纱，这是本章的重点。 Storm-Yar 使您能够在 Hadoop 群集上运行 Storm 拓扑。 从可管理性和运营的角度来看，这是有帮助的。 最后，我们了解了如何在运行在纱线上的 Storm 上部署拓扑。

在下一章中，我们将看到 Storm 如何与其他大数据技术集成，如 HBase、Redis 等。