# 第六章。机器学习

不管我们是否注意到，我们每天都是机器学习的消费者。谷歌等电子邮件提供商会自动将一些收到的邮件推送到`Spam`文件夹，亚马逊等在线购物网站或脸书等社交网站会主动提供意想不到的有用建议。那么，是什么让这些软件产品能够重新连接久违的朋友呢？这些只是机器学习在行动中的几个例子。

形式上，机器学习是**人工智能** ( **AI** )的一部分，它处理一类可以从数据中学习并做出预测的算法。技术和基本概念来自统计领域。机器学习存在于计算机科学和统计学的交叉领域，被认为是数据科学最重要的组成部分之一。它已经存在了相当长的时间，但是它的复杂性只是随着数据和可伸缩性需求的增加而增加。机器学习算法本质上倾向于资源密集型和迭代性，这使得它们不太适合 MapReduce 范式。MapReduce 非常适合单程算法，但不太适合多程算法。火花研究项目正是为了应对这一挑战而启动的。Apache Spark 在其 MLlib 库中配备了高效算法，即使在迭代计算需求中也能表现出色。

前一章概述了数据分析的生命周期及其各种组件，如数据清理、数据转换、采样技术和可视化数据的图形技术，以及涵盖描述性统计和推理性统计的概念。我们还研究了一些可以在 Spark 平台上执行的统计测试。除了我们在上一章中建立的基础知识，我们将在本章中介绍大多数机器学习算法以及如何使用它们在 Spark 上建立模型。

作为本章的先决条件，对机器学习算法和计算机科学基础的基本理解是很好的。然而，我们已经用一组正确的实例介绍了算法的一些理论基础，以使这些算法更容易理解和实现。本章涵盖的主题有:

*   Introduction to machine learning
    *   进化
    *   监督学习
    *   无监督学习
*   MLlib and the Pipeline API
    *   MLlib(密西西比州)
    *   毫升管道
*   Introduction to machine learning
    *   参数方法
    *   非参数方法
*   Regression methods
    *   线性回归
    *   回归正则化
*   Classification methods
    *   逻辑回归
    *   线性支持向量机
*   Decision trees
    *   杂质测量
    *   停止规则
    *   分裂候选人
    *   决策树的优势
    *   例子
*   Ensembles
    *   随机森林
    *   梯度增强树
*   多层感知机分类器
*   Clustering techniques
    *   k-均值聚类
*   摘要

# 简介

机器学习就是通过示例数据进行学习；为给定输入产生特定输出的示例。机器学习有各种各样的业务用例。让我们看几个例子来了解一下它到底是什么:

*   向用户推荐他们可能感兴趣的商品的推荐引擎
*   营销活动的客户细分(将具有相似特征的客户分组)
*   癌症疾病分类-恶性/良性
*   预测建模，例如，销售预测、天气预测
*   例如，从商业角度推断，了解产品价格的变化会对销售产生什么影响

## 进化

甚至在第一个计算机系统出现之前，统计学习的概念就已经存在了。在十九世纪，最小二乘技术(现在称为线性回归)已经发展起来。对于分类问题，费希尔提出了**线性判别分析** ( **LDA** )。大约在 20 世纪 40 年代，一种被称为**逻辑回归**的 LDA 替代方法被提出，所有这些方法不仅随着时间的推移而改进，而且启发了其他新算法的发展。

在那个时期，计算是一个大问题，因为它是用纸和笔完成的。因此拟合非线性方程不太可行，因为它需要大量的计算。20 世纪 80 年代以后，随着技术的进步和计算机系统的引入，分类/回归树被引入。慢慢地，随着技术和计算系统的进一步进步，统计学习在某种程度上与现在所知的机器学习相融合。

## 监督学习

如前一节所述，机器学习就是通过示例数据进行学习。根据算法如何理解数据并对其进行训练，它们大致分为两类:**监督学习**和**无监督学习**。

监督统计学习包括基于特定输出的一个或多个输入建立模型。这意味着我们得到的输出可以根据我们提供的输入来监督我们的分析。换句话说，对于预测变量(例如，年龄、教育和费用变量)的每个观察，都有一个结果变量(例如，工资)的相关响应测量。参考下表，了解示例数据集，其中我们试图基于**年龄**、**教育、**和**费用**变量预测**工资**:

![Supervised learning](graphics/image_06_001.jpg)

监督算法可以用于预测、估计、分类和其他类似的需求，我们将在下面的章节中介绍。

## 无监督学习

无监督统计学习包括基于一个或多个输入建立模型，但无意产生特定的输出。这意味着没有响应/输出变量可以明确预测；但是输出通常是具有相似特征的数据点组。与监督学习不同，你不知道要将数据点分类到哪些组/标签中，而是让算法自己决定。

这里不存在`training`数据集的概念，该数据集通过建立模型然后使用`test`数据集验证模型来将`relate`结果变量与`predictor`变量进行比较。无监督算法的输出不能根据您提供的输入来监督您的分析。这样的算法可以从数据中学习关系和结构。*聚类*和*关联规则学习*是无监督学习技术的例子。

下图描述了如何使用聚类对具有相似特征的数据项进行分组:

![Unsupervised learning](graphics/image_06_002.jpg)

# MLlib 和管道应用编程接口

让我们首先学习一些 Spark 基础知识，以便能够对其执行机器学习操作。我们将在这一节讨论 MLlib 和管道 API。

## MLlib

MLlib 是建立在 Apache Spark 之上的机器学习库，其中包含了大多数可以大规模实现的算法。MLlib 与 GraphX、SQL 和 Streaming 等其他组件的无缝集成为开发人员提供了相对容易地组装复杂、可扩展和高效的工作流的机会。MLlib 库由常见的学习算法和实用程序组成，包括分类、回归、聚类、协同过滤和降维。

MLlib 与提供高级管道应用编程接口的`spark.ml`包一起工作。这两个包的根本区别在于，MLlib ( `spark.mllib`)在 RDDs 之上工作，而 ML ( `spark.ml`)包在数据帧之上工作，并支持 ML 管道。目前，这两个软件包都受 Spark 支持，但建议使用`spark.ml`软件包。

这个库中的基本数据类型是向量和矩阵。向量是局部的，可以是密集的，也可以是稀疏的。密集向量存储为一组值。稀疏向量存储为两个数组；第一个数组存储非零值索引，第二个数组存储实际值。所有元素值都存储为双精度值，索引存储为从零开始的整数。理解基本结构对有效使用库有很大帮助，它应该有助于从头开始编写任何新算法。让我们看一些示例代码，以便更好地理解这两种矢量表示:

**斯卡拉**

```scala
//Create vectors
scala> import org.apache.spark.ml.linalg.{Vector, Vectors}
import org.apache.spark.ml.linalg.{Vector, Vectors}

//Create dense vector
scala> val dense_v: Vector = Vectors.dense(10.0,0.0,20.0,30.0,0.0)
dense_v: org.apache.spark.ml.linalg.Vector = [10.0,0.0,20.0,30.0,0.0]
scala>

//Create sparse vector: pass size, position index array and value array
scala> val sparse_v1: Vector = Vectors.sparse(5,Array(0,2,3),
       Array(10.0,20.0,30.0))
sparse_v1: org.apache.spark.ml.linalg.Vector = (5,[0,2,3],[10.0,20.0,30.0])
scala>

//Another way to create sparse vector with position, value tuples
scala> val sparse_v2: Vector = Vectors.sparse(5,
        Seq((0,10.0),(2,20.0),(3,30.0)))
sparse_v2: org.apache.spark.ml.linalg.Vector = (5,[0,2,3],[10.0,20.0,30.0])
scala>  
 Compare vectors 
--------------- cala> sparse_v1 == sparse_v2
res0: Boolean = true
scala> sparse_v1 == dense_v
res1: Boolean = true      //All three objects are equal but...
scala> dense_v.toString()
res2: String = [10.0,0.0,20.0,30.0,0.0]
scala> sparse_v2.toString()
res3: String = (5,[0,2,3],[10.0,20.0,30.0]) //..internal representation
differs
scala> sparse_v2.toArray
res4: Array[Double] = Array(10.0, 0.0, 20.0, 30.0, 0.0)

Interchangeable ---------------
scala> dense_v.toSparse
res5: org.apache.spark.mllib.linalg.SparseVector = (5,[0,2,3]
[10.0,20.0,30.0])
scala> sparse_v1.toDense
res6: org.apache.spark.mllib.linalg.DenseVector = [10.0,0.0,20.0,30.0,0.0]
scala>

A common operation ------------------
scala> Vectors.sqdist(sparse_v1,
        Vectors.dense(1.0,2.0,3.0,4.0,5.0))
res7: Double = 1075.0
```

Python:

```scala
//Create vectors
>>> from pyspark.ml.linalg import Vector, Vectors
//Create vectors
>>> dense_v = Vectors.dense(10.0,0.0,20.0,30.0,0.0)
//Pass size, position index array and value array
>>> sparse_v1 = Vectors.sparse(5,[0,2,3],
                    [10.0,20.0,30.0])
>>> 

//Another way to create sparse vector with position, value tuples
>>> sparse_v2 = Vectors.sparse(5,
                  [[0,10.0],[2,20.0],[3,30.0]])
>>> 

Compare vectors 
--------------- >>> sparse_v1 == sparse_v2
True
>>> sparse_v1 == dense_v
True      //All three objects are equal but...
>>> dense_v
DenseVector([10.0, 0.0, 20.0, 30.0, 0.0])
>>> sparse_v1
SparseVector(5, {0: 10.0, 2: 20.0, 3: 30.0}) //..internal representation
differs
>>> sparse_v2
SparseVector(5, {0: 10.0, 2: 20.0, 3: 30.0})

Interchangeable 
---------------- //Note: as of Spark 2.0.0, toDense and toSparse are not available in pyspark
 A common operation 
------------------- >>> Vectors.squared_distance(sparse_v1,
        Vectors.dense(1.0,2.0,3.0,4.0,5.0))
1075.0
```

矩阵可以是局部的或分布的，密集的或稀疏的。局部矩阵作为一维数组存储在单台机器上。密集局部矩阵以列主顺序存储(列成员是连续的)，而稀疏矩阵值以列主顺序以**压缩稀疏列** ( **CSC** )格式存储。在这种格式中，矩阵以三个数组的形式存储。第一个数组包含非零值的行索引，第二个数组包含每列的起始值索引，第三个数组包含所有非零值。索引是从零开始的整数类型。第一个数组包含从零到行数减一的值。第三个数组包含 double 类型的元素。第二个数组需要一些解释。该数组中的每个条目对应于每列中第一个非零元素的索引。例如，假设在一个 3 乘 3 的矩阵中，每列只有一个非零元素。那么第二个数组将包含 0，1，2 作为它的元素。第一个数组包含行位置，第三个数组包含三个值。如果一列中没有非零元素，您将注意到相同的索引在第二个数组中重复。让我们检查一些示例代码:

**斯卡拉:**

```scala
scala> import org.apache.spark.ml.linalg.{Matrix,Matrices}
import org.apache.spark.ml.linalg.{Matrix, Matrices}

Create dense matrix 
------------------- //Values in column major order
Matrices.dense(3,2,Array(9.0,0,0,0,8.0,6))
res38: org.apache.spark.mllib.linalg.Matrix =
9.0  0.0
0.0  8.0
0.0  6.0
 Create sparse matrix 
-------------------- //1.0 0.0 4.0
0.0 3.0 5.0
2.0 0.0 6.0//
val sm: Matrix = Matrices.sparse(3,3,
        Array(0,2,3,6), Array(0,2,1,0,1,2),
        Array(1.0,2.0,3.0,4.0,5.0,6.0))
sm: org.apache.spark.mllib.linalg.Matrix =
3 x 3 CSCMatrix
(0,0) 1.0
(2,0) 2.0
(1,1) 3.0
(0,2) 4.0
(1,2) 5.0
(2,2) 6.0
 Sparse matrix, a column of all zeros 
------------------------------------ //third column all zeros
Matrices.sparse(3,4,Array(0,2,3,3,6),
    Array(0,2,1,0,1,2),values).toArray
res85: Array[Double] = Array(1.0, 0.0, 2.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.0,
4.0, 5.0, 6.0)

```

**蟒蛇:**

```scala
//Create dense matrix
>>> from pyspark.ml.linalg import Matrix, Matrices

//Values in column major order
>>> Matrices.dense(3,2,[9.0,0,0,0,8.0,6])
DenseMatrix(3, 2, [9.0, 0.0, 0.0, 0.0, 8.0, 6.0], False)
>>> 

//Create sparse matrix
//1.0 0.0 4.0
0.0 3.0 5.0
2.0 0.0 6.0//
>>> sm = Matrices.sparse(3,3,
        [0,2,3,6], [0,2,1,0,1,2],
        [1.0,2.0,3.0,4.0,5.0,6.0])
>>> 

//Sparse matrix, a column of all zeros
//third column all zeros
>>> Matrices.sparse(3,4,[0,2,3,3,6],
        [0,2,1,0,1,2],
    values=[1.0,2.0,3.0,4.0,5.0,6.0]).toArray()
array([[ 1.,  0.,  0.,  4.],
       [ 0.,  3.,  0.,  5.],
       [ 2.,  0.,  0.,  6.]])
>>> 
```

分布式矩阵是最复杂的矩阵，选择合适的分布式矩阵非常重要。分布式矩阵由一个或多个关系数据库支持。行和列索引属于`long`类型，以支持非常大的矩阵。分布式矩阵的基本类型是`RowMatrix`，它简单地由它的行的 RDD 支持。

每一行依次是一个局部向量。这适用于列数很少的情况。请记住，我们需要通过 rdd 来创建分布式矩阵，这与本地矩阵不同。让我们看一个例子:

**斯卡拉:**

```scala
scala> import org.apache.spark.mllib.linalg.{Vector,Vectors}
import org.apache.spark.mllib.linalg.{Vector, Vectors}
scala> import org.apache.spark.mllib.linalg.distributed.RowMatrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix

scala>val dense_vlist: Array[Vector] = Array(
    Vectors.dense(11.0,12,13,14),
    Vectors.dense(21.0,22,23,24),
    Vectors.dense(31.0,32,33,34))
dense_vlist: Array[org.apache.spark.mllib.linalg.Vector] =
Array([11.0,12.0,13.0,14.0], [21.0,22.0,23.0,24.0], [31.0,32.0,33.0,34.0])
scala>

//Distribute the vector list
scala> val rows  = sc.parallelize(dense_vlist)
rows: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] =
ParallelCollectionRDD[0] at parallelize at <console>:29
scala> val m: RowMatrix = new RowMatrix(rows)
m: org.apache.spark.mllib.linalg.distributed.RowMatrix =
org.apache.spark.mllib.linalg.distributed.RowMatrix@5c5043fe
scala> print("Matrix size is " + m.numRows()+"X"+m.numCols())
Matrix size is 3X4
scala>
```

**蟒蛇:**

```scala
>>> from pyspark.mllib.linalg import Vector,Vectors
>>> from pyspark.mllib.linalg.distributed import RowMatrix

>>> dense_vlist = [Vectors.dense(11.0,12,13,14),
         Vectors.dense(21.0,22,23,24), Vectors.dense(31.0,32,33,34)]
>>> rows  = sc.parallelize(dense_vlist)
>>> m = RowMatrix(rows)
>>> "Matrix size is {0} X {1}".format(m.numRows(), m.numCols())
'Matrix size is 3 X 4'
```

一个`IndexedRowMatrix`存储行条目前面的行索引。这在执行联接时很有用。你需要通过`IndexedRow`对象来创建一个`IndexedRowMatrix`。一个`IndexedRow`对象是一个带有一个长`Index`和一个`Vector`行元素的包装器。

A `CoordinatedMatrix`将数据存储为行、列索引和元素值的元组。A `BlockMatrix`表示局部矩阵块中的分布式矩阵。提供了将矩阵从一种类型转换为另一种类型的方法，但这些操作成本较高，应谨慎使用。

## ML 管道

真实的机器学习工作流是数据提取、数据清理、预处理、探索、特征提取、模型拟合和评估的迭代循环。Spark 上的 ML 管道是一个简单的 API，供用户设置复杂的 ML 工作流。它旨在解决一些棘手的问题，如参数调整，或基于不同的数据分割(交叉验证)或不同的参数集训练许多模型。编写脚本来自动化这一切不再是一个要求，可以在管道应用编程接口本身中处理。

管道应用编程接口由一系列管道阶段(作为抽象实现，如*转换器*和*估计器*)组成，以期望的顺序执行。

在 ML 管道中，您可以调用上一章中讨论的数据清理/转换函数，并调用 MLlib 中可用的机器学习算法。这可以以迭代的方式完成，直到您获得模型的期望性能。

![ML pipeline](graphics/image_06_003.jpg)

### 变压器

转换器是一种抽象，它实现`transform()`方法将一个数据帧转换成另一个数据帧。如果该方法是一个特征转换程序，根据您执行的操作，生成的数据框可能包含一些额外的转换列。然而，如果该方法是一个学习模型，那么产生的数据框架将包含一个额外的列，其中包含预测的结果。

### 估计值

估计器是一种抽象，可以是任何实现`fit()`方法的学习算法，以在数据帧上得到训练来产生模型。从技术上讲，这个模型是给定数据帧的转换器。

示例:逻辑回归是一种学习算法，因此是一种估计量。调用`fit()`训练一个逻辑回归模型，这是一个合成模型，因此是一个可以产生包含预测列的数据帧的转换器。

下面的示例演示了一个简单的单阶段管道。

**斯卡拉:**

```scala
//Pipeline example with single stage to illustrate syntax
scala> import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.Pipeline
scala> import org.apache.spark.ml.feature._
import org.apache.spark.ml.feature._

//Create source data frame
scala> val df = spark.createDataFrame(Seq(
         ("Oliver Twist","Charles Dickens"),
        ("Adventures of Tom Sawyer","Mark Twain"))).toDF(
        "Title","Author")

//Split the Title to tokens
scala> val tok = new Tokenizer().setInputCol("Title").
          setOutputCol("words")
tok: org.apache.spark.ml.feature.Tokenizer = tok_2b2757a3aa5f

//Define a pipeline with a single stage
scala> val p = new Pipeline().setStages(Array(tok))
p: org.apache.spark.ml.Pipeline = pipeline_f5e0de400666

//Run an Estimator (fit) using the pipeline
scala> val model = p.fit(df)
model: org.apache.spark.ml.PipelineModel = pipeline_d00989625bb2

//Examine stages
scala> p.getStages   //Returns a list of stage objects
res1: Array[org.apache.spark.ml.PipelineStage] = Array(tok_55af0061af6d)

// Examine the results
scala> val m = model.transform(df).select("Title","words")
m: org.apache.spark.sql.DataFrame = [Title: string, words: array<string>]
scala> m.select("words").collect().foreach(println)
[WrappedArray(oliver, twist)]
[WrappedArray(adventures, of, tom, sawyer)]
```

**蟒蛇:**

```scala
//Pipeline example with single stage to illustrate syntax
//Create source data frame
>>> from pyspark.ml.pipeline import Pipeline
>>> from pyspark.ml.feature import Tokenizer
>>>  df = sqlContext.createDataFrame([
    ("Oliver Twist","Charles Dickens"),
    ("Adventures of Tom Sawyer","Mark Twain")]).toDF("Title","Author")
>>> 

//Split the Title to tokens
>>> tok = Tokenizer(inputCol="Title",outputCol="words")

//Define a pipeline with a single stage
>>> p = Pipeline(stages=[tok])

//Run an Estimator (fit) using the pipeline
>>> model = p.fit(df)

//Examine stages
>>> p.getStages()  //Returns a list of stage objects
[Tokenizer_4f35909c4c504637a263]

// Examine the results
>>> m = model.transform(df).select("Title","words")
>>> [x[0] for x in m.select("words").collect()]
[[u'oliver', u'twist'], [u'adventures', u'of', u'tom', u'sawyer']]
>>> 
```

上面的例子展示了管道的创建和执行，虽然只有一个阶段，在这个上下文中是一个标记器。Spark 提供了几款开箱即用的“特色变压器€”。这些功能转换器在数据清理和数据准备阶段非常方便。

以下示例显示了将原始文本转换为特征向量的真实示例。如果您不熟悉 TF-IDF，请阅读来自[http://www.tfidf.com](http://www.tfidf.com)的这篇简短教程。

**斯卡拉:**

```scala
scala> import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.Pipeline
scala> import org.apache.spark.ml.feature._
import org.apache.spark.ml.feature._
scala> 

//Create a dataframe
scala> val df2 = spark.createDataset(Array(
         (1,"Here is some text to illustrate pipeline"),
         (2, "and tfidf, which stands for term frequency inverse document
frequency"
         ))).toDF("LineNo","Text")

//Define feature transformations, which are the pipeline stages
// Tokenizer splits text into tokens
scala> val tok = new Tokenizer().setInputCol("Text").
             setOutputCol("Words")
tok: org.apache.spark.ml.feature.Tokenizer = tok_399dbfe012f8

// HashingTF maps a sequence of words to their term frequencies using hashing
// Larger value of numFeatures reduces hashing collision possibility
scala> val tf = new HashingTF().setInputCol("Words").setOutputCol("tf").setNumFeatures(100)
tf: org.apache.spark.ml.feature.HashingTF = hashingTF_e6ad936536ea
// IDF, Inverse Docuemnt Frequency is a statistical weight that reduces weightage of commonly occuring words
scala> val idf = new IDF().setInputCol("tf").setOutputCol("tf_idf")
idf: org.apache.spark.ml.feature.IDF = idf_8af1fecad60a
// VectorAssembler merges multiple columns into a single vector column
scala> val va = new VectorAssembler().setInputCols(Array("tf_idf")).setOutputCol("features")
va: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_23205c3f92c8
//Define pipeline
scala> val tfidf_pipeline = new Pipeline().setStages(Array(tok,tf,idf,va))
val tfidf_pipeline = new Pipeline().setStages(Array(tok,tf,idf,va))
scala> tfidf_pipeline.getStages
res2: Array[org.apache.spark.ml.PipelineStage] = Array(tok_399dbfe012f8, hashingTF_e6ad936536ea, idf_8af1fecad60a, vecAssembler_23205c3f92c8)
scala>

//Now execute the pipeline
scala> val result = tfidf_pipeline.fit(df2).transform(df2).select("words","features").first()
result: org.apache.spark.sql.Row = [WrappedArray(here, is, some, text, to, illustrate, pipeline),(100,[0,3,35,37,69,81],[0.4054651081081644,0.4054651081081644,0.4054651081081644,0.4054651081081644,0.4054651081081644,0.4054651081081644])]
```

**蟒蛇:**

```scala
//A realistic, multi-step pipeline that converts text to TF_ID
>>> from pyspark.ml.pipeline import Pipeline
>>> from pyspark.ml.feature import Tokenizer, HashingTF, IDF, VectorAssembler, \
               StringIndexer, VectorIndexer

//Create a dataframe
>>> df2 = sqlContext.createDataFrame([
    [1,"Here is some text to illustrate pipeline"],
    [2,"and tfidf, which stands for term frequency inverse document
frequency"
    ]]).toDF("LineNo","Text")

//Define feature transformations, which are the pipeline stages
//Tokenizer splits text into tokens
>>> tok = Tokenizer(inputCol="Text",outputCol="words")

// HashingTF maps a sequence of words to their term frequencies using
hashing

// Larger the numFeatures, lower the hashing collision possibility
>>> tf = HashingTF(inputCol="words", outputCol="tf",numFeatures=1000)

// IDF, Inverse Docuemnt Frequency is a statistical weight that reduces
weightage of commonly occuring words
>>> idf = IDF(inputCol = "tf",outputCol="tf_idf")

// VectorAssembler merges multiple columns into a single vector column
>>> va = VectorAssembler(inputCols=["tf_idf"],outputCol="features")

//Define pipeline
>>> tfidf_pipeline = Pipeline(stages=[tok,tf,idf,va])
>>> tfidf_pipeline.getStages()
[Tokenizer_4f5fbfb6c2a9cf5725d6, HashingTF_4088a47d38e72b70464f, IDF_41ddb3891541821c6613, VectorAssembler_49ae83b800679ac2fa0e]
>>>

//Now execute the pipeline
>>> result = tfidf_pipeline.fit(df2).transform(df2).select("words","features").collect()
>>> [(x[0],x[1]) for x in result]
[([u'here', u'is', u'some', u'text', u'to', u'illustrate', u'pipeline'], SparseVector(1000, {135: 0.4055, 169: 0.4055, 281: 0.4055, 388: 0.4055, 400: 0.4055, 603: 0.4055, 937: 0.4055})), ([u'and', u'tfidf,', u'which', u'stands', u'for', u'term', u'frequency', u'inverse', u'document', u'frequency'], SparseVector(1000, {36: 0.4055, 188: 0.4055, 333: 0.4055, 378: 0.4055, 538: 0.4055, 597: 0.4055, 727: 0.4055, 820: 0.4055, 960: 0.8109}))]
>>> 
```

该示例创建并执行了一个多阶段管道，该管道将文本转换为可由机器学习算法处理的特征向量。在继续之前，让我们再看一些特性。

**斯卡拉:**

```scala
scala> import org.apache.spark.ml.feature._
import org.apache.spark.ml.feature._
scala>

//Basic examples illustrating features usage
//Look at model examples for more feature examples
//Binarizer converts continuous value variable to two discrete values based on given threshold
scala> import scala.util.Random
import scala.util.Random
scala> val nums = Seq.fill(10)(Random.nextDouble*100)
...
scala> val numdf = spark.createDataFrame(nums.map(Tuple1.apply)).toDF("raw_nums")
numdf: org.apache.spark.sql.DataFrame = [raw_nums: double]
scala> val binarizer = new Binarizer().setInputCol("raw_nums").
            setOutputCol("binary_vals").setThreshold(50.0)
binarizer: org.apache.spark.ml.feature.Binarizer = binarizer_538e392f56db
scala> binarizer.transform(numdf).select("raw_nums","binary_vals").show(2)
+------------------+-----------+
|          raw_nums|binary_vals|
+------------------+-----------+
|55.209245003482884|        1.0|
| 33.46202184060426|        0.0|
+------------------+-----------+
scala>

//Bucketizer to convert continuous value variables to desired set of discrete values
scala> val split_vals:Array[Double] = Array(0,20,50,80,100) //define intervals
split_vals: Array[Double] = Array(0.0, 20.0, 50.0, 80.0, 100.0)
scala> val b = new Bucketizer().
           setInputCol("raw_nums").
           setOutputCol("binned_nums").
           setSplits(split_vals)
b: org.apache.spark.ml.feature.Bucketizer = bucketizer_a4dd599e5977
scala> b.transform(numdf).select("raw_nums","binned_nums").show(2)
+------------------+-----------+
|          raw_nums|binned_nums|
+------------------+-----------+
|55.209245003482884|        2.0|
| 33.46202184060426|        1.0|
+------------------+-----------+
scala>

//Bucketizer is effectively equal to binarizer if only two intervals are
given 
scala> new Bucketizer().setInputCol("raw_nums").
        setOutputCol("binned_nums").setSplits(Array(0,50.0,100.0)).
        transform(numdf).select("raw_nums","binned_nums").show(2)
+------------------+-----------+
|          raw_nums|binned_nums|
+------------------+-----------+
|55.209245003482884|        1.0|
| 33.46202184060426|        0.0|
+------------------+-----------+
scala>
```

**蟒蛇:**

```scala
//Some more features
>>> from pyspark.ml import feature, pipeline
>>> 

//Basic examples illustrating features usage
//Look at model examples for more examples
//Binarizer converts continuous value variable to two discrete values based on given threshold
>>> import random
>>> nums = [random.random()*100 for x in range(1,11)]
>>> numdf = sqlContext.createDataFrame(
             [[x] for x in nums]).toDF("raw_nums")
>>> binarizer = feature.Binarizer(threshold= 50,
       inputCol="raw_nums", outputCol="binary_vals")
>>> binarizer.transform(numdf).select("raw_nums","binary_vals").show(2)
+------------------+-----------+
|          raw_nums|binary_vals|
+------------------+-----------+
| 95.41304359504672|        1.0|
|41.906045589243405|        0.0|
+------------------+-----------+
>>> 

//Bucketizer to convert continuous value variables to desired set of discrete values
>>> split_vals = [0,20,50,80,100] //define intervals
>>> b =
feature.Bucketizer(inputCol="raw_nums",outputCol="binned_nums",splits=split
vals)
>>> b.transform(numdf).select("raw_nums","binned_nums").show(2)
+------------------+-----------+
|          raw_nums|binned_nums|
+------------------+-----------+
| 95.41304359504672|        3.0|
|41.906045589243405|        1.0|
+------------------+-----------+

//Bucketizer is effectively equal to binarizer if only two intervals are
given 
>>> feature.Bucketizer(inputCol="raw_nums",outputCol="binned_nums",                  
                       splits=[0,50.0,100.0]).transform(numdf).select(
                       "raw_nums","binned_nums").show(2)
+------------------+-----------+
|          raw_nums|binned_nums|
+------------------+-----------+
| 95.41304359504672|        1.0|
|41.906045589243405|        0.0|
+------------------+-----------+
>>> 
```

# 机器学习入门

在本书的前几节中，我们学习了响应/结果变量如何与预测变量相关，通常是在监督学习环境中。如今人们使用的这两类变量有各种不同的名称。让我们看看它们的一些同义术语，我们将在本书中交替使用它们:

*   **输入变量(X)** :特征、预测因子、解释变量、自变量
*   **输出变量(Y)** :响应变量，因变量

如果 *Y* 和 *X* 之间存在关系，其中 *X=X <sub>1</sub> ，X <sub>2</sub> ，X <sub>3</sub> ，...，X <sub>n</sub>* (n 个不同的预测因子)那么可以写成如下:

![Introduction to machine learning](graphics/image_06_004.jpg)

这里![Introduction to machine learning](graphics/image_06_005.jpg)是表示 *X* 如何描述 *Y* 的函数，未知！这是我们利用手头观察到的数据点得出的结论。术语

![Introduction to machine learning](graphics/image_06_006.jpg)

是均值为零的随机误差项，与 *X* 无关。

与这种方程相关的误差基本上有两种类型——可约误差和不可约误差。顾名思义，可还原的错误与函数相关联，可以通过提高

![Introduction to machine learning](graphics/image_06_007.jpg)

通过使用更好的学习算法或通过调整相同的算法。既然 *Y* 也是函数

![Introduction to machine learning](graphics/image_06_008.jpg)

，它独立于 *X* ，仍然会有一些无法解决的相关错误。这被称为不可约误差(

![Introduction to machine learning](graphics/image_06_009.jpg)

).总有一些因素会影响结果变量，但在建立模型时没有考虑到(因为它们在大多数情况下是未知的)，并导致不可约的误差项。因此，我们在本书中讨论的方法将只集中在最小化可约误差上。

我们建立的大多数机器学习模型可以用于预测或推理，或者两者的结合。对于某些算法，函数

![Introduction to machine learning](graphics/image_06_010.jpg)

可以表示为一个方程，告诉我们因变量 *Y* 与自变量( *X1* 、 *X2* ，...， *Xn* )。在这种情况下，我们既可以做推断，也可以做预测。但是有些算法是黑箱，我们只能预测，不能推断，因为 *Y* 和 *X* 是如何相关的，不得而知。

请注意，线性机器学习模型更适合于推理设置，因为它们对业务用户来说更容易解释。然而，在预测环境中，可以有更好的算法提供更准确的预测，但它们不太容易解释。当以推理为目标时，我们应该更喜欢线性回归等限制性模型，以获得更好的可解释性，而当仅以预测为目标时，我们可能会选择使用可解释性更低、准确性更高的高度灵活的模型，如**支持向量机**(**【SVM】**)(然而，这可能并非在所有情况下都成立)。考虑到可解释性和准确性之间的权衡，您需要根据业务需求谨慎选择算法。让我们深入了解这些概念背后的基本原理。

基本上，我们需要一组数据点(训练数据)来构建一个模型进行估计

![Introduction to machine learning](graphics/image_06_011.jpg)

*(X)* 使 *Y =*

![Introduction to machine learning](graphics/image_06_012.jpg)

*(X)* 。概括地说，这种学习方法可以是参数的或非参数的。

## 参数方法

参数化方法遵循两个步骤。在第一步中，假设形状为

![Parametric methods](graphics/image_06_013.jpg)

*()* 。例如 *X* 与 *Y* 线性相关，那么 *X、*的作用是

![Parametric methods](graphics/image_06_014.jpg)

*(X)*可以用如下所示的线性方程表示:

![Parametric methods](graphics/Beta1.jpg)

模型选定后，第二步是估算参数*0*、*1*，...，*通过使用手头的数据点来训练模型，从而:*

![Parametric methods](graphics/Beta-2.jpg)

这种参数方法的一个缺点是，我们对![Parametric methods](graphics/image_06_016.jpg) *()* 的线性假设在现实生活中可能不成立。

## 非参数方法

我们不对 *Y* 和 *X* 之间的线性关系以及变量的数据分布做出任何假设，因此

![Non-parametric methods](graphics/image_06_017.jpg)

*()* 为非参数。因为它不采用任何形式的

![Non-parametric methods](graphics/image_06_018.jpg)

*()* ，通过与数据点的良好拟合，可以产生更好的结果，这可能是一个优势。

因此，与参数方法相比，非参数方法需要更多的数据点来估计

![Non-parametric methods](graphics/image_06_019.jpg)

*()* 准确来说。但是请注意，如果处理不当，可能会导致过拟合问题。随着我们的深入，我们将对此进行更多的讨论。

# 回归方法

回归方法是监督学习的一种。如果响应变量是定量/连续的(采用数字值，如年龄、工资、身高等)，那么无论解释变量的类型如何，问题都可以称为回归问题。有各种各样的建模技术来解决回归问题。在这一节中，我们将重点讨论线性回归技术及其一些不同的变体。

回归方法可以用来预测任何真正有价值的结果。以下是几个例子:

*   根据员工的教育水平、工作地点、工作类型等预测其工资
*   预测股票价格
*   预测客户的购买潜力
*   预测机器发生故障前需要的时间

## 线性回归

继我们在上一节*中讨论的参数方法*之后，在假设线性之后

![Linear regression](graphics/image_06_020.jpg)

*(X)* ，我们需要训练数据来拟合描述解释变量(表示为 *X* )和响应变量(表示为 *Y* 之间关系的模型。当只存在一个解释变量时，称为简单线性回归，当存在多个解释变量时，称为多元线性回归。简单的线性回归就是在二维环境中拟合一条直线，当有两个预测变量时，它会在三维环境中拟合一个平面，当有两个以上的变量时，它会在高维环境中拟合一个平面。

线性回归方程的通常形式可以表示为:

y ′=

![Linear regression](graphics/image_06_021.jpg)

(十)+

![Linear regression](graphics/image_06_022.jpg)

这里*Y’*代表预测的结果变量。

只有一个预测变量的线性回归方程可以表示为:

![Linear regression](graphics/Beta11.jpg)

具有多个预测变量的线性回归方程可以表示为:

![Linear regression](graphics/Beta22.jpg)

这里![Linear regression](graphics/image_06_025.jpg)是独立于 *X* 的不可约误差项，平均值为零。我们无法控制它，但我们可以努力优化

![Linear regression](graphics/image_06_026.jpg)

*(X)* 。由于没有一个模型能够达到 100%的精度，因此由于不可约误差分量(

![Linear regression](graphics/image_06_027.jpg)

).

拟合线性回归最常见的方法叫做**最小二乘**，也称为**普通最小二乘** ( **OLS** )方法。该方法通过最小化从每个数据点到回归线的垂直偏差的平方和来找到最适合观测数据点的回归线。为了更好地理解线性回归是如何工作的，现在让我们来看一个下面形式的简单线性回归:

![Linear regression](graphics/Beta33.jpg)

其中，*0*为回归线的 Y 截距，*1*定义直线的斜率。意思是*1*是 *Y* 每变化一个单位 *X* 的平均变化。我们以 *X* 和 *Y* 为例:

<colgroup><col> <col></colgroup> 
| **X** | **Y** |
| **1** | Twelve |
| **2** | Twenty |
| **3** | Thirteen |
| **4** | Thirty-eight |
| **5** | Twenty-seven |

如果我们通过上表中所示的数据点拟合一条线性回归线，那么它将如下所示:

![Linear regression](graphics/image_06_028.jpg)

上图红色竖线表示预测误差，可定义为实际 *Y* 值与预测 *Y'* 值之差。如果对这些差异进行平方并求和，则称之为**平方误差之和** ( **SSE** )，这是用于寻找最佳拟合线的最常见的度量。下表显示了如何计算上证综指:

<colgroup><col> <col> <col> <col> <col></colgroup> 
| **X** | **Y** | **Y’** | **Y-Y’** | **(Y-Y’)2** |
| **1** | Twelve | Twelve point four | Zero point four | Zero point one six |
| **2** | Twenty | Seventeen point two | Two point eight | Seven point eight four |
| **3** | Thirteen | Twenty-two | -9 | Eighty-one |
| **4** | Thirty-eight | Twenty-six point eight | Eleven point two | One hundred and twenty-five point four four |
| **5** | Twenty-seven | Thirty-one point six | -4.6 | Twenty-one point one six |
|  |  |  | 总和 | Two hundred and thirty-five point six |

在上表中，术语**(Y-Y’)**称为残差。**残差平方和** ( **RSS** )可以表示为:

*RSS =残 <sub>1</sub> <sup>2</sup> +残<sub>2</sub>T7】2+残 <sub>3</sub> <sup>2</sup> +......+残 <sub>n</sub> <sup>2</sup>*

请注意，回归非常容易受到异常值的影响，如果在应用回归之前不进行处理，可能会引入巨大的 RSS 错误。

将回归线拟合到观察到的数据点后，您应该通过将残差绘制在 Y 轴上来检查残差，而不是解释 X 轴上的变量。如果曲线几乎是一条直线，那么你关于线性关系的假设是有效的，否则它可能表明某种非线性关系的存在。在存在非线性关系的情况下，您可能必须考虑非线性。其中一种方法是在方程中加入高阶多项式。

我们看到 RSS 是拟合回归线的一个重要特征(在建立模型时)。现在，要评估你的回归拟合有多好(一旦模型建立)，你需要另外两个统计量- **残差标准误差** ( **RSE** )和**R<sup>2</sup>T7】统计量。**

我们讨论了不可约误差分量*，因此回归总会有一定程度的误差(即使你的方程完全符合你的数据点并且你已经正确估计了系数)。RSE 是*标准偏差的估计值，可定义如下:**

 **![Linear regression](graphics/image_06_029.jpg)

这意味着实际值平均偏离真实回归线一个 RSE 因子。

由于 RSE 实际上是以 *Y* 为单位进行测量的(参考上一节我们是如何计算 RSS 的)，所以很难说它是模型精度的唯一最佳统计量。

因此，引入了一种替代方法，称为 R <sup>2</sup> 统计量(也称为确定系数)。计算 R <sup>2</sup> 的公式如下:

![Linear regression](graphics/image_06_030.jpg)

**平方和** ( **TSS** )可计算为:

![Linear regression](graphics/image_06_031.jpg)

请注意，即使在执行回归预测 *Y* 之前，TSS 也会测量 *Y* 中固有的总方差。注意里面没有 *Y'* 。相反，RSS 代表 *Y* 中回归后无法解释的变异性。这意味着( *TSS - RSS* )能够解释执行回归后响应的可变性。

*R <sup>2</sup>* 统计通常在 0 到 1 的范围内，但是如果拟合比仅拟合一条水平线更差，则可以是负的，但这种情况很少发生。接近 1 的值表明回归方程可以解释响应变量中的大部分可变性，并且是很好的拟合。相反，接近 0 的值表示回归没有解释响应变量中的大部分方差，并且不是很好的拟合。举例来说，0.25 的 *R <sup>2</sup>* 意味着 *Y* 中 25%的方差由 *X* 来解释，并表示要调整模型进行改进。

现在让我们讨论如何通过回归解决数据集中的非线性问题。如前所述，当您发现非线性关系时，需要正确处理。要使用相同的线性回归技术对非线性方程建模，您必须创建更高阶的特征，这些特征将被回归技术视为另一个变量。例如，如果*工资*是预测*购买潜力*的特征/变量，并且我们发现它们之间存在非线性关系，那么我们可能会根据需要解决的非线性程度创建一个名为(*工资 3* 的特征。请注意，在创建此类高阶要素的同时，还必须保留基本要素。在这个例子中，你必须在回归方程中同时使用(*薪水*)和(*薪水 3* )。

到目前为止，我们假设所有的预测变量都是连续的。如果有明确的预测者呢？在这种情况下，我们必须对这些变量进行虚拟编码(比如男性为 1，女性为 0)，以便回归技术生成两个方程，一个用于性别=男性(该方程将有性别变量)，另一个用于性别=女性(该方程将没有性别变量，因为它将被丢弃为编码 0)。有时，在分类变量很少的情况下，根据分类变量的级别划分数据集并为它们构建单独的模型可能是一个好主意。

最小二乘线性回归的一个主要优点是它解释了结果变量与预测变量之间的关系。这使得它非常容易解释，可以用来推断和做预测。

### 损失函数

许多机器学习问题可以表述为凸优化问题。这个问题的目的是找到平方损失最小的系数的值。这个目标函数基本上有两个组成部分——正则化函数和损失函数。正则化器用于控制模型的复杂性(因此不会过度拟合)，损失函数用于估计平方损失最小的回归函数的系数。

用于最小二乘的损失函数称为**平方损失**，如下图所示:

![Loss function](graphics/image_06_032.jpg)

这里 *Y* 是响应变量(实值) *W* 是权重向量(系数的值) *X* 是特征向量。因此

![Loss function](graphics/Capture-1.jpg)

给出预测值，我们将其与实际值 *Y* 等同，以找到需要最小化的平方损失。

用于估计系数的算法称为**梯度下降**。对于不同类型的机器学习算法，有不同类型的损失函数和优化算法，我们将在需要时进行介绍。

### 优化

最终，线性方法必须优化损失函数。在引擎盖下，线性方法使用凸优化方法来优化目标函数。MLlib 开箱支持**随机梯度下降** ( **SGD** )和**有限记忆-布赖登-弗莱彻-戈德法布-尚诺** ( **L-BFGS** )。目前大多数算法 API 支持 SGD，少数支持 L-BFGS。

SGD 是一种一阶优化技术，最适合大规模数据和分布式计算环境。目标函数(损失函数)被写成和的最优化问题最适合用 SGD 来解决。

L-BFGS 算法是拟牛顿法族中求解优化问题的一种优化算法。与其他一阶优化技术(如 SGD)相比，L-BFGS 算法往往能更快地收敛。

MLlib 中可用的一些线性方法支持 SGD 和 L-BFGS。你应该根据所考虑的目标函数选择一个。一般来说，推荐使用 BFGS 滤波器，因为它收敛速度更快，但你需要根据要求仔细评估。

## 回归的正则化

权重(系数值)越大，模型越容易过度拟合。正则化是一种主要用于通过控制模型的复杂性来消除过拟合问题的技术。当您在训练数据和测试数据上看到模型性能之间的差异时，通常会这样做。如果训练性能大于测试数据，则可能是过度拟合的情况(高方差情况)。

为了解决这个问题，引入了一种正则化技术来惩罚损失函数。总是建议使用任何正则化技术，尤其是当训练数据具有少量观察值时。

在我们进一步讨论正则化技术之前，我们必须了解*偏差*和*方差*在监督学习环境中意味着什么，以及为什么总是存在相关的权衡。虽然两者都与误差有关，但是*偏差*模型意味着它偏向于一些错误的假设，并且可能在某种程度上错过预测变量和响应变量之间的关系。这是装配不足的情况！另一方面，*高方差*模型意味着它试图接触每个数据点，并最终对数据集中存在的随机噪声进行建模。它代表了过度拟合的情况。

带有 L2 惩罚(L2 正则化)的线性回归被称为**岭回归**，带有 L1 惩罚(L1 正则化)的线性回归被称为**套索回归**。当 L1 和 L2 的点球同时使用时，称为**弹性净回归**。我们将在下一节逐一讨论它们。

由于平滑性，L2 正则化问题通常比 L1 正则化问题更容易解决，但是 L1 正则化问题会导致权重稀疏，从而导致模型更小且更易于解释。因此，套索有时用于特征选择。

### 岭回归

当我们将 L2 罚函数(也称为**收缩罚函数**)加到最小二乘的损失函数中时，它就变成了岭回归，如下图所示:

![Ridge regression](graphics/image_06_034.jpg)

这里 *λ* (大于 0)是单独确定的调谐参数。上式中的第二项称为收缩罚分，只有当系数(*0*、*1*时，该罚分才会很小...等等)很小并且接近 0。当 *λ = 0* 时，岭回归变为最小二乘。随着λ趋近于无穷大，回归系数趋近于零(但永远不为零)。

岭回归为 *λ* 的每个值生成不同组的系数值。因此，需要使用交叉验证仔细选择 lambda 值。当我们增加λ值时，回归线的灵活性会降低，从而降低方差并增加偏差。

请注意，收缩惩罚适用于除截距项*0*之外的所有解释变量。

当训练数据较少时，或者甚至在预测器或特征的数量多于观测值的情况下，岭回归确实工作得很好。此外，脊线所需的计算几乎与最小二乘法相同。

由于 ridge 不会将任何系数值减少到零，因此所有变量都将出现在模型中，如果变量数量较多，这可能会降低模型的可解释性。

### 套索回归

拉索是在山脊之后引入的。当我们将 L1 惩罚加到最小二乘的损失函数中时，它就变成了套索回归，如下所示:

![Lasso regression](graphics/image_06_035.jpg)

不同的是，它不是取系数的平方，而是取系数的模。与 ridge 不同，它可以强制它的一些系数恰好为零，这可以消除一些变量。所以，套索也可以用于变量选择！

Lasso 为每个λ值生成不同的系数值集。因此，需要使用交叉验证仔细选择 lambda 值。像 ridge 一样，随着λ的增加，方差减小，偏差增大。

与 ridge 相比，Lasso 产生了更好的可解释模型，因为它通常包含变量总数的子集。当有许多分类变量时，最好选择套索而不是脊。

在现实中，无论是山脊还是套索，都不会比其他的更好。Lasso 通常在少量具有显著系数的预测变量上表现良好，其余的具有非常小的系数。当有许多预测因子，并且几乎所有预测因子都具有实质上相似的系数大小时，Ridge 通常表现得更好。

岭有利于分组选择，也可以解决多重共线性问题。另一方面，套索不能进行分组选择，倾向于只选择其中一个预测因子。此外，如果一组预测因子之间高度相关，拉索倾向于只选择其中一个，而将其他的收缩为零。

### 弹性净回归

当我们将 L1 和 L2 惩罚加到最小二乘的损失函数中时，它变成弹性净回归，如下所示:

![Elastic net regression](graphics/image_06_036.jpg)

以下是弹性净回归的优点:

*   强制稀疏性并帮助移除最无效的变量
*   鼓励分组效应
*   结合了脊和套索的优点

弹性网回归的朴素版本会导致双重收缩问题，这会导致偏差增加和预测精度降低。为了解决这个问题，一种方法可以是通过将( *1+ λ2* )乘以估计系数来重新调整估计系数:

**斯卡拉**

```scala
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.regression.LinearRegressionModel
import org.apache.spark.mllib.regression.LinearRegressionWithSGD
scala> import org.apache.spark.ml.regression.{LinearRegression,LinearRegressionModel}
import org.apache.spark.ml.regression.{LinearRegression,LinearRegressionModel}
// Load the data
scala> val data = spark.read.format("libsvm").load("data/mllib/sample_linear_regression_data.txt")
data: org.apache.spark.sql.DataFrame = [label: double, features: vector]

// Build the model
scala> val lrModel = new LinearRegression().fit(data)

//Note: You can change ElasticNetParam, MaxIter and RegParam
// Defaults are 0.0, 100 and 0.0
lrModel: org.apache.spark.ml.regression.LinearRegressionModel = linReg_aa788bcebc42

//Check Root Mean Squared Error
scala> println("Root Mean Squared Error = " + lrModel.summary.rootMeanSquaredError)
Root Mean Squared Error = 10.16309157133015
```

**蟒蛇**:

```scala
>>> from pyspark.ml.regression import LinearRegression, LinearRegressionModel
>>>

// Load the data
>>> data = spark.read.format("libsvm").load("data/mllib/sample_linear_regression_data.txt")
>>> 

// Build the model
>>> lrModel = LinearRegression().fit(data)

//Note: You can change ElasticNetParam, MaxIter and RegParam
// Defaults are 0.0, 100 and 0.0
//Check Root Mean Squared Error
>>> print "Root Mean Squared Error = ", lrModel.summary.rootMeanSquaredError
Root Mean Squared Error = 10.16309157133015
>>> 
```

# 分类方法

如果响应变量是定性/分类的(采用分类值，如性别、贷款违约、婚姻状况等)，那么无论解释变量的类型如何，问题都可以被称为分类问题。有各种类型的分类方法，但我们将在这一节集中讨论逻辑回归和支持向量机。

以下是分类方法的一些含义的几个例子:

*   顾客购买或不购买产品
*   一个人是否患有糖尿病

*   申请贷款的个人是否会违约
*   一个电子邮件接收者会看电子邮件或者不看

## 逻辑回归

逻辑回归测量解释变量和分类反应变量之间的关系。我们不对分类反应变量使用线性回归，因为反应变量不是连续的，因此误差项不是正态分布的。

所以逻辑回归是一种分类算法。逻辑回归不是直接对响应变量 *Y* 建模，而是对*P(Y*|*X)**Y*属于特定类别的概率分布建模。( *Y* | *X* )的条件分布是伯努利分布而不是高斯分布。逻辑回归方程可以表示如下:

![Logistic regression](graphics/image_06_037.jpg)

对于两类分类，模型的输出应该只限于两类中的一类(比如 0 或 1)。由于逻辑回归预测概率，而不是直接预测类别，因此我们使用逻辑函数(也称为， *sigmoid 函数*)将输出限制为单个类别:

![Logistic regression](graphics/image_06_038.jpg)

求解前面的方程，我们得到如下结果:

![Logistic regression](graphics/Capture-2.jpg)

它可以进一步简化为:

![Logistic regression](graphics/image_06_040.jpg)

左边的数量 *P(X)/1-P(X)* 称为*赔率*。赔率的值从 0 到无穷大不等。接近 0 的值表示概率非常小，数字大的值表示概率高。根据情况，有时会直接使用赔率而不是概率。

如果我们取概率的对数，它就变成了对数奇数或对数，可以如下所示:

![Logistic regression](graphics/image_06_041.jpg)

从上式可以看出，logit 与 *X* 线性相关。

在有两个类 1 和 0 的情况下，当 *p < 0.5* 时，如果 *p > = 0.5* 和 *Y = 0* ，那么我们预测 *Y = 1* 。所以逻辑回归实际上是一个线性分类器，决策边界在 *p = 0.5* 。可能会有这样的商业案例:默认情况下 *p* 没有设置为 0.5，您可能需要使用一些数学技术计算出正确的值。

一种称为最大似然的方法被用于通过计算回归系数来拟合模型，并且该算法可以是类似于线性回归设置中的梯度下降。

在逻辑回归中，损失函数应该解决误分类率。因此，用于逻辑回归的损失函数称为*逻辑损失*，如下图所示:

![Logistic regression](graphics/image_06_042.jpg)

### 注

请注意，当您使用高阶多项式来更好地拟合模型时，逻辑回归也容易过度拟合。要解决这个问题，可以像线性回归一样使用正则化项。截至本文撰写之时，Spark 不支持正则化逻辑回归，因此我们将暂时跳过这一部分。

# 线性支持向量机(SVM)

**支持向量机** ( **SVM** )是一种有监督的机器学习算法，可用于分类和回归。然而，它在解决分类问题时更受欢迎，由于 Spark 提供它作为 SVM 分类器，我们将只讨论分类设置。当用作分类器时，与逻辑回归不同，它是非概率分类器。

SVM 从一个简单的分类器进化而来，称为**最大边际分类器**。由于最大边缘分类器要求类可以通过线性边界分离，因此它不能应用于许多数据集。因此，它被扩展到一个被称为**支持向量分类器**的改进版本，可以解决类重叠和类之间没有明确分离的情况。支持向量分类器被进一步扩展到我们所说的 SVM，以适应非线性的类边界。让我们一步一步地讨论 SVM 的演变，以便我们清楚地了解它是如何工作的。

如果一个数据集中有 *p* 个维度(特征)，那么我们在该 p 维空间中拟合一个超平面，其方程可以定义如下:

![Linear Support Vector Machines (SVM)](graphics/image_06_043.jpg)

这个超平面被称为形成决策边界的分离超平面。将根据结果对结果进行分类；如果大于 0，则在一侧，如果小于 0，则在另一侧，如下图所示:

![Linear Support Vector Machines (SVM)](graphics/image_06_044.jpg)

在上图中观察到可以有多个超平面(它们可以是无限的)。应该有一个合理的方法来选择最佳超平面。这是我们选择最大边缘超平面的地方。如果计算所有数据点到分离超平面的垂直距离，那么最小的距离将被称为边距。因此，对于最大边缘分类器，超平面应该具有最高的边缘。

距离分离超平面较近但等距的训练观测值称为支持向量。对于支持向量的任何微小变化，超平面也会重新定向。这些支持向量实际上定义了边际。现在，如果考虑中的两个类是不可分的呢？我们可能想要一个分类器，它不能完美地将两个类分开，并且有一个更软的边界，允许一定程度的错误分类。这一要求导致了支持向量分类器(也称为软边界分类器)的引入。

从数学上讲，是等式中的松弛变量导致了错误分类。此外，在支持向量分类器中有一个调整参数，应该使用交叉€”验证来选择。这个调优参数是在偏差和方差之间进行权衡的参数，应该小心处理。当它较大时，裕度较宽，包括许多支持向量，并且具有低方差和高偏差。如果它很小，那么边缘将具有较少的支持向量，并且分类器将具有低偏差但高方差。

SVM 的损失函数可以表示如下:

![Linear Support Vector Machines (SVM)](graphics/image_06_045.jpg)

截至本文撰写之时，Spark 仅支持线性支持向量机。默认情况下，线性支持向量机是用 L2 正则化训练的。Spark 还支持替代的 L1 正则化。

目前为止还不错！但是，当类之间存在非线性边界时，支持向量分类器将如何工作，如下图所示:

![Linear Support Vector Machines (SVM)](graphics/image_06_046.jpg)

任何线性分类器，如支持向量分类器，在前面的情况下表现都很差。如果它在数据点之间画一条直线，那么类就不会被正确地分开。这是一个非线性类边界的例子。这个问题的一个解决方案是 SVM。换句话说，当支持向量分类器与非线性核融合时，它就变成了 SVM。

类似于我们在回归方程中引入高阶多项式项来说明非线性的方式，在 SVM 背景下也可以做一些事情。SVM 使用一种叫做核的东西来处理数据集中不同种类的非线性；不同非线性的不同内核。内核方法将数据映射到更高维的空间，因为如果这样做，数据可能会被很好地分离。此外，它使区分不同的类变得更加容易。让我们讨论几个重要的内核，以便能够选择正确的一个。

## 线性核

这是最基本的内核类型之一，允许我们只挑选线条或超平面。它相当于一个支持向量分类器。如果数据集中存在非线性，则无法解决。

## 多项式核

这允许我们解决多项式阶的某种程度的非线性。当训练数据正常化时，这种方法效果很好。这个核通常有更多的超参数，因此增加了模型的复杂性。

## 径向基函数核

当你不确定使用哪个内核时，**径向基函数** ( **径向基函数**)可以是一个不错的默认选择。它允许你挑选出均匀的圆或超球面。虽然这通常比线性或多项式核性能更好，但当特征数量巨大时，它的性能并不好。

## 乙状结肠核

sigmoid 内核源于神经网络。所以，一个具有 sigmoid 核的 SVM 相当于一个具有两层感知器的神经网络。

# 训练一个 SVM

在训练 SVM 时，建模者必须做出许多决定:

*   如何对数据进行预处理(转换和缩放)。分类变量应该通过实体模型化来转换成数字变量。此外，还需要缩放数值(0 到 1 或-1 到+1)。
*   使用哪个内核(如果您无法可视化数据和/或得出结论，请使用交叉€验证进行检查)。
*   为 SVM 设置什么参数:惩罚参数和内核参数(使用交叉€“验证或网格搜索找到)

如果需要，可以使用基于熵的特征选择来仅包括模型中的重要特征。

**Scala** :

```scala
scala> import org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}
import org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}
scala> import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
scala> import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.mllib.util.MLUtils
scala>

// Load training data in LIBSVM format.
scala> val data = MLUtils.loadLibSVMFile(sc, "data/mllib/sample_libsvm_data.txt")
data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[6] at map at MLUtils.scala:84
scala>

// Split data into training (60%) and test (40%).
scala> val splits = data.randomSplit(Array(0.6, 0.4), seed = 11L)
splits: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] = Array(MapPartitionsRDD[7] at randomSplit at <console>:29, MapPartitionsRDD[8] at randomSplit at <console>:29)
scala> val training = splits(0).cache()
training: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[7] at randomSplit at <console>:29
scala> val test = splits(1)
test: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[8] at randomSplit at <console>:29
scala>

// Run training algorithm to build the model
scala> val model = SVMWithSGD.train(training, numIterations=100)
model: org.apache.spark.mllib.classification.SVMModel = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 692, numClasses = 2, threshold = 0.0
scala>

// Clear the default threshold.
scala> model.clearThreshold()
res1: model.type = org.apache.spark.mllib.classification.SVMModel: intercept =
0.0, numFeatures = 692, numClasses = 2, threshold = None
scala>

// Compute raw scores on the test set.
scala> val scoreAndLabels = test.map { point =>
       val score = model.predict(point.features)
      (score, point.label)
      }
scoreAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] =
MapPartitionsRDD[213] at map at <console>:37
scala>

// Get evaluation metrics.
scala> val metrics = new BinaryClassificationMetrics(scoreAndLabels)
metrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@3106aebb
scala> println("Area under ROC = " + metrics.areaUnderROC())
Area under ROC = 1.0
scala>
```

### 注

`mllib`已经进入维护模式，SVM 在 ml 下仍然不可用，因此仅提供 Scala 代码进行说明。

# 决策树

决策树是一种非参数监督学习算法，可用于分类和回归。决策树就像倒排树，根节点在顶部，叶节点向下。有不同的算法将数据集分割成类似分支的段。每个叶节点被分配给一个代表最合适的目标值的类。

决策树不需要对数据集进行任何缩放或转换，并且像数据一样工作。它们可以处理分类特征和连续特征，还可以解决数据集中的非线性问题。决策树的核心是一种贪婪算法(它考虑当前的最佳分割，不考虑未来的场景)，对特征空间进行递归二进制划分。拆分是基于每个节点的信息增益来完成的，因为信息增益衡量给定属性按照目标类或值将训练示例分隔得有多好。第一个分裂发生在产生最大信息增益并成为根节点的特征上。

一个节点的信息增益是父节点杂质和两个子节点杂质的加权和之差。为了估计信息增益，Spark 目前有两个用于分类问题的杂质度量和一个用于回归的杂质度量，如下所述。

## 杂质测量

杂质是同质性的度量，也是递归划分的最佳标准。通过计算杂质，确定最佳分裂候选。大多数杂质测量是基于概率的:

*一个类别的概率=该类别的观测数/观测总数*

让我们花一些时间在 Spark 支持的不同类型的重要杂质测量上。

### 基尼指数

基尼指数主要针对数据集中的连续属性或特征。如果不是，它将假设所有的属性和特征都是连续的。分割使子节点比父节点更纯。基尼倾向于寻找最大的类别——得到最大观测值的反应变量类别。它可以定义如下:

![Gini Index](graphics/image_06_047.jpg)

如果一个响应的所有观测值都属于一个类，那么该类的概率 *j* ，即( *Pj* )将为 1，因为只有一个类， *(Pj)2* 也将为 1。这使得基尼指数为零。

### 熵

熵主要用于数据集中的分类属性或特征。它可以定义如下:

![Entropy](graphics/image_06_048.jpg)

如果一个响应的所有观测值都属于一个类，那么该类的概率( *Pj* )将为 1， *log(P)* 将为零。这使得熵为零。

下图描述了公平抛硬币的概率:

![Entropy](graphics/Capture-3.jpg)

只是为了解释前面的图表，如果你掷一个公平的硬币，头部或尾部的概率将是 0.5，所以在 0.5 的概率下会有最大的观察值。

如果数据样本是完全齐次的，那么熵将为零，如果样本可以等分为二，那么熵将为一。

计算速度比基尼系数慢一点，因为它也要计算日志。

### 方差

与基尼指数和熵不同，方差用于计算回归问题的信息增益。差异可以定义为:

![Variance](graphics/image_06_050.jpg)

## 停止规则

当满足以下条件之一时，递归树构造在节点处停止:

*   节点深度等于`maxDepth`训练参数
*   没有分裂候选导致信息增益大于`minInfoGain`
*   没有分裂候选产生子节点，每个子节点至少有一个`minInstancesPerNode`训练实例

## 分裂候选人

数据集通常混合了分类特征和连续特征。这些特性如何被进一步分割成分割候选项是我们应该理解的，因为我们有时需要对它们进行某种程度的控制来构建一个更好的模型。

### 分类特征

对于具有 *M* 个可能值(类别)的分类特征，可以提出*2(M-1)-1*个分割候选项。无论是二进制分类还是回归，通过平均标签对分类特征值进行排序，分裂候选数可以减少到*M-1*。

例如，考虑一个二元分类(0/1)问题，其中一个分类特征有三个类别 A、B 和 C，它们对应的 label-1 响应变量的比例分别为 0.2、0.6 和 0.4。在这种情况下，分类特征可以排序为 A、C、B。因此，两个拆分候选(*M-1*=*3-1*=*2*)可以是 *A | (C、B)* 和 *A、(C | B)* ，其中“*|”*表示拆分。

### 连续特征

对于连续特征变量，可能没有两个值是相同的(至少我们可以这样假设)。如果有 *n* 个观察值，那么 *n* 个分裂候选可能不是一个好主意，尤其是在大数据环境下。

在 Spark 中，这是通过对数据样本执行分位数计算，并相应地对数据进行宁滨运算来实现的。使用`maxBins`参数，您仍然可以控制您想要允许的最大箱数。`maxBins`的最大默认值为`32`。

## 决策树的优势

*   它们易于理解和解释，因此易于向业务用户解释
*   它们对分类和回归都有效
*   定性和定量数据都可以用于构建决策树

决策树中的信息增益偏向于具有更多层次的属性。

## 决策树的缺点

*   对于有效的连续结果变量，它们没有那么大的作用
*   当有许多类并且数据集很小时，性能很差
*   轴平行分割降低了精度
*   因为他们试图拟合几乎所有的数据点，所以他们遭受高方差

## 示例

实现-€“明智的分类和回归树之间没有重大区别。让我们看看它在 Spark 上的实际实现。

**斯卡拉:**

```scala
//Assuming ml.Pipeline and ml.features are already imported
scala> import org.apache.spark.ml.classification.{
        DecisionTreeClassifier, DecisionTreeClassificationModel}
import org.apache.spark.ml.classification.{DecisionTreeClassifier,
DecisionTreeClassificationModel}
scala>
/prepare train data
scala> val f:String = "<Your path>/simple_file1.csv"
f: String = <your path>/simple_file1.csv
scala> val trainDF = spark.read.options(Map("header"->"true",
            "inferSchema"->"true")).csv(f)
trainDF: org.apache.spark.sql.DataFrame = [Text: string, Label: int]

scala>

 //define DecisionTree pipeline
//StringIndexer maps labels(String or numeric) to label indices
//Maximum occurrence label becomes 0 and so on
scala> val lblIdx = new StringIndexer().
                setInputCol("Label").
                setOutputCol("indexedLabel")
lblIdx: org.apache.spark.ml.feature.StringIndexer = strIdx_3a7bc9c1ed0d
scala>

// Create labels list to decode predictions
scala> val labels = lblIdx.fit(trainDF).labels
labels: Array[String] = Array(2, 1, 3)
scala>

//Define Text column indexing stage
scala> val fIdx = new StringIndexer().
                setInputCol("Text").
              setOutputCol("indexedText")
fIdx: org.apache.spark.ml.feature.StringIndexer = strIdx_49253a83c717

// VectorAssembler
scala> val va = new VectorAssembler().
              setInputCols(Array("indexedText")).
              setOutputCol("features")
va: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_764720c39a85

//Define Decision Tree classifier. Set label and features vector
scala> val dt = new DecisionTreeClassifier().
            setLabelCol("indexedLabel").
            setFeaturesCol("features")
dt: org.apache.spark.ml.classification.DecisionTreeClassifier = dtc_84d87d778792

//Define label converter to convert prediction index back to string
scala> val lc = new IndexToString().
                setInputCol("prediction").
                setOutputCol("predictedLabel").
                setLabels(labels)
lc: org.apache.spark.ml.feature.IndexToString = idxToStr_e2f4fa023665
scala>

//String the stages together to form a pipeline
scala> val dt_pipeline = new Pipeline().setStages(
          Array(lblIdx,fIdx,va,dt,lc))
dt_pipeline: org.apache.spark.ml.Pipeline = pipeline_d4b0e884dcbf
scala>
//Apply pipeline to the train data
scala> val resultDF = dt_pipeline.fit(trainDF).transform(trainDF)

//Check results. Watch Label and predictedLabel column values match
resultDF: org.apache.spark.sql.DataFrame = [Text: string, Label: int ... 6 more
fields]
scala>
resultDF.select("Text","Label","features","prediction","predictedLabel").show()
+----+-----+--------+----------+--------------+
|Text|Label|features|prediction|predictedLabel|
+----+-----+--------+----------+--------------+
|   A|    1|   [1.0]|       1.0|             1|
|   B|    2|   [0.0]|       0.0|             2|
|   C|    3|   [2.0]|       2.0|             3|
|   A|    1|   [1.0]|       1.0|             1|
|   B|    2|   [0.0]|       0.0|             2|
+----+-----+--------+----------+--------------+
scala>

//Prepare evaluation data
scala> val eval:String = "€œ<Your path>/simple_file2.csv"
eval: String = <Your path>/simple_file2.csv
scala> val evalDF = spark.read.options(Map("header"->"true",
            "inferSchema"->"true")).csv(eval)
evalDF: org.apache.spark.sql.DataFrame = [Text: string, Label: int]
scala>

//Apply the same pipeline to the evaluation data
scala> val eval_resultDF = dt_pipeline.fit(evalDF).transform(evalDF)
eval_resultDF: org.apache.spark.sql.DataFrame = [Text: string, Label: int ... 7
more fields]

//Check evaluation results
scala>
eval_resultDF.select("Text","Label","features","prediction","predictedLabel").sh
w()
+----+-----+--------+----------+--------------+
|Text|Label|features|prediction|predictedLabel|
+----+-----+--------+----------+--------------+
|   A|    1|   [0.0]|       1.0|             1|
|   A|    1|   [0.0]|       1.0|             1|
|   A|    2|   [0.0]|       1.0|             1|
|   B|    2|   [1.0]|       0.0|             2|
|   C|    3|   [2.0]|       2.0|             3|
+----+-----+--------+----------+--------------+
//Note that predicted label for the third row is 1 as against Label(2) as
expected

Python:

//Model training example
>>> from pyspark.ml.pipeline import Pipeline
>>> from pyspark.ml.feature import StringIndexer, VectorIndexer, VectorAssembler,
IndexToString
>>> from pyspark.ml.classification import DecisionTreeClassifier,
DecisionTreeClassificationModel
>>> 

//prepare train data
>>> file_location = "../work/simple_file1.csv"
>>> trainDF = spark.read.csv(file_location,header=True,inferSchema=True)

 //Read file
>>>

//define DecisionTree pipeline
//StringIndexer maps labels(String or numeric) to label indices
//Maximum occurrence label becomes 0 and so on
>>> lblIdx = StringIndexer(inputCol = "Label",outputCol = "indexedLabel")

// Create labels list to decode predictions
>>> labels = lblIdx.fit(trainDF).labels
>>> labels
[u'2', u'1', u'3']
>>> 

//Define Text column indexing stage
>>> fidx = StringIndexer(inputCol="Text",outputCol="indexedText")

// Vector assembler
>>> va = VectorAssembler(inputCols=["indexedText"],outputCol="features")

//Define Decision Tree classifier. Set label and features vector
>>> dt = DecisionTreeClassifier(labelCol="indexedLabel",featuresCol="features")

//Define label converter to convert prediction index back to string
>>> lc = IndexToString(inputCol="prediction",outputCol="predictedLabel",
                       labels=labels)

//String the stages together to form a pipeline
>>> dt_pipeline = Pipeline(stages=[lblIdx,fidx,va,dt,lc])
>>>
>>> 

//Apply decision tree pipeline
>>> dtModel = dt_pipeline.fit(trainDF)
>>> dtDF = dtModel.transform(trainDF)
>>> dtDF.columns
['Text', 'Label', 'indexedLabel', 'indexedText', 'features', 'rawPrediction',
'probability', 'prediction', 'predictedLabel']
>>> dtDF.select("Text","Label","indexedLabel","prediction",
"predictedLabel").show()
+----+-----+------------+----------+--------------+
|Text|Label|indexedLabel|prediction|predictedLabel|
+----+-----+------------+----------+--------------+
|   A|    1|         1.0|       1.0|             1|
|   B|    2|         0.0|       0.0|             2|
|   C|    3|         2.0|       2.0|             3|
|   A|    1|         1.0|       1.0|             1|
|   B|    2|         0.0|       0.0|             2|
+----+-----+------------+----------+--------------+

>>>

>>> //prepare evaluation dataframe
>>> eval_file_path = "../work/simple_file2.csv"
>>> evalDF = spark.read.csv(eval_file_path,header=True, inferSchema=True) 

//Read eval file
>>> eval_resultDF = dt_pipeline.fit(evalDF).transform(evalDF)
>>> eval_resultDF.columns
['Text', 'Label', 'indexedLabel', 'indexedText', 'features', 'rawPrediction', 'probability', 'prediction', 'predictedLabel']
>>> eval_resultDF.select("Text","Label","indexedLabel","prediction",
"predictedLabel").show()
+----+-----+------------+----------+--------------+
|Text|Label|indexedLabel|prediction|predictedLabel|
+----+-----+------------+----------+--------------+
|   A|    1|         1.0|       1.0|             1|
|   A|    1|         1.0|       1.0|             1|
|   A|    2|         0.0|       1.0|             1|
|   B|    2|         0.0|       0.0|             2|
|   C|    3|         2.0|       2.0|             3|
+----+-----+------------+----------+--------------+
>>> 

Accompanying data files:
simple_file1.csv Text,Label
A,1
B,2
C,3
A,1
B,2simple_file2.csv Text,Label
A,1
A,1
A,2
B,2
C,3
```

# 在一起

顾名思义，集成方法使用多种学习算法来获得预测精度方面更精确的模型。通常这些技术需要更多的计算能力，并使模型更加复杂，这使得它难以解释。让我们讨论 Spark 上可用的各种类型的合奏技术。

## 随机森林

随机森林是决策树的集成技术。在我们到达随机森林之前，让我们看看它是如何进化的。我们知道决策树通常具有高方差问题，并且倾向于过度拟合模型。为了解决这个问题，引入了一个名为*打包*(也称为引导聚合)的概念。对于决策树，想法是从数据集中提取多个训练集(自举训练集)，并从中创建单独的决策树，然后对它们进行平均以得到回归树。对于分类树，我们可以从所有树中选择多数票或最常见的类别。这些树长得很深，根本没有修剪。这确实降低了方差，尽管单个树可能有很高的方差。

普通装袋方法的一个问题是，对于大多数自举训练集来说，强预测器位于顶部裂口处，这几乎使装袋的树看起来相似。这意味着预测看起来也是相似的，如果你平均它们，那么它不会将方差减少到预期的程度。为了解决这个问题，需要一种技术，该技术采用与袋装树相似的方法，但消除了树之间的相关性，因此产生了*随机森林*。

在这种方法中，您构建自举训练样本来创建决策树，但唯一的区别是，每次分裂发生时，从总共 K 个预测器中选择一个随机样本的 P 个预测器。这就是随机森林如何为这种方法注入随机性。作为经验法则，我们可以把 P 作为 q 的平方根。

就像在打包的情况下一样，在这种方法中，如果你的目标是回归，你也可以对预测进行平均，如果目标是分类，你可以获得多数票。Spark 提供了一些调整参数来调整该模型，如下所示:

*   `numTrees`:可以指定随机森林中要考虑的树木数量。如果数字很高，那么预测的方差会更小，但所需的时间会更多。
*   `maxDepth`:可以指定每棵树的最大深度。深度的增加使得树木在预测精度方面更加强大。尽管它们倾向于过度种植单棵树，但总体产量还是不错的，因为我们对结果进行了平均，从而减少了方差。

*   `subsamplingRate`:该参数主要用于加速训练。它用于设置自举训练样本大小。小于 1.0 的值会加快性能。
*   `featureSubsetStrategy`:这个参数也可以帮助加快执行速度。它用于设置用作每个节点的分割候选的要素数量。应该小心设置，因为过低或过高的值都会影响模型的准确性。

### 随机森林的优势

*   当并行执行时，它们运行得更快
*   它们不太容易过度适应
*   它们很容易调谐
*   与树或袋装树相比，预测精度更高
*   即使预测变量是分类和连续特征的混合，它们也能很好地工作，并且不需要缩放

## 梯度增强树

像随机森林一样，**梯度增强树** ( **GBTs** )也是一组树。它们可以应用于分类和回归问题。与袋装树或随机森林不同，其中树是在独立的数据集上并行构建的，并且相互独立，而 gbt 是按顺序构建的。每棵树都是使用先前生长的树的结果来生长的。请注意，GBTs 不适用于引导样本。

在每次迭代中，gbt 使用当前的集合来预测训练实例的标签，并将它们与真实标签进行比较并估计误差。预测精度差的训练实例被重新标记，以便决策树在下一次迭代中基于先前错误的错误率被纠正。

发现错误率和重新标记实例背后的机制基于损失函数。gbt 旨在减少每次迭代的损失函数。Spark 支持以下类型的损失函数:

*   **日志丢失**:用于分类问题。

*   **平方误差(L2 损失)**:用于回归问题，默认设置。它是所有观测值的实际输出和预测输出之间的平方差总和。应该很好地处理异常值，这样损失函数才能很好地运行。
*   **绝对误差(L1 损失)**:这也用于回归问题。它是所有观测值的实际输出和预测输出之间的绝对差值的总和。与平方误差相比，它对异常值更稳健。

Spark 提供了一些调整参数来调整该模型，如下所示:

*   `loss`:您可以传递一个损失函数，如前一节所述，这取决于您正在处理的数据集以及您是否打算进行分类或回归。
*   `numIterations`:每次迭代只产生一棵树！如果将该值设置得很高，那么执行所需的时间也会很长，因为操作将是顺序的，并且还会导致过度拟合。应该仔细设置，以获得更好的性能和准确性。
*   `learningRate`:这不是真正的调优参数。如果算法的行为不稳定，那么减少这一点可以帮助稳定模型。
*   `algo` : *分类*或*回归*是根据你想要的设定的。

GBTs 可以用更多的树过度拟合模型，所以 Spark 提供了`runWithValidation`方法来防止过度拟合。

### 类型

截至本文撰写之时，Spark 上的 gbt 尚不支持多类分类。

让我们看一个例子来说明 gbt 的作用。示例数据集包含 20 名学生的平均分数和出勤率。数据还包含符合一组标准的“通过”或“失败”结果。然而，有几名学生(ids 1009 和 1020)获得了“授予”通过状态事件，尽管他们并不真正合格。现在我们的任务是检查接这两个学生的模特是否不在。

通过标准如下:

*   分数至少应为 40，出席率至少应为“够了”
*   如果分数在 40 到 60 之间，那么出席率应该是“满”才能通过

以下示例还强调了跨多个模型重用管道阶段。因此，我们首先构建决策树分类器，然后构建 GBT。我们构建了两个共享阶段的不同管道。

**输入**:

```scala
// Marks < 40 = Fail
// Attendence == Poor => Fail
// Marks >40 and attendence Full => Pass
// Marks > 60 and attendence Enough or Full => Pass
// Two exceptions were studentId 1009 and 1020 who were granted Pass
//This example also emphasizes the reuse of pipeline stages
// Initially the code trains a DecisionTreeClassifier
// Then, same stages are reused to train a GBT classifier
```

**斯卡拉:**

```scala
scala> import org.apache.spark.ml.feature._
scala> import org.apache.spark.ml.Pipeline
scala> import org.apache.spark.ml.classification.{DecisionTreeClassifier,
                                   DecisionTreeClassificationModel}
scala> case class StResult(StudentId:String, Avg_Marks:Double,
        Attendance:String, Result:String)
scala> val file_path = "../work/StudentsPassFail.csv"
scala> val source_ds = spark.read.options(Map("header"->"true",
            "inferSchema"->"true")).csv(file_path).as[StResult]
source_ds: org.apache.spark.sql.Dataset[StResult] = [StudentId: int, Avg_Marks:
double ... 2 more fields]
scala>
//Examine source data
scala> source_ds.show(4)
+---------+---------+----------+------+
|StudentId|Avg_Marks|Attendance|Result|
+---------+---------+----------+------+
|     1001|     48.0|      Full|  Pass|
|     1002|     21.0|    Enough|  Fail|
|     1003|     24.0|    Enough|  Fail|
|     1004|      4.0|      Poor|  Fail|
+---------+---------+----------+------+

scala>           
//Define preparation pipeline
scala> val marks_bkt = new Bucketizer().setInputCol("Avg_Marks").
        setOutputCol("Mark_bins").setSplits(Array(0,40.0,60.0,100.0))
marks_bkt: org.apache.spark.ml.feature.Bucketizer = bucketizer_5299d2fbd1b2
scala> val att_idx = new StringIndexer().setInputCol("Attendance").
        setOutputCol("Att_idx")
att_idx: org.apache.spark.ml.feature.StringIndexer = strIdx_2db54ba5200a
scala> val label_idx = new StringIndexer().setInputCol("Result").
        setOutputCol("Label")
label_idx: org.apache.spark.ml.feature.StringIndexer = strIdx_20f4316d6232
scala>

//Create labels list to decode predictions
scala> val resultLabels = label_idx.fit(source_ds).labels
resultLabels: Array[String] = Array(Fail, Pass)
scala> val va = new VectorAssembler().setInputCols(Array("Mark_bins","Att_idx")).
                  setOutputCol("features")
va: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_5dc2dbbef48c
scala> val dt = new DecisionTreeClassifier().setLabelCol("Label").
         setFeaturesCol("features")
dt: org.apache.spark.ml.classification.DecisionTreeClassifier = dtc_e8343ae1a9eb
scala> val lc = new IndexToString().setInputCol("prediction").
             setOutputCol("predictedLabel").setLabels(resultLabels)
lc: org.apache.spark.ml.feature.IndexToString = idxToStr_90b6693d4313
scala>

//Define pipeline
scala>val dt_pipeline = new
Pipeline().setStages(Array(marks_bkt,att_idx,label_idx,va,dt,lc))
dt_pipeline: org.apache.spark.ml.Pipeline = pipeline_95876bb6c969
scala> val dtModel = dt_pipeline.fit(source_ds)
dtModel: org.apache.spark.ml.PipelineModel = pipeline_95876bb6c969
scala> val resultDF = dtModel.transform(source_ds)
resultDF: org.apache.spark.sql.DataFrame = [StudentId: int, Avg_Marks: double ...
10 more fields]
scala> resultDF.filter("Label != prediction").select("StudentId","Label","prediction","Result","predictedLabel").show()
+---------+-----+----------+------+--------------+
|StudentId|Label|prediction|Result|predictedLabel|
+---------+-----+----------+------+--------------+\
|     1009|  1.0|       0.0|  Pass|          Fail|
|     1020|  1.0|       0.0|  Pass|          Fail|
+---------+-----+----------+------+--------------+

//Note that the difference is in the student ids that were granted pass

//Same example using Gradient boosted tree classifier, reusing the pipeline stages
scala> import org.apache.spark.ml.classification.GBTClassifier
import org.apache.spark.ml.classification.GBTClassifier
scala> val gbt = new GBTClassifier().setLabelCol("Label").
              setFeaturesCol("features").setMaxIter(10)
gbt: org.apache.spark.ml.classification.GBTClassifier = gbtc_cb55ae2174a1
scala> val gbt_pipeline = new
Pipeline().setStages(Array(marks_bkt,att_idx,label_idx,va,gbt,lc))
gbt_pipeline: org.apache.spark.ml.Pipeline = pipeline_dfd42cd89403
scala> val gbtResultDF = gbt_pipeline.fit(source_ds).transform(source_ds)
gbtResultDF: org.apache.spark.sql.DataFrame = [StudentId: int, Avg_Marks: double ... 8 more fields]
scala> gbtResultDF.filter("Label !=
prediction").select("StudentId","Label","Result","prediction","predictedLabel").show()
+---------+-----+------+----------+--------------+
|StudentId|Label|Result|prediction|predictedLabel|
+---------+-----+------+----------+--------------+
|     1009|  1.0|  Pass|       0.0|          Fail|
|     1020|  1.0|  Pass|       0.0|          Fail|
+---------+-----+------+----------+--------------+
```

**蟒蛇**:

```scala
>>> from pyspark.ml.pipeline import Pipeline
>>> from pyspark.ml.feature import Bucketizer, StringIndexer, VectorAssembler, IndexToString
>>> from pyspark.ml.classification import DecisionTreeClassifier,
DecisionTreeClassificationModel
>>> 

//Get source file
>>> file_path = "../work/StudentsPassFail.csv"
>>> source_df = spark.read.csv(file_path,header=True,inferSchema=True)
>>> 

//Examine source data
>>> source_df.show(4)
+---------+---------+----------+------+
|StudentId|Avg_Marks|Attendance|Result|
+---------+---------+----------+------+
|     1001|     48.0|      Full|  Pass|
|     1002|     21.0|    Enough|  Fail|
|     1003|     24.0|    Enough|  Fail|
|     1004|      4.0|      Poor|  Fail|
+---------+---------+----------+------+

//Define preparation pipeline
>>> marks_bkt = Bucketizer(inputCol="Avg_Marks",
        outputCol="Mark_bins", splits=[0,40.0,60.0,100.0])
>>> att_idx = StringIndexer(inputCol = "Attendance",
        outputCol="Att_idx")
>>> label_idx = StringIndexer(inputCol="Result",
                   outputCol="Label")
>>> 

//Create labels list to decode predictions
>>> resultLabels = label_idx.fit(source_df).labels
>>> resultLabels
[u'Fail', u'Pass']
>>> 
>>> va = VectorAssembler(inputCols=["Mark_bins","Att_idx"],
                         outputCol="features")
>>> dt = DecisionTreeClassifier(labelCol="Label", featuresCol="features")
>>> lc = IndexToString(inputCol="prediction",outputCol="predictedLabel",
             labels=resultLabels)
>>> dt_pipeline = Pipeline(stages=[marks_bkt, att_idx, label_idx,va,dt,lc])
>>> dtModel = dt_pipeline.fit(source_df)
>>> resultDF = dtModel.transform(source_df)
>>>

//Look for obervatiuons where prediction did not match
>>> resultDF.filter("Label != prediction").select(
         "StudentId","Label","prediction","Result","predictedLabel").show()
+---------+-----+----------+------+--------------+
|StudentId|Label|prediction|Result|predictedLabel|
+---------+-----+----------+------+--------------+
|     1009|  1.0|       0.0|  Pass|          Fail|
|     1020|  1.0|       0.0|  Pass|          Fail|
+---------+-----+----------+------+--------------+

//Note that the difference is in the student ids that were granted pass
>>> 
//Same example using Gradient boosted tree classifier, reusing the pipeline
stages
>>> from pyspark.ml.classification import GBTClassifier
>>> gbt = GBTClassifier(labelCol="Label", featuresCol="features",maxIter=10)
>>> gbt_pipeline = Pipeline(stages=[marks_bkt,att_idx,label_idx,va,gbt,lc])
>>> gbtResultDF = gbt_pipeline.fit(source_df).transform(source_df)
>>> gbtResultDF.columns
['StudentId', 'Avg_Marks', 'Attendance', 'Result', 'Mark_bins', 'Att_idx',
'Label', 'features', 'prediction', 'predictedLabel']
>>> gbtResultDF.filter("Label !=
prediction").select("StudentId","Label","Result","prediction","predictedLabel").show()
+---------+-----+------+----------+--------------+
|StudentId|Label|Result|prediction|predictedLabel|
+---------+-----+------+----------+--------------+
|     1009|  1.0|  Pass|       0.0|          Fail|
|     1020|  1.0|  Pass|       0.0|          Fail|
+---------+-----+------+----------+--------------+
```

# 多层感知器分类器

**多层感知器分类器** ( **MLPC** )是一种前馈人工神经网络，多层节点以定向方式相互连接。它使用一种称为*反向传播*的监督学习技术来训练网络。

中间层的节点使用 sigmoid 函数将输出限制在 0 到 1 之间，输出层的节点使用`softmax`函数，这是 sigmoid 函数的广义版本。

**Scala** :

```scala
scala> import org.apache.spark.ml.classification.MultilayerPerceptronClassifier
import org.apache.spark.ml.classification.MultilayerPerceptronClassifier
scala> import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
scala> import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.mllib.util.MLUtils

// Load training data
scala> val data = MLUtils.loadLibSVMFile(sc,
"data/mllib/sample_multiclass_classification_data.txt").toDF()
data: org.apache.spark.sql.DataFrame = [label: double, features: vector]

//Convert mllib vectors to ml Vectors for spark 2.0+. Retain data for previous versions
scala> val data2 = MLUtils.convertVectorColumnsToML(data)
data2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]

// Split the data into train and test
scala> val splits = data2.randomSplit(Array(0.6, 0.4), seed = 1234L)
splits: Array[org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]] = Array([label: double, features: vector], [label: double, features: vector])
scala> val train = splits(0)
train: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]
scala> val test = splits(1)
test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]

// specify layers for the neural network:
// input layer of size 4 (features), two intermediate of size 5 and 4 and output of size 3 (classes)
scala> val layers = Array[Int](4, 5, 4, 3)
layers: Array[Int] = Array(4, 5, 4, 3)

// create the trainer and set its parameters
scala> val trainer = new MultilayerPerceptronClassifier().
           setLayers(layers).setBlockSize(128).
           setSeed(1234L).setMaxIter(100)
trainer: org.apache.spark.ml.classification.MultilayerPerceptronClassifier = mlpc_edfa49fbae3c

// train the model
scala> val model = trainer.fit(train)
model: org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel = mlpc_edfa49fbae3c

// compute accuracy on the test set
scala> val result = model.transform(test)
result: org.apache.spark.sql.DataFrame = [label: double, features: vector ... 1 more field]
scala> val predictionAndLabels = result.select("prediction", "label")
predictionAndLabels: org.apache.spark.sql.DataFrame = [prediction: double, label: double]
scala> val evaluator = new MulticlassClassificationEvaluator().setMetricName("accuracy")
evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_a4f43d85f261
scala> println("Accuracy:" + evaluator.evaluate(predictionAndLabels))
Accuracy:0.9444444444444444

Python: >>> from pyspark.ml.classification import MultilayerPerceptronClassifier
>>> from pyspark.ml.evaluation import MulticlassClassificationEvaluator
>>> from pyspark.mllib.util import MLUtils
>>>

  //Load training data
>>> data = spark.read.format("libsvm").load(      "data/mllib/sample_multiclass_classification_data.txt")

//Convert mllib vectors to ml Vectors for spark 2.0+. Retain data for previous versions
>>> data2 = MLUtils.convertVectorColumnsToML(data)
>>>

 // Split the data into train and test
>>> splits = data2.randomSplit([0.6, 0.4], seed = 1234L)
>>> train, test = splits[0], splits[1]
>>>

 // specify layers for the neural network:
 // input layer of size 4 (features), two intermediate of size 5 and 4 and output of size 3 (classes)
>>> layers = [4,5,4,3] 

// create the trainer and set its parameters
>>> trainer = MultilayerPerceptronClassifier(layers=layers, blockSize=128,
                 seed=1234L, maxIter=100)
// train the model
>>> model = trainer.fit(train)
>>>

// compute accuracy on the test set
>>> result = model.transform(test)
>>> predictionAndLabels = result.select("prediction", "label")
>>> evaluator = MulticlassClassificationEvaluator().setMetricName("accuracy")
>>> print "Accuracy:",evaluator.evaluate(predictionAndLabels)
Accuracy: 0.901960784314
>>> 
```

# 聚类技术

聚类是一种无监督的学习技术，其中没有响应变量来监督模型。其思想是将具有某种相似度的数据点进行聚类。除了探索性数据分析之外，它还被用作监督管道的一部分，分类器或回归器可以建立在不同的聚类上。有很多聚类技术可用。让我们来看看 Spark 支持的几个重要项目。

## K-均值聚类

K-means 是最常见的聚类技术之一。k-means 问题是寻找最小化类内方差的聚类中心，即从被聚类的每个数据点到其聚类中心(最接近它的中心)的平方距离之和。您必须预先指定数据集中所需的集群数量。

由于它使用欧几里德距离度量来查找数据点之间的差异，因此在使用 k 均值之前，需要将要素缩放到可比较的单位。欧几里德距离可以用如下图形方式更好地解释:

![K-means clustering](graphics/image_06_051.jpg)

给定一组数据点( *x1* ， *x2* ，...， *xn* )与变量数量一样多的维度，k-means 聚类旨在将 n 个观测值划分为 k 个(小于 *n* )集合，其中 *S = {S1，S2，...，Sk}* ，以最小化**簇内平方和** ( **WCSS** )。换句话说，它的目标是找到:

![K-means clustering](graphics/image_06_052.jpg)

Spark 要求将以下参数传递给该算法:

*   `k`:这是需要的簇数。
*   `maxIterations`:这是要运行的最大迭代次数。
*   `initializationMode`:指定随机初始化或通过 k 均值初始化||。
*   `runs`:这是运行 k-means 算法的次数(k-means 不保证能找到全局最优解，在给定数据集上运行多次时，算法返回最佳聚类结果)。
*   `initializationSteps`:这决定了 k 均值||算法中的步数。
*   `epsilon`:这决定了我们认为 k 均值已经收敛的距离阈值。
*   `initialModel`:这是一组可选的用于初始化的集群中心。如果提供了此参数，则只执行一次运行。

### k-means 的缺点

*   它仅适用于数字特征
*   它需要在实现算法之前进行缩放
*   它易受局部最优的影响(解决这个问题的方法是 k-means++)

### 示例

让我们对相同的学生数据运行 k 均值聚类。

```scala
scala> import org.apache.spark.ml.clustering.{KMeans, KMeansModel}
import org.apache.spark.ml.clustering.{KMeans, KMeansModel}
scala> import org.apache.spark.ml.linalg.Vectors
import org.apache.spark.ml.linalg.Vectors
scala>

//Define pipeline for kmeans. Reuse the previous stages in ENSEMBLES
scala> val km = new KMeans()
km: org.apache.spark.ml.clustering.KMeans = kmeans_b34da02bd7c8
scala> val kmeans_pipeline = new
Pipeline().setStages(Array(marks_bkt,att_idx,label_idx,va,km,lc))
kmeans_pipeline: org.apache.spark.ml.Pipeline = pipeline_0cd64aa93a88

//Train and transform
scala> val kmeansDF = kmeans_pipeline.fit(source_ds).transform(source_ds)
kmeansDF: org.apache.spark.sql.DataFrame = [StudentId: int, Avg_Marks: double ... 8 more fields]

//Examine results
scala> kmeansDF.filter("Label != prediction").count()
res17: Long = 13

```

**蟒蛇**:

```scala
>>> from pyspark.ml.clustering import KMeans, KMeansModel
>>> from pyspark.ml.linalg import Vectors
>>> 

//Define pipeline for kmeans. Reuse the previous stages in ENSEMBLES
>>> km = KMeans()
>>> kmeans_pipeline = Pipeline(stages = [marks_bkt, att_idx, label_idx,va,km,lc])

//Train and transform
>>> kmeansDF = kmeans_pipeline.fit(source_df).transform(source_df)
>>> kmeansDF.columns
['StudentId', 'Avg_Marks', 'Attendance', 'Result', 'Mark_bins', 'Att_idx', 'Label', 'features', 'prediction', 'predictedLabel']
>>> kmeansDF.filter("Label != prediction").count()
4
```

# 总结

在本章中，我们解释了各种机器学习算法，它们是如何在 MLlib 库中实现的，以及它们如何与管道应用编程接口一起使用来简化执行。Python 和 Scala 代码示例涵盖了这些概念，以供参考。

在下一章中，我们将讨论 Spark 如何支持 R 编程语言，重点是一些算法及其执行，类似于我们在本章中介绍的。

# 参考文献

MLlib 中支持的算法:

*   [http://spark.apache.org/docs/latest/mllib-guide.html](http://spark.apache.org/docs/latest/mllib-guide.html)
*   [http://spark . Apache . org/docs/latest/mllib-决策树. html](http://spark.apache.org/docs/latest/mllib-decision-tree.html)

火花毫升编程指南:

*   [http://spark.apache.org/docs/latest/ml-guide.html](http://spark.apache.org/docs/latest/ml-guide.html)

2015 年 6 月峰会幻灯片中的 spark.pdf 先进数据科学:

*   [https://databricks . com/blog/2015/07/29/新增功能-机器学习-pipelines-in-spark-1-4 . html](https://databricks.com/blog/2015/07/29/new-features-in-machine-learning-pipelines-in-spark-1-4.html)
*   [https://databricks . com/blog/2015/06/02/statistical-and-mathematical-functions-with-data frames-in-spark . html](https://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html)
*   [https://databricks . com/blog/2015/01/07/ml-pipelines-a-new-high-level-API-for-mllib . html](https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html)**