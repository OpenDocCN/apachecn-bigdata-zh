# 三、数据帧简介

为了解决任何现实世界的大数据分析问题，访问高效且可扩展的计算系统绝对是强制性的。然而，如果目标用户无法以一种简单且熟悉的方式获得计算能力，这几乎没有任何意义。交互式数据分析对于可以表示为命名列的数据集变得更加容易，而普通的 RDD 则不是这样。因此，需要一种基于模式的方法来以标准化的方式表示数据是数据框架背后的灵感。

前一章概述了 Spark 的一些设计方面。我们学习了 Spark 如何通过内存计算在分布式数据集合上实现分布式数据处理。它涵盖了显示 Spark 是一个快速、高效和可扩展的计算平台的大部分要点。在这一章中，我们将看到 Spark 如何引入 DataFrame API，让数据科学家能够像在家里一样轻松地进行他们通常的数据分析活动。

本主题将作为许多后续章节的基础，我们强烈建议您非常好地理解这里涵盖的概念。作为本章的先决条件，需要对 SQL 和 Spark 有基本的了解。本章涵盖的主题如下:

*   为什么是数据帧？
*   Spark SQL
    *   催化剂优化器
*   DataFrame API
    *   数据框基础
    *   RDD 对数据框

*   Creating DataFrames
    *   来自 RDDs
    *   来自 JSON
    *   来自 JDBC 的消息
    *   来自其他数据源
*   操作数据帧

# 为什么是数据帧？

除了大规模、可扩展的计算能力之外，大数据应用程序还需要多种功能的组合，例如支持交互式数据分析的关系系统(简单的 SQL 风格)、异构数据源、不同的存储格式以及不同的处理技术。

尽管 Spark 提供了一个函数式编程 API 来操作分布式数据集合，但它最终得到了元组(_1，_2，...).对元组进行操作的编码有点复杂和混乱，有时很慢。因此，需要一个具有以下特征的标准化层:

*   用模式命名列(比元组更高级别的抽象),以便于操作和跟踪它们
*   整合来自各种数据源的数据的功能，如 Hive、Parquet、SQL Server、PostgreSQL、JSON 以及 Spark 的本机 rdd，并将它们统一为一种通用格式
*   能够利用特殊文件格式的内置模式，如 Avro、CSV、JSON 等。
*   支持简单的关系和复杂的逻辑操作
*   消除了为 ML 算法定义基于特定领域任务的列对象的需要，并作为 MLlib 中所有算法的公共数据层
*   一种独立于语言的实体，可以在不同语言的函数之间传递

为了满足上述需求，数据框架应用编程接口被构建为 Spark SQL 之上的又一个抽象层次。

# Spark SQL

为基本的业务需求执行 SQL 查询是非常常见的，并且几乎每个业务都使用某种类型的数据库。因此，Spark SQL 还支持使用基本 SQL 语法或 HiveQL 编写的 SQL 查询的执行。Spark SQL 还可以用来从现有的 Hive 安装中读取数据。除了这些简单的 SQL 操作，Spark SQL 还解决了一些棘手的问题。通过关系查询设计复杂的逻辑很麻烦，有时几乎是不可能的。因此，Spark SQL 旨在集成关系处理和功能编程的能力，以便在分布式计算环境中实现、优化和扩展复杂的逻辑。与 Spark SQL 交互的方式基本上有三种，包括 SQL、数据框架 API 和数据集 API。数据集应用编程接口是在撰写本书时在 Spark 1.6 中添加的一个实验层，因此我们将只讨论数据帧。

Spark SQL 将数据框公开为更高级别的应用编程接口，并处理所有涉及的复杂性，还执行所有后台任务。通过声明性语法，用户可以专注于程序应该完成什么，而不必担心控制流，这将由构建在 Spark SQL 中的 Catalyst 优化器来处理。

## 催化剂优化器

Catalyst 优化器是 Spark SQL 和 DataFrame 的支点。它是用 Scala 的函数式编程结构构建的，具有以下特性:

*   Schema inference from various data formats:
    *   Spark 内置了对 JSON 模式推理的支持。用户只需将任何 JSON 文件注册为一个表，并使用 SQL 语法进行查询，就可以创建一个表。
    *   Scala 对象的 RDDs 类型信息是从 Scala 的类型系统中提取的，也就是**案例类**，如果它们包含案例类的话。
    *   作为 Python 对象的 RDDs 用不同的方法提取类型信息。由于 Python 不是静态类型的，并且遵循动态类型系统，因此 RDD 可以包含多种类型。因此，Spark SQL 对数据集进行采样，并使用类似于 JSON 模式推断的算法来推断模式。
    *   未来，将提供对 CSV、XML 和其他格式的内置支持。

*   Built-in support for a wide range of data sources and query federation for efficient data import:
    *   Spark 有一个内置的机制，可以通过查询联邦从一些外部数据源(例如 JSON、JDBC、Parquet、MySQL、Hive、PostgreSQL、HDFS、S3 等)获取数据。它可以通过使用现成的 SQL 数据类型和其他复杂的数据类型(如 Struct、Union、Array 等)来精确地建模源数据。
    *   它还允许用户使用**数据源 API** 从不支持的现成数据源(例如 CSV、Avro HBase、Cassandra 等)中获取数据。
    *   Spark 使用谓词下推(将过滤或聚合推入外部存储系统)来优化来自外部系统的数据来源，并将它们组合起来形成数据管道。
*   Control and optimization of code generation:
    *   优化实际上发生在整个执行流水线的最后阶段。
    *   Catalyst 旨在优化查询执行的所有阶段:分析、逻辑优化、物理规划和代码生成，以将部分查询编译为 Java 字节码。

# 数据帧应用编程接口

像数据表示或数据库投影输出(select 语句的输出)这样的 Excel 电子表格，最接近人类的数据表示一直是一组具有多行的统一列。这种通常有标记行和列的二维数据结构在某些领域被称为数据框架，例如 R 数据框架和 Python 的熊猫数据框架。在 DataFrame 中，通常一个单独的列具有相同类型的数据，而行描述了关于该列的数据点，这些数据点共同表示某种东西，无论是关于个人、购买或棒球比赛结果的数据。你可以把它想象成一个矩阵，或者一个电子表格，或者一个关系数据库表。

R 和 Pandas 中的数据帧在切片、整形和分析数据方面非常方便——这是任何数据争论和数据分析工作流中必不可少的操作。这激发了 Spark 上一个类似概念的发展，称为数据帧。

## 数据帧基础

数据框架应用编程接口最早是在 2015 年 3 月发布的 Spark 1.3.0 中引入的。它是 Spark SQL 的一个编程抽象，用于结构化和半结构化数据处理。它使开发人员能够通过 Python、Java、Scala 和 R 来利用数据框架、数据结构的力量。与 RDDs 一样，Spark 数据框架是组织成命名列的记录的分布式集合，类似于 RDBMS 表或 R 或 Pandas 的数据框架。然而，与关系数据库不同，它们跟踪模式并促进关系操作以及程序操作，如`map`。在内部，数据框以列格式存储数据，但在过程函数需要时会动态构造行对象。

数据框应用编程接口带来了两个特性:

*   内置支持多种数据格式，如拼花、蜂巢和 JSON。尽管如此，通过 Spark SQL 的外部数据源应用编程接口，数据框可以访问大量第三方数据源，如数据库和 NoSQL 商店。
*   A more robust and feature-rich DSL with functions designed for common tasks such as:
    *   [计]元数据
    *   抽样
    *   关系数据处理-项目、筛选、聚合、连接
    *   UDF

数据框架应用编程接口建立在 Spark SQL 查询优化器的基础上，可以在一群机器上自动有效地执行代码。

## RDDs 与数据帧

RDDs 和 DataFrames 是 Spark 提供的两种不同类型的容错和分布式数据抽象。它们在某种程度上是相似的，但是在实现上有很大的不同。开发人员需要清楚地了解他们的差异，以便能够将他们的需求与正确的抽象相匹配。

### 相似之处

以下是关系数据库和数据帧之间的相似之处:

*   在 Spark 中，两者都是容错的分区数据抽象
*   两者都可以处理不同的数据源
*   两者都是延迟评估的(当对它们执行输出操作时执行)，因此能够采用最优化的执行计划
*   这两种 API 都有四种语言版本:Scala、Python、Java 和 R

### 差异

以下是关系数据库和数据帧之间的区别:

*   数据帧是比关系数据库更高层次的抽象。
*   RDD 的定义意味着定义一个**有向无环图** ( **DAG** )而定义一个数据帧导致创建一个**抽象语法树** ( **AST** )。Spark SQL 催化剂引擎将利用和优化 AST。
*   RDD 是一个通用的数据结构抽象，而数据框架是一个专门处理二维表状数据的数据结构。

数据框架应用编程接口实际上是 SchemaRDD 重命名的。更名是为了表明它不再从 RDD 继承，并以一个熟悉的名字和概念安慰数据科学家。

# 创建数据帧

Spark 数据框创作类似于 RDD 创作。要访问数据框架应用编程接口，您需要 SQLContext 或 HiveContext 作为入口点。在本节中，我们将演示如何从各种数据源创建数据帧，从内存中集合的基本代码示例开始:

![Creating DataFrames](img/image_03_001.jpg)

## 从关系数据库创建数据帧

下面的代码从颜色列表中创建一个 RDD，后面是包含颜色名称及其长度的元组集合。它使用`toDF`方法创建一个数据帧，将 RDD 转换成一个数据帧。`toDF`方法将列标签列表作为可选参数:

**蟒蛇**:

```scala
   //Create a list of colours 
>>> colors = ['white','green','yellow','red','brown','pink'] 
//Distribute a local collection to form an RDD 
//Apply map function on that RDD to get another RDD containing colour, length tuples 
>>> color_df = sc.parallelize(colors) 
        .map(lambda x:(x,len(x))).toDF(["color","length"]) 

>>> color_df 
DataFrame[color: string, length: bigint] 

>>> color_df.dtypes        //Note the implicit type inference 
[('color', 'string'), ('length', 'bigint')] 

>>> color_df.show()  //Final output as expected. Order need not be the same as shown 
+------+------+ 
| color|length| 
+------+------+ 
| white|     5| 
| green|     5| 
|yellow|     6| 
|   red|     3| 
| brown|     5| 
|  pink|     4| 
+------+------+ 

```

**Scala** :

```scala
//Create a list of colours 
Scala> val colors = List("white","green","yellow","red","brown","pink") 
//Distribute a local collection to form an RDD 
//Apply map function on that RDD to get another RDD containing colour, length tuples 
Scala> val color_df = sc.parallelize(colors) 
         .map(x => (x,x.length)).toDF("color","length") 

Scala> color_df 
res0: org.apache.spark.sql.DataFrame = [color: string, length: int] 

Scala> color_df.dtypes  //Note the implicit type inference   
res1: Array[(String, String)] = Array((color,StringType), (length,IntegerType)) 

Scala> color_df.show()//Final output as expected. Order need not be the same as shown 
+------+------+ 
| color|length| 
+------+------+ 
| white|     5| 
| green|     5| 
|yellow|     6| 
|   red|     3| 
| brown|     5| 
|  pink|     4| 
+------+------+ 

```

从前面的例子中可以看出，从开发人员的角度来看，创建数据帧类似于创建 RDD。我们在这里创建了一个 RDD，然后将其转换为元组，然后发送到`toDF`方法。注意`toDF`取元组列表而不是标量元素。您甚至需要传递元组来创建单列数据帧。每个元组类似于一行。您可以选择标记列；否则，Spark 会创建模糊的名称，如`_1`、`_2`。列的类型推断隐式发生。

如果您已经将数据作为关系数据库，Spark SQL 支持两种不同的方法将现有的关系数据库转换为数据帧:

*   第一种方法使用反射来推断包含特定类型对象的 RDD 模式，这意味着您知道该模式。
*   第二种方法是通过编程接口，让您构建一个模式，然后将其应用于现有的 RDD。虽然这种方法更详细，但它允许您在运行时才知道列类型时构造数据帧。

## 从 JSON 创建数据帧

JavaScript 对象符号，或 JSON，是一种独立于语言、自描述的轻量级数据交换格式。JSON 已经成为一种流行的数据交换格式，并且变得无处不在。除了 JavaScript 和 RESTful 接口之外，像 MySQL 这样的数据库已经接受了 JSON 作为数据类型，MongoDB 以二进制形式将所有数据存储为 JSON 文档。对于任何现代数据分析工作流来说，数据到 JSON 的转换都是必不可少的。Spark 数据帧应用编程接口允许开发人员将 JSON 对象转换成数据帧，反之亦然。让我们仔细看看下面的例子，以便更好地理解:

**蟒蛇**:

```scala
//Pass the source json data file path 
>>> df = sqlContext.read.json("./authors.json") 
>>> df.show() //json parsed; Column names and data    types inferred implicitly 
+----------+---------+ 
|first_name|last_name| 
+----------+---------+ 
|      Mark|    Twain| 
|   Charles|  Dickens| 
|    Thomas|    Hardy| 
+----------+---------+ 

```

**Scala** :

```scala
//Pass the source json data file path 
Scala> val df = sqlContext.read.json("./authors.json") 
Scala> df.show()  //json parsed; Column names and    data types inferred implicitly 
+----------+---------+ 
|first_name|last_name| 
+----------+---------+ 
|      Mark|    Twain| 
|   Charles|  Dickens| 
|    Thomas|    Hardy| 
+----------+---------+ 

```

Spark 从键中自动推断模式，并相应地创建一个数据帧。

## 使用 JDBC 从数据库创建数据帧

Spark 允许开发人员使用 JDBC 从其他数据库创建数据帧，前提是您确保预期数据库的 JDBC 驱动程序是可访问的。JDBC 驱动程序是允许 Java 应用程序与数据库交互的软件组件。不同的数据库需要不同的驱动程序。通常，数据库提供者如 MySQL 提供这些驱动程序组件来访问他们的数据库。您必须确保您有合适的驱动程序来驱动您想要使用的数据库。

以下示例假设您已经有一个运行在给定网址上的 MySQL 数据库，一个名为`test`的数据库中名为`people`的表，其中包含一些数据，以及登录的有效凭据。还有一个用适当的 JAR 文件重新启动 REPL shell 的附加步骤:

### 注

如果你的系统中还没有这个 JAR 文件，可以通过以下链接从 MySQL 网站下载:[https://dev.mysql.com/downloads/connector/j/](https://dev.mysql.com/downloads/connector/j/)。

**蟒蛇**:

```scala
//Launch shell with driver-class-path as a command line argument 
pyspark --driver-class-path /usr/share/   java/mysql-connector-java.jar 
   //Pass the connection parameters 
>>> peopleDF = sqlContext.read.format('jdbc').options( 
                        url = 'jdbc:mysql://localhost', 
                        dbtable = 'test.people', 
                        user = 'root', 
                        password = 'mysql').load() 
   //Retrieve table data as a DataFrame 
>>> peopleDF.show() 
+----------+---------+------+----------+----------+---------+ 
|first_name|last_name|gender|       dob|occupation|person_id| 
+----------+---------+------+----------+----------+---------+ 
|    Thomas|    Hardy|     M|1840-06-02|    Writer|      101| 
|     Emily|   Bronte|     F|1818-07-30|    Writer|      102| 
| Charlotte|   Bronte|     F|1816-04-21|    Writer|      103| 
|   Charles|  Dickens|     M|1812-02-07|    Writer|      104| 
+----------+---------+------+----------+----------+---------+ 

```

**Scala** :

```scala
//Launch shell with driver-class-path as a command line argument 
spark-shell --driver-class-path /usr/share/   java/mysql-connector-java.jar 
   //Pass the connection parameters 
scala> val peopleDF = sqlContext.read.format("jdbc").options( 
           Map("url" -> "jdbc:mysql://localhost", 
               "dbtable" -> "test.people", 
               "user" -> "root", 
               "password" -> "mysql")).load() 
peopleDF: org.apache.spark.sql.DataFrame = [first_name: string, last_name: string, gender: string, dob: date, occupation: string, person_id: int] 
//Retrieve table data as a DataFrame 
scala> peopleDF.show() 
+----------+---------+------+----------+----------+---------+ 
|first_name|last_name|gender|       dob|occupation|person_id| 
+----------+---------+------+----------+----------+---------+ 
|    Thomas|    Hardy|     M|1840-06-02|    Writer|      101| 
|     Emily|   Bronte|     F|1818-07-30|    Writer|      102| 
| Charlotte|   Bronte|     F|1816-04-21|    Writer|      103| 
|   Charles|  Dickens|     M|1812-02-07|    Writer|      104| 
+----------+---------+------+----------+----------+---------+ 

```

## 从 Apache 拼花创建数据帧

Apache Parquet 是一种高效、压缩的柱状数据表示，可用于 Hadoop 生态系统中的任何项目。与按行存储数据的传统方法相反，列数据表示按列存储数据。需要频繁查询几列中的两到三列的用例从这种安排中受益匪浅，因为列是连续存储在磁盘上的，并且您不必在面向行的存储中读取不需要的列。另一个优点是在压缩方面。单列中的数据属于单一类型。这些值往往是相似的，有时是相同的。这些质量大大提高了压缩和编码效率。Parquet 允许在每列级别指定压缩方案，并允许在发明和实现编码时添加更多编码。

Apache Spark 提供读写 Parquet 文件的支持，自动保留原始数据的模式。以下示例将上一示例中加载到数据框中的人员数据写入拼花格式，然后将其重新读入 RDD:

**蟒蛇**:

```scala
//Write DataFrame contents into Parquet format 
>>> peopleDF.write.parquet('writers.parquet') 
//Read Parquet data into another DataFrame 
>>> writersDF = sqlContext.read.parquet('writers.parquet')  
writersDF: org.apache.spark.sql.DataFrame = [first_name:    string, last_name: string, gender: string, dob:    date, occupation: string, person_id: int]
```

**Scala** :

```scala
//Write DataFrame contents into Parquet format 
scala> peopleDF.write.parquet("writers.parquet") 
//Read Parquet data into another DataFrame 
scala> val writersDF = sqlContext.read.parquet("writers.parquet")  
writersDF: org.apache.spark.sql.DataFrame = [first_name:    string, last_name: string, gender: string, dob:    date, occupation: string, person_id: int]
```

## 从其他数据源创建数据帧

Spark 为 JSON、JDBC、HDFS、Parquet、MYSQL、亚马逊 S3 等多个数据源提供内置支持。此外，它还提供了一个数据源应用编程接口，该接口为通过 Spark SQL 访问结构化数据提供了一个可插拔的机制。有几个库建立在这个可插拔组件之上，例如，CSV、Avro、Cassandra 和 MongoDB，仅举几个例子。这些库不是 Spark 代码库的一部分。这些是为单个数据源构建的，托管在社区站点 Spark 包中。

# 数据帧操作

在本章的前一节中，我们学习了许多不同的创建数据帧的方法。在本节中，我们将重点介绍可以在数据帧上执行的各种操作。开发人员链接多个操作来过滤、转换、聚合和排序数据框中的数据。底层的 Catalyst 优化器确保了这些操作的高效执行。您在这里找到的这些函数类似于您通常在表的 SQL 操作中找到的函数:

**蟒蛇**:

```scala
//Create a local collection of colors first 
>>> colors = ['white','green','yellow','red','brown','pink'] 
//Distribute the local collection to form an RDD 
//Apply map function on that RDD to get another RDD containing colour, length tuples and convert that RDD to a DataFrame 
>>> color_df = sc.parallelize(colors) 
        .map(lambda x:(x,len(x))).toDF(['color','length']) 
//Check the object type 
>>> color_df 
DataFrame[color: string, length: bigint] 
//Check the schema 
>>> color_df.dtypes 
[('color', 'string'), ('length', 'bigint')] 

//Check row count 
>>> color_df.count() 
6 
//Look at the table contents. You can limit displayed rows by passing parameter to show 
color_df.show() 
+------+------+ 
| color|length| 
+------+------+ 
| white|     5| 
| green|     5| 
|yellow|     6| 
|   red|     3| 
| brown|     5| 
|  pink|     4| 
+------+------+ 

//List out column names 
>>> color_df.columns 
[u'color', u'length'] 

//Drop a column. The source DataFrame color_df remains the same. //Spark returns a new DataFrame which is being passed to show 
>>> color_df.drop('length').show() 
+------+ 
| color| 
+------+ 
| white| 
| green| 
|yellow| 
|   red| 
| brown| 
|  pink| 
+------+ 
//Convert to JSON format 
>>> color_df.toJSON().first() 
u'{"color":"white","length":5}' 
//filter operation is similar to WHERE clause in SQL 
//You specify conditions to select only desired columns and rows 
//Output of filter operation is another DataFrame object that is usually passed on to some more operations 
//The following example selects the colors having a length of four or five only and label the column as "mid_length" 
filter 
------ 
>>> color_df.filter(color_df.length.between(4,5)) 
      .select(color_df.color.alias("mid_length")).show() 
+----------+ 
|mid_length| 
+----------+ 
|     white| 
|     green| 
|     brown| 
|      pink| 
+----------+ 

//This example uses multiple filter criteria 
>>> color_df.filter(color_df.length > 4) 
     .filter(color_df[0]!="white").show() 
+------+------+ 
| color|length| 
+------+------+ 
| green|     5| 
|yellow|     6| 
| brown|     5| 
+------+------+ 

//Sort the data on one or more columns 
sort 
---- 
//A simple single column sorting in default (ascending) order 
>>> color_df.sort("color").show() 
+------+------+ 
| color|length| 
+------+------+ 
| brown|     5| 
| green|     5| 
|  pink|     4| 
|   red|     3| 
| white|     5| 
|yellow|     6| 
+------+------+ 
//First filter colors of length more than 4 and then sort on multiple columns 
//The Filtered rows are sorted first on the column length in default ascending order. Rows with same length are sorted on color in descending order   
>>> color_df.filter(color_df['length']>=4).sort("length", 'color',ascending=False).show()
+------+------+ 
| color|length| 
+------+------+ 
|yellow|     6| 
| white|     5| 
| green|     5| 
| brown|     5| 
|  pink|     4| 
+------+------+ 

//You can use orderBy instead, which is an alias to sort 
>>> color_df.orderBy('length','color').take(4)
[Row(color=u'red', length=3), Row(color=u'pink', length=4), Row(color=u'brown', length=5), Row(color=u'green', length=5)]

//Alternative syntax, for single or multiple columns.  
>>> color_df.sort(color_df.length.desc(),   color_df.color.asc()).show() 
+------+------+ 
| color|length| 
+------+------+ 
|yellow|     6| 
| brown|     5| 
| green|     5| 
| white|     5| 
|  pink|     4| 
|   red|     3| 
+------+------+ 
//All the examples until now have been acting on one row at a time, filtering or transforming or reordering.  
//The following example deals with regrouping the data 
//These operations require "wide dependency" and often involve shuffling.  
groupBy 
------- 
>>> color_df.groupBy('length').count().show() 
+------+-----+ 
|length|count| 
+------+-----+ 
|     3|    1| 
|     4|    1| 
|     5|    3| 
|     6|    1| 
+------+-----+ 
//Data often contains missing information or null values. We may want to drop such rows or replace with some filler information. dropna is provided for dropping such rows 
//The following json file has names of famous authors. Firstname data is missing in one row. 
dropna 
------ 
>>> df1 = sqlContext.read.json('./authors_missing.json')
>>> df1.show() 
+----------+---------+ 
|first_name|last_name| 
+----------+---------+ 
|      Mark|    Twain| 
|   Charles|  Dickens| 
|      null|    Hardy| 
+----------+---------+ 

//Let us drop the row with incomplete information 
>>> df2 = df1.dropna() 
>>> df2.show()  //Unwanted row is dropped 
+----------+---------+ 
|first_name|last_name| 
+----------+---------+ 
|      Mark|    Twain| 
|   Charles|  Dickens| 
+----------+---------+ 

```

**Scala** :

```scala
//Create a local collection of colors first 
Scala> val colors = List("white","green","yellow","red","brown","pink") 
//Distribute a local collection to form an RDD 
//Apply map function on that RDD to get another RDD containing color, length tuples and convert that RDD to a DataFrame 
Scala> val color_df = sc.parallelize(colors) 
        .map(x => (x,x.length)).toDF("color","length") 
//Check the object type 
Scala> color_df 
res0: org.apache.spark.sql.DataFrame = [color: string, length: int] 
//Check the schema 
Scala> color_df.dtypes 
res1: Array[(String, String)] = Array((color,StringType), (length,IntegerType)) 
//Check row count 
Scala> color_df.count() 
res4: Long = 6 
//Look at the table contents. You can limit displayed rows by passing parameter to show 
color_df.show() 
+------+------+ 
| color|length| 
+------+------+ 
| white|     5| 
| green|     5| 
|yellow|     6| 
|   red|     3| 
| brown|     5| 
|  pink|     4| 
+------+------+ 
//List out column names 
Scala> color_df.columns 
res5: Array[String] = Array(color, length) 
//Drop a column. The source DataFrame color_df remains the same. 
//Spark returns a new DataFrame which is being passed to show 
Scala> color_df.drop("length").show() 
+------+ 
| color| 
+------+ 
| white| 
| green| 
|yellow| 
|   red| 
| brown| 
|  pink| 
+------+ 
//Convert to JSON format 
color_df.toJSON.first() 
res9: String = {"color":"white","length":5} 

//filter operation is similar to WHERE clause in SQL 
//You specify conditions to select only desired columns and rows 
//Output of filter operation is another DataFrame object that is usually passed on to some more operations 
//The following example selects the colors having a length of four or five only and label the column as "mid_length" 
filter 
------ 
Scala> color_df.filter(color_df("length").between(4,5)) 
       .select(color_df("color").alias("mid_length")).show() 
+----------+ 
|mid_length| 
+----------+ 
|     white| 
|     green| 
|     brown| 
|      pink| 
+----------+ 

//This example uses multiple filter criteria. Notice the not equal to operator having double equal to symbols  
Scala> color_df.filter(color_df("length") > 4).filter(color_df( "color")!=="white").show() 
+------+------+ 
| color|length| 
+------+------+ 
| green|     5| 
|yellow|     6| 
| brown|     5| 
+------+------+ 
//Sort the data on one or more columns 
sort 
---- 
//A simple single column sorting in default (ascending) order 
Scala> color_df..sort("color").show() 
+------+------+                                                                  
| color|length| 
+------+------+ 
| brown|     5| 
| green|     5| 
|  pink|     4| 
|   red|     3| 
| white|     5| 
|yellow|     6| 
+------+------+ 
//First filter colors of length more than 4 and then sort on multiple columns 
//The filtered rows are sorted first on the column length in default ascending order. Rows with same length are sorted on color in descending order  
Scala> color_df.filter(color_df("length")>=4).sort($"length", $"color".desc).show() 
+------+------+ 
| color|length| 
+------+------+ 
|  pink|     4| 
| white|     5| 
| green|     5| 
| brown|     5| 
|yellow|     6| 
+------+------+ 
//You can use orderBy instead, which is an alias to sort. 
scala> color_df.orderBy("length","color").take(4) 
res19: Array[org.apache.spark.sql.Row] = Array([red,3], [pink,4], [brown,5], [green,5]) 
//Alternative syntax, for single or multiple columns 
scala> color_df.sort(color_df("length").desc, color_df("color").asc).show() 
+------+------+ 
| color|length| 
+------+------+ 
|yellow|     6| 
| brown|     5| 
| green|     5| 
| white|     5| 
|  pink|     4| 
|   red|     3| 
+------+------+ 
//All the examples until now have been acting on one row at a time, filtering or transforming or reordering. 
//The following example deals with regrouping the data.  
//These operations require "wide dependency" and often involve shuffling. 
groupBy 
------- 
Scala> color_df.groupBy("length").count().show() 
+------+-----+ 
|length|count| 
+------+-----+ 
|     3|    1| 
|     4|    1| 
|     5|    3| 
|     6|    1| 
+------+-----+ 
//Data often contains missing information or null values.  
//The following json file has names of famous authors. Firstname data is missing in one row. 
dropna 
------ 
Scala> val df1 = sqlContext.read.json("./authors_missing.json") 
Scala> df1.show() 
+----------+---------+ 
|first_name|last_name| 
+----------+---------+ 
|      Mark|    Twain| 
|   Charles|  Dickens| 
|      null|    Hardy| 
+----------+---------+ 
//Let us drop the row with incomplete information 
Scala> val df2 = df1.na.drop() 
Scala> df2.show()  //Unwanted row is dropped 
+----------+---------+ 
|first_name|last_name| 
+----------+---------+ 
|      Mark|    Twain| 
|   Charles|  Dickens| 
+----------+---------+ 

```

## 在引擎盖下

您现在已经知道，数据框架应用编程接口是由 Spark SQL 授权的，Spark SQL 的催化剂优化器在优化性能方面起着至关重要的作用。

虽然查询执行缓慢，但它使用 Catalyst 的*目录*组件来识别程序或表达式中使用的列名是否存在于正在使用的表中以及数据类型是否正确，并且还采取许多其他类似的预防措施。这种方法的优点是，用户只要输入一个无效的表达式，就会弹出一个错误，而不是等到程序执行。

# 总结

在这一章中，我们解释了在 Spark 中开发 DataFrame API 背后的动机，以及在 Spark 中开发如何变得比以往任何时候都更容易。我们简要介绍了数据框架应用编程接口的设计方面，以及它是如何建立在 Spark SQL 之上的。我们讨论了从不同的数据源(如关系数据库、JSON、拼花和 JDBC)创建数据框架的各种方法。在这一章的最后，我们向您介绍了如何对数据帧执行操作。在接下来的章节中，我们将在数据科学和机器学习的背景下更详细地讨论数据框架操作。

在下一章中，我们将学习 Spark 如何支持统一数据访问，并详细讨论数据集和结构化流组件。

# 参考文献

Apache Spark 官方资源的 SQL 编程指南上的数据框架参考:

*   [https://spark . Apache . org/docs/latest/SQL-programming-guide . html # creating-data frames](https://spark.apache.org/docs/latest/sql-programming-guide.html#creating-dataframes)

数据框:在 Apache Spark 中为大规模数据科学引入数据框:

*   [https://databricks . com/blog/2015/02/17/introduction-data frames-in-spark-for-scale-data-science . html](https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html)

数据框:从熊猫到 ApacheSpark 的数据框:

*   [https://databricks . com/blog/2015/08/12/from-pandas-to-Apache-sparks-data frame . html](https://databricks.com/blog/2015/08/12/from-pandas-to-apache-sparks-dataframe.html)

Spark 数据帧的 Scala 应用编程接口参考指南:

*   [https://spark . Apache . org/docs/1 . 5 . 1/API/Java/org/Apache/spark/SQL/data frame . html](https://spark.apache.org/docs/1.5.1/api/java/org/apache/spark/sql/DataFrame.html)

Parquet 上的 Cloudera blog post——Apache Hadoop 的一种高效的通用柱状文件格式:

*   [http://blog . cloud era . com/blog/2013/03/introduction-parquet-column-storage-for-Apache-Hadoop/](http://blog.cloudera.com/blog/2013/03/introducing-parquet-columnar-storage-for-apache-hadoop/)