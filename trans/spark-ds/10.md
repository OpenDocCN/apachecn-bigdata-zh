# 第十章。把它们放在一起

大数据分析正在彻底改变企业的运营方式，并为一些迄今为止无法想象的机会铺平了道路。几乎每个企业、个人研究员或调查记者都有大量数据需要处理。我们需要一种简洁的方法，从原始数据开始，根据手头的问题得出有意义的见解。

在前几章中，我们已经使用 Apache Spark 介绍了数据科学的各个方面。我们开始讨论大数据分析需求以及 Apache spark 如何融入其中。渐渐地，我们研究了 Spark 编程模型、RDDs 和 DataFrame 抽象，并了解了 Spark 数据集如何支持统一数据访问以及连续应用程序的流方面。然后，我们使用 Apache Spark 和机器学习覆盖了数据分析生命周期的整个广度。我们在 Spark 上学习了结构化和非结构化数据分析，并为数据工程师和科学家以及业务用户探索了可视化方面。

前面讨论的所有章节帮助我们理解每章的一个简洁的方面。我们现在已经准备好穿越整个数据科学生命周期。在这一章中，我们将进行一个端到端的案例研究，并应用我们到目前为止所学的一切。我们不会引入任何新概念；这将有助于应用到目前为止获得的知识，并加强我们的理解。然而，我们已经重申了一些概念，没有过多的细节，使这一章自成一体。本章涵盖的主题与数据分析生命周期中的步骤大致相同:

*   快速回顾
*   介绍案例研究
*   构建业务问题
*   数据采集和数据清理
*   发展假设
*   数据探索

*   数据准备
*   模型结构
*   数据可视化
*   向业务用户传达结果
*   摘要

# 快速回顾一下

我们已经在不同的章节中详细讨论了典型数据科学项目中涉及的各个步骤。让我们快速浏览一下我们已经介绍过的内容，并触及一些重要的方面。下图显示了所涉及步骤的高级概述:

![A quick recap](graphics/image_10_001.jpg)

在前面的图示中，我们试图在更高的层次上解释数据科学项目中涉及的步骤，这些步骤大多是许多数据科学作业的通用步骤。更多的子步骤实际上存在于每个阶段，但可能因项目而异。

对于数据科学家来说，在开始时找到最佳的方法和步骤是非常困难的。通常，数据科学项目没有明确定义的生命周期，例如**软件开发生命周期** ( **SDLC** )。通常情况下，由于生命周期中的大多数步骤都是迭代的，数据科学项目会因为重复的延迟而陷入交付延迟。此外，跨团队的循环依赖会增加复杂性并导致执行延迟。然而，在从事大数据分析项目时，数据科学家遵循定义明确的数据科学工作流程是重要且有利的，无论不同的业务案例如何。这不仅有助于有组织的执行，也有助于我们专注于目标，因为数据科学项目在大多数情况下本质上是敏捷的。此外，建议您对任何给定项目的数据、领域和算法进行一定程度的研究。

在本章中，我们可能无法在单个流程中容纳所有的细化步骤，但将讨论重要的领域，以提醒您。我们将尝试查看一些不同的编码示例，这些示例我们在前面的章节中没有涉及到。

# 介绍案例研究

我们将在本章探讨奥斯卡奖的人口统计数据。你可以通过[https://www . crowdflow . com/WP-content/uploads/2016/03/Oscars-persistics-dfe . CSV](https://www.crowdflower.com/wp-content/uploads/2016/03/Oscars-demographics-DFE.csv)从 GitHub 资源库下载数据。

该数据集基于在[http://www.crowdflower.com/data-for-everyone](http://www.crowdflower.com/data-for-everyone)提供的数据。它包含人口统计细节，如种族、出生地和年龄。行数约为 400 行，可以在简单的家用电脑上轻松处理，因此您可以在 Spark 上执行数据科学项目时进行**概念验证** ( **概念验证**)。

只要从下载文件和检查数据开始。数据可能看起来很好，但当你仔细观察时，你会发现它并不“干净”。例如，出生日期列不遵循相同的格式。有些年份是两位数格式，而有些年份是四位数格式。出生地没有美国境内的国家/地区。

同样，你还会注意到数据看起来是倾斜的，更多的“白人”来自美国。但你可能会觉得，这种趋势已经向晚年转变了。到目前为止，您还没有使用任何工具或技术，只是快速浏览了一下数据。在数据科学的现实世界中，这种看似微不足道的活动在生命周期的更深处会很有帮助。你可以对手头的数据产生感觉，同时对数据进行假设。这将带您进入工作流程的第一步。

# 商业问题

如前所述，任何数据科学项目最重要的方面是手头的问题。对*有一个清晰的认识，我们试图解决什么问题？*这对项目的成功至关重要。它还驱动哪些被认为是相关数据，哪些不是。例如，在当前的案例研究中，如果我们想看的是人口统计数据，那么电影名和人名就无关紧要了。有时候，手边没有具体的问题！*然后呢？*即使在没有具体问题的时候，业务可能还是有一些客观的，或者数据科学家和领域专家可以一起合作，找到业务需要努力的领域。为了理解业务、功能、问题陈述或数据，数据科学家从“提问”开始。它不仅有助于定义工作流，而且有助于寻找合适的数据。

例如，如果业务重点是人口统计信息，则正式的业务问题陈述可以定义为:

*奥斯卡奖得主的种族和原籍国有什么影响？*

在现实场景中，这一步不会这么简单。制定正确的问题是数据科学家、策略团队、领域专家和项目所有者的集体责任。由于如果不符合目的，整个工作都是徒劳的，数据科学家必须咨询所有利益相关者，并试图从他们那里获取尽可能多的信息。然而，他们最终可能会获得无价的见解或“预感”。所有这些结合起来形成了初始假设的核心，也帮助数据科学家了解他们应该寻找什么。

在没有具体问题可供企业寻找答案的情况下，处理起来更有意思，但执行起来可能很复杂！

# 数据采集和数据清理

**数据采集**是顺理成章的下一步。它可能像从单个电子表格中选择数据一样简单，也可能是一个精心设计的几个月项目。数据科学家必须收集尽可能多的相关数据。“相关”是这里的关键词。记住，更相关的数据胜过聪明的算法。

我们已经介绍了如何从异构数据源中获取数据，并将其整合以形成一个单一的数据矩阵，因此在这里我们将不重复相同的基本原理。相反，我们从单一来源获取数据，并从中提取一个子集。

现在是时候查看数据并开始清理它了。本章中呈现的脚本往往比前面的例子更长，但仍然不是生产质量的手段。现实工作需要更多的异常检查和性能调整:

**斯卡拉**

```scala
//Load tab delimited file 
scala> val fp = "<YourPath>/Oscars.txt" 
scala> val init_data = spark.read.options(Map("header"->"true", "sep" -> "\t","inferSchema"->"true")).csv(fp) 
//Select columns of interest and ignore the rest 
>>> val awards = init_data.select("birthplace", "date_of_birth", 
        "race_ethnicity","year_of_award","award").toDF( 
         "birthplace","date_of_birth","race","award_year","award") 
awards: org.apache.spark.sql.DataFrame = [birthplace: string, date_of_birth: string ... 3 more fields] 
//register temporary view of this dataset 
scala> awards.createOrReplaceTempView("awards") 

//Explore data 
>>> awards.select("award").distinct().show(10,false) //False => do not truncate 
+-----------------------+                                                        
|award                  | 
+-----------------------+ 
|Best Supporting Actress| 
|Best Director          | 
|Best Actress           | 
|Best Actor             | 
|Best Supporting Actor  | 
+-----------------------+ 
//Check DOB quality. Note that length varies based on month name 
scala> spark.sql("SELECT distinct(length(date_of_birth)) FROM awards ").show() 
+---------------------+                                                          
|length(date_of_birth)| 
+---------------------+ 
|                   15| 
|                    9| 
|                    4| 
|                    8| 
|                   10| 
|                   11| 
+---------------------+ 

//Look at the value with unexpected length 4 Why cant we show values for each of the length type ?  
scala> spark.sql("SELECT date_of_birth FROM awards WHERE length(date_of_birth) = 4").show() 
+-------------+ 
|date_of_birth| 
+-------------+ 
|         1972| 
+-------------+ 
//This is an invalid date. We can either drop this record or give some meaningful value like 01-01-1972 

```

**蟒蛇**

```scala
    //Load tab delimited file
    >>> init_data = spark.read.csv("<YOURPATH>/Oscars.txt",sep="\t",header=True)
    //Select columns of interest and ignore the rest
    >>> awards = init_data.select("birthplace", "date_of_birth",
            "race_ethnicity","year_of_award","award").toDF(
             "birthplace","date_of_birth","race","award_year","award")
    //register temporary view of this dataset
    >>> awards.createOrReplaceTempView("awards")
    scala>
    //Explore data
    >>> awards.select("award").distinct().show(10,False) //False => do not truncate
    +-----------------------+                                                       
    |award                  |
    +-----------------------+
    |Best Supporting Actress|
    |Best Director          |
    |Best Actress           |
    |Best Actor             |
    |Best Supporting Actor  |
    +-----------------------+
    //Check DOB quality
    >>> spark.sql("SELECT distinct(length(date_of_birth)) FROM awards ").show()
    +---------------------+                                                         
    |length(date_of_birth)|
    +---------------------+
    |                   15|
    |                    9|
    |                    4|
    |                    8|
    |                   10|
    |                   11|
    +---------------------+
    //Look at the value with unexpected length 4\. Note that length varies based on month name
    >>> spark.sql("SELECT date_of_birth FROM awards WHERE length(date_of_birth) = 4").show()
    +-------------+
    |date_of_birth|
    +-------------+
    |         1972|
    +-------------+
    //This is an invalid date. We can either drop this record or give some meaningful value like 01-01-1972

```

前面的代码片段下载了一个制表符分隔的文本文件，将所需的列加载到 DataFrame 中，并注册了一个临时表。其余的代码与刚刚探索数据的基本 SQL 语句非常相似。

大多数数据集都包含一个`date`字段，除非它们来自一个单一的受控数据源，否则它们的格式很可能会有所不同，并且几乎总是需要清理的对象。

对于手边的数据集，你可能也注意到了 `date_of_birth`和`birthplace`需要大量的清理。下面的代码显示了两个**用户自定义函数** ( **UDFs** )分别清洁`date_of_birth`和`birthplace`。这些 UDF 一次只处理一个数据元素，它们只是普通的 Scala/Python 函数。应该注册这些用户定义的函数，以便可以在 SQL 语句中使用它们。最后一步是创建一个将参与进一步分析的干净数据帧。

请注意以下用于清理的逻辑`birthplace.`这是一个弱逻辑，因为我们假设任何以两个字符结尾的字符串都是美国州。我们必须将它们与一系列有效的缩写进行比较。同样，假设两位数的年份总是来自二十世纪是另一个容易出错的假设。根据不同的用例，数据科学家/数据工程师必须决定保留更多的行是重要的，还是只包括高质量的数据。所有此类决定都应清楚地记录在案，以供参考:

**斯卡拉:**

```scala
//UDF to clean date 
//This function takes 2 digit year and makes it 4 digit 
// Any exception returns an empty string 
scala> def fncleanDate(s:String) : String = {  
  var cleanedDate = "" 
  val dateArray: Array[String] = s.split("-") 
  try{    //Adjust year 
     var yr = dateArray(2).toInt 
     if (yr < 100) {yr = yr + 1900 } //make it 4 digit 
     cleanedDate = "%02d-%s-%04d".format(dateArray(0).toInt, 
                dateArray(1),yr) 
     } catch { case e: Exception => None } 
     cleanedDate } 
fncleanDate: (s: String)String 

```

**蟒蛇:**

```scala
    //This function takes 2 digit year and makes it 4 digit
    // Any exception returns an empty string
    >>> def fncleanDate(s):
          cleanedDate = ""
          dateArray = s.split("-")
          try:    //Adjust year
             yr = int(dateArray[2])
             if (yr < 100):
                  yr = yr + 1900 //make it 4 digit
             cleanedDate = "{0}-{1}-{2}".format(int(dateArray[0]),
                      dateArray[1],yr)
          except :
              None
          return cleanedDate

```

要清理的 UDF 日期接受带连字符的日期字符串并将其拆分。如果最后一个成分，即年份，是两位数长，那么它被认为是二十世纪的日期，并添加 1900 使其成为四位数格式。

如果国家/地区字符串是纽约市或最后一个组件是两个字符长，并且假定它是美国的一个州，则下面的 UDF 将该国家/地区附加为美国:

```scala
//UDF to clean birthplace 
// Data explorartion showed that  
// A. Country is omitted for USA 
// B. New York City does not have State code as well 
//This function appends country as USA if 
// A. the string contains New York City  (OR) 
// B. if the last component is of length 2 (eg CA, MA) 
scala> def fncleanBirthplace(s: String) : String = { 
        var cleanedBirthplace = "" 
        var strArray : Array[String] =  s.split(" ") 
        if (s == "New York City") 
           strArray = strArray ++ Array ("USA") 
        //Append country if last element length is 2 
        else if (strArray(strArray.length-1).length == 2) 
            strArray = strArray ++ Array("USA") 
        cleanedBirthplace = strArray.mkString(" ") 
        cleanedBirthplace } 

```

Python:

```scala
    >>> def fncleanBirthplace(s):
            cleanedBirthplace = ""
            strArray = s.split(" ")
            if (s == "New York City"):
                strArray += ["USA"]  //Append USA
            //Append country if last element length is 2
            elif (len(strArray[len(strArray)-1]) == 2):
                strArray += ["USA"]
            cleanedBirthplace = " ".join(strArray)
            return cleanedBirthplace

```

如果您想从选择字符串访问它们，应该注册 UDF:

**斯卡拉:**

```scala
//Register UDFs 
scala> spark.udf.register("fncleanDate",fncleanDate(_:String)) 
res10: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StringType,Some(List(StringType))) 
scala> spark.udf.register("fncleanBirthplace", fncleanBirthplace(_:String)) 
res11: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StringType,Some(List(StringType))) 

```

**蟒蛇:**

```scala
    >>> from pyspark.sql.types import StringType
    >>> sqlContext.registerFunction("cleanDateUDF",fncleanDate, StringType())
    >>> sqlContext.registerFunction( "cleanBirthplaceUDF",fncleanBirthplace, StringType())

```

使用 UDFs 清理数据帧。执行以下清理操作:

1.  调用 UDFs `fncleanDate`和`fncleanBirthplace`来固定出生地和国家。
2.  从`award_year`中减去出生年份，即可获得领奖时的`age`。
3.  保持`race`和`award`不变。

**斯卡拉:**

```scala
//Create cleaned data frame 
scala> var cleaned_df = spark.sql ( 
            """SELECT fncleanDate (date_of_birth) dob, 
               fncleanBirthplace(birthplace) birthplace, 
               substring_index(fncleanBirthplace(birthplace),' ',-1)  
                               country, 
               (award_year - substring_index(fncleanDate( date_of_birth),'-',-1)) age, race, award FROM awards""") 
cleaned_df: org.apache.spark.sql.DataFrame = [dob: string, birthplace: string ... 4 more fields] 

```

**蟒蛇:**

```scala
//Create cleaned data frame 
>>> from pyspark.sql.functions import substring_index>>> cleaned_df = spark.sql (            """SELECT cleanDateUDF (date_of_birth) dob,               cleanBirthplaceUDF(birthplace) birthplace,               substring_index(cleanBirthplaceUDF(birthplace),' ',-1) country,               (award_year - substring_index(cleanDateUDF( date_of_birth),               '-',-1)) age, race, award FROM awards""")
```

最后一行需要一些解释。UDF 的使用类似于 SQL 函数，表达式别名为有意义的名称。我们添加了一个计算列`age`，因为我们也想验证年龄的影响。`substring_index`函数在第一个参数中搜索第二个参数。`-1`表示从右侧寻找第一个事件。

# 发展假设

假设是你对结果的最佳猜测。您根据问题、与利益相关者的对话以及查看数据来形成您的初始假设。对于给定的问题，你可以形成一个或多个假设。这个初始假设作为一个路线图，指导你完成探索性分析。开发一个假设对于统计上批准或不批准一个陈述非常重要，而不仅仅是通过将数据视为一个数据矩阵，甚至通过视觉效果。这是因为我们仅仅通过观察数据建立起来的认知可能是不正确的，有时还会带有欺骗性。

现在你知道你的最终结果可能会也可能不会证明这个假设是正确的。来到我们在本课中考虑的案例研究，我们得出以下初步假设:

*   获奖者大多是白人
*   大多数获奖者来自美国
*   最佳男女演员往往比最佳导演年轻

现在，我们已经正式确定了我们的假设，我们都准备好继续生命周期中的下一步了..

# 数据探索

现在我们有了一个干净的数据框架，有了相关的数据和最初的假设，是时候真正探索我们所拥有的了。数据框架抽象提供了开箱即用的功能，例如`group by`供您查看。您可以将清理后的数据框注册为表，并运行经过时间测试的 SQL 语句来执行同样的操作。

这也是绘制一些图表的时候。可视化的这个阶段是数据可视化一章中提到的探索性分析。这种探索的目标很大程度上受到你从商业利益相关者那里获得的初始信息和假设的影响。换句话说，你与利益相关者的讨论有助于你知道要寻找什么。

有一些通用的指导方针适用于几乎所有的数据科学任务，但是对于不同的用例还是很主观的。让我们看看一些通用的:

*   寻找缺失的数据并加以处理。我们已经在[第 5 章](05.html "Chapter 5. Data Analysis on Spark")、*火花*数据分析中讨论了各种方法。
*   找出数据集中的异常值并加以处理。我们也讨论过这方面。请注意，在某些情况下，我们认为的异常值和正常数据点可能会根据用例而改变。
*   执行单变量分析，其中分别研究数据集中的每个变量。频率分布或百分位数分布很常见。也许绘制一些图表来获得更好的想法。这也将帮助您在进入数据建模之前准备数据。
*   验证你最初的假设。
*   检查数值数据的最小值和最大值。如果任何列中的变化太大，这可能是数据规范化或缩放的候选。

*   检查分类数据中的不同值(字符串值，如城市名称)及其频率。如果任何一列中有太多不同的值(即级别)，您可能必须寻找减少级别数量的方法。如果一个级别几乎总是发生，那么这个专栏并不能帮助模型区分可能的结果。此类列可能会被删除。在探索阶段，您只需找出这样的候选列，并让数据准备阶段处理实际操作。

在我们当前的数据集中，我们没有任何缺失的数据，也没有任何可能带来任何挑战的数字数据。但是，在处理无效日期时，一些缺失的值可能会悄悄出现。因此，下面的代码涵盖了剩余的操作项。该代码假设`cleaned_df`已经创建:

**Scala/Python:**

```scala
cleaned_df = cleaned_df.na.drop //Drop rows with missing values 
cleaned_df.groupBy("award","country").count().sort("country","award","count").show(4,False) 
+-----------------------+---------+-----+                                        
|award                  |country  |count| 
+-----------------------+---------+-----+ 
|Best Actor             |Australia|1    | 
|Best Actress           |Australia|1    | 
|Best Supporting Actor  |Australia|1    | 
|Best Supporting Actress|Australia|1    | 
+-----------------------+---------+-----+ 
//Re-register data as table 
cleaned_df.createOrReplaceTempView("awards") 
//Find out levels (distinct values) in each categorical variable 
spark.sql("SELECT count(distinct country) country_count, count(distinct race) race_count, count(distinct award) award_count from awards").show() 
+-------------+----------+-----------+                                           
|country_count|race_count|award_count| 
+-------------+----------+-----------+ 
|           34|         6|          5| 
+-------------+----------+-----------+ 

```

以下可视化对应于最初的假设。请注意，我们的两个假设被发现是正确的，但第三个不是。这些可视化是使用齐柏林飞船创建的:

![Data exploration](graphics/image_10_002.jpg)

请注意，所有的假设不能仅仅通过视觉来验证，因为它们有时可能是欺骗性的。因此，需要进行适当的统计检验，如 t 检验、方差分析、卡方检验、相关检验等。我们不会在这一部分详细讨论。详见[第五章](05.html "Chapter 5. Data Analysis on Spark")、*星火*数据分析。

# 数据准备

数据探索阶段帮助我们确定了在进入建模阶段之前需要解决的所有问题。每一个单独的问题都需要仔细思考和深思熟虑，以选择最佳的解决方案。以下是一些常见问题和可能的解决方法。最佳解决方案取决于手头的问题和/或业务环境。

## 分类变量中的级别太多

这是我们面临的最常见的问题之一。这个问题的处理取决于多种因素:

*   如果该列几乎总是唯一的，例如，它是事务标识或时间戳，那么它不参与建模，除非您从它派生新的特性。您可以安全地删除该列，而不会丢失任何信息内容。您通常会在数据清理阶段将其删除。
*   如果有可能用在当前上下文中有意义的粗粒度级别(例如，州或国家，而不是城市)来替换这些级别，那么通常这是解决这个问题的最佳方法。
*   您可能希望为每个不同的级别添加 0 或 1 值的虚拟列。例如，如果一列中有 100 个级别，则改为添加 100 列。在任何观察点(行)，一列最多有 1 个。这被称为**一键编码**，Spark 通过`ml.features`包提供开箱即用的编码。
*   另一个选择是保留最频繁的级别。你甚至可以将这些等级中的每一个附加到一个主导等级上，这个等级被认为是“更接近”这个等级。另外，你可以把剩下的捆成一个桶，比如说`Others`。
*   没有硬性规定绝对限制级别的数量。这取决于您在每个单独的特性中需要的粒度和性能限制。

当前数据集在分类变量`country`中有太多级别。我们选择保留最频繁的级别，并将剩余的级别捆绑到`Others`中:

**斯卡拉:**

```scala
//Country has too many values. Retain top ones and bundle the rest 
//Check out top 6 countries with most awards. 
scala> val top_countries_df = spark.sql("SELECT country, count(*) freq FROM awards GROUP BY country ORDER BY freq DESC LIMIT 6") 
top_countries_df: org.apache.spark.sql.DataFrame = [country: string, freq: bigint] 
scala> top_countries_df.show() 
+-------+----+                                                                   
|country|freq| 
+-------+----+ 
|    USA| 289| 
|England|  57| 
| France|   9| 
| Canada|   8| 
|  Italy|   7| 
|Austria|   7| 
+-------+----+ 
//Prepare top_countries list 
scala> val top_countries = top_countries_df.select("country").collect().map(x => x(0).toString) 
top_countries: Array[String] = Array(USA, England, New York City, France, Canada, Italy) 
//UDF to fix country. Retain top 6 and bundle the rest into "Others" 
scala> import org.apache.spark.sql.functions.udf 
import org.apache.spark.sql.functions.udf 
scala > val setCountry = udf ((s: String) => 
        { if (top_countries.contains(s)) {s} else {"Others"}}) 
setCountry: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StringType,Some(List(StringType))) 
//Apply udf to overwrite country 
scala> cleaned_df = cleaned_df.withColumn("country", setCountry(cleaned_df("country"))) 
cleaned_df: org.apache.spark.sql.DataFrame = [dob: string, birthplace: string ... 4 more fields] 

```

**蟒蛇:**

```scala
    //Check out top 6 countries with most awards.
    >>> top_countries_df = spark.sql("SELECT country, count(*) freq FROM awards GROUP BY country ORDER BY freq DESC LIMIT 6")
    >>> top_countries_df.show()
    +-------+----+                                                                  
    |country|freq|
    +-------+----+
    |    USA| 289|
    |England|  57|
    | France|   9|
    | Canada|   8|
    |  Italy|   7|
    |Austria|   7|
    +-------+----+
    >>> top_countries = [x[0] for x in top_countries_df.select("country").collect()]
    //UDF to fix country. Retain top 6 and bundle the rest into "Others"
    >>> from pyspark.sql.functions import udf
    >>> from pyspark.sql.types import StringType
    >>> setCountry = udf(lambda s: s if s in top_countries else "Others", StringType())
    //Apply UDF
    >>> cleaned_df = cleaned_df.withColumn("country", setCountry(cleaned_df["country"]))

```

## 变异过大的数值变量

有时数字数据值可能会变化几个数量级。比如，如果你看的是个人的年收入，可能相差很大。Z-score 规范化(标准化)和 min-max 缩放是处理此类数据的两种流行选择。Spark 在`ml.features`包中包含了这两种开箱即用的转换。

我们当前的数据集没有任何这样的变量。我们唯一的数字变量是年龄，它的值都是两位数。少了一个需要解决的问题。

请注意，并不总是需要对这些数据进行标准化。如果您正在比较两个不同尺度的两个变量，或者如果您正在使用聚类算法或 SVM 分类器，或者任何其他确实需要标准化数据的情况，您可以标准化数据。

### 缺失数据

这是一个令人关切的主要领域。任何缺少目标本身的观察结果都应该从训练数据中删除。剩余的观察值可以保留一些估算值，也可以根据要求删除。在输入缺少的值时，您应该非常小心；否则可能会导致误导输出！在连续变量的空白单元格中替换平均值似乎非常容易，但这可能不是正确的方法。

我们目前的案例研究没有任何缺失的数据，因此没有治疗的余地。然而，让我们看一个例子。

让我们假设您有一个正在处理的学生数据集，它包含从 1 班到 5 班的数据。如果有一些缺失的`Age`值，而你只是找到整列的平均值并替换，那么这将成为一个异常值，并可能导致模糊的结果。你可以选择只找出学生所在班级的平均值，然后估算这个值。这至少是一个更好的方法，但可能不是一个完美的方法。在大多数情况下，你还必须给其他变量赋予权重。如果你这样做，你可能最终会建立一个预测模型来寻找缺失的值，这可能是一个很好的方法！

### 连续数据

数值数据通常是连续的，必须离散化，因为它是某些算法的先决条件。它通常被分成不同的桶或数值范围。但是，在某些情况下，您可能不仅仅是根据数据的范围统一进行桶操作，您可能还需要考虑方差或标准偏差或任何其他适用的原因来进行适当的桶操作。现在，决定桶的数量也由数据科学家决定，但这也需要仔细分析。太少的桶会降低粒度，太多的桶与太多的分类级别差不多。在我们的案例研究中，`age`就是这样的数据的一个例子，我们需要将其离散化。我们把它分成不同的桶。例如，看看这个管道阶段，它将`age`转换为 10 个桶:

**斯卡拉:**

```scala
scala> val splits = Array(Double.NegativeInfinity, 35.0, 45.0, 55.0, 
          Double.PositiveInfinity) 
splits: Array[Double] = Array(-Infinity, 35.0, 45.0, 55.0, Infinity) 
scala> val bucketizer = new Bucketizer().setSplits(splits). 
                 setInputCol("age").setOutputCol("age_buckets") 
bucketizer: org.apache.spark.ml.feature.Bucketizer = bucketizer_a25c5d90ac14 

```

**蟒蛇:**

```scala
    >>> splits = [-float("inf"), 35.0, 45.0, 55.0,
                   float("inf")]
    >>> bucketizer = Bucketizer(splits = splits, inputCol = "age",
                        outputCol = "age_buckets")

```

### 分类数据

我们已经讨论了离散化连续数据并将其转换为类别或桶的必要性。我们还讨论了虚拟变量的引入，每个虚拟变量对应一个分类变量的不同值。还有一种更常见的数据准备实践，我们将分类级别转换为数字(离散)数据。这是必需的，因为许多机器学习算法处理数字数据、整数和实数，或者一些其他情况可能需要它。所以，我们需要把分类数据转换成数字数据。

这种方法可能有缺点。将一个顺序引入固有的无序数据有时可能不符合逻辑。例如，分别为颜色“红”、“绿”、“蓝”和“黑”分配数字(如 0、1、2、3)没有意义。这是因为我们不能说红色离“绿”有一个单位的距离，“绿”离“蓝”也有一个单位的距离！如果适用，引入虚拟变量在许多这样的情况下更有意义。

### 准备数据

讨论了常见问题和可能的修复后，让我们看看如何准备我们当前的数据集。我们已经介绍了太多级别问题相关的代码修复。以下示例显示了其余部分。它将所有要素转换为单个要素列。它还为测试模型留出了一些数据。该代码严重依赖于`ml.features`包，该包旨在支持数据准备阶段。请注意，这段代码只是定义了需要做什么。转换尚未执行。这些将成为随后定义的管道中的阶段。尽可能推迟执行，直到实际模型构建完成。Catalyst 优化器会找到实现管道的最佳路径:

**斯卡拉:**

```scala
//Define pipeline to convert categorical labels to numerical labels 
scala> import org.apache.spark.ml.feature.{StringIndexer, Bucketizer, VectorAssembler} 
import org.apache.spark.ml.feature.{StringIndexer, Bucketizer, VectorAssembler} 
scala> import org.apache.spark.ml.Pipeline 
import org.apache.spark.ml.Pipeline 
//Race 
scala> val raceIdxer = new StringIndexer(). 
           setInputCol("race").setOutputCol("raceIdx") 
raceIdxer: org.apache.spark.ml.feature.StringIndexer = strIdx_80eddaa022e6 
//Award (prediction target) 
scala> val awardIdxer = new StringIndexer(). 
         setInputCol("award").setOutputCol("awardIdx") 
awardIdxer: org.apache.spark.ml.feature.StringIndexer = strIdx_256fe36d1436 
//Country 
scala> val countryIdxer = new StringIndexer(). 
         setInputCol("country").setOutputCol("countryIdx") 
countryIdxer: org.apache.spark.ml.feature.StringIndexer = strIdx_c73a073553a2 

//Convert continuous variable age to buckets 
scala> val splits = Array(Double.NegativeInfinity, 35.0, 45.0, 55.0, 
          Double.PositiveInfinity) 
splits: Array[Double] = Array(-Infinity, 35.0, 45.0, 55.0, Infinity) 

scala> val bucketizer = new Bucketizer().setSplits(splits). 
                 setInputCol("age").setOutputCol("age_buckets") 
bucketizer: org.apache.spark.ml.feature.Bucketizer = bucketizer_a25c5d90ac14 

//Prepare numerical feature vector by clubbing all individual features 
scala> val assembler = new VectorAssembler().setInputCols(Array("raceIdx", 
          "age_buckets","countryIdx")).setOutputCol("features") 
assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_8cf17ee0cd60 

//Define data preparation pipeline 
scala> val dp_pipeline = new Pipeline().setStages( 
          Array(raceIdxer,awardIdxer, countryIdxer, bucketizer, assembler)) 
dp_pipeline: org.apache.spark.ml.Pipeline = pipeline_06717d17140b 
//Transform dataset 
scala> cleaned_df = dp_pipeline.fit(cleaned_df).transform(cleaned_df) 
cleaned_df: org.apache.spark.sql.DataFrame = [dob: string, birthplace: string ... 9 more fields] 
//Split data into train and test datasets 
scala> val Array(trainData, testData) = 
        cleaned_df.randomSplit(Array(0.7, 0.3)) 
trainData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [dob: string, birthplace: string ... 9 more fields] 
testData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [dob: string, birthplace: string ... 9 more fields] 

```

**蟒蛇:**

```scala
    //Define pipeline to convert categorical labels to numcerical labels
    >>> from pyspark.ml.feature import StringIndexer, Bucketizer, VectorAssembler
    >>> from pyspark.ml import Pipelin
    //Race
    >>> raceIdxer = StringIndexer(inputCol= "race", outputCol="raceIdx")
    //Award (prediction target)
    >>> awardIdxer = StringIndexer(inputCol = "award", outputCol="awardIdx")
    //Country
    >>> countryIdxer = StringIndexer(inputCol = "country", outputCol = "countryIdx")

    //Convert continuous variable age to buckets
    >>> splits = [-float("inf"), 35.0, 45.0, 55.0,
                   float("inf")]
    >>> bucketizer = Bucketizer(splits = splits, inputCol = "age",
                        outputCol = "age_buckets")
    >>>
    //Prepare numerical feature vector by clubbing all individual features
    >>> assembler = VectorAssembler(inputCols = ["raceIdx", 
              "age_buckets","countryIdx"], outputCol = "features")

    //Define data preparation pipeline
    >>> dp_pipeline = Pipeline(stages = [raceIdxer,
             awardIdxer, countryIdxer, bucketizer, assembler])
    //Transform dataset
    >>> cleaned_df = dp_pipeline.fit(cleaned_df).transform(cleaned_df)
    >>> cleaned_df.columns
    ['dob', 'birthplace', 'country', 'age', 'race', 'award', 'raceIdx', 'awardIdx', 'countryIdx', 'age_buckets', 'features']

    //Split data into train and test datasets
    >>> trainData, testData = cleaned_df.randomSplit([0.7, 0.3])

```

执行完所有数据准备活动后，您将得到一个完全数字的数据，没有缺失值，并且每个属性都有可管理的级别。您可能已经删除了任何可能不会给现有分析增加多少价值的属性。这就是我们所说的**最终数据矩阵**。现在，您已经准备好开始对数据建模了。因此，首先您将源数据分为训练数据和测试数据。使用训练数据“训练”模型，使用测试数据“测试”模型。请注意，拆分是随机的，如果您重做拆分，可能会得到不同的训练和测试分区。

# 模型搭建

模型是事物的表示，是对现实的渲染或描述。就像物理建筑的模型一样，数据科学模型试图理解现实；在这种情况下，现实是特征和预测变量之间的潜在关系。它们可能不是 100%准确，但仍然非常有助于根据数据深入了解我们的业务领域。

有几种机器学习算法可以帮助我们对数据进行建模，Spark 提供了许多现成的算法。然而，构建哪个模型仍然是一个百万美元的问题。这取决于各种因素，如可解释性-准确性的权衡、手头有多少数据、分类或数字变量、时间和内存限制等。在下面的代码示例中，我们只是随机训练了几个模型，向您展示了如何做到这一点。

我们将根据种族、年龄和国家预测奖项类型。我们将使用决策树分类器、随机森林分类器和 OneVsRest 算法。这三个是任意选择的。它们都使用多类标签，并且简单易懂。我们使用了`ml`包提供的以下评估指标:

*   **精度**:正确预测观测值的比率。

*   **加权精度**:精度是正确的正观测值与所有正观测值的比值。加权精度考虑了单个类的频率。
*   **加权召回**:召回是阳性与实际阳性的比值。实际阳性是真阳性和假阴性的总和。加权召回考虑了单个类别的频率。
*   **F1** :默认评价尺度。这是精度和召回率的加权平均值。

**斯卡拉:**

```scala
scala> import org.apache.spark.ml.Pipeline 
import org.apache.spark.ml.Pipeline 
scala> import org.apache.spark.ml.classification.DecisionTreeClassifier 
import org.apache.spark.ml.classification.DecisionTreeClassifier 

//Use Decision tree classifier 
scala> val dtreeModel = new DecisionTreeClassifier(). 
           setLabelCol("awardIdx").setFeaturesCol("features"). 
           fit(trainData) 
dtreeModel: org.apache.spark.ml.classification.DecisionTreeClassificationModel = DecisionTreeClassificationModel (uid=dtc_76c9e80680a7) of depth 5 with 39 nodes 

//Run predictions using testData 
scala> val dtree_predictions = dtreeModel.transform(testData) 
dtree_predictions: org.apache.spark.sql.DataFrame = [dob: string, birthplace: string ... 12 more fields] 

//Examine results. Your results may vary due to randomSplit 
scala> dtree_predictions.select("award","awardIdx","prediction").show(4) 
+--------------------+--------+----------+ 
|               award|awardIdx|prediction| 
+--------------------+--------+----------+ 
|       Best Director|     1.0|       1.0| 
|        Best Actress|     0.0|       0.0| 
|        Best Actress|     0.0|       0.0| 
|Best Supporting A...|     4.0|       3.0| 
+--------------------+--------+----------+ 

//Compute prediction mismatch count 
scala> dtree_predictions.filter(dtree_predictions("awardIdx") =!= dtree_predictions("prediction")).count() 
res10: Long = 88 
scala> testData.count 
res11: Long = 126 
//Predictions match with DecisionTreeClassifier model is about 30% ((126-88)*100/126) 

//Train Random forest 
scala> import org.apache.spark.ml.classification.RandomForestClassifier 
import org.apache.spark.ml.classification.RandomForestClassifier 
scala> import org.apache.spark.ml.classification.RandomForestClassificationModel 
import org.apache.spark.ml.classification.RandomForestClassificationModel 
scala> import org.apache.spark.ml.feature.{StringIndexer, IndexToString, VectorIndexer} 
import org.apache.spark.ml.feature.{StringIndexer, IndexToString, VectorIndexer} 

//Build model 
scala> val RFmodel = new RandomForestClassifier(). 
        setLabelCol("awardIdx"). 
        setFeaturesCol("features"). 
        setNumTrees(6).fit(trainData) 
RFmodel: org.apache.spark.ml.classification.RandomForestClassificationModel = RandomForestClassificationModel (uid=rfc_c6fb8d764ade) with 6 trees 
//Run predictions on the same test data using Random Forest model 
scala> val RF_predictions = RFmodel.transform(testData) 
RF_predictions: org.apache.spark.sql.DataFrame = [dob: string, birthplace: string ... 12 more fields] 
//Check results 
scala> RF_predictions.filter(RF_predictions("awardIdx") =!= RF_predictions("prediction")).count() 
res29: Long = 87 //Roughly the same as DecisionTreeClassifier 

//Try OneVsRest Logistic regression technique 
scala> import org.apache.spark.ml.classification.{LogisticRegression, OneVsRest} 
import org.apache.spark.ml.classification.{LogisticRegression, OneVsRest} 
//This model requires a base classifier 
scala> val classifier = new LogisticRegression(). 
            setLabelCol("awardIdx"). 
            setFeaturesCol("features"). 
            setMaxIter(30). 
            setTol(1E-6). 
            setFitIntercept(true) 
classifier: org.apache.spark.ml.classification.LogisticRegression = logreg_82cd24368c87 

//Fit OneVsRest model 
scala> val ovrModel = new OneVsRest(). 
           setClassifier(classifier). 
           setLabelCol("awardIdx"). 
           setFeaturesCol("features"). 
           fit(trainData) 
ovrModel: org.apache.spark.ml.classification.OneVsRestModel = oneVsRest_e696c41c0bcf 
//Run predictions 
scala> val OVR_predictions = ovrModel.transform(testData) 
predictions: org.apache.spark.sql.DataFrame = [dob: string, birthplace: string ... 10 more fields] 
//Check results 
scala> OVR_predictions.filter(OVR_predictions("awardIdx") =!= OVR_predictions("prediction")).count()          
res32: Long = 86 //Roughly the same as other models 

```

**蟒蛇:**

```scala
    >>> from pyspark.ml import Pipeline
    >>> from pyspark.ml.classification import DecisionTreeClassifier

    //Use Decision tree classifier
    >>> dtreeModel = DecisionTreeClassifier(labelCol = "awardIdx", featuresCol="features").fit(trainData)

    //Run predictions using testData
    >>> dtree_predictions = dtreeModel.transform(testData)

    //Examine results. Your results may vary due to randomSplit
    >>> dtree_predictions.select("award","awardIdx","prediction").show(4)
    +--------------------+--------+----------+
    |               award|awardIdx|prediction|
    +--------------------+--------+----------+
    |       Best Director|     1.0|       4.0|
    |       Best Director|     1.0|       1.0|
    |       Best Director|     1.0|       1.0|
    |Best Supporting A...|     4.0|       3.0|
    +--------------------+--------+----------+

    >>> dtree_predictions.filter(dtree_predictions["awardIdx"] != dtree_predictions["prediction"]).count()
    92
    >>> testData.count()
    137
    >>>
    //Predictions match with DecisionTreeClassifier model is about 31% ((133-92)*100/133)

    //Train Random forest
    >>> from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel
    >>> from pyspark.ml.feature import StringIndexer, IndexToString, VectorIndexer
    >>> from pyspark.ml.evaluation import MulticlassClassificationEvaluator

    //Build model
    >>> RFmodel = RandomForestClassifier(labelCol = "awardIdx", featuresCol = "features", numTrees=6).fit(trainData)

    //Run predictions on the same test data using Random Forest model
    >>> RF_predictions = RFmodel.transform(testData)
    //Check results
    >>> RF_predictions.filter(RF_predictions["awardIdx"] != RF_predictions["prediction"]).count()
    94     //Roughly the same as DecisionTreeClassifier

    //Try OneVsRest Logistic regression technique
    >>> from pyspark.ml.classification import LogisticRegression, OneVsRest

    //This model requires a base classifier
    >>> classifier = LogisticRegression(labelCol = "awardIdx", featuresCol="features",
                  maxIter = 30, tol=1E-6, fitIntercept = True)
    //Fit OneVsRest model
    >>> ovrModel = OneVsRest(classifier = classifier, labelCol = "awardIdx",
                    featuresCol = "features").fit(trainData)
    //Run predictions
    >>> OVR_predictions = ovrModel.transform(testData)
    //Check results
    >>> OVR_predictions.filter(OVR_predictions["awardIdx"] != OVR_predictions["prediction"]).count()
    90  //Roughly the same as other models

```

到目前为止，我们已经尝试了几个模型，发现它们给我们的性能大致相同。还有各种其他方法来验证模型性能。这同样取决于您所使用的算法、业务环境和产生的结果。让我们来看看`spark.ml.evaluation`包中开箱即用的一些指标:

**斯卡拉:**

```scala
scala> import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator 
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator 
//F1 
scala> val f1_eval = new MulticlassClassificationEvaluator(). 
                     setLabelCol("awardIdx") //Default metric is F1 
f1_eval: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_e855a949bb0e 

//WeightedPrecision 
scala> val wp_eval = new MulticlassClassificationEvaluator(). 
                     setMetricName("weightedPrecision").setLabelCol("awardIdx") 
wp_eval: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_44fd64e29d0a 

//WeightedRecall 
scala> val wr_eval = new MulticlassClassificationEvaluator(). 
                     setMetricName("weightedRecall").setLabelCol("awardIdx") 
wr_eval: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_aa341966305a 
//Compute measures for all models 
scala> val f1_eval_list = List (dtree_predictions, RF_predictions, OVR_predictions) map ( 
           x => f1_eval.evaluate(x)) 
f1_eval_list: List[Double] = List(0.2330854098674473, 0.2330854098674473, 0.2330854098674473) 
scala> val wp_eval_list = List (dtree_predictions, RF_predictions, OVR_predictions) map ( 
           x => wp_eval.evaluate(x)) 
wp_eval_list: List[Double] = List(0.2661599224979506, 0.2661599224979506, 0.2661599224979506) 

scala> val wr_eval_list = List (dtree_predictions, RF_predictions, OVR_predictions) map ( 
           x => wr_eval.evaluate(x)) 
wr_eval_list: List[Double] = List(0.31746031746031744, 0.31746031746031744, 0.31746031746031744) 

```

**蟒蛇:**

```scala
    >>> from pyspark.ml.evaluation import MulticlassClassificationEvaluator

    //F1
    >>> f1_eval = MulticlassClassificationEvaluator(labelCol="awardIdx") //Default metric is F1
    //WeightedPrecision
    >>> wp_eval = MulticlassClassificationEvaluator(labelCol="awardIdx", metricName="weightedPrecision")
    //WeightedRecall
    >>> wr_eval = MulticlassClassificationEvaluator(labelCol="awardIdx", metricName="weightedRecall")
    //Accuracy
    >>> acc_eval = MulticlassClassificationEvaluator(labelCol="awardIdx", metricName="Accuracy")
    //Compute measures for all models
    >>> f1_eval_list = [ f1_eval.evaluate(x) for x in [dtree_predictions, RF_predictions, OVR_predictions]]
    >>> wp_eval_list = [ wp_eval.evaluate(x) for x in [dtree_predictions, RF_predictions, OVR_predictions]]
    >>> wr_eval_list = [ wr_eval.evaluate(x) for x in [dtree_predictions, RF_predictions, OVR_predictions]]
    //Print results for DecisionTree, Random Forest and OneVsRest
    >>> f1_eval_list
    [0.2957949866055487, 0.2645186821042419, 0.2564967990214734]
    >>> wp_eval_list
    [0.3265407181548341, 0.31914852065228005, 0.25295826631254753]
    >>> wr_eval_list
    [0.3082706766917293, 0.2932330827067669, 0.3233082706766917]

```

**输出:**

<colgroup><col> <col> <col> <col></colgroup> 
|  | **决策树** | **随机森林** | onevsrest |
| 子一代 | 0.29579 | 0.26451 | 0.25649 |
| 加权精度 | 0.32654 | 0.26451 | 0.25295 |
| 加权积分 | 0.30827 | 0.29323 | 0.32330 |

验证模型性能后，您必须尽可能地调整模型。现在，调优可以双向进行，在数据级别和算法级别。提供算法期望的正确数据非常重要。问题是，无论你输入什么数据，算法可能还是会给出一些输出——它从不抱怨！因此，除了通过处理缺失值、处理单变量和多变量异常值等方式适当地清理数据之外，您还可以创建更多相关的特征。这种特征工程通常被视为数据科学最重要的方面。拥有合适的领域专业知识有助于设计更好的特性。现在，谈到优化的算法方面，我们总是有机会优化传递给算法的参数。您可以选择使用网格搜索来查找最佳参数。此外，数据科学家应该问自己使用哪个损失函数和为什么，以及在 GD、SGD、L-BFGS 等中，使用哪种算法来优化损失函数和为什么。

请注意，前面的方法只是为了演示如何在 Spark 上执行这些步骤。只看准确度来选择一种算法可能不是最好的方法。选择算法取决于您处理的数据类型、结果变量、业务问题/需求、计算挑战、可解释性以及许多其他因素。

# 数据可视化

**数据可视化**是从你承担数据科学任务时起就不时需要的东西。在构建任何模型之前，最好是，您必须可视化每个变量，以查看它们的分布，从而了解它们的特征，并找到异常值，以便您可以处理它们。简单的工具，如散点图、箱线图、条形图等，是一些通用的、方便的工具。此外，你必须在大多数步骤中使用视觉效果，以确保你朝着正确的方向前进。

每当您想要与业务用户或利益相关者合作时，通过视觉传达您的分析总是一种好的做法。视觉可以以更有意义的方式容纳更多的数据，本质上是直观的。

请注意，大多数数据科学任务的结果最好通过视觉效果和仪表盘呈现给业务用户。我们已经有专门关于这个主题的一章，所以我们不会深入探讨。

# 将结果传达给业务用户

在现实生活场景中，大多数情况下，您不得不断断续续地与业务部门保持沟通。在最终的生产就绪模型结束之前，您可能需要构建几个模型，并将结果传达给业务部门。

一个可实现的模型并不总是依赖于准确性；你可能需要引入其他的测量方法，例如灵敏度、特异性或 ROC 曲线，并且通过视觉效果，例如增益/提升图或具有统计学意义的 K-S 测试的输出来表示你的结果。请注意，这些技术需要业务用户的输入。该输入通常会指导您构建模型或设置阈值的方式。让我们看几个例子来更好地理解它是如何工作的:

*   如果一个回归者预测了一个事件发生的概率，那么盲目地将阈值设置为 0.5，假设任何高于 0.5 的都是 1，低于 0.5 的都是 0 可能不是最好的方法！你可以使用 ROC 曲线，做出更科学或更符合逻辑的决定。
*   癌症检测诊断的假阴性预测可能根本不可取！这是一个极端的生命危险案例。

*   与发送硬拷贝相比，电子邮件竞选更便宜。因此，企业可能会决定向预测概率小于 0.5(比如 0.35)的收件人发送电子邮件。

请注意，前面的决策很大程度上受业务用户或问题所有者的影响，数据科学家与他们密切合作来处理这类情况。

正如已经讨论过的，正确的视觉效果是向企业传达结果的最佳方式。

# 总结

在本章中，我们进行了一个案例研究，并完成了端到端的数据分析生命周期。在构建数据产品的过程中，我们已经应用了前面章节中所获得的知识。我们陈述了一个业务问题，形成了一个初始假设，获取了数据，并为模型构建做好了准备。我们尝试建立多个模型，找到了一个合适的模型。

在下一章，也是最后一章，我们将讨论使用 Spark 构建真实世界的应用程序。

# 参考文献

[http://www2.sas.com/proceedings/forum2007/073-2007.pdf](http://www2.sas.com/proceedings/forum2007/073-2007.pdf)。

[https://azure . Microsoft . com/en-in/documentation/articles/machine-learning-algorithm-choice/](https://azure.microsoft.com/en-in/documentation/articles/machine-learning-algorithm-choice/)。

[http://www . cs . Cornell . edu/courses/cs 578/2003 fa/performance _ measures . pdf](http://www.cs.cornell.edu/courses/cs578/2003fa/performance_measures.pdf)。