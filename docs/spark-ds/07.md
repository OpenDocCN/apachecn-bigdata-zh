# 七、使用 SparkR 扩展 Spark

统计学家和数据科学家一直在使用 R 来解决几乎每个领域的挑战性问题，从生物信息学到竞选活动。他们更喜欢 R，因为它强大的可视化能力、强大的社区以及丰富的统计和机器学习包生态系统。世界各地的许多学术机构使用 R 语言教授数据科学和统计学。

r 最初是由统计学家在 20 世纪 90 年代中期创建的，目的是提供一种更好、更方便用户的数据分析方法。r 最初用于学术和研究。随着企业越来越意识到数据科学在其业务增长中的作用，在企业部门使用 R 的数据分析师的数量也开始增长。R 语言的用户群在存在了二十年后，被认为超过了两百万。

所有这些成功背后的驱动因素之一是，R 的设计目的是让分析师的生活更轻松，而不是电脑。r 本质上是单线程的，它只能处理完全适合单台机器内存的数据集。但是现在，R 用户正在处理越来越大的数据集。在成熟的 R 语言下，现代分布式处理能力的无缝集成使数据科学家能够充分利用两者的优势。他们可以跟上不断增长的业务需求，并继续受益于他们最喜欢的 R 语言的灵活性。

本章介绍了 SparkR，这是一个为 R 程序员设计的 R API，这样他们就可以利用 Spark 的力量，而不用学习一门新的语言。由于已经假设了 R、R Studio 和数据分析技能的先前知识，本章不试图介绍 R。作为快速回顾，提供了对 Spark 计算引擎的非常简短的概述。读者应该阅读本书的前三章，以更深入地了解 Spark 编程模型和数据框架。这些知识非常重要，因为开发人员必须了解他的代码的哪一部分在本地 R 环境中执行，哪一部分由 Spark 计算引擎处理。本章涵盖的主题如下:

*   SparkR 基础知识
*   带 Spark 的 R 的优点及其局限性
*   使用 SparkR 编程
*   迷你图数据框
*   机器学习

# Spark 基础

r 是一种用于统计计算和图形的语言和环境。SparkR 是一个 R 包，它提供了一个轻量级的前端，使 Apache Spark 能够从 R 访问。SparkR 的目标是结合 R 环境提供的灵活性和易用性以及 Spark 计算引擎提供的可扩展性和容错性。在讨论 SparkR 如何实现其目标之前，让我们回顾一下 Spark 架构。

Apache Spark 是一个快速、通用、容错的框架，用于大型分布式数据集上的交互式和迭代计算。它支持多种数据源和存储层。它提供统一的数据访问，以组合不同的数据格式，流式传输数据，并使用高级、可组合的运算符定义复杂的操作。您可以使用 Scala、Python 或 R 外壳(或没有外壳的 Java)交互式开发应用程序。您可以将它部署在您的家庭桌面上，也可以在数千个节点的大型集群上运行它，处理数千兆字节的数据。

### 注

SparkR 起源于 AMPLab([https://amplab.cs.berkeley.edu/](https://amplab.cs.berkeley.edu/))探索不同的技术，将 R 的可用性与 Spark 的可扩展性相结合。它在 2015 年 6 月发布的 Apache Spark 1.4 中作为 alpha 组件发布。Spark 1.5 版本提高了 R 可用性，并引入了带有**广义线性模型** ( **GLMs** )的 MLlib 机器学习包。发生在 2016 年 1 月的 Spark 1.6 版本增加了一些更多的特性，比如模型总结和特性交互。发生在 2016 年 7 月的 Spark 2.0 版本带来了几个重要的特性，比如 UDF、改进的模型覆盖、数据框窗口函数 API 等等。

## 从 R 环境访问迷你图

可以从 R shell 或 R Studio 启动 SparkR。SparkR 的入口点是 SparkSession 对象，它表示与 Spark 集群的连接。运行 R 的节点成为驱动程序。R 程序创建的任何对象都驻留在这个驱动程序上。通过迷你会话创建的任何对象都是在集群中的工作节点上创建的。下图描述了 R 与运行在集群上的 Spark 交互的运行时视图。请注意，R 解释器存在于集群中的每个工作节点上。下图没有显示群集管理器，也没有显示存储层。您可以使用任何集群管理器(例如，纱或介子)和任何存储选项，如 HDFS、Cassandra 或亚马逊 S3:

![Accessing SparkR from the R environment](img/image_07_001.jpg)

来源:http://www . slide share . net/Hadoop _ Summit/w-145 p 210-avenkataraman。

SparkSession 对象是通过传递应用程序名称、内存、内核数量和要连接的集群管理器等信息来创建的。与 Spark 引擎的任何交互都是通过这个 Spark 会话对象启动的。如果您使用 SparkR shell，已经为您创建了一个 SparkSession 对象。否则，您必须显式创建它。该对象替换了 SparkContext 和 SQLContext 对象，这些对象存在于 Spark 1.x 版本中。为了向后兼容，这些对象仍然存在。甚至上图也描述了 SparkContext，在 Spark 2.0 之后，您应该将其视为 SparkSession。

既然我们已经理解了如何从 R 环境访问 Spark，那么让我们来检查 Spark 引擎提供的核心数据抽象。

## RDDs 和数据帧

Spark 引擎的核心是其主要的数据抽象，称为**弹性分布式数据集** ( **RDD** )。RDD 由一个或多个数据源组成，并由用户定义为一个或多个稳定(具体)数据源上的一系列转换(又称谱系)。每个 RDD 或 RDD 分区都知道如何使用沿袭图在故障时重建自己，从而提供容错能力。RDD 是一个不可变的数据结构，这意味着它可以在线程之间共享，没有同步开销，因此可以并行化。RDDs 上的操作要么是转换，要么是动作。转换是谱系中的单个步骤。换句话说，它们是创建 RDD 的操作，因为每次转换都是从稳定的数据源获取数据，或者转换一个不可变的 RDD 并创建另一个 RDD。转换只是声明；在对该 RDD 实施*行动*操作之前，不会对其进行评估。动作是利用关系数据库的操作。

Spark 基于当前的操作优化了 RDD 计算。例如，如果操作是读取第一行，则只计算一个分区，跳过其余的分区。它自动执行内存中的计算，性能下降很小(内存不足时会溢出到磁盘上)，并在所有内核之间分配处理。如果 RDD 在程序逻辑中被频繁访问，您可以缓存它，从而避免重新计算开销。

R 语言提供了一种称为*数据帧*的二维数据结构，这使得数据操作变得方便。Apache Spark 自带数据框，灵感来自 R 和 Python 中的数据框(通过 Pandas)。Spark DataFrame 是建立在 RDD 数据结构抽象之上的专用数据结构。它提供了分布式数据框架实现，从开发人员的角度来看，它与 R 数据框架非常相似，同时可以支持非常大的数据集。Spark 数据集应用编程接口为数据帧添加了结构，这种结构为幕后的更多优化提供了信息。

## 开始

现在我们已经理解了底层数据结构和运行时视图，是时候运行一些命令了。在本节中，我们假设您已经成功安装了 R 和 Spark 并将其添加到路径中。我们还假设设置了`SPARK_HOME`环境变量。让我们看看如何从 R shell 或 R Studio 访问 SparkR:

```scala
> R  // Start R shell  
> Sys.getenv("SPARK_HOME") //Confirm SPARK_HOME is set 
  <Your SPARK_HOME path> 
> library(SparkR, lib.loc = 
    c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"))) 

Attaching package: 'SparkR' 
The following objects are masked from 'package:stats': 

    cov, filter, lag, na.omit, predict, sd, var, window 

The following objects are masked from 'package:base': 

    as.data.frame, colnames, colnames<-, drop, endsWith, intersect, 
    rank, rbind, sample, startsWith, subset, summary, transform, union 
> 

> //Try help(package=SparkR) if you want to more information 
//initialize SparkSession object 
>  sparkR.session()  
Java ref type org.apache.spark.sql.SparkSession id 1  
> 
Alternatively, you may launch sparkR shell which comes with predefined SparkSession. 

> bin/sparkR  // Start SparkR shell  
>      // For simplicity sake, no Log messages are shown here 
> //Try help(package=SparkR) if you want to more information 
> 

```

这就是从 R 环境中访问 Spark 数据帧的能力所需要做的一切。

# 优点和局限性

R 语言长期以来一直是数据科学家的通用语言。它简单易懂的数据框架抽象、富于表现力的 API 和充满活力的包生态系统正是分析师所需要的。主要挑战在于可扩展性。SparkR 通过在不离开 R 生态系统的情况下提供分布式内存数据帧来弥补这一差距。这样的共生关系可以让用户获得以下好处:

*   分析师没有必要学习一门新的语言
*   迷你程序接口类似于迷你程序接口
*   您可以从 R studio 访问 SparkR，以及自动完成功能
*   对非常大的数据集执行交互式探索性分析不再受到内存限制或周转时间长的阻碍
*   从不同类型的数据源访问数据变得更加容易。大多数以前势在必行的任务都变成了声明性的。查看[第 4 章](http://Chapter%204)、*统一数据访问*，了解更多信息
*   您可以自由混合 dplyr，如 Spark 函数、SQL 和 R 库，这些在 Spark 中仍然不可用

尽管结合两个世界的优点有很多令人兴奋的优点，但这种结合仍然有一些局限性。这些限制可能不会影响每一个用例，但是我们无论如何都需要意识到它们:

*   R 固有的动态特性限制了催化剂优化器可用的信息。与 Scala 这样的静态类型语言相比，我们可能无法充分利用谓词推回等优化。
*   SparkR 不支持其他 API(如 Scala API)中已经提供的所有机器学习算法。

总之，使用 Spark 进行数据预处理，使用 R 进行分析和可视化似乎是近期最好的方法。

# 使用 SparkR 编程

到目前为止，我们已经理解了 SparkR 的运行时模型和提供容错和可伸缩性的基本数据抽象。我们已经了解了如何从 R shell 或 R studio 访问 Spark API。是时候尝试一些基本且熟悉的操作了:

```scala
> 
> //Open the shell 
> 
> //Try help(package=SparkR) if you want to more information 
> 
> df <- createDataFrame(iris) //Create a Spark DataFrame 
> df    //Check the type. Notice the column renaming using underscore 
SparkDataFrame[Sepal_Length:double, Sepal_Width:double, Petal_Length:double, Petal_Width:double, Species:string] 
> 
> showDF(df,4) //Print the contents of the Spark DataFrame 
+------------+-----------+------------+-----------+-------+ 
|Sepal_Length|Sepal_Width|Petal_Length|Petal_Width|Species| 
+------------+-----------+------------+-----------+-------+ 
|         5.1|        3.5|         1.4|        0.2| setosa| 
|         4.9|        3.0|         1.4|        0.2| setosa| 
|         4.7|        3.2|         1.3|        0.2| setosa| 
|         4.6|        3.1|         1.5|        0.2| setosa| 
+------------+-----------+------------+-----------+-------+ 
>  
> head(df,2)  //Returns an R data.frame. Default 6 rows 
  Sepal_Length Sepal_Width Petal_Length Petal_Width Species 
1          5.1         3.5          1.4         0.2  setosa 
2          4.9         3.0          1.4         0.2  setosa 
> //You can use take(df,2) to get the same results 
//Check the dimensions 
> nrow(df) [1] 150 > ncol(df) [1] 5 

```

这些操作看起来非常类似于 R 数据帧函数，因为 spark 数据帧是基于 R 数据帧和 Python(熊猫)数据帧建模的。但是如果你不小心，这种相似性可能会造成混乱。你可能会因为在一个 R `data.frame`上运行一个计算密集型函数而意外阻塞你的本地机器，以为负载会被分配。例如，intersect 函数在两个包中具有相同的签名。你需要注意对象是属于类`SparkDataFrame`(Spark 数据框)还是`data.frame` (R 数据框)。您还需要最小化本地 R `data.frame`对象和 Spark 数据框对象之间的来回转换。让我们通过一些例子来感受一下这种区别:

```scala
> 
> //Open the SparkR shell 
> df <- createDataFrame(iris) //Create a Spark DataFrame 
> class(df) [1] "SparkDataFrame" attr(,"package") [1] "SparkR" 
> df2 <- head(df,2) //Create an R data frame 
> class(df2) 
 [1] "data.frame" 
> //Now try running some R command on both data frames 
> unique(df2$Species)   //Works fine as expected [1] "setosa" > unique(df$Species)    //Should fail Error in unique.default(df$Species) : unique() applies only to vectors > class(df$Species)   //Each column is a Spark's Column class [1] "Column" attr(,"package") [1] "SparkR" > class(df2$Species) [1] "character" 

```

## 功能名称屏蔽

现在我们已经尝试了一些基本操作，让我们离题一点。我们必须了解当一个加载的库与基本包或其他已经加载的包有重叠的函数名时会发生什么。这有时被称为函数名重叠、函数屏蔽或名称冲突。您可能已经注意到在加载 SparkR 包时提到被屏蔽的对象的消息。这对于加载到 R 环境中的任何包来说都是常见的，并且不仅限于 SparkR。如果 R 环境已经包含任何与正在加载的包中的函数同名的函数，那么对该函数的任何后续调用都会表现出该函数在最新加载的包中的行为。如果您想访问前一个函数而不是`SparkR`函数，您需要显式地将该函数的包名作为前缀，如图所示:

```scala
//First try in R environment, without loading sparkR 
//Try sampling from a column in an R data.frame 
>sample(iris$Sepal.Length,6,FALSE) //Returns any n elements [1] 5.1 4.9 4.7 4.6 5.0 5.4 >sample(head(iris),3,FALSE) //Returns any 3 columns 
//Try sampling from an R data.frame 
//The Boolean argument is for with_replacement 
> sample(head 
> head(sample(iris,3,TRUE)) //Returns any 3 columns
  Species Species.1 Petal.Width
1  setosa    setosa         0.2 
2  setosa    setosa         0.2 
3  setosa    setosa         0.2 
4  setosa    setosa         0.2 
5  setosa    setosa         0.2 
6  setosa    setosa         0.4 

//Load sparkR, initialize sparkSession and then execute this  
> df <- createDataFrame(iris) //Create a Spark DataFrame 
> sample_df <- sample(df,TRUE,0.3) //Different signature 
> dim(sample_df)  //Different behavior [1] 44  5 
> //Returned 30% of the original data frame and all columns 
> //Try with base prefix 
> head(base::sample(iris),3,FALSE)  //Call base package's sample
  Species Petal.Width Petal.Length 
1  setosa         0.2          1.4
2  setosa         0.2          1.4 
3  setosa         0.2          1.3 
4  setosa         0.2          1.5 
5  setosa         0.2          1.4 
6  setosa         0.4          1.7 

```

## 子集数据

R 数据帧上的子集操作非常灵活，SparkR 试图用相同或相似的等价物保留这些操作。我们已经在前面的示例中看到了一些操作，但是本节以有序的方式介绍了它们:

```scala
//Subsetting data examples 
> b1 <- createDataFrame(beaver1) 
//Get one column 
> b1$temp 
Column temp    //Column class and not a vector 
> //Select some columns. You may use positions too 
> select(b1, c("day","temp")) 
SparkDataFrame[day:double, temp:double] 
>//Row subset based on conditions 
> head(subset(b1,b1$temp>37,select= c(2,3))) 
  time  temp 
1 1730 37.07 
2 1740 37.05 
3 1940 37.01 
4 1950 37.10 
5 2000 37.09 
6 2010 37.02 
> //Multiple conditions with AND and OR 
> head(subset(b1, between(b1$temp,c(36.0,37.0)) |  
        b1$time %in% 900 & b1$activ == 1,c(2:4)),2) 
 time  temp activ 
1  840 36.33     0 
2  850 36.34     0 

```

### 类型

在撰写本书时(Apache Spark 2.o 发行版)，基于行索引的切片不可用。您将无法使用`df[n,]`或`df[m:n,]`语法获得特定的行或行范围。

```scala
//For example, try on a normal R data.frame 
> beaver1[2:4,] 
  day time  temp activ 
2 346  850 36.34     0 
3 346  900 36.35     0 
4 346  910 36.42     0 
//Now, try on Spark Data frame 
> b1[2:4,] //Throws error 
Expressions other than filtering predicates are not supported in the first parameter of extract operator [ or subset() method. 
> 

```

## 列功能

您已经注意到了子集数据部分的列函数`between`。这些功能在`Column`类上运行。顾名思义，这些函数一次只对一列进行操作，通常用于设置数据框的子集。对于常见操作，如排序、转换和格式化，还有其他几个方便的列函数。除了处理列中的值之外，您还可以将列追加到数据框中，或者从数据框中删除一列或多列。与 r 类似，负列下标可用于省略列。以下示例显示了`Column`类函数在子集操作中的使用，随后是添加和删除列:

```scala
> //subset using Column operation using airquality dataset as df 
> head(subset(df,isNull(df$Ozone)),2) 
  Ozone Solar_R Wind Temp Month Day 
1    NA      NA 14.3   56     5   5 
2    NA     194  8.6   69     5  10 
> 
> //Add column and drop column examples 
> b1 <- createDataFrame(beaver1) 

//Add new column 
> b1$inRetreat <- otherwise(when(b1$activ == 0,"No"),"Yes") 
 head(b1,2) 
  day time  temp activ inRetreat 
1 346  840 36.33     0        No 
2 346  850 36.34     0        No 
> 
//Drop a column.  
> b1$day <- NULL 
> b1  // Example assumes b1$inRetreat does not exist 
SparkDataFrame[time:double, temp:double, activ:double] 
> //Drop columns using negative subscripts 
> b2 <- b1[,-c(1,4)]  > head(b2) 
   time  temp 
1  840 36.33 
2  850 36.34 
3  900 36.35 
4  910 36.42 
5  920 36.55 
6  930 36.69 
>  

```

## 分组数据

可以使用类似于 SQL 的`group_by`函数对数据框数据进行子分组。执行此类操作有多种方式。我们在这一节介绍一个稍微复杂的例子。此外，我们使用`magrittr`库提供的`%>%`，也就是前向管道操作符，它提供了一种链接命令的机制:

```scala
> //GroupedData example using iris data as df 
> //Open SparkR shell and create df using iris dataset  
> groupBy(df,"Species") 
GroupedData    //Returns GroupedData object 
> library(magrittr)  //Load the required library 
//Get group wise average sepal length 
//Report results sorted by species name 
>df2 <- df %>% groupBy("Species") %>%  
          avg("Sepal_Length") %>%  
          withColumnRenamed("avg(Sepal_Length)","avg_sepal_len") %>% 
          orderBy ("Species") 
//Format the computed double column 
df2$avg_sepal_len <- format_number(df2$avg_sepal_len,2) 
showDF(df2) 
+----------+-------------+ 
|   Species|avg_sepal_len| 
+----------+-------------+ 
|    setosa|         5.01| 
|versicolor|         5.94| 
| virginica|         6.59| 
+----------+-------------+ 

```

您可以继续使用正向管道操作符来链接操作。仔细看列重命名部分的代码。列名参数是先前操作的输出，在此操作开始之前已经完成，因此您可以放心地假设`avg(sepal_len)`列已经存在。`format_number`工作正常，这又是一次得心应手的`Column`操作。

下一节有另一个类似的例子`GroupedData`及其使用`dplyr`的等效实现。

# Spark 数据框

在本节中，我们将尝试一些有用的、常用的操作。首先，我们尝试传统的 R/ `dplyr`操作，然后使用 SparkR API 显示等效的操作:

```scala
> //Open the R shell and NOT SparkR shell  
> library(dplyr,warn.conflicts=FALSE)  //Load dplyr first 
//Perform a common, useful operation  
> iris %>%               
+   group_by(Species) %>% +   summarise(avg_length = mean(Sepal.Length),  
+             avg_width = mean(Sepal.Width)) %>% +   arrange(desc(avg_length)) 
Source: local data frame [3 x 3] 
     Species avg_length avg_width 
      (fctr)      (dbl)     (dbl) 
1  virginica      6.588     2.974 
2 versicolor      5.936     2.770 
3     setosa      5.006     3.428 

//Remove from R environment 
> detach("package:dplyr",unload=TRUE) 

```

这个操作非常类似于 SQL 组，后面是顺序。它在 SparkR 中的等效实现也与`dplyr`示例非常相似。请看下面的例子。注意方法名称，并将其定位与前面的`dplyr`示例进行比较:

```scala
> //Open SparkR shell and create df using iris dataset  
> collect(arrange(summarize(groupBy(df,df$Species),  +     avg_sepal_length = avg(df$Sepal_Length), +     avg_sepal_width = avg(df$Sepal_Width)), +     "avg_sepal_length", decreasing = TRUE))  
     Species avg_sepal_length avg_sepal_width 
1     setosa            5.006           3.428 
2 versicolor            5.936           2.770 
3  virginica            6.588           2.974 

```

SparkR 旨在尽可能接近现有的 R API。因此，方法名称看起来非常类似于`dplyr`方法。例如，看看有`groupBy`而`dplyr`有`group_by`的例子。SparkR 支持冗余函数名。例如，它有`group_by`和`groupBy`来迎合来自不同编程环境的开发人员。`dplyr`和 SparkR 中的方法名称也非常接近 SQL 关键字`GROUP BY`。但是这些方法调用的顺序是不一样的。该示例还显示了使用`collect`将 Spark 数据帧转换为 R `data.frame`的附加步骤。这些方法是由内而外排列的，也就是说，首先对数据进行分组，然后进行汇总，然后进行排列。这是可以理解的，因为在 SparkR 中，在最里面的方法中创建的 DataFrame 成为其直接前身的参数，以此类推。

## SQL 操作

如果您对前面示例中的语法不太满意，您可能希望尝试编写如图所示的 SQL 字符串，它与前面的语法完全相同，但使用了良好的旧 SQL 语法:

```scala
> //Register the Spark DataFrame as a table/View 
> createOrReplaceTempView(df,"iris_vw")  
//Look at the table structure and some rows
> collect(sql(sqlContext, "SELECT * FROM iris_tbl LIMIT 5"))
    Sepal_Length Sepal_Width Petal_Length Petal_Width Species 
1          5.1         3.5          1.4         0.2  setosa 
2          4.9         3.0          1.4         0.2  setosa 
3          4.7         3.2          1.3         0.2  setosa 
4          4.6         3.1          1.5         0.2  setosa 
5          5.0         3.6          1.4         0.2  setosa 
> //Try out the above example using SQL syntax 
> collect(sql(sqlContext, "SELECT Species,       avg(Sepal_Length) avg_sepal_length,      avg(Sepal_Width) avg_sepal_width       FROM iris_tbl        GROUP BY Species       ORDER BY avg_sepal_length desc")) 

  Species avg_sepal_length avg_sepal_width 

1  virginica            6.588           2.974 
2 versicolor            5.936           2.770 
3     setosa            5.006           3.428 

```

如果您习惯于从 RDBMS 表中获取数据，那么前面的示例看起来是实现现有操作的最自然的方式。但是我们要怎么做呢？第一条语句告诉 Spark 注册一个临时表(或者，顾名思义，一个视图，一个表的逻辑抽象)。这与数据库表不完全相同。它是暂时的，因为当 SparkSession 对象被销毁时，它会被销毁。您没有显式地将数据写入任何 RDBMS 数据存储(您必须为此使用`SaveAsTable`)。但是，一旦将 Spark 数据框注册为临时表，就可以自由地使用 SQL 语法对该数据框进行操作。下一个语句是一个基本的`SELECT`语句，显示列名后跟五行，由`LIMIT`关键字指定。下一条 SQL 语句创建了一个 Spark 数据框，该数据框包含一个“物种”列，后跟两个按平均萼片长度排序的平均列。通过使用收集，该数据帧又被收集为 R `data.frame`。最终结果与前面的例子完全相同。您可以自由使用任何一种语法。有关更多信息和示例，请查看[第 4 章](http://chapter%204)、*统一数据访问*中的 SQL 部分。

## 设置操作

常用的设置操作，如`union`、`intersection`和`minus`，在 SparkR 中是开箱可用的。事实上，当 SparkR 加载时，警告信息显示`intersect`为屏蔽功能之一。以下示例基于`beaver`数据集:

```scala
> //Create b1 and b2 DataFrames using beaver1 and beaver2 datasets 
> b1 <- createDataFrame(beaver1) 
> b2 <- createDataFrame(beaver2) 
//Get individual and total counts 
> > c(nrow(b1), nrow(b2), nrow(b1) + nrow(b2)) 
[1] 114 100 214 
//Try adding both data frames using union operation 
> nrow(unionAll(b1,b2)) 
[1] 214     //Sum of two datsets 
> //intersect example 
//Remove the first column (day) and find intersection 
showDF(intersect(b1[,-c(1)],b2[,-c(1)])) 

+------+-----+-----+ 
|  time| temp|activ| 
+------+-----+-----+ 
|1100.0|36.89|  0.0| 
+------+-----+-----+ 
> //except (minus or A-B) is covered in machine learning examples   

```

## 合并数据帧

下一个示例说明了使用`merge`命令连接两个数据帧。该示例的第一部分显示了 R 实现，下一部分显示了 SparkR 实现:

```scala
> //Example illustrating data frames merging using R (Not SparkR) 
> //Create two data frames with a matching column 
//Products df with two rows and two columns 
> products_df <- data.frame(rbind(c(101,"Product 1"), 
                    c(102,"Product 2"))) 
> names(products_df) <- c("Prod_Id","Product") 
> products_df 
 Prod_Id   Product 
1     101 Product 1 
2     102 Product 2 

//Sales df with sales for each product and month 24x3 
> sales_df <- data.frame(cbind(rep(101:102,each=12), month.abb, 
                    sample(1:10,24,replace=T)*10)) 
> names(sales_df) <- c("Prod_Id","Month","Sales") 

//Look at first 2 and last 2 rows in the sales_df 
> sales_df[c(1,2,23,24),] 
   Prod_Id Month Sales 
1      101   Jan    60 
2      101   Feb    40 
23     102   Nov    20 
24     102   Dec   100 

> //merge the data frames and examine the data 
> total_df <- merge(products_df,sales_df) 
//Look at the column names 
> colnames(total_df) 
> [1] "Prod_Id" "Product" "Month"   "Sales" 

//Look at first 2 and last 2 rows in the total_df 
> total_df[c(1,2,23,24),]     
   Prod_Id   Product Month Sales 
1      101 Product 1   Jan    10 
2      101 Product 1   Feb    20 
23     102 Product 2   Nov    60 
24     102 Product 2   Dec    10 

```

前面这段代码完全依赖于 R 的基本包。为了简单起见，我们在两个数据框中使用了相同的联接列名称。下一段代码演示了使用 SparkR 的相同示例。它看起来类似于前面的代码，所以请仔细查找不同之处:

```scala
> //Example illustrating data frames merging using SparkR 
> //Create an R data frame first and then pass it on to Spark 
> //Watch out the base prefix for masked rbind function 
> products_df <- createDataFrame(data.frame( 
    base::rbind(c(101,"Product 1"), 
    c(102,"Product 2")))) 
> names(products_df) <- c("Prod_Id","Product") 
>showDF(products_df) 
+-------+---------+ 
|Prod_Id|  Product| 
+-------+---------+ 
|    101|Product 1| 
|    102|Product 2| 
+-------+---------+ 
> //Create Sales data frame 
> //Notice the as.data.frame similar to other R functions 
> //No cbind in SparkR so no need for base:: prefix 
> sales_df <- as.DataFrame(data.frame(cbind( 
             "Prod_Id" = rep(101:102,each=12), 
"Month" = month.abb, 
"Sales" = base::sample(1:10,24,replace=T)*10))) 
> //Check sales dataframe dimensions and some random rows  
> dim(sales_df) 
[1] 24  3 
> collect(sample(sales_df,FALSE,0.20)) 
  Prod_Id Month Sales 
1     101   Sep    50 
2     101   Nov    80 
3     102   Jan    90 
4     102   Jul   100 
5     102   Nov    20 
6     102   Dec    50 
> //Merge the data frames. The following merge is from SparkR library 
> total_df <- merge(products_df,sales_df) 
// You may try join function for the same purpose 
//Look at the columns in total_df 
> total_df 
SparkDataFrame[Prod_Id_x:string, Product:string, Prod_Id_y:string, Month:string, Sales:string] 
//Drop duplicate column 
> total_df$Prod_Id_y <- NULL    
> head(total_df) 
  Prod_Id_x   Product Month Sales 
1       101 Product 1   Jan    40 
2       101 Product 1   Feb    10 
3       101 Product 1   Mar    90 
4       101 Product 1   Apr    10 
5       101 Product 1   May    50 
6       101 Product 1   Jun    70 
> //Note: As of Spark 2.0 version, SparkR does not support 
    row sub-setting  

```

您可能希望使用不同类型的联接，例如左外联接和右外联接，或者使用不同的列名来更好地理解这个函数。

# 机器学习

SparkR 为现有的 MLLib 函数提供包装器。r 公式被实现为 MLLib 特征转换器。转换器是一个 ML 管道(`spark.ml`)级，它将一个数据帧作为输入，并产生另一个数据帧作为输出，后者通常包含一些附加的列。特征转换器是一种将输入列转换为特征向量的转换器，这些特征向量被附加到源数据帧。例如，在线性回归中，字符串输入列是单向编码的，数值转换为双精度值。标签列将作为响应变量的副本被追加(如果数据帧中还没有标签列的话)。

在本节中，我们将介绍朴素贝叶斯和高斯 GLM 模型的示例代码。我们不解释模型本身或它们产生的摘要。相反，我们直接使用 SparkR 来完成。

## 朴素贝叶斯模型

nave Bayes 模型是一个直观简单的模型，可以处理分类数据。我们将使用朴素贝叶斯模型训练样本数据集。我们不会解释模型是如何工作的，而是直接使用 SparkR 训练模型。如需了解更多信息，请参考[第 6 章](06.html "Chapter 6.  Machine Learning")、*机器学习*。

这个例子使用了一个数据集，其中有 20 名学生的平均分数和出勤率。事实上，这个数据集已经在[第 6 章](http://Chapter 6)、*机器学习*中引入，用于训练合奏。然而，让我们重温一下它的内容。

学生将根据一套明确的规则获得`Pass`或`Fail`。两个有身份证的学生`1009`和`1020`被授予`Pass`，尽管否则他们会失败。即使我们没有给模型提供实际的规则，我们也期望模型能够预测这两个学生的结果`Fail`。以下是`Pass` / `Fail`标准:

*   标记< 40 =>失败
*   出勤率低= >失败
*   分数超过 40 分且出席人数满= >通过
*   分数> 60 且出席率至少足够= >及格以下是训练朴素贝叶斯模型的示例:

```scala
//Example to train NaÃ¯ve Bayes model 

//Read file 
> myFile <- read.csv("../work/StudentsPassFail.csv") //R data.frame 
> df <- createDataFrame(myFile) //sparkDataFrame 
//Look at the data 
> showDF(df,4) 
+---------+---------+----------+------+ 
|StudentId|Avg_Marks|Attendance|Result| 
+---------+---------+----------+------+ 
|     1001|     48.0|      Full|  Pass| 
|     1002|     21.0|    Enough|  Fail| 
|     1003|     24.0|    Enough|  Fail| 
|     1004|      4.0|      Poor|  Fail| 
+---------+---------+----------+------+ 

//Make three buckets out of Avg_marks 
// A >60; 40 < B < 60; C > 60 
> df$marks_bkt <- otherwise(when(df$Avg_marks < 40, "C"), 
                           when(df$Avg_marks > 60, "A")) 
> df$marks_bkt <- otherwise(when(df$Avg_marks < 40, "C"), 
                           when(df$Avg_marks > 60, "A")) 
> df <- fillna(df,"B",cols="marks_bkt") 
//Split train and test 
> trainDF <- sample(df,TRUE,0.7) 
> testDF <- except(df, trainDF) 

//Build model by supplying RFormula, training data 
> model <- spark.naiveBayes(Result ~ Attendance + marks_bkt, data = trainDF) 
> summary(model) 
$apriori 
          Fail      Pass 
[1,] 0.6956522 0.3043478 

$tables 
     Attendance_Poor Attendance_Full marks_bkt_C marks_bkt_B 
Fail 0.5882353       0.1764706       0.5882353   0.2941176   
Pass 0.125           0.875           0.125       0.625       

//Run predictions on test data 
> predictions <- predict(model, newData= testDF) 
//Examine results 
> showDF(predictions[predictions$Result != predictions$prediction, 
     c("StudentId","Attendance","Avg_Marks","marks_bkt", "Result","prediction")]) 
+---------+----------+---------+---------+------+----------+                     
|StudentId|Attendance|Avg_Marks|marks_bkt|Result|prediction| 
+---------+----------+---------+---------+------+----------+ 
|     1010|      Full|     19.0|        C|  Fail|      Pass| 
|     1019|    Enough|     45.0|        B|  Fail|      Pass| 
|     1014|      Full|     12.0|        C|  Fail|      Pass| 
+---------+----------+---------+---------+------+----------+ 
//Note that the predictions are not exactly what we anticipate but models are usually not 100% accurate 

```

## 高斯 GLM 模型

在本例中，我们尝试根据臭氧、太阳辐射和风的值来预测温度:

```scala
> //Example illustrating Gaussian GLM model using SparkR 
> a <- createDataFrame(airquality) 
//Remove rows with missing values 
> b <- na.omit(a) 
> //Inspect the dropped rows with missing values 
> head(except(a,b),2)    //MINUS set operation 
  Ozone Solar_R Wind Temp Month Day 
1    NA     186  9.2   84     6   4 
2    NA     291 14.9   91     7  14 

> //Prepare train data and test data 
traindata <- sample(b,FALSE,0.8) //Not base::sample 
testdata <- except(b,traindata) 

> //Build model 
> model <- glm(Temp ~ Ozone + Solar_R + Wind,  
          data = traindata, family = "gaussian") 
> // Get predictions 
> predictions <- predict(model, newData = testdata) 
> head(predictions[,c(predictions$Temp, predictions$prediction)], 
                 5) 
  Temp prediction 
1   90   81.84338 
2   79   80.99255 
3   88   85.25601 
4   87   76.99957 
5   76   71.75683 

```

# 总结

到目前为止，SparkR 并不支持 Spark 中所有可用的算法，但正在进行积极的开发来弥补这一差距。Spark 2.0 版本提高了算法覆盖率，包括 nave Bayes、k-means 聚类和生存回归。查看支持算法的最新文档。更多的工作正在进行中，推出一个更好的集成了 R 包和 Spark 包的 SparkR 的 CRAN 版本，以及更好的 RFormula 支持。

# 参考文献

*   *Spark:过去、现在和未来*作者:*希瓦拉姆·文卡塔拉曼:*T4】http://shivaram.org/talks/sparkr-summit-2015.pdf
*   *通过 spark 和 R* 启用探索性数据科学 *Shivaram Venkataraman* 和*Hossein Falaki:*[http://www . slide share . net/databricks/通过 Spark 和 r](http://www.slideshare.net/databricks/enabling-exploratory-data-science-with-spark-and-r) 启用探索性数据科学
*   *SparkR:使用 Spark* 缩放 R 程序由 *Shivaram Venkataraman* 和其他人:[http://shivaram.org/publications/sparkr-sigmod.pdf](http://shivaram.org/publications/sparkr-sigmod.pdf)
*   *高级分析的 SparkR 最新动态*作者:*孟祥瑞*:[http://files . meetup . com/4439192/最近% 20 发展% 20 in % 20 SparkR % 20 for % 20 Advanced % 20 Analytics . pdf](http://files.meetup.com/4439192/Recent%20Development%20in%20SparkR%20for%20Advanced%20Analytics.pdf)
*   To understand RFormula, try out the following links:
    *   [https://stat . ethz . ch/R-manual/R-dev/library/stats/html/formula . html](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html)
    *   [http://spark . Apache . org/docs/latest/ml-features . html # rformula](http://spark.apache.org/docs/latest/ml-features.html#rformula)