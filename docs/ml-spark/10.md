# 十、Spark 高级文本处理

在[章](04.html)，*使用 Spark*获取、处理和准备数据中，我们讨论了与特征提取和数据处理相关的各种主题，包括从文本数据中提取特征的基础知识。 在本章中，我们将介绍 Spark ML 中提供的更多高级文本处理技术，以处理大规模文本数据集。

在本章中，我们将：

*   完成详细的示例，说明数据处理、要素提取和建模管道，因为它们与文本数据相关
*   根据文档中的词语评估两个文档之间的相似度
*   使用提取的文本要素作为分类模型的输入
*   介绍自然语言处理的最新发展，将单词本身建模为向量，并说明如何使用 Spark 的 word2vec 模型根据单词的含义评估两个单词之间的相似性

我们将了解如何将 Spark 的 MLlib 和 Spark ML 用于文本处理示例以及文档集群。

# 文本数据有什么特别之处？

文本数据使用起来可能很复杂，主要原因有两个。 首先，文本和语言有一种固有的结构，这是不容易使用原始词(例如，含义、上下文、不同类型的词、句子结构和不同的语言，等等)捕捉到的。 因此，单纯的特征提取通常效果相对较差。

第二，文本数据的有效维度非常大，潜在地是无限的。 单单考虑一下英语中的单词数量，再加上各种特殊的单词、字符、俚语等等。 然后，加入其他语言和你可能在互联网上找到的所有类型的文本。 即使在相对较小的数据集中，文本数据的维度也很容易超过数千万甚至数亿个单词。 例如，数十亿个网站的公共爬虫数据集包含超过 8400 亿个单词。

为了解决这些问题，我们需要提取更多结构化特征的方法和处理海量文本数据的方法。

# 从数据中提取正确的要素

**自然语言处理领域**(**NLP**)涵盖了处理文本的广泛技术，从文本处理和特征提取到建模和机器学习。 在本章中，我们将重点介绍 Spark MLlib 和 Spark ML 中可用的两种特征提取技术：**术语频率-文档频率**(**TF-IDF**)术语加权方案和特征散列。

通过 TF-IDF 的例子，我们还将探索特征提取过程中的处理、标记化和过滤如何帮助我们降低输入数据的维数，以及提高我们提取的特征的信息量和有用性。

# 术语加权方案

在[第 4 章](04.html)，*使用 Spark*获取、处理和准备数据中，我们研究了向量表示，其中文本特征被映射到一个简单的二进制向量，称为**词袋**模型。 另一种在实践中常用的表示方法称为术语频率-文档频率倒置。

TF-IDF 根据其在文档中的频率(**术语频率**)对一段文本(称为**文档**)中的每个术语进行加权。 然后，基于该术语在所有文档(数据集中的文档集合通常被称为**语料库**)中的频率，应用称为**逆文档频率**的全局归一化。 TF-IDF 的标准定义如下所示：

*tf-idf(t，d)=tf(t，d)x idf(T)*

这里，*Tf(t，d)*是术语*t*在文档*d*中的出现频率(出现次数)，*IDF(T)*是术语*t*在语料库中的倒数文档频率；其定义如下：

*idf(T)=log(N/d)*

这里，*N*是文档总数，*d*是出现术语*t*的文档数量。

TF-IDF 公式意味着相对于文档中出现几次的词语，文档中出现多次的词语在矢量表示中获得更高的权重。 但是，IDF 标准化可以减轻所有文档中非常常见的术语的权重。 最终结果是，真正罕见或重要的术语应该被赋予较高的权重，而较常见的术语(假定重要性较低)在权重方面的影响应该较小。

要了解更多关于词袋模型(或向量空间模型)的信息，一个很好的资源是 Christopher D.Manning、Prabhakar Raghavan 和 Hinrich Schütze 著的 Introduction to Information Retrieval 一书，剑桥大学出版社(在[http://nlp.stanford.edu/IR-book/html/htmledition/irbook.html](http://nlp.stanford.edu/IR-book/html/htmledition/irbook.html)上有 HTML 版)。

它包含有关文本处理技术的部分，包括标记化、停用词删除、词干提取和向量空间模型，以及 TF-IDF 等加权方案。

*概览也可以在[http://en.wikipedia.org/wiki/Tf%E2%80%93idf](http://en.wikipedia.org/wiki/Tf%E2%80%93idf)上找到。

# 要素散列

**特征散列**是一种处理高维数据的技术，通常用于文本和分类数据集，在文本和分类数据集中，特征可以采用许多唯一值(通常是数百万个值)。 在前面的章节中，我们经常对分类特征(包括文本)使用*1 of-K*编码方法。 虽然这种方法简单有效，但在面对极高维度的数据时可能会失效。

构建和使用*1-of-K*特征编码要求我们保持每个可能的特征值到向量中的索引的映射。 此外，创建映射的过程本身至少需要通过数据集进行一次额外的遍历，在并行方案中可能很难做到这一点。 到目前为止，我们经常使用一种简单的方法，即收集不同的特征值，并用一组索引压缩该集合，以创建要索引的特征值地图。 然后，该映射(在我们的代码中显式地或由 Spark 隐式地)广播给每个 Worker。

但是，在处理处理文本时常见的数千万或更多的巨大特征维度时，这种方法可能会很慢，并且可能需要大量的内存和网络资源，无论是在 Spark Master(收集唯一值)和 Worker(向每个 Worker 广播生成的映射，这会将其保存在内存中，以便将特征编码应用于其本地输入数据)上都是如此。

特征散列的工作原理是，根据使用散列函数将该特征散列为一个数字(通常为整数值)而获得的值，为该特征分配矢量索引。 例如，假设`United States`的地理位置的分类要素的散列值为`342`。 我们将使用散列值作为向量索引，该索引处的值将是`1.0`，以指示`United States`特性的存在。 使用的散列函数必须是一致的(即，对于给定的输入，它每次返回相同的输出)。

此编码的工作方式与基于映射的编码相同，不同之处在于我们预先选择了特征向量的大小。 由于最常见的散列函数返回整个整数范围内的值，我们将使用*模*运算将索引值限制为向量的大小，向量的大小通常要小得多(根据我们的要求，从几万到几百万)。

功能散列的优势在于，我们不需要构建映射并将其保存在内存中。 它也很容易实现，速度非常快，可以在线实时完成，因此不需要首先通过我们的数据集。 最后，由于我们选择的特征向量维度明显小于数据集的原始维度，因此我们在训练和生产中都限制了模型的内存使用量；因此，内存使用量不会随着数据的大小和维度而扩展。

然而，有两个重要的缺点，这两个缺点如下：

*   由于我们不创建要素到索引值的映射，因此也不能进行要素索引到值的反向映射。 例如，这使得我们更难确定我们的模型中哪些功能信息最丰富。
*   当我们限制特征向量的大小时，我们可能会遇到**个散列冲突**。 当两个不同的特征被散列到我们的特征向量中的相同索引中时，就会发生这种情况。 令人惊讶的是，只要我们相对于输入数据的维度选择一个合理的特征向量维度，这似乎不会对模型性能产生严重影响。 如果散列向量较大，则碰撞的影响很小，但增益仍然很大。 有关更多详细信息，请参阅本白皮书：[http://www.cs.jhu.edu/~mdredze/publications/mobile_nlp_feature_mixing.pdf](http://www.cs.jhu.edu/~mdredze/publications/mobile_nlp_feature_mixing.pdf)

有关散列的更多信息，请参见[http://en.wikipedia.org/wiki/Hash_function](http://en.wikipedia.org/wiki/Hash_function)。

介绍使用散列进行特征提取和机器学习的一篇关键文章是：

*Kilian Weinberger*，*Anirban Dasgupta*，*John Langford*，*Alex Smola*和*Josh Attenberg*。 *大规模多任务学习的特征散列算法*。 *工艺。 ICML2009*，可在[http://alex.smola.org/papers/2009/Weinbergeretal09.pdf](http://alex.smola.org/papers/2009/Weinbergeretal09.pdf)上查阅。

# 从 20 个新闻组数据集中提取 TF-IDF 特征

为了说明本章中的概念，我们将使用名为**20 个新闻组**的熟知文本数据集；此数据集通常用于文本分类任务。 这是跨 20 个不同主题发布的新闻组消息的集合。 有各种形式的数据可用。 出于我们的目的，我们将使用数据集的`bydate`版本，该版本可从[http://qwone.com/~jason/20Newsgroups](http://qwone.com/~jason/20Newsgroups)获得。

该数据集将可用数据分成训练集和测试集，分别包含原始数据的 60%和 40%。 这里，测试集中的消息出现在训练集中的消息之后。 此数据集还排除了标识实际新闻组的某些邮件头；因此，它是测试分类模型实际性能的合适数据集。

Further information on the original dataset can be found in the *UCI Machine Learning Repository* page at [http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.data.html](http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.data.html).

要开始，请使用以下命令下载数据并解压缩文件：

```scala
>tar xfvz 20news-bydate.tar.gz

```

这将创建两个文件夹：一个名为`20news-bydate-train`，另一个名为`20news-bydate-test`。 让我们来看看 Training DataSet 文件夹下的目录结构：

```scala
>cd 20news-bydate-train/ >ls

```

您将看到它包含多个子文件夹，每个新闻组对应一个子文件夹：

```scala
alt.atheism                comp.windows.x          rec.sport.hockey
  soc.religion.christian
comp.graphics              misc.forsale            sci.crypt
  talk.politics.guns comp.os.ms-windows.misc    rec.autos               sci.electronics
  talk.politics.mideast
comp.sys.ibm.pc.hardware   rec.motorcycles         sci.med
  talk.politics.misc
comp.sys.mac.hardware      rec.sport.baseball      sci.space
  talk.religion.misc

```

每个新闻组文件夹下都有多个文件；每个文件都包含一个单独的邮件帖子：

```scala
> ls rec.sport.hockey
52550 52580 52610 52640 53468 53550 53580 53610 53640 53670 53700 
53731 53761 53791
...

```

我们可以查看其中一条消息的一部分，以了解其格式：

```scala
> head -20 rec.sport.hockey/52550
From: dchhabra@stpl.ists.ca (Deepak Chhabra)
Subject: Superstars and attendance (was Teemu Selanne, was +/-
  leaders)
Nntp-Posting-Host: stpl.ists.ca
Organization: Solar Terresterial Physics Laboratory, ISTS
Distribution: na
Lines: 115

Dean J. Falcione (posting from jrmst+8@pitt.edu) writes:
[I wrote:]

>>When the Pens got Mario, granted there was big publicity,etc, etc,
>>and interest was immediately generated. Gretzky did the same thing for
>>LA.
>>However, imnsho, neither team would have seen a marked improvement in
>>attendance if the team record did not improve. In the year before Lemieux
>>came, Pittsburgh finished with 38 points. Following his arrival, the Pens
>>finished with 53, 76, 72, 81, 87, 72, 88, and 87 points, with a couple of
 ^^
>>Stanley Cups thrown in.
...

```

正如我们所看到的，每封邮件都包含一些头字段，其中包含发件人、主题和其他元数据，后跟邮件的原始内容。

# 浏览 20 个新闻组数据

我们将使用 Spark Program 加载和分析数据集。

```scala
object TFIDFExtraction { 

  def main(args: Array[String]) { 

 } 
}

```

查看目录结构，您可能会再次认识到，我们的数据包含在各个文本文件中(每条消息一个文本文件)。 因此，我们将再次使用 Spark 的`wholeTextFiles`方法将每个文件的内容读取到 RDD 中的一个记录中。

在下面的代码中，`PATH`指的是解压`20news-bydate`ZIP 文件的目录：

```scala
val sc = new SparkContext("local[2]", "First Spark App") 

val path = "../data/20news-bydate-train/*" 
val rdd = sc.wholeTextFiles(path) 
// count the number of records in the dataset 
println(rdd.count)

```

如果放置断点，将显示以下行，指示 Spark 检测到的文件总数：

```scala
...
INFO FileInputFormat: Total input paths to process : 11314
...

```

命令运行完成后，您将看到记录总数，应该与前面的`Total input paths to process`屏幕输出相同：

```scala
11314

```

现在让我们打印已加载数据的`rdd`的第一个元素：

```scala
16/12/30 20:42:02 INFO DAGScheduler: Job 1 finished: first at 
TFIDFExtraction.scala:27, took 0.067414 s
(file:/home/ubuntu/work/ml-resources/spark- 
ml/Chapter_10/data/20news- bydate-train/alt.atheism/53186,From:  
ednclark@kraken.itc.gu.edu.au (Jeffrey Clark)
Subject: Re: some thoughts.
Keywords: Dan Bissell
Nntp-Posting-Host: kraken.itc.gu.edu.au
Organization: ITC, Griffith University, Brisbane, Australia
Lines: 70
....

```

接下来，我们来看看可用的新闻组主题：

```scala
val newsgroups = rdd.map { case (file, text) => 
  file.split("/").takeRight(2).head } 
println(newsgroups.first()) 
val countByGroup = newsgroups.map(n => (n, 1)).reduceByKey(_ +
  _).collect.sortBy(-_._2).mkString("n") 
println(countByGroup)

```

这将显示以下结果：

```scala
(rec.sport.hockey,600)
(soc.religion.christian,599)
(rec.motorcycles,598)
(rec.sport.baseball,597)
(sci.crypt,595)
(rec.autos,594)
(sci.med,594)
(comp.windows.x,593)
(sci.space,593)
(sci.electronics,591)
(comp.os.ms-windows.misc,591)
(comp.sys.ibm.pc.hardware,590)
(misc.forsale,585)
(comp.graphics,584)
(comp.sys.mac.hardware,578)
(talk.politics.mideast,564)
(talk.politics.guns,546)
(alt.atheism,480)
(talk.politics.misc,465)
(talk.religion.misc,377)

```

我们可以看到，主题之间的消息数量大致相等。

# 应用基本标记化

文本处理管道的第一步是将每个文档中的原始文本内容拆分成一个术语集合(也称为**标记**)。 这称为**标记化**。 我们将首先应用一个简单的**空格**标记化，并将每个文档的每个标记转换为小写：

```scala
val text = rdd.map { case (file, text) => text } 
val whiteSpaceSplit = text.flatMap(t => t.split(" 
  ").map(_.toLowerCase)) 
println(whiteSpaceSplit.distinct.count)

```

In the preceding code, we used the `flatMap` function instead of `map`, as for now, we want to inspect all the tokens together for exploratory analysis. Later in this chapter, we will apply our tokenization scheme on a per-document basis, so we will use the `map` function.

运行此代码片段后，您将看到应用我们的标记化后的唯一令牌总数：

```scala
402978

```

正如您所看到的，即使是相对较少的文本，原始标记的数量(因此，我们的特征向量的维度)也可能非常高。

让我们看看随机选择的文档。 我们将使用 RDD 的示例方法：

```scala
def sample( 
      withReplacement: Boolean, 
      fraction: Double, 
      seed: Long = Utils.random.nextLong): RDD[T] 

Return a sampled subset of this RDD. 
@param withReplacement can elements be sampled multiple times    
  (replaced when sampled out) 
@param fraction expected size of the sample as a fraction of this   
  RDD's size without replacement: probability that each element is    
  chosen; fraction must be [0, 1] with replacement: expected number   
  of times each element is chosen; fraction must be >= 0 
@param seed seed for the random number generator 

      println(nonWordSplit.distinct.sample( 
      true, 0.3, 42).take(100).mkString(","))

```

Note that we set the third parameter to the `sample` function, which is the random seed. We set this function to `42` so that we get the same results from each call to `sample` so that your results match those in this chapter.

这将显示以下结果：

```scala
atheist,resources
summary:,addresses,,to,atheism
keywords:,music,,thu,,11:57:19,11:57:19,gmt
distribution:,cambridge.,290

archive-name:,atheism/resources
alt-atheism-archive-  
name:,december,,,,,,,,,,,,,,,,,,,,,,addresses,addresses,,,,,,,
religion,to:,to:,,p.o.,53701.
telephone:,sell,the,,fish,on,their,cars,,with,and,written

inside.,3d,plastic,plastic,,evolution,evolution,7119,,,,,san,san,
san,mailing,net,who,to,atheist,press

aap,various,bible,,and,on.,,,one,book,is:

"the,w.p.,american,pp.,,1986.,bible,contains,ball,,based,based,
james,of

```

# 改进我们的标记化

前面的简单方法会产生很多标记，并且不会过滤掉许多非单词字符(如标点符号)。 大多数标记化方案将删除这些字符。 我们可以通过使用正则表达式模式将每个原始文档拆分成非单词字符来实现这一点：

```scala
val nonWordSplit = text.flatMap(t => 
  t.split("""W+""").map(_.toLowerCase)) 
println(nonWordSplit.distinct.count)

```

这显著减少了唯一令牌的数量：

```scala
130126

```

如果我们检查前几个标记，我们会发现我们已经删除了文本中大多数不太有用的字符：

```scala
println( 
nonWordSplit.distinct.sample(true, 0.3, 
  50).take(100).mkString(","))

```

您将看到显示以下结果：

```scala
jejones,ml5,w1w3s1,k29p,nothin,42b,beleive,robin,believiing,749,
steaminess,tohc4,fzbv1u,ao,
instantaneous,nonmeasurable,3465,tiems,tiems,tiems,eur,3050,pgva4,
animating,10011100b,413,randall_clark,
mswin,cannibal,cannibal,congresswoman,congresswoman,theoreticians,
34ij,logically,kxo,contoler,
contoler,13963,13963,ets,sask,sask,sask,uninjured,930420,pws,vfj,
jesuit,kocharian,6192,1tbs,octopi,
012537,012537,yc0,dmitriev,icbz,cj1v,bowdoin,computational,
jkis_ltd,
caramate,cfsmo,springer,springer,
005117,shutdown,makewindow,nowadays,mtearle,discernible,
discernible,qnh1,hindenburg,hindenburg,umaxc,
njn2e5,njn2e5,njn2e5,x4_i,x4_i,monger,rjs002c,rjs002c,rjs002c,
warms,ndallen,g45,herod,6w8rg,mqh0,suspects,
floor,flq1r,io21087,phoniest,funded,ncmh,c4uzus

```

虽然我们的非单词模式可以很好地拆分文本，但我们仍然会遇到包含数字字符的数字和记号。 在某些情况下，数字可能是语料库的重要组成部分。 出于我们的目的，我们流水线中的下一步将是过滤掉数字和混合了数字的单词和令牌。

我们可以通过应用另一个正则表达式模式来实现这一点，并使用该模式过滤出与该模式不匹配的令牌`val regex = """[^0-9]*""".r`。

```scala
val regex = """[^0-9]*""".r 
val filterNumbers = nonWordSplit.filter(token => 
  regex.pattern.matcher(token).matches) 
println(filterNumbers.distinct.count)

```

这进一步减小了令牌集的大小：

```scala
84912

```

```scala
println(filterNumbers.distinct.sample(true, 0.3,      
50).take(100).mkString(","))

```

让我们看一下过滤后的令牌的另一个随机样本。

您将看到如下所示的输出：

```scala
jejones,silikian,reunion,schwabam,nothin,singen,husky,tenex,
eventuality,beleive,goofed,robin,upsets,aces,nondiscriminatory,
underscored,bxl,believiing,believiing,believiing,historians,
nauseam,kielbasa,collins,noport,wargame,isv,bellevue,seetex,seetex,
negotiable,negotiable,viewed,rolled,unforeseen,dlr,museum,museum,
wakaluk,wakaluk,dcbq,beekeeper,beekeeper,beekeeper,wales,mop,win,
ja_jp,relatifs,dolphin,strut,worshippers,wertheimer,jaze,jaze,
logically,kxo,nonnemacher,sunprops,sask,bbzx,jesuit,logos,aichi,
remailing,remailing,winsor,dtn,astonished,butterfield,miserable,
icbz,icbz,poking,sml,sml,makeing,deterministic,deterministic,
deterministic,rockefeller,rockefeller,explorers,bombardments,
bombardments,bombardments,ray_bourque,hour,cfsmo,mishandles,
scramblers,alchoholic,shutdown,almanac_,bruncati,karmann,hfd,
makewindow,perptration,mtearle

```

我们可以看到，我们已经删除了所有数字字符。 这仍然给我们留下了一些奇怪的*词*，但是我们在这里不会太担心这些。

# 删除停用字词

**停用词**是指在语料库中几乎所有文档(以及大多数语料库)中多次出现的常用词。 典型的英语停用词的例子包括 and、But、the、of 等。 从提取的记号中排除停用词是文本特征提取中的标准做法。

当使用 TF-IDF 加权时，加权方案实际上为我们解决了这一问题。 由于停用词的 IDF 得分非常低，因此它们的 TF-IDF 权重往往很低，因此重要性较低。 在某些情况下，对于信息检索和搜索任务，可能需要包含停用字词。 然而，在特征提取期间排除停用词仍然是有益的，因为它降低了最终特征向量的维度以及训练数据的大小。

我们可以查看语料库中在所有文档中出现频率最高的一些标记，以了解要排除的其他一些停用词：

```scala
val tokenCounts = filterNumbers.map(t => (t, 1)).reduceByKey(_ + 
  _) 
val oreringDesc = Ordering.by[(String, Int), Int](_._2) 
println(tokenCounts.top(20)(oreringDesc).mkString("n"))

```

在前面的代码中，我们在过滤掉数字字符之后获得了令牌，并生成了每个令牌在语料库中出现的次数计数。 现在，我们可以使用 Spark 的 top 函数按计数检索前 20 个令牌。 请注意，我们需要为 top 函数提供一个排序，告诉 Spark 如何对 RDD 的元素进行排序。 在本例中，我们希望按计数排序，因此我们将指定键-值对的第二个元素。

运行前面的代码片段将产生以下顶级标记：

```scala
(the,146532)
(to,75064)
(of,69034)
(a,64195)
(ax,62406)
(and,57957)
(i,53036)
(in,49402)
(is,43480)
(that,39264)
(it,33638)
(for,28600)
(you,26682)
(from,22670)
(s,22337)
(edu,21321)
(on,20493)
(this,20121)
(be,19285)
(t,18728)

```

正如我们可能预料的那样，这个列表中有很多常见的词，我们可以潜在地将其标记为停用词。 让我们用这些
中的一些以及其他常用词来创建一组停用词。 然后，我们将在过滤掉这些停用词后查看这些令牌：

```scala
val stopwords = Set( 
  "the","a","an","of","or","in","for","by","on","but", "is", 
  "not", "with", "as", "was", "if", 
  "they", "are", "this", "and", "it", "have", "from", "at", "my",  
  "be", "that", "to" 
val tokenCountsFilteredStopwords = tokenCounts.filter {  
  case (k, v) => !stopwords.contains(k)  
  } 

println(tokenCountsFilteredStopwords.top(20)   
  (oreringDesc).mkString("n"))

```

您将看到以下输出：

```scala
(ax,62406)
(i,53036)
(you,26682)
(s,22337)
(edu,21321)
(t,18728)
(m,12756)
(subject,12264)
(com,12133)
(lines,11835)
(can,11355)
(organization,11233)
(re,10534)
(what,9861)
(there,9689)
(x,9332)
(all,9310)
(will,9279)
(we,9227)
(one,9008)

```

你可能会注意到，在这个排行榜上仍然有相当多的常用词。 在实践中，我们可能会有更多的停用词。 但是，我们将保留一些(部分是为了说明稍后使用 TF-IDF 权重时常用单词的影响)。

您可以在此处找到常用停用词列表：[http://xpo6.com/list-of-english-stop-words/](http://xpo6.com/list-of-english-stop-words/)

我们将使用的另一个过滤步骤是删除所有长度仅为一个字符的标记。 这背后的推理类似于删除停用词-这些单字符标记在我们的文本模型中不太可能是信息性的，并且可以进一步降低特征维度和模型大小。 我们将通过另一个筛选步骤来完成此操作：

```scala
val tokenCountsFilteredSize =  
  tokenCountsFilteredStopwords.filter {  
    case (k, v) => k.size >= 2  
  } 
println(tokenCountsFilteredSize.top(20)  
  (oreringDesc).mkString("n"))

```

同样，我们将检查此筛选步骤之后剩余的令牌：

```scala
(ax,62406)
(you,26682)
(edu,21321)
(subject,12264)
(com,12133)
(lines,11835)
(can,11355)
(organization,11233)
(re,10534)
(what,9861)
(there,9689)
(all,9310)
(will,9279)
(we,9227)
(one,9008)
(would,8905)
(do,8674)
(he,8441)
(about,8336)
(writes,7844)

```

除了一些我们没有排除在外的常用词之外，我们看到一些潜在的更具信息性的词开始出现。

# 根据频率排除术语

当术语在语料库中的总出现次数非常低时，在标记化过程中排除这些术语也是一种常见的做法。 例如，让我们检查一下语料库中出现次数最少的术语(请注意，我们在这里使用不同的顺序来返回按升序排序的结果)：

```scala
val oreringAsc = Ordering.by[(String, Int), Int](-_._2) 
println(tokenCountsFilteredSize.top(20)(oreringAsc)
  .mkString("n"))

```

您将获得以下结果：

```scala
(lennips,1)
(bluffing,1)
(preload,1)
(altina,1)
(dan_jacobson,1)
(vno,1)
(actu,1)
(donnalyn,1)
(ydag,1)
(mirosoft,1)
(xiconfiywindow,1)
(harger,1)
(feh,1)
(bankruptcies,1)
(uncompression,1)
(d_nibby,1)
(bunuel,1)
(odf,1)
(swith,1)
(lantastic,1)

```

正如我们所看到的，有许多术语在整个语料库中只出现一次。 由于通常情况下，我们希望将提取的特征用于其他任务，如文档相似性或机器学习模型，因此只出现一次的令牌对学习没有用处，因为我们将没有足够的训练数据与这些令牌相关。 我们可以应用另一个筛选器来排除这些罕见的令牌：

```scala
val rareTokens = tokenCounts.filter{ case (k, v) => v < 2 }.map {  
  case (k, v) => k }.collect.toSet 
val tokenCountsFilteredAll = tokenCountsFilteredSize.filter {    
  case (k, v) => !rareTokens.contains(k) } 
println(tokenCountsFilteredAll.top(20)    
  (oreringAsc).mkString("n"))

```

我们可以看到，我们剩下的标记在语料库中至少出现了两次：

```scala
(sina,2)
(akachhy,2)
(mvd,2)
(hizbolah,2)
(wendel_clark,2)
(sarkis,2)
(purposeful,2)
(feagans,2)
(wout,2)
(uneven,2)
(senna,2)
(multimeters,2)
(bushy,2)
(subdivided,2)
(coretest,2)
(oww,2)
(historicity,2)
(mmg,2)
(margitan,2)
(defiance,2)

```

现在，让我们计算一下唯一令牌的数量：

```scala
println(tokenCountsFilteredAll.count)

```

您将看到以下输出：

```scala
51801

```

正如我们所看到的，通过在我们的标记化管道中应用所有过滤步骤，我们已经将特征维度从`402,978`降低到了`51,801`。

现在，我们可以将所有过滤逻辑合并到一个函数中，该函数可以应用于 RDD 中的每个文档：

```scala
def tokenize(line: String): Seq[String] = { 
  line.split("""W+""") 
    .map(_.toLowerCase) 
    .filter(token => regex.pattern.matcher(token).matches) 
    .filterNot(token => stopwords.contains(token)) 
    .filterNot(token => rareTokens.contains(token)) 
    .filter(token => token.size >= 2) 
    .toSeq 
}

```

我们可以使用以下代码片段检查该函数是否给出了相同的结果：

```scala
println(text.flatMap(doc => tokenize(doc)).distinct.count)

```

这将输出`51801`，为我们提供与逐步流水线相同的唯一令牌计数。

我们可以将 RDD 中的每个文档标记化如下：

```scala
val tokens = text.map(doc => tokenize(doc)) 
println(tokens.first.take(20))

```

您将看到类似以下内容的输出，其中显示了第一个文档的标记化版本的第一部分：

```scala
WrappedArray(mathew, mantis, co, uk, subject, alt, atheism, 
faq, atheist, resources, summary, books, addresses, music,         
anything, related, atheism, keywords, faq)

```

# 关于词干去除的注记

文本处理和标记化中的一个常见步骤是**词干**。 这是将整个单词转换为**基本形式**(称为**词干**)。 例如，复数可以转换成单数(*狗*变成*狗*)，而像*步行*和*Walker*这样的形式可以变成步行。 词干分析可能会变得相当复杂，通常需要使用专门的 NLP 或搜索引擎软件(例如 NLTK、OpenNLP 和 Lucene)来处理。 出于本例的目的，我们将忽略词干处理。

A full treatment of stemming is beyond the scope of this book. You can find more details at [http://en.wikipedia.org/wiki/Stemming](http://en.wikipedia.org/wiki/Stemming).

# 要素散列

首先，我们解释什么是功能散列，以便在下一节中更容易理解 TF-IDF 模型。

特征散列将字符串或单词转换为固定长度的向量，这使得处理文本变得容易。

Spark 目前使用 Austin Appleby 的 MurMurHash 3 算法(MurMurHash3_X86_32)将文本散列为数字。

您可以在此处找到实现

```scala
private[spark] def murmur3Hash(term: Any): Int = {
  term match {
  case null => seed
  case b: Boolean => hashInt(if (b) 1 else 0, seed)
  case b: Byte => hashInt(b, seed)
  case s: Short => hashInt(s, seed)
  case i: Int => hashInt(i, seed)
  case l: Long => hashLong(l, seed)
  case f: Float => hashInt(java.lang.Float
    .floatToIntBits(f), seed)
  case d: Double => hashLong(java.lang.Double.
    doubleToLongBits(d), seed)
  case s: String => val utf8 = UTF8String.fromString(s)
    hashUnsafeBytes(utf8.getBaseObject, utf8.getBaseOffset, 
    utf8.numBytes(), seed)
  case _ => throw new SparkException( 
  "HashingTF with murmur3 algorithm does not " +
    s"support type ${term.getClass.getCanonicalName} of input  
  data.")
  }
}

```

请注意，函数 hashInt、hasnLong 等是从 Util.scala 调用的

# 构建 TF-IDF 模型

现在，我们将使用 Spark ML 将每个文档以经过处理的令牌的形式转换为向量表示形式。 第一步是使用`HashingTF`实现，它利用特征散列将输入文本中的每个标记映射到词频矢量中的索引。 然后，我们将计算全局 IDF，并使用它将词频向量转换为 TF-IDF 向量。

因此，对于每个令牌，索引将是令牌的散列(依次映射到特征向量的维度上)。 每个令牌的值将是该令牌的 TF-IDF 权重(即术语频率乘以倒数文档频率)。

首先，我们将导入所需的类并创建我们的`HashingTF`实例，传入一个`dim`维度参数。 虽然默认特征维度是 2<sup>20</sup>(或大约 100 万)，但我们将选择 2<sup>18</sup>(或大约 260,000)，因为对于大约 50,000 个令牌，我们应该不会遇到大量的散列冲突，出于说明的目的，较小的维度将对内存和处理更友好：

```scala
import org.apache.spark.mllib.linalg.{ SparseVector => SV } 
import org.apache.spark.mllib.feature.HashingTF 
import org.apache.spark.mllib.feature.IDF 
val dim = math.pow(2, 18).toInt 
val hashingTF = new HashingTF(dim) 
val tf = hashingTF.transform(tokens) 
tf.cache

```

Note that we imported MLlib's `SparseVector` using an alias of `SV`. This is because later, we will use Breeze's `linalg` module, which itself also imports `SparseVector`. This way, we will avoid namespace collisions.

`HashingTF`的`transform`函数将每个输入文档(即一系列令牌)映射到 MLlib`Vector`。 我们还将调用`cache`将数据固定在内存中，以加快后续操作。

让我们检查一下转换后的数据集的第一个元素：

请注意，`HashingTF.transform`返回`RDD[Vector]`，因此我们将返回的结果强制转换为 MLlib`SparseVector`的实例。

通过接受`Iterable`参数(例如，文档作为`Seq[String]`)，`transform`方法也可以对单个文档起作用。 这将返回单个向量。

```scala
val v = tf.first.asInstanceOf[SV] 
println(v.size) 
println(v.values.size) 
println(v.values.take(10).toSeq) 
println(v.indices.take(10).toSeq)

```

您将看到显示以下输出：

```scala
262144
706
WrappedArray(1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0)
WrappedArray(313, 713, 871, 1202, 1203, 1209, 1795, 1862, 3115,     
3166)

```

我们可以看到，词频的每个稀疏向量的维数是 262,144(或我们指定的 2<sup>18</sup>)。 但是，向量中非零条目的数量只有 706 个。 输出的最后两行显示向量中前几个条目的频率计数和向量索引。

现在，我们将通过创建一个新的`IDF`实例并使用术语频率向量的 RDD 作为输入调用`fit`来计算语料库中每个术语的反向文档频率。 然后，我们将通过`IDF`的`transform`函数将词频向量转换为 TF-IDF 向量：

```scala
val idf = new IDF().fit(tf) 
val tfidf = idf.transform(tf) 
val v2 = tfidf.first.asInstanceOf[SV] 
println(v2.values.size) 
println(v2.values.take(10).toSeq) 
println(v2.indices.take(10).toSeq)

```

当您检查 TF-IDF 变换向量的 RDD 中的第一个元素时，您将看到类似如下所示的输出：

```scala
706
WrappedArray(2.3869085659322193, 4.670445463955571, 
6.561295835827856, 4.597686109673142,  ...
WrappedArray(313, 713, 871, 1202, 1203, 1209, 1795, 1862, 3115,     
3166)

```

我们可以看到，非零条目的数量没有变化(在`706`)，术语的向量索引也没有变化。 更改的是每个术语的值。 以前，这些值表示文档中每个术语的频率，但现在，新值表示 IDF 加权的频率。

当我们执行以下两行代码时，IDF 权重就出现了

```scala
val idf = new IDF().fit(tf) 
val tfidf = idf.transform(tf)

```

# TF-IDF 权重分析

接下来，让我们研究几个术语的 TF-IDF 权重，以说明术语的共性或稀有性的影响。

首先，我们可以计算整个语料库的最小和最大 TF-IDF 权重：

```scala
val minMaxVals = tfidf.map { v => 
  val sv = v.asInstanceOf[SV] 
  (sv.values.min, sv.values.max) 
} 
val globalMinMax = minMaxVals.reduce { case ((min1, max1), 
  (min2, max2)) => 
  (math.min(min1, min2), math.max(max1, max2)) 
} 
println(globalMinMax)

```

如我们所见，最小 TF-IDF 为零，而最大值明显更大：

```scala
(0.0,66155.39470409753)

```

我们现在将探讨附加到各个术语的 TF-IDF 权重。 在上一节关于停用词的讨论中，我们过滤掉了许多经常出现的常用词。 回想一下，我们并没有删除所有这样的潜在停用词。 相反，我们在语料库中保留了一些，以便我们可以说明应用 TF-IDF 加权方案对这些术语的影响。

TF-IDF 权重将倾向于为常用术语分配较低的权重。 要了解这一点，我们可以计算出现在前面计算的顶级匹配项列表中的几个术语的 TF-IDF 表示，例如`you`、`do`和`we`：

```scala
val common = sc.parallelize(Seq(Seq("you", "do", "we"))) 
val tfCommon = hashingTF.transform(common) 
val tfidfCommon = idf.transform(tfCommon) 
val commonVector = tfidfCommon.first.asInstanceOf[SV] 
println(commonVector.values.toSeq)

```

如果我们形成该文档的 TF-IDF 向量表示，我们将看到分配给每个术语的以下值。 请注意，由于功能散列，我们不能确切地确定哪个术语代表什么。 但是，这些值表明应用于这些术语的权重相对较低：

```scala
WrappedArray(0.9965359935704624, 1.3348773448236835, 
0.5457486182039175)

```

现在，让我们对几个不太常见的术语应用相同的转换，这些术语可能会直观地与特定主题或概念更紧密地联系在一起：

```scala
val uncommon = sc.parallelize(Seq(Seq("telescope", 
  "legislation", "investment"))) 
val tfUncommon = hashingTF.transform(uncommon) 
val tfidfUncommon = idf.transform(tfUncommon) 
val uncommonVector = tfidfUncommon.first.asInstanceOf[SV] 
println(uncommonVector.values.toSeq)

```

从以下结果可以看出，TF-IDF 的权重确实明显高于更常见的术语：

```scala
WrappedArray(5.3265513728351666, 5.308532867332488, 
5.483736956357579)

```

# 使用 TF-IDF 模型

虽然我们经常提到训练 TF-IDF 模型，但它实际上是一个特征提取过程或变换，而不是机器学习模型。 TF-IDF 加权通常用作其他模型的预处理步骤，例如降维、分类或回归。

为了说明 TF-IDF 加权的潜在用途，我们将探索两个示例。 第一种是利用 TF-IDF 向量计算文档相似度，第二种是以 TF-IDF 向量作为输入特征训练多标签分类模型。

# 文档与 20 个新闻组数据集和 TF-IDF 功能相似

您可能还记得，在[第 5 章](05.html)，*使用 Spark*构建推荐引擎时，可以使用距离度量计算两个向量之间的相似性。 两个向量越接近(即距离度量越小)，它们就越相似。 我们用来计算电影之间相似度的一个这样的度量是余弦相似度。

就像我们对电影所做的那样，我们还可以计算两个文档之间的相似度。 使用 TF-IDF，我们将每个文档转换为向量表示。 因此，我们可以使用与电影矢量相同的技术来比较两个文档。

直观地说，如果两个文档共享很多术语，我们可能会认为它们彼此更相似。 相反，如果两个文档各自包含许多彼此不同的术语，我们可能会认为它们不太相似。 当我们通过计算两个向量的点积来计算余弦相似度时，每个向量都由每个文档中的术语组成，我们可以看到，术语重叠程度高的文档往往具有更高的余弦相似度。

现在，我们可以看到 TF-IDF 正在工作。 我们可以合理地预期，即使是非常不同的文档也可能包含许多相对常见的重叠术语(例如，我们的停用词)。 但是，由于 TF-IDF 权重较低，这些术语不会对点积产生重大影响，因此对计算的相似度不会有太大影响。

例如，我们可能期望从`hockey`新闻组中随机选择的两条消息彼此相对相似。 让我们看看是不是这样：

```scala
val hockeyText = rdd.filter { case (file, text) => 
  file.contains("hockey") } 
val hockeyTF = hockeyText.mapValues(doc => 
  hashingTF.transform(tokenize(doc))) 
val hockeyTfIdf = idf.transform(hockeyTF.map(_._2))

```

在前面的代码中，我们首先过滤了原始输入 RDD，以便只保留曲棍球主题中的消息。 然后，我们应用我们的标记化和词频转换函数。 请注意，使用的`transform`方法是在单个文档上工作的版本(以`Seq[String]`的形式)，而不是在文档的 RDD 上工作的版本。

最后，我们应用了`IDF`转换(请注意，我们使用的 IDF 与我们已经在整个语料库上计算的 IDF 相同)。

一旦我们有了`hockey`个文档向量，我们就可以随机选择其中两个向量并计算它们之间的余弦相似度(正如我们前面所做的那样，我们将使用 Breeze 实现线性代数功能，特别是首先将 MLlib 向量转换为 Breeze`SparseVector`实例)：

```scala
import breeze.linalg._ 
val hockey1 = hockeyTfIdf.sample( 
  true, 0.1, 42).first.asInstanceOf[SV] 
val breeze1 = new SparseVector(hockey1.indices,
  hockey1.values, hockey1.size) 
val hockey2 = hockeyTfIdf.sample(true, 0.1, 
  43).first.asInstanceOf[SV] 
val breeze2 = new SparseVector(hockey2.indices,
  hockey2.values, hockey2.size) 
val cosineSim = breeze1.dot(breeze2) / 
  (norm(breeze1) * norm(breeze2)) 
println(cosineSim)

```

我们可以看到文档之间的余弦相似度约为 0.06：

```scala
0.06700095047242809

```

虽然这看起来可能相当低，但请记住，我们的功能的有效维度很高，这是因为在处理文本数据时具有大量典型的唯一术语。 因此，我们可以预期，即使任何两个文档关于同一主题，它们的术语重叠度也可能相对较低，因此绝对相似性得分也会较低。

相反，我们可以使用相同的方法，将此相似性得分与我们的一个`hockey`文档和从`comp.graphics`新闻组中随机选择的另一个文档之间计算的相似性得分进行比较：

```scala
val graphicsText = rdd.filter { case (file, text) => 
  file.contains("comp.graphics") } 
val graphicsTF = graphicsText.mapValues(doc => 
  hashingTF.transform(tokenize(doc))) 
val graphicsTfIdf = idf.transform(graphicsTF.map(_._2)) 
val graphics = graphicsTfIdf.sample(true, 0.1, 
  42).first.asInstanceOf[SV] 
val breezeGraphics = new SparseVector(graphics.indices, 
  graphics.values, graphics.size) 
val cosineSim2 = breeze1.dot(breezeGraphics) / (norm(breeze1) * 
  norm(breezeGraphics)) 
println(cosineSim2)

```

余弦相似度在`0.0047`处显著降低：

```scala
0.001950124251275256

```

最后，来自另一个体育相关主题的文档可能比来自计算机相关主题的文档更类似于我们的`hockey`文档。 但是，我们可能希望`baseball`文档与我们的`hockey`文档不太相似。 让我们通过计算来自`baseball`新闻组的随机消息与我们的`hockey`文档之间的相似度来看看情况是否如此：

```scala
// compare to sport.baseball topic 
val baseballText = rdd.filter { case (file, text) => 
  file.contains("baseball") } 
val baseballTF = baseballText.mapValues(doc => 
  hashingTF.transform(tokenize(doc))) 
val baseballTfIdf = idf.transform(baseballTF.map(_._2)) 
val baseball = baseballTfIdf.sample(true, 0.1, 
  42).first.asInstanceOf[SV] 
val breezeBaseball = new SparseVector(baseball.indices, 
  baseball.values, baseball.size) 
val cosineSim3 = breeze1.dot(breezeBaseball) / (norm(breeze1) * 
   norm(breezeBaseball)) 
println(cosineSim3)

```

实际上，正如我们所预期的那样，我们发现`baseball`和`hockey`文档的余弦相似度为`0.05`，明显高于`comp.graphics`文档，但也略低于其他`hockey`文档：

```scala
0.05047395039466008

```

来源：

[https：//github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_10/scala-2.0.x/src/main/scala/TFIDFExtraction.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_10/scala-2.0.x/src/main/scala/TFIDFExtraction.scala)

# 使用 TF-IDF 在 20 个新闻组数据集上训练文本分类器

当使用 TF-IDF 向量时，我们预计余弦相似性度量将基于文档之间的术语重叠来捕捉文档之间的相似性。 以类似的方式，我们预计机器学习模型(如分类器)将能够学习单个术语的权重；这将允许它区分来自不同类别的文档。 也就是说，应该可以了解某些术语的存在(和权重)与特定主题之间的映射。

在 20 个新闻组示例中，每个新闻组主题都是一个类，我们可以使用 TF-IDF 转换后的向量作为输入来训练分类器。

由于我们正在处理一个多类分类问题，因此我们将在 MLlib 中使用支持多类的朴素贝叶斯模型。 作为第一步，我们将导入将要使用的 Spark 类：

```scala
import org.apache.spark.mllib.regression.LabeledPoint 
import org.apache.spark.mllib.classification.NaiveBayes 
import org.apache.spark.mllib.evaluation.MulticlassMetrics.

```

我们将把集群代码保存在一个名为`Document clustering`的对象中

```scala
object DocumentClassification { 

  def main(args: Array[String]) { 
    val sc = new SparkContext("local[2]", "") 
    ... 
}

```

接下来，我们需要提取 20 个主题并将它们转换为类映射。 我们可以按照与*1-of-K*功能编码完全相同的方式完成此操作，方法是为每个类分配一个数字索引：

```scala
val newsgroupsMap = 
  newsgroups.distinct.collect().zipWithIndex.toMap 
val zipped = newsgroups.zip(tfidf) 
val train = zipped.map { case (topic, vector) => 
  LabeledPoint(newsgroupsMap(topic), vector) } 
train.cache

```

在前面的代码片段中，我们采用了`newsgroups`RDD，其中每个元素都是主题，并使用`zip`函数将其与 TF-IDF 向量的`tfidf`RDD 中的每个元素组合在一起。 然后，我们在新压缩的 RDD 中映射每个键值元素，并创建一个`LabeledPoint`实例，其中`label`是类索引，`features`是 TF-IDF 向量。

Note that the `zip` operator assumes that each RDD has the same number of partitions as well as the same number of elements in each partition. It will fail if this is not the case. We can make this assumption here because we have effectively created both our `tfidf` RDD and `newsgroups` RDD from a series of `map` transformations on the same original RDD that preserved the partitioning structure.

现在我们有了正确格式的输入 RDD，我们只需将其传递给朴素贝叶斯`train`函数：

```scala
val model = NaiveBayes.train(train, lambda = 0.1)

```

让我们在测试数据集上评估该模型的性能。 我们将从`20news-bydate-test`目录加载原始测试数据，再次使用`wholeTextFiles`将每条消息读取到 RDD 元素中。 然后，我们将使用与`newsgroups`RDD 相同的方式从文件路径中提取类标签：

```scala
val testPath = "/PATH/20news-bydate-test/*" 
val testRDD = sc.wholeTextFiles(testPath) 
val testLabels = testRDD.map { case (file, text) => 
  val topic = file.split("/").takeRight(2).head 
  newsgroupsMap(topic) 
}

```

转换测试数据集中的文本的过程与转换训练数据的过程相同-我们将先应用`tokenize`函数，然后进行词频转换，然后再次使用从训练数据计算的相同 IDF 将 TF 向量转换为 TF-IDF 向量。 最后，我们将使用 TF-IDF 向量压缩测试类标签，并创建我们的测试`RDD[LabeledPoint]`：

```scala
val testTf = testRDD.map { case (file, text) => 
  hashingTF.transform(tokenize(text)) } 
val testTfIdf = idf.transform(testTf) 
val zippedTest = testLabels.zip(testTfIdf) 
val test = zippedTest.map { case (topic, vector) => 
  LabeledPoint(topic, vector) }

```

Note that it is important that we use the training set IDF to transform the test data, as this creates a more realistic estimation of model performance on new data, which might potentially contain terms that the model has not yet been trained on. It would be "cheating" to recompute the IDF vector based on the test dataset and, more importantly, would potentially lead to incorrect estimates of optimal model parameters selected through cross-validation.

现在，我们准备好计算模型的预测和真实类标签。 我们将使用此 RDD 来计算模型的精度和多类加权 F 度量：

```scala
val predictionAndLabel = test.map(p =>       
  (model.predict(p.features),   p.label)) 
val accuracy = 1.0 * predictionAndLabel.filter
  (x => x._1 == x._2).count() / test.count() 
val metrics = new MulticlassMetrics(predictionAndLabel) 
println(accuracy) 
println(metrics.weightedFMeasure)

```

The weighted F-measure is an overall measure of precision and recall performance (where, like area under an ROC curve, values closer to 1.0 indicate better performance), which is then combined through a weighted averaged across the classes.

我们可以看到，我们的简单多类朴素贝叶斯模型在准确率和 F-度量方面都达到了接近 80%：

```scala
0.7928836962294211
0.7822644376431702

```

# 评估文本处理的影响

文本处理和 TF-IDF 加权是设计用于降低原始文本数据的维度并从原始文本数据中提取某些结构的特征提取技术的示例。 我们可以通过比较在原始文本数据上训练的模型和在处理后的文本数据和 TF-IDF 加权文本数据上训练的模型的性能，来看到应用这些处理技术的影响。

# 在 20 个新闻组数据集中比较原始要素和处理后的 TF-IDF 要素

在本例中，我们将简单地将散列术语频率转换应用于使用文档文本的简单空格拆分获得的原始文本标记。 我们将根据此数据训练一个模型，并评估测试集上的性能，就像我们对使用 TF-IDF 功能训练的模型所做的那样：

```scala
val rawTokens = rdd.map { case (file, text) => text.split(" ") } 
val rawTF = texrawTokenst.map(doc => hashingTF.transform(doc)) 
val rawTrain = newsgroups.zip(rawTF).map { case (topic, vector)  
  => LabeledPoint(newsgroupsMap(topic), vector) } 
val rawModel = NaiveBayes.train(rawTrain, lambda = 0.1) 
val rawTestTF = testRDD.map { case (file, text) => 
  hashingTF.transform(text.split(" ")) } 
val rawZippedTest = testLabels.zip(rawTestTF) 
val rawTest = rawZippedTest.map { case (topic, vector) => 
  LabeledPoint(topic, vector) } 
val rawPredictionAndLabel = rawTest.map(p => 
  (rawModel.predict(p.features), p.label)) 
val rawAccuracy = 1.0 * rawPredictionAndLabel.filter(x => x._1 
  == x._2).count() / rawTest.count() 
println(rawAccuracy) 
val rawMetrics = new MulticlassMetrics(rawPredictionAndLabel) 
println(rawMetrics.weightedFMeasure)

```

也许令人惊讶的是，原始模型做得相当好，尽管精确度和 F-度量都比 TF-IDF 模型低几个百分点。 这也在一定程度上反映了朴素贝叶斯模型非常适合原始频率计数形式的数据：

```scala
0.7661975570897503
0.7653320418573546

```

# 基于 Spark 2.0 的文本分类

在本节中，我们将使用 libsvm 版本的*20newsgroup*数据来使用基于 Spark DataFrame 的 API 对文本文档进行分类。 在当前版本的 Spark libsvm 中支持 3.22 版([https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/))

从以下链接下载 libsvm 格式的数据，并将输出文件夹复制到 Spark-2.0.x 下。

有关*20 新闻组 libsvm*数据，请访问以下链接：[https://1drv.ms/f/s！Av6fk5nQi2j-iF84quUlDnJc6G6D](https://1drv.ms/f/s!Av6fk5nQi2j-iF84quUlDnJc6G6D)

从`org.apache.spark.ml`导入适当的包并创建包装器 Scala：

```scala
package org.apache.spark.examples.ml 

import org.apache.spark.SparkConf 
import org.apache.spark.ml.classification.NaiveBayes 
import        

org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator 

import org.apache.spark.sql.SparkSession 

object DocumentClassificationLibSVM { 
  def main(args: Array[String]): Unit = { 

  } 
}

```

接下来，我们将把`libsvm`数据加载到 Spark DataFrame 中：

```scala
val spConfig = (new SparkConf).setMaster("local")
  .setAppName("SparkApp") 
val spark = SparkSession 
  .builder() 
  .appName("SparkRatingData").config(spConfig) 
  .getOrCreate() 

val data = spark.read.format("libsvm").load("./output/20news-by-
  date-train-libsvm/part-combined") 

val Array(trainingData, testData) = data.randomSplit(Array(0.7,
  0.3), seed = 1L)

```

从`org.apache.spark.ml.classification.NaiveBayes`类实例化`NaiveBayes`模型并训练该模型：

```scala
val model = new NaiveBayes().fit(trainingData) 
val predictions = model.transform(testData) 
predictions.show()

```

下表是 Predictions DataFrame`.show()`命令的输出：

```scala
+----+-------------------+--------------------+-----------------+----------+
|label|     features     |    rawPrediction   |   probability   |prediction|
+-----+------------------+--------------------+-----------------+----------+
|0.0|(262141,[14,63,64...|[-8972.9535882773...|[1.0,0.0,1.009147...| 0.0|
|0.0|(262141,[14,329,6...|[-5078.5468878602...|[1.0,0.0,0.0,0.0,...| 0.0|
|0.0|(262141,[14,448,5...|[-3376.8302696656...|[1.0,0.0,2.138643...| 0.0|
|0.0|(262141,[14,448,5...|[-3574.2782864683...|[1.0,2.8958758424...| 0.0|
|0.0|(262141,[14,535,3...|[-5001.8808481928...|[8.85311976855360...| 12.0|
|0.0|(262141,[14,573,8...|[-5950.1635030844...|[1.0,0.0,1.757049...| 0.0|
|0.0|(262141,[14,836,5...|[-8795.2012408412...|[1.0,0.0,0.0,0.0,...| 0.0|
|0.0|(262141,[14,991,2...|[-1892.8829282793...|[0.99999999999999...| 0.0|
|0.0|(262141,[14,1176,...|[-4746.2275710890...|[1.0,5.8201E-319,...| 0.0|
|0.0|(262141,[14,1379,...|[-7104.8373572933...|[1.0,8.9577444139...| 0.0|
|0.0|(262141,[14,1582,...|[-5473.6206675848...|[1.0,5.3185120345...| 0.0|
|0.0|(262141,[14,1836,...|[-11289.582479676...|[1.0,0.0,0.0,0.0,...| 0.0|
|0.0|(262141,[14,2325,...|[-3957.9187837274...|[1.0,2.1880375223...| 0.0|
|0.0|(262141,[14,2325,...|[-7131.2028421844...|[1.0,2.6110663778...| 0.0|
|0.0|(262141,[14,3033,...|[-3014.6430319605...|[1.0,2.6341580467...| 0.0|
|0.0|(262141,[14,4335,...|[-8283.7207917560...|[1.0,8.9559011053...| 0.0|
|0.0|(262141,[14,5173,...|[-6811.3466537480...|[1.0,7.2593916980...| 0.0|
|0.0|(262141,[14,5232,...|[-2752.8846541292...|[1.0,1.8619374091...| 0.0|
|0.0|(262141,[15,5173,...|[-8741.7756643949...|[1.0,0.0,2.606005...| 0.0|
|0.0|(262141,[168,170,...|[-41636.025208445...|[1.0,0.0,0.0,0.0,...| 0.0|
+----+--------------------+-------------------+-------------------+--------+

```

测试模型的准确性：

```scala
val accuracy = evaluator.evaluate(predictions) 
println("Test set accuracy = " + accuracy) 
spark.stop()

```

该模型的精度高于`0.8`，如以下输出所示：

```scala
Test set accuracy = 0.8768458357944477
Accuracy is better as the Naive Bayes implementation has improved 
from Spark 1.6 to Spark 2.0

```

# Word2vec 型号

到目前为止，我们一直使用词袋向量，可选择使用某种权重方案(如 TF-IDF)来表示文档中的文本。 最近流行的另一类模型与将单个单词表示为向量有关。

这些通常以某种方式基于语料库中单词之间的共现统计。 一旦计算出矢量表示，我们就可以像使用 TF-IDF 矢量一样使用这些矢量(例如将它们用作其他机器学习模型的功能)。 一种这样的常见用例是基于两个单词的向量表示来计算它们之间相对于其含义的相似度。

Word2vec 指的是这些模型之一的具体实现，通常称为**分布式矢量表示**。 MLlib 模型使用**跳过语法**模型，该模型寻求学习考虑单词出现的上下文的向量表示。

虽然详细介绍 word2vec 超出了本书的范围，但是 Spark 在[http://spark.apache.org/docs/latest/mllib-feature-extraction.html#word2vec](http://spark.apache.org/docs/latest/mllib-feature-extraction.html#word2vec)上的文档包含了关于该算法的更多细节以及到参考实现的链接。

2vec 一词背后的主要学术论文之一是*Tomas Mikolov*，*Kai Chen*，*Greg Corrado*和*Jeffrey Dean*。 *向量空间中词表示的有效估计*。 *载于 ICLR 研讨会论文集*，*2013*。

它在[http://arxiv.org/pdf/1301.3781.pdf](http://arxiv.org/pdf/1301.3781.pdf)上提供。

单词向量表示领域的另一个最近的模型是[http://www-nlp.stanford.edu/projects/glove/](http://www-nlp.stanford.edu/projects/glove/)的手套。

您还可以利用第三方库来进行词性标记。 例如，Stanford NLP 库可以与 Scala 代码挂钩。 请参考此讨论主题([http://stackoverflow.com/questions/18416561/pos-tagging-in-scala](http://stackoverflow.com/questions/18416561/pos-tagging-in-scala))了解有关如何操作的更多详细信息。

# 在 20 个新闻组数据集上使用 Spark MLlib 的 word2vec

在 Spark 中训练 word2vec 模型相对简单。 我们将传入一个 RDD，其中每个元素都是一个术语序列。 我们可以使用已经创建的标记化文档的 RDD 作为模型的输入。

```scala
object Word2VecMllib {
  def main(args: Array[String]) {
  val sc = new SparkContext("local[2]", "Word2Vector App")
  val path = "./data/20news-bydate-train/alt.atheism/*"
  val rdd = sc.wholeTextFiles(path)
  val text = rdd.map { case (file, text) => text }
  val newsgroups = rdd.map { case (file, text) =>             
    file.split("/").takeRight(2).head }
  val newsgroupsMap =       
    newsgroups.distinct.collect().zipWithIndex.toMap
  val dim = math.pow(2, 18).toInt
  var tokens = text.map(doc => TFIDFExtraction.tokenize(doc))
  import org.apache.spark.mllib.feature.Word2Vec
  val word2vec = new Word2Vec()
  val word2vecModel = word2vec.fit(tokens)
    word2vecModel.findSynonyms("philosophers", 5).foreach(println)
  sc.stop()
  }
}

```

我们的代码在 Scala 对象`Word2VecMllib`中：

```scala
import org.apache.spark.SparkContext
import org.apache.spark.mllib.linalg.{SparseVector => SV}
object Word2VecMllib {
  def main(args: Array[String]) {
  }
}

```

让我们从加载文本文件开始：

```scala
val sc = new SparkContext("local[2]", "Word2Vector App")
val path = "./data/20news-bydate-train/alt.atheism/*"
val rdd = sc.wholeTextFiles(path)
val text = rdd.map { case (file, text) => text }
val newsgroups = rdd.map { case (file, text) =>
  file.split("/").takeRight(2).head }
val newsgroupsMap =      
  newsgroups.distinct.collect().zipWithIndex.toMap
val dim = math.pow(2, 18).toInt
  var tokens = text.map(doc => TFIDFExtraction.tokenize(doc))

```

我们使用 TF-IDF 创建的令牌作为 word2vec 的起点。 让我们首先初始化对象并设置种子：

```scala
import org.apache.spark.mllib.feature.Word2Vec
 val word2vec = new Word2Vec()

```

现在，让我们通过对 TF-IDF 标记调用`word2vec.fit()`来创建模型：。

```scala
val word2vecModel = word2vec.fit(tokens)

```

在对模型进行训练时，您将看到一些输出。

经过训练后，我们可以很容易地找到给定术语的前 20 个同义词(即，通过词向量之间的余弦相似度计算出与输入术语最相似的术语)。 例如，要查找与`philosopher`最相似的 20 个术语，请使用以下代码行：

```scala
word2vecModel.findSynonyms(philosophers", 5).foreach(println)
sc.stop()

```

从下面的输出中我们可以看到，大多数术语都与曲棍球或其他有关：

```scala
(year,0.8417112940969042) (motivations,0.833017707021745) (solution,0.8284719617235932) (whereas,0.8242997325042509) (formed,0.8042383351975712)

```

# 在 20 个新闻组数据集上使用 Spark ML 的 word2vec

在本节中，我们将了解如何使用 Spark ML DataFrame 和 Spark 2.0.x 的更新实现来创建 Word2Vector 模型。

我们将从数据集创建一个 DataFrame：

```scala
val spConfig = (new SparkConf).setMaster("local").setAppName("SparkApp")
val spark = SparkSession
  .builder
  .appName("Word2Vec Sample").config(spConfig)
  .getOrCreate()
import spark.implicits._
val rawDF = spark.sparkContext
  .wholeTextFiles("./data/20news-bydate-train/alt.atheism/*")
  val temp = rawDF.map( x => {
    (x._2.filter(_ >= ' ').filter(! _.toString.startsWith("(")) )
    })
  val textDF = temp.map(x => x.split(" ")).map(Tuple1.apply)
    .toDF("text")

```

然后创建`Word2Vec`类并在上面创建的 DataFrame`textDF`上训练模型：

```scala
val word2Vec = new Word2Vec()
  .setInputCol("text")
  .setOutputCol("result")
  .setVectorSize(3)
  .setMinCount(0)
val model = word2Vec.fit(textDF)
val result = model.transform(textDF)
  result.select("result").take(3).foreach(println)
)

```

现在让我们试着找出`hockey`的一些同义词：
如下

```scala
val ds = model.findSynonyms("philosophers", 5).select("word")
  ds.rdd.saveAsTextFile("./output/philiosphers-synonyms" +             System.nanoTime())
  ds.show(

```

将生成以下输出：

```scala
 +--------------+ | word         | +--------------+ | Fess         | | guide        | |validinference| | problems.    | | paperback    | +--------------+

```

如您所见，结果与我们使用 RDD 得到的结果大不相同。 这是因为 Spark 1.6 和 Spark 2.0/2.1 中的 Word2Vector 转换的两种实现不同。

# 简略的 / 概括的 / 简易判罪的 / 简易的

在本章中，我们更深入地研究了更复杂的文本处理，并探索了 MLlib 的文本特征提取能力，特别是 TF-IDF 术语加权方案。 我们介绍了使用生成的 TF-IDF 特征向量计算文档相似度和训练新闻组主题分类模型的示例。 最后，您了解了如何使用 MLlib 的尖端 word2vec 模型计算文本语料库中单词的向量表示，并使用训练好的模型查找上下文含义与给定单词相似的单词。 我们还研究了在 Spark ML 中使用 word2vec

在下一章中，我们将介绍在线学习，您将了解 Spark Streaming 与在线学习模式的关系。