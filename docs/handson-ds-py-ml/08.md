# 八、处理真实世界的数据

在本章中，我们将讨论处理真实世界数据的挑战，以及您可能会遇到的一些怪癖。 本章首先讨论偏差-方差权衡，这是一种更有原则的方式来讨论数据可能过度拟合和不足拟合的不同方式，以及它们之间的相互关系。 然后，我们将讨论 k-折交叉验证技术，它是您克服过度匹配的重要工具，并了解如何使用 Python 实现它。

接下来，我们将分析在对数据实际应用任何算法之前清理和规范化数据的重要性。 我们看到了一个确定网站上最受欢迎页面的例子，它将展示清理数据的重要性。 本章还介绍了记住对数字数据进行标准化的重要性。 最后，我们来看看如何检测和处理离群值。

具体而言，本章涵盖以下主题：

*   分析偏差/方差权衡
*   K 重交叉验证的概念及其实现
*   清理和规范化数据的重要性
*   确定网站热门页面的示例
*   对数字数据进行归一化处理
*   检测异常值并对其进行处理

# 偏差/方差权衡

在处理真实世界的数据时，我们面临的一个基本挑战是对数据、模型或预测的回归过高，而不是过低。 当我们谈论拟合不足和拟合过高时，我们通常可以在偏差和方差的背景下谈论这一点，以及偏差和方差之间的权衡。 那么，让我们来谈谈这意味着什么。

所以从概念上讲，偏差和方差非常简单。 偏差就是你离正确的值有多远，也就是说，在预测正确的整体价值时，你的预测总体上有多好。 如果你把你所有的预测都取其均值，它们是或多或少是正确的呢？ 还是你的错误总是偏向一个方向或另一个方向？ 如果是这样的话，那么你的预测就偏向了某个方向。

方差仅仅是衡量你的预测分布有多广，分散程度有多大。 所以，如果你的预测到处都是，那就是很高的方差。 但是，如果他们非常关注正确的值是什么，或者在高偏差的情况下甚至是不正确的值，那么您的方差就很小。

让我们来看一些例子。 让我们设想一下，下面的仪表板代表了我们正在进行的一系列预测，其中我们试图预测的真正价值位于靶心：

![](img/fae2cf23-43e6-4526-b005-39df58463d04.png)

*   从左上角的飞镖开始，你可以看到我们的点都散落在中心周围。 所以总体而言，你知道平均误差是相当接近实际的。 我们的偏差实际上很低，因为我们的预测都围绕着相同的正确点。 但是，我们有很高的方差，因为这些点散布在各地。 所以，这是一个低偏差和高方差的例子。
*   如果我们继续移动到右上角的飞镖，我们会发现我们的点都一直偏离它们应该在的位置，向西北方向倾斜。 所以这是我们预测中高度偏向的一个例子，它们总是有一定的偏差。 我们的方差很小，因为它们都紧紧地聚集在错误的点上，但至少它们离得很近，所以我们的预测是一致的。 这是很低的方差。 但是，这种偏见是很高的。 再说一遍，这是高度偏向，低方差。
*   在左下角的飞镖图中，你可以看到我们的预测分散在错误的平均值附近。 因此，我们有很高的偏见；一切都偏向了不应该出现的地方。 但我们的差异也很大。 所以，这是两个世界中最糟糕的；在这个例子中，我们有很高的偏差和很高的方差。
*   最后，在一个美妙的完美世界里，你会有一个例子，比如右下角的仪表盘，我们有低的偏向，所有的东西都围绕着它应该在的位置居中，而低的方差，所有的东西都紧紧地围绕着它们应该在的位置聚集。 所以，在一个完美的世界里，这就是你最终得到的结果。

在现实中，你经常需要在偏差和方差之间做出选择。 归根结底是数据拟合过高，而不是数据拟合不足。 让我们看一下下面的例子：

![](img/7a856ab7-9d88-404c-8cfb-97365495b525.png)![](img/0386221c-18f7-4cc9-88aa-9139c77d4ba5.png)

这是对偏见和差异的一种不同的思考方式。 所以，在左边的图表中，我们有一条直线，你可以认为它的方差非常小，相对于这些观测值。 所以这条线的方差不是很大，也就是说，方差很小。 但偏差，即每个点的误差，实际上是很高的。

现在，对比一下右边图表中的过度拟合数据，在那里我们已经不遗余力地与观测结果进行了拟合。 这条线有很高的方差，但偏差很小，因为每个单独的点都非常接近它应该在的位置。 这是一个例子，我们用方差换取了偏见。

归根结底，你不是要减少偏差或者只是减少方差，你想要减少错误。 这才是真正重要的，事实证明，你可以将误差表达为偏差和方差的函数：

![](img/459a0f30-4b0c-4dfd-a4fb-91b39c005369.png)

看看这个，误差等于偏差的平方加上方差。 因此，这两个因素都会导致整体误差，而偏见实际上对整体误差的贡献更大。 但请记住，你真正想要最小化的是误差，而不是具体的偏差或方差，而且过于复杂的模型最终可能会有高方差和低偏差，而过于简单的模型将有低方差和高偏差。 然而，它们最终可能都会有类似的错误术语。 当您尝试拟合您的数据时，您只需要在这两件事之间找到合适的中间点即可。 在接下来的小节中，我们将讨论一些更有原则的方法来实际避免过度适应。 但是，这只是我想要理解的偏见和差异的概念，因为人们确实在谈论它，你会被期望知道这是什么意思。

现在让我们把它与本书前面的一些概念联系起来。 例如，在 k 最近的邻居中，如果我们增加 K 的值，我们开始将我们的邻居平均分布到更大的区域。 这有减少方差的效果，因为我们在一个更大的空间里把事情理顺了，但这可能会增加我们的偏见，因为我们将获得更多的人口，他们可能与我们开始的点越来越不相关。 通过在更多的邻居上平滑 kNN，我们可以减少方差，因为我们在更多的值上进行平滑。 但是，我们可能会引入偏见，因为我们引入了越来越多的观点，而这些观点与我们开始时的观点几乎没有什么关系。

决策树是另一个例子。 我们知道单个决策树容易过度拟合，因此这可能意味着它具有很高的方差。 但是，随机森林试图通过减少偏差来折衷这种差异，它通过拥有多棵随机变化的树来实现这一点，并将它们的所有解平均在一起。 这就像我们通过增加 kNN 中的 K 来求平均值：我们可以通过使用多个决策树来求取决策树的结果的平均值，使用随机森林类似的想法。

这是一种偏差-方差的权衡。 你知道你必须在你的价值观总体上有多准确，它们的分布有多广，或者它们有多紧密地聚集在一起之间做出决定。 这就是偏差-方差的权衡，它们都会导致总体误差，这才是您真正关心的最小化误差。 所以，请记住这些术语！

# K-折交叉验证，避免过度拟合

在这本书的前面，我们谈到训练和测试是防止过度拟合和实际测量模型在以前从未见过的数据上表现良好的一种很好的方法。 我们可以用一种叫做 k-折交叉验证的技术将这一点提升到一个新的水平。 因此，让我们来讨论一下您的武器库中用于对抗过度匹配的强大工具；k-折交叉验证，并了解它是如何工作的。

回想一下培训/测试，我们的想法是将我们正在构建机器学习模型的所有数据分成两个部分：训练数据集和测试数据集。 我们的想法是，我们只使用训练数据集中的数据来训练我们的模型，然后使用我们为测试数据集保留的数据来评估它的性能。 这可以防止我们过度拟合已有的数据，因为我们是在根据以前从未见过的数据来测试模型。

然而，列车/测试仍有其局限性：您最终仍可能过度适应您的特定列车/测试分离。 也许您的训练数据集并不能真正代表整个数据集，而过多的内容最终出现在您的训练数据集中，这就扭曲了事情。 所以，这就是 k-折交叉验证的用武之地，它需要训练/测试，并将其提升一个档次。

这个想法虽然听起来很复杂，但其实相当简单：

1.  我们没有将数据分成两个桶，一个用于训练，一个用于测试，而是将其分为 K 个桶。
2.  我们保留了其中一个桶用于测试，用于评估我们模型的结果。
3.  我们根据剩余的桶 K-1 来训练我们的模型，然后我们使用我们的测试数据集，并使用它来评估我们的模型在所有不同的训练数据集中的表现如何。
4.  我们将这些最终的误差度量(即那些 r 平方的值)平均在一起，以从 k 重交叉验证中获得最终的误差度量。

仅此而已。 这是一种更健壮的训练/测试方式，这也是其中一种方式。

现在，您可能会想，如果我过多地适应了我保留的那个测试数据集，该怎么办？ 对于每个训练数据集，我仍然使用相同的测试数据集。 如果测试数据集也不能真正代表事物，该怎么办？

还有一些 k-折交叉验证的变体也会将这一点随机化。 所以，你可以随机选择每次训练数据集，然后继续随机地将东西分配到不同的桶中，并测量结果。 但通常，当人们谈论 k-折交叉验证时，他们谈论的是这种特定的技术，即您预留一个存储桶用于测试，其余的存储桶用于训练，当您为每个训练数据集构建模型时，您将根据测试数据集评估所有的训练数据集。

# 使用 SCRICKIT-LEARN 的 k-折交叉验证示例

幸运的是，SCRKIT-LEARN 让这件事变得非常容易，甚至比做普通的训练/测试更容易！ 做 k-折交叉验证非常简单，所以你也可以直接做。

现在，这一切在实践中的工作方式是，你会有一个你想要调整的模型，你会有这个模型的不同变体，不同的参数，你可能想要对它进行调整，对吗？

例如，多项式拟合的多项式次。 因此，我们的想法是尝试您的模型的不同值、不同的变量，使用 k-折交叉验证来测量它们，并找到针对您的测试数据集的最小误差的值。 这就是你的甜蜜点。 在实践中，您希望使用 k-折交叉验证来根据测试数据集来度量模型的准确性，并且只需要不断改进该模型，不断尝试其中的不同值，不断尝试该模型的不同变体，甚至可能完全不同的模型，直到您找到使用 k-折交叉验证来最大限度地减少误差的技术。

让我们深入研究一个示例，看看它是如何工作的。 我们将再次将其应用于我们的 Iris 数据集，重新访问 SVC，我们将尝试 k-折交叉验证，看看它有多简单。 让我们在这里使用一些真实的 Python 代码实际实施 k-折交叉验证和训练/测试。 你会发现它实际上非常容易使用，这是一件好事，因为这是一种你应该用来衡量你的模型在监督学习中的准确性和有效性的技术。

请往前走，打开`KFoldCrossValidation.ipynb`，如果你愿意，就跟着走。 我们将再次查看虹膜数据集；还记得我们在讨论降维时介绍过这一点吗？

提醒您一下，虹膜数据集包含 150 个虹膜花尺寸，其中每朵花都有其花瓣的长度和宽度，以及其萼片的长度和宽度。 我们也知道每朵花属于 3 种不同的蝴蝶花中的哪一种。 这里的挑战是建立一个模型，能够成功地预测虹膜花的种类，只要给出它的花瓣和萼片的长度和宽度。 那么，让我们继续这样做吧。

我们将使用 SVC 模型。 如果你还记得的话，这只是一种对数据进行分类的方法，非常可靠。 如果你需要去重温一下你的记忆，这里有一节是关于这方面的：

```py
import numpy as np 
from sklearn import cross_validation 
from sklearn import datasets 
from sklearn import svm 

iris = datasets.load_iris() # Split the iris data into train/test data sets with #40% reserved for testing 
X_train, X_test, y_train, y_test = cross_validation.train_test_split(iris.data,                                     iris.target, test_size=0.4, random_state=0) 

# Build an SVC model for predicting iris classifications #using training data 
clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train) 

# Now measure its performance with the test data 
clf.score(X_test, y_test) 

```

我们所做的是使用 SCRICKIT-LEARN 的`cross_validation`库，我们从做一个传统的列车测试拆分开始，只做一个单一的列车/测试拆分，看看它会如何工作。

要做到这一点，我们有一个`train_test_split()`函数，它使它变得非常简单。 因此，我们的工作方式是向`train_test_split()`提供一组特征数据。 `iris.data`只包含每朵花的所有实际尺寸。 `iris.target`基本上是我们试图预测的事情。

在本例中，它包含每朵花的所有物种。 `test_size`说明我们要培训的百分比与测试的百分比。 因此，0.4 意味着我们将随机抽取 40%的数据用于测试，60%用于培训目的。 这给我们带来的是 4 个数据集，基本上是针对特征数据和目标数据的训练数据集和测试数据集。 因此，`X_train`最终包含了我们虹膜测量值的 60%，而`X_test`包含了用于测试我们模型结果的 40%的测量值。 `y_train`和`y_test`包含每一段的实际物种。

然后，我们继续构建一个 SVC 模型，用于预测给定的测量结果的虹膜种类，并且我们只使用训练数据来构建该模型。 我们使用线性核，仅使用训练特征数据和训练物种数据(即目标数据)来拟合该 SVC 模型。 我们称这个模型为`clf`。 然后，我们调用`clf`上的`score()`函数，根据我们的测试数据集来测量它的性能。 因此，我们根据为虹膜测量保留的测试数据和测试虹膜物种对此模型进行评分，看看它做得有多好：

![](img/7d9b6519-0e08-4385-af2a-4dc643935313.jpg)

事实证明，它做得真的很好！ 在 96%以上的时间里，我们的模型能够正确地预测出以前从未见过的虹膜的种类，仅仅是基于对虹膜的测量。 这真是太酷了！

但是，这是一个相当小的数据集，如果我没记错的话，大约 150 朵花。 所以，我们只用 150 朵花中的 60%进行训练，只用 40%的 150 朵花进行测试。 这仍然是一个相当小的数字，所以我们可能仍然过度适应我们所做的特定的列车/测试拆分。 因此，让我们使用 k-折交叉验证来防止这种情况。 事实证明，使用 k-折交叉验证，尽管它是一种更健壮的技术，但实际上比训练/测试更容易使用。 所以，这真是太酷了！ 那么，让我们看看它是如何工作的：

```py
# We give cross_val_score a model, the entire data set and its "real" values, and the number of folds: 
scores = cross_validation.cross_val_score(clf, iris.data, iris.target, cv=5) 

# Print the accuracy for each fold: 
print scores 

# And the mean accuracy of all 5 folds: 
print scores.mean() 

```

我们已经有了一个模型，也就是我们为这个预测定义的 SVC 模型，您所需要做的就是在`cross_validation`包上调用`cross_val_score()`。 因此，您在此函数中传递一个给定类型的模型(`clf`)，即您拥有的所有测量值的整个数据集，即我的所有特征数据(`iris.data`)和我的所有目标数据(所有物种)`iris.target`。

我想要`cv=5`，这意味着它实际上将使用 5 个不同的训练数据集，同时保留`1`用于测试。 基本上，它将运行 5 次，这就是我们需要做的全部工作。 这将根据整个数据集自动评估我们的模型，分成五种不同的方式，并返回给我们单独的结果。

如果我们打印回它的输出，它会给我们返回一个列表，其中列出了每一次迭代的实际误差度量，也就是每一次折叠的实际误差度量。 我们可以将它们相加，以基于 k 重交叉验证获得总体误差度量：

![](img/705c48f3-a3b4-4d63-ae15-c4159c0e5c76.png)

当我们做超过 5 倍的时候，我们可以看到我们的结果比我们想象的还要好！98%的准确率。 这真是太酷了！ 事实上，在几次跑动中，我们都有完美的准确度。 所以这是相当令人惊叹的东西。

现在让我们看看我们是否能做得更好。 我们以前用的是线性核，如果我们用多项式核，会不会更花哨呢？ 这是否会过度拟合，或者实际上会更好地符合我们现有的数据？ 这在某种程度上取决于这些花瓣尺寸和实际物种之间是否存在线性关系或多项式关系。 所以，让我们来试一试：

```py
clf = svm.SVC(kernel='poly', C=1).fit(X_train, y_train)scores = cross_validation.cross_val_score(clf, iris.data, iris.target, cv=5)print scoresprint scores.mean()

```

我们将使用相同的技术重新运行一次。 但这一次，我们使用的是多项式核。 我们会将其与我们的训练数据集相匹配，在这种情况下，您适合的位置并不重要，因为`cross_val_score()`会不断地为您重新运行它：

![](img/705c48f3-a3b4-4d63-ae15-c4159c0e5c76.png)

事实证明，当我们使用多项式拟合时，我们最终得到的总得分甚至比我们最初的得分还要低。 所以，这告诉我们多项式核可能是过拟合的。 当我们使用 k-折交叉验证时，它显示的分数实际上比线性核要低。

这里重要的一点是，如果我们只使用单一的列车/测试拆分，我们就不会意识到我们过度适应了。 如果我们只在这里进行一次训练/测试拆分，实际上会得到相同的结果，就像我们在线性内核上所做的那样。 因此，如果我们没有使用 k-折交叉验证，我们可能会无意中过度拟合那里的数据，甚至不知道它的存在。 因此，这是一个很好的例子，说明了 k-折叠法的用武之地，并警告你过度拟合，在这种情况下，单一的列车/测试拆分可能无法捕捉到这一点。 所以，把它放在你的工具箱里。

如果你还想玩这个，那就试试不同的程度吧。 因此，您实际上可以指定不同的度数。 多项式核的默认值为 3 度，但您可以尝试不同的角度，也可以尝试两个角度。

这样做好点了吗？ 如果你降到 1，它基本上退化成线性核，对吗？ 所以，可能仍然存在多项式关系，也可能只是二次多项式。 试一试，看看你能得到什么回报。 这是 k 重交叉验证。 正如你所看到的，它非常容易使用，这要归功于 SCRKIT-LISTING。 这是以一种非常健壮的方式衡量你的模型有多好的一个重要方法。

# 数据清理和规范化

现在，这是最简单的部分之一，但它可能是整本书中最重要的部分。 我们将讨论如何清理您的输入数据，这是您将花费大量时间来做的事情。

如何清理输入数据和理解原始输入数据将对结果质量产生巨大影响-甚至可能比您选择的模型或调整模型的效果更好。 所以，注意了；这是很重要的事情！

Cleaning your raw input data is often the most important, and time-consuming, part of your job as a data scientist!

让我们来谈谈数据科学的一个令人不快的事实，那就是，你实际上花了大部分时间来清理和准备你的数据，而实际上只花了相对较少的时间来分析它和尝试新的算法。 它并不像人们一直认为的那样光彩照人。 但是，这是一件非常重要的事情，需要注意。

你可能会在原始数据中发现很多不同的东西。 进入你的数据，仅仅是原始数据，将会变得非常肮脏，它将会受到许多不同方式的污染。 如果你不处理它，它会歪曲你的结果，最终会让你的企业做出错误的决定。

如果你犯了一个错误，你摄取了一大堆错误的数据，没有解释它，没有清理这些数据，你告诉你的企业根据这些结果做一些后来被证明是完全错误的事情，你将会有很大的麻烦！。 所以，请注意！

您需要注意很多不同类型的问题和数据：

*   **局外人**：也许你有一些人在你的数据中表现得有些奇怪，当你挖掘他们的时候，发现他们是你一开始就不应该看的数据。 一个很好的例子是，如果你正在查看网络日志数据，你会看到一个会话 ID 一遍又一遍地重复，它一直在以荒谬的速度做一些人类永远做不到的事情。 你可能看到的是一个机器人，一个在某个地方运行的脚本，它实际上是在抓取你的网站。 这甚至可能是某种恶意攻击。 但不管怎样，你不想让那些行为数据告诉你的模型，这些模型是为了预测使用你网站的真人的行为。 因此，观察离群值是识别在构建模型时可能想要从模型中剔除的数据类型的一种方法。
*   **缺失数据**：如果没有数据，您会怎么做？ 回到 Web 日志的例子中，您可能在该行有推荐人，也可能没有推荐人。 如果它不在那里你会怎么做？ 您是为丢失创建新的分类，还是为未指定的分类创建新分类？ 或者你会把这句话完全抛诸脑后？ 你必须考虑在那里做什么才是正确的。
*   **恶意数据**：可能有人试图利用您的系统，可能有人试图欺骗系统，而您不希望这些人逍遥法外。 比方说你在做一个推荐系统。 可能会有人试图捏造行为数据来宣传他们的新产品，对吗？ 所以，你需要警惕这类事情，并确保你识别出先令攻击，或其他类型的对你的输入数据的攻击，并从结果中过滤掉它们，不要让它们获胜。
*   **错误数据**：如果某个系统中的某个软件错误只是在某些情况下写出了错误的值，该怎么办？ 这是有可能发生的。 不幸的是，没有什么好办法让你知道这件事。 但是，如果您看到的数据看起来可疑，或者结果对您没有任何意义，深入挖掘有时会发现一个潜在的错误，而这个错误正是导致错误数据写入的首要原因。 也许在某种程度上，事情没有被正确地组合在一起。 也许并不是在整个会议期间都在举行会议。 例如，人们在浏览网站时可能会丢弃他们的会话 ID，而获得新的会话 ID。
*   **无关数据**：这里非常简单。 也许你只对纽约市人的数据感兴趣，或者出于某种原因。 在这种情况下，所有来自世界其他地方的人的数据都与你想要找出的东西无关。 您要做的第一件事就是丢弃所有这些数据，限制您的数据，将其缩减为您真正关心的数据。
*   **数据不一致**：这是一个大问题。 例如，在地址中，人们可以用许多不同的方式写相同的地址：他们可以缩写 Street，也可以不缩写 Street，他们可能根本不会在街道名称的末尾加上 Street。 他们可能会以不同的方式将行组合在一起，他们可能会拼写不同的东西，他们可能在美国使用邮政编码，或者在美国使用邮政编码加 4 代码，他们可能在上面有国家，他们可能没有国家。 你需要想办法弄清楚你看到的变化是什么，以及你如何才能将它们全部正常化。
*   也许我在看有关电影的数据。 一部电影在不同的国家可能有不同的名字，或者一本书在不同的国家可能有不同的名字，但它们的意思是一样的。 因此，您需要注意这些需要规范化数据的地方，其中相同的数据可以用多种不同的方式表示，您需要将它们组合在一起才能获得正确的结果。
*   **格式化**：这也可能是一个问题；内容的格式可能不一致。 以日期为例：在美国，我们总是做月、日、年(MM/DD/YY)，但在其他国家，他们可能会做日、月、年(DD/MM/YY)，谁知道呢。 您需要注意这些格式差异。 也许电话号码的区号周围有圆括号，也许没有；也许号码的每一部分之间有破折号，也许没有；也许社会保险号码有破折号，也可能没有。这些都是你需要注意的事情，你需要确保在处理过程中，格式上的差异不会被视为不同的实体或不同的分类。

因此，有很多事情需要注意，前面的列表只列出了主要的需要注意的事情。 记住：垃圾进，垃圾出。 你的模型和你给它的数据一样好，这是非常，非常正确的！ 如果您提供大量干净的数据，您可以拥有一个非常简单的模型，它的性能非常好，而且在一个更脏的数据集上，它实际上可能比一个复杂的模型执行得更好。

因此，确保你有足够的数据，高质量的数据往往是最重要的。 你会惊讶于现实世界中使用的一些最成功的算法是多么简单。 它们的成功取决于进入其中的数据的质量，以及进入其中的数据量。 你并不总是需要花哨的技巧才能获得好的结果。 通常，数据的质量和数量与其他任何东西一样重要。

永远要质疑你的结果！ 您不想只在得到不喜欢的结果时才回去查找输入数据中的异常。 这会给你的结果带来一种无意的偏见，让你喜欢或期待的结果不受质疑地通过，对吗？ 你总是想要质疑事情，以确保你一直在关注这些事情，因为即使你找到了你喜欢的结果，如果结果证明是错的，它仍然是错误的，它仍然会给你的公司带来错误的方向。 它可能会回来咬你一口。

举个例子，我有一个名为“不恨新闻”的网站。 这是非盈利性的，所以我不想通过告诉你这件事来赚钱。 比方说，我只是想在我拥有的这个网站上找到最受欢迎的页面。 这听起来是个很简单的问题，不是吗？ 我应该能够浏览我的网络日志，计算出每个页面有多少点击量，然后对它们进行排序，对吗？ 能有多难呢？！ 嗯，原来真的很难！ 因此，让我们深入到这个示例中，看看为什么它很困难，并查看一些必须进行的实际数据清理的示例。

# 正在清除 Web 日志数据

我们将展示清理数据的重要性。 我有一些来自我自己的小网站的网络日志数据。 我们只是想找出该网站上浏览量最高的页面。 听起来很简单，但正如您将看到的，这实际上很有挑战性！ 所以，如果你想跟着做，`TopPages.ipynb`就是我们从这里开始工作的笔记本。 我们开始吧！

实际上，我有一个访问日志，这是我从我的实际网站上获取的。 这是来自 Apache 的真实 HTTP 访问日志，包含在您的图书资料中。 因此，如果您确实想在这里继续下去，请确保更新路径以将访问日志移动到保存图书材料的位置：

```py
logPath = "E:\\sundog-consult\\Packt\\DataScience\\access_log.txt" 

```

# 在 Web 日志上应用正则表达式

因此，我从 Internet 上获取了以下小代码片段，它们将把 Apache 访问日志行解析为一系列字段：

```py
format_pat= re.compile( 
    r"(?P<host>[\d\.]+)\s" 
    r"(?P<identity>\S*)\s" 
    r"(?P<user>\S*)\s" 
    r"\[(?P<time>.*?)\]\s" 
    r'"(?P<request>.*?)"\s' 
    r"(?P<status>\d+)\s" 
    r"(?P<bytes>\S*)\s" 
    r'"(?P<referer>.*?)"\s' 
    r'"(?P<user_agent>.*?)"\s*' 
) 

```

此代码包含主机、用户、时间、实际页面请求、状态、引用人、`user_agent`(表示实际使用哪个浏览器查看此页面)等信息。 它构建了所谓的正则表达式，我们使用`re`库来使用它。 对于在大字符串上进行模式匹配，这基本上是一种非常强大的语言。 因此，我们实际上可以将该正则表达式应用于访问日志的每一行，并自动将该访问日志行中的信息位分组到这些不同的字段中。 让我们继续运行这个。

这里要做的显而易见的事情是，让我们快速创建一个小脚本，对我们遇到的每个被请求的 URL 进行计数，并统计它被请求的次数。 然后我们就可以对列表进行排序，得到我们的首页，对吗？ 听起来很简单！

因此，我们将构建一个名为`URLCounts`的小型 Python 字典。 我们将打开我们的日志文件，并且对于每一行，我们将应用我们的正则表达式。 如果它确实返回了与我们试图匹配的模式匹配成功的结果，我们会说，好的，这看起来像是我们访问日志中的一个像样的行。

让我们从中提取请求字段，它是实际的 HTTP 请求，也就是浏览器实际请求的页面。 我们将把它分为三个组件：它由 GET 或 POST 之类的操作组成；被请求的实际 URL；以及正在使用的协议。 如果信息被拆分，那么我们就可以查看该 URL 是否已经存在于我的字典中。 如果是，我将增加该 URL 被`1`遇到的次数；否则，我将为该 URL 引入一个新的字典条目，并将其初始化为`1`的值。 我对日志中的每一行执行此操作，以相反的数字顺序对结果进行排序，并将其打印出来：

```py
URLCounts = {}with open(logPath, "r") as f:    for line in (l.rstrip() for l in f):        match= format_pat.match(line)        if match:            access = match.groupdict()            request = access['request']            (action, URL, protocol) = request.split()            if URLCounts.has_key(URL):                URLCounts[URL] = URLCounts[URL] + 1            else:                URLCounts[URL] = 1results = sorted(URLCounts, key=lambda i: int(URLCounts[i]), reverse=True)for result in results[:20]:    print result + ": " + str(URLCounts[result])

```

所以，让我们继续运行它：

![](img/5299efb0-51ac-451c-b6ac-43d72797555d.png)

哎呀！ 我们在这里结束了这个古老的大错误。 它告诉我们，我们需要的不只是`1`的价值。 显然，我们得到的一些请求字段不包含操作、URL 和协议，它们包含其他内容。

让我们看看那里发生了什么事！ 因此，如果我们打印出不包含三个项目的所有请求，我们将看到实际显示的内容。 所以，我们在这里要做的是一个类似的小代码片段，但我们将在请求字段上进行拆分，并打印出没有得到预期的三个字段的案例。

```py
URLCounts = {}with open(logPath, "r") as f:    for line in (l.rstrip() for l in f):        match= format_pat.match(line)        if match:            access = match.groupdict()            request = access['request']            fields = request.split()            if (len(fields) != 3):                print fields

```

让我们看看里面到底有什么：

![](img/2e3d1bcb-e929-4fa6-a80f-8616c134b54d.png)

所以，我们有一堆空地。 这是我们的第一个问题。 但是，然后我们有了第一个填满垃圾的区域。 谁知道这是从哪里来的，但这显然是错误的数据。 好吧，好吧，让我们修改一下剧本。

# 修改一-过滤请求字段

实际上，我们只会丢弃请求中没有预期 3 个字段的任何行。 这似乎是一件合法的事情，因为这里面确实有完全无用的数据，我们这样做并不会错过任何东西。 因此，我们将修改我们的脚本来实现这一点。 在它实际尝试处理它之前，我们引入了一条`if (len(fields) == 3)`行。 我们将运行该命令：

```py
URLCounts = {}with open(logPath, "r") as f:    for line in (l.rstrip() for l in f):        match= format_pat.match(line)        if match:            access = match.groupdict()            request = access['request']            fields = request.split()            if (len(fields) == 3):                URL = fields[1]                if URLCounts.has_key(URL):                    URLCounts[URL] = URLCounts[URL] + 1                else:                    URLCounts[URL] = 1results = sorted(URLCounts, key=lambda i: int(URLCounts[i]), reverse=True)for result in results[:20]:    print result + ": " + str(URLCounts[result])

```

嘿，我们有结果了！

![](img/0f8ec1d7-97ce-4c7c-b1ed-ff868e626183.png)

但这看起来不像是我网站的首页。 记住，这是一个新闻网站。 所以，我们得到了一堆 PHP 文件命中，那就是 Perl 脚本。 那是怎么回事？ 我们的最高结果是这个`xmlrpc.php`脚本，然后是`WP_login.php`，然后是主页。 所以，不是很有用。 然后是`robots.txt`，然后是一堆 XML 文件。

你知道，当我后来调查这件事时，发现我的网站实际上受到了恶意攻击；有人试图侵入它。 这个`xmlrpc.php`脚本是他们试图猜测我密码的方式，他们试图使用登录脚本登录。 幸运的是，我在他们还没来得及访问这个网站之前就把他们关掉了。

这是将恶意数据引入我的数据流的一个例子，我必须将其过滤掉。 因此，通过观察这一点，我们可以看到，恶意攻击不仅查看 PHP 文件，而且还试图执行某些东西。 它不仅仅是在执行 GET 请求，它还在脚本上执行 POST 请求，以实际尝试在我的网站上执行代码。

# 修改两个-过滤 POST 请求

现在，我知道我关心的数据，你知道，本着我试图弄清楚的精神，人们从我的网站上获得网页。 因此，我应该做的一件合法的事情就是从这些日志中过滤掉任何非 GET 请求。 那么，接下来我们来做这件事。 我们将再次检查请求字段中是否有三个字段，然后还将检查操作是否为 GET。 如果不是，我们将完全忽略这一行：

```py
URLCounts = {}with open(logPath, "r") as f:    for line in (l.rstrip() for l in f):        match= format_pat.match(line)        if match:            access = match.groupdict()            request = access['request']            fields = request.split()            if (len(fields) == 3):                (action, URL, protocol) = fields                if (action == 'GET'):                    if URLCounts.has_key(URL):                        URLCounts[URL] = URLCounts[URL] + 1                    else:                        URLCounts[URL] = 1results = sorted(URLCounts, key=lambda i: int(URLCounts[i]), reverse=True)for result in results[:20]:    print result + ": " + str(URLCounts[result])

```

我们现在应该更接近我们想要的了，以下是前面代码的输出：

![](img/49ea2d9a-f169-429a-8873-46b8e18fbbf0.png)

是啊，这看起来更合理了。 但是，它仍然没有真正通过一项健全的检查。 这是一个新闻网站，人们访问它是为了阅读新闻。 他们真的在读我在上面只有几篇文章的小博客吗？ 我不这样认为!。 这看起来有点可疑。 那么，让我们稍微深入一下，看看到底是谁在浏览这些博客页面。 如果您实际进入该文件并手动检查它，您会发现许多博客请求实际上没有任何用户代理。 他们只有一个用户代理`-`，这非常不寻常：

![](img/b4326ae3-6106-4c4d-8aff-af6acc33730a.png)

如果一个拥有真实浏览器的真人试图获取这个页面，它会显示类似 Mozilla、Internet Explorer 或 Chrome 之类的内容。 因此，这些请求似乎来自某种铲运机。 同样，潜在的恶意流量无法识别其身份。

# 修改三-检查用户代理

也许，我们也应该查看 UserAgents，看看他们是否真的是发出请求的人。 让我们继续打印出我们遇到的所有不同的 UserAgent。 因此，本着实际上总结了我们看到的不同 URL 的代码精神，我们可以查看我们看到的所有不同的 UserAgent，并按照日志中最流行的`user_agent`字符串对它们进行排序：

```py
UserAgents = {}with open(logPath, "r") as f:    for line in (l.rstrip() for l in f):        match= format_pat.match(line)        if match:            access = match.groupdict()            agent = access['user_agent']            if UserAgents.has_key(agent):                UserAgents[agent] = UserAgents[agent] + 1            else:                UserAgents[agent] = 1results = sorted(UserAgents, key=lambda i: int(UserAgents[i]), reverse=True)for result in results:    print result + ": " + str(UserAgents[result])

```

我们得到以下结果：

![](img/bd809047-195b-4c82-b51a-b55840e01965.png)

你可以看到大部分看起来都是合法的。 所以，如果它是一个刮板，在这种情况下，它实际上是一个恶意攻击，但他们实际上假装是一个合法的浏览器。 但是这个破折号`user_agent`也经常出现。 我不知道那是什么，但我知道它不是真正的浏览器。

我看到的另一件事是来自蜘蛛和网络爬虫的大量流量。 所以，有百度，这是中国的搜索引擎，有 Googlebot 在爬行页面。 我想我也在里面看到了 Yandex，一个俄罗斯搜索引擎。 因此，我们的数据正受到许多爬虫的污染，这些爬虫只是为了搜索引擎的目的而试图挖掘我们的网站。 再说一次，这些流量不应该计入我分析的预期目的，也就是看到这些真人在我的网站上看的是什么页面。 这些都是自动脚本。

# 过滤蜘蛛/机器人的活动

好的，这就有点棘手了。 仅仅基于用户字符串没有真正好的方法来识别蜘蛛或机器人。 但我们至少可以合法地破解它，过滤掉任何包含“bot”一词的内容，或者我的缓存插件中可能会提前请求页面的任何内容。 我们还会脱光我们的朋友 Single Dash。 因此，我们将再次优化我们的脚本，除了其他所有内容之外，还要剔除所有看起来可疑的 UserAgents：

```py
URLCounts = {}with open(logPath, "r") as f:    for line in (l.rstrip() for l in f):        match= format_pat.match(line)        if match:            access = match.groupdict()            agent = access['user_agent']            if (not('bot' in agent or 'spider' in agent or                     'Bot' in agent or 'Spider' in agent or                    'W3 Total Cache' in agent or agent =='-')):                request = access['request']                fields = request.split()                if (len(fields) == 3):                    (action, URL, protocol) = fields                    if (action == 'GET'):                        if URLCounts.has_key(URL):                            URLCounts[URL] = URLCounts[URL] + 1                        else:                            URLCounts[URL] = 1results = sorted(URLCounts, key=lambda i: int(URLCounts[i]), reverse=True)for result in results[:20]:    print result + ": " + str(URLCounts[result])

```

```py
URLCounts = {}with open(logPath, "r") as f:    for line in (l.rstrip() for l in f):        match= format_pat.match(line)        if match:            access = match.groupdict()            agent = access['user_agent']            if (not('bot' in agent or 'spider' in agent or                     'Bot' in agent or 'Spider' in agent or                    'W3 Total Cache' in agent or agent =='-')):                request = access['request']                fields = request.split()                if (len(fields) == 3):                    (action, URL, protocol) = fields                    if (URL.endswith("/")):                        if (action == 'GET'):                            if URLCounts.has_key(URL):                                URLCounts[URL] = URLCounts[URL] + 1                            else:                                URLCounts[URL] = 1results = sorted(URLCounts, key=lambda i: int(URLCounts[i]), reverse=True)for result in results[:20]:    print result + ": " + str(URLCounts[result])

```

我们能得到什么？

![](img/c8f00b0c-8e18-4276-8046-548376597f18.png)

好的，我们开始吧！ 对于前两个条目来说，这开始看起来更合理了，主页最受欢迎，这是意料之中的。 奥兰多头条也很受欢迎，因为我使用这个网站的次数比其他任何人都多，而且我住在奥兰多。 但在那之后，我们得到了一堆根本不是网页的东西：一堆脚本，一堆 CSS 文件。 那些不是网页。

# 修改四-应用特定于网站的过滤器

我只需要应用一些关于我的网站的知识，我碰巧知道我网站上所有的合法页面都是以它们的 URL 中的一个斜杠结尾的。 因此，让我们继续修改它，去掉所有不以斜杠结尾的内容：

```py
URLCounts = {}with open (logPath, "r") as f:    for line in (l.rstrip() for 1 in f):        match= format_pat.match(line)        if match:            access = match.groupdict()            agent = access['user_agent']            if (not('bot' in agent or 'spider' in agent or                    'Bot' in agent or 'Spider' in agent or                    'W3 Total Cache' in agent or agent =='-')):                request = access['request']                fields = request.split()                if (len(fields) == 3):                    (action, URL, protocol) = fields                    if (URL.endswith("/")):                        if (action == 'GET'):                            if URLCounts.has_key(URL):                                URLCounts[URL] = URLCounts[URL] + 1                            else:                                URLCounts[URL] = 1results = sorted(URLCounts, key=lambda i: int(URLCounts[i]), reverse=True)for result in results[:20]:    print result + ": " + str(URLCounts[result])

```

让我们来运行它！

![](img/a5aa652b-c518-403f-821e-576828f6d14a.png)

最后，我们得到了一些似乎有意义的结果！ 所以，看起来，在我的小网站 No-Hate News 上，从真人那里请求的首页是主页，然后是`orlando-headlines`，然后是世界新闻，然后是漫画，然后是天气和关于屏幕。 所以，这看起来开始变得更合法了。

但是，如果您深入挖掘，就会发现这种分析仍然存在问题。 例如，那些提要页面仍然来自试图从我的网站获取 RSS 数据的机器人。 因此，这是一个很好的寓言，说明一个看似简单的分析需要对源数据进行大量的预处理和清理，然后才能得到有意义的结果。

再说一次，确保你在清理数据的过程中所做的事情都是有原则的，而且你不是在挑剔那些与你的先入为主的观念不符的问题。 因此，始终质疑您的结果，始终查看您的源数据，并寻找其中的奇怪之处。

# Web 日志数据的活动

好的，如果你想再搞点这个的话，你可以解决那个提要问题。 继续去掉包含提要的内容，因为我们知道这不是一个真正的网页，只是为了熟悉一下代码。 或者，更仔细地查看日志，了解这些提要页面的实际来源。

也许还有一种更好、更健壮的方法来将流量识别为一个更大的类别。 所以，你可以随意摆弄它。 但我希望您学到了教训：数据清理--这非常重要，而且会占用您很多时间！

所以，在一个简单的问题上，比如“我的网站上浏览量最大的页面是什么？”想要得到一些合理的结果是多么的困难，这是相当令人惊讶的。 您可以想象，如果要为这么简单的问题清理数据需要做那么多工作，想想脏数据实际上可能会影响更复杂问题的结果的所有细微差别，以及复杂的算法。

了解源数据、查看它、查看具有代表性的样本非常重要，确保您了解进入系统的内容。 始终质疑您的结果，并将其与原始源数据联系起来，以查看可疑结果从何而来。

# 对数字数据进行归一化处理

这是一个非常快速的部分：我只想提醒您规范化数据的重要性，确保您的各种输入要素数据具有相同的比例，并且具有可比性。 而且，有时重要，有时不重要。但是，你只需要意识到它什么时候发生就行了。 请记住这一点，因为如果你不这样做，有时它会影响你的结果的质量。

因此，有时模型将基于几个不同的数字属性。 如果你记得多变量车型，我们可能会看到一辆车的不同属性，它们可能不是直接可比较的测量结果。 或者，例如，如果我们研究年龄和收入之间的关系，年龄可能从 0 到 100 岁，但以美元计算的收入可能从 0 到数十亿美元，而且根据货币的不同，这个范围可能更大！ 有些模特可以接受这一点。

如果你在做回归，通常这没什么大不了的。 但是，其他模型表现不佳，除非首先将这些值缩小到一个共同的比例。 如果您不小心，可能会导致某些属性比其他属性更重要。 如果你试图在你的模型中把这两个值当作可比值，也许收入最终会比年龄重要得多。

因此，这也可能在属性中引入偏差，这也可能是一个问题。 也许你的一组数据是歪曲的，你知道，有时候你需要对这组值的实际范围进行归一化，而不仅仅是 0 到任何最大值的范围。 关于何时应该和不应该进行这种标准化，没有固定的规则。 我所能说的就是始终阅读文档以了解您正在使用的任何技术。

例如，在 SCISKIT 中，了解他们的 PCA 实现有一个`whiten`选项，它会自动为您标准化您的数据。 你也许应该用这句话。 它还提供了一些预处理模块，可以自动为您规格化和缩放内容。

还要注意实际应该用数字或顺序表示的文本数据。 如果您有`yes`或`no`数据，则可能需要将其转换为`1`或`0`，并以一致的方式进行转换。 所以，再说一遍，只需阅读文档即可。 大多数技术确实可以很好地处理原始的、未标准化的数据，但是在第一次开始使用新技术之前，只需阅读文档并了解输入是否应该首先进行缩放、标准化或白化即可。 如果是这样的话，SCRKIT-LEARN 可能会让你很容易做到这一点，你只需要记住去做就行了！ 如果要缩放输入数据，完成后不要忘记重新缩放结果。

如果您想要能够解释得到的结果，有时需要在完成后将它们缩回到原来的范围。 如果您正在缩放对象，甚至在将其输入到模型之前将其偏向某一特定数量，请确保在将这些结果实际呈现给某人之前取消缩放和偏向。 否则他们就没有任何意义了！ 需要提醒的是，在将数据传递到给定的模型之前，一定要检查是否应该对数据进行标准化或白化，如果您愿意，也可以说这是一个寓言。

这一节没有相关的练习，我只是想让你记住这一点。 我只是想把这一点讲清楚。 有些算法需要白化或归一化，有些则不需要。所以，一定要阅读文档！ 如果您确实需要将进入算法的数据归一化，它通常会告诉您，这将使您非常容易做到这一点。 请注意这一点！

# 检测异常值

现实世界数据的一个常见问题是离群值。 您总是会有一些奇怪的用户，或者一些污染您的数据的奇怪的代理，它们的行为与普通用户的行为不正常和非典型。 它们可能是合法的异常值；它们可能是由真人造成的，而不是由某种恶意流量或虚假数据引起的。 所以，有时候移除它们是合适的，有时候是不合适的。确保你做出负责任的决定。 因此，让我们深入研究一些处理异常值的例子。

例如，如果我在做协作过滤，我试着推荐电影或类似的东西，你可能有几个超级用户看过每部电影，并对每部电影进行评分。 他们最终可能会对其他所有人的推荐产生过大的影响。

你不会真的想让一小撮人在你的系统中拥有那么大的权力。 因此，这可能是一个例子，在这个例子中，过滤掉离群值是合法的，并通过他们在系统中实际投入的评级来识别他们。 或者，一个离群值可能是没有足够评级的人。

我们可能正在查看 Web 日志数据，就像我们在前面的示例中看到的那样，当我们进行数据清理时，离群值可能会告诉您，您的数据从一开始就有很大的问题。 它可能是恶意流量，也可能是机器人，或者其他应该被丢弃的代理，它们不代表您试图建模的真人。

如果有人真的想要美国的平均收入(而不是中位数)，你不应该因为不喜欢唐纳德·特朗普(Donald Trump)就把他赶走。 你知道，事实是，他的数十亿美元将推动这一平均值上升，即使它不会改变中间值。 因此，不要抛出离群值来捏造数据。 但是，如果一开始就与您试图建立的模型不一致，就抛出异常值。

现在，我们如何识别异常值呢？ 还记得我们的老朋友标准差吗？ 我们在这本书中很早就谈到了这一点。 它是检测异常值的一个非常有用的工具。 在一个非常有原则的问题上，你可以计算一个数据集的标准差，它应该是或多或少的正态分布。 如果你看到一个数据点在一个或两个标准差之外，你就有一个异常值。

记住，我们在前面也谈到了盒子和胡须图，这些图也有一种内置的检测和可视化异常值的方法。 这些图表将异常值定义为位于四分位数范围 1.5 之外。

你选的倍数是多少？ 嗯，你必须使用常识，你知道，对于什么是异常值没有硬性规定。 你必须看一看你的数据，看一下它，看一下它的分布，看一看柱状图。 看看有没有真正的东西让你觉得是明显的异常值，在你扔掉它们之前先弄清楚它们是什么。

# 处理异常值

因此，让我们来看一些示例代码，看看在实践中如何处理离群值。 让我们来处理一些异常值吧。 这是一个相当简单的部分。 实际上是稍微回顾一下。 如果你想跟上，我们在`Outliers.ipynb`。 所以，如果你愿意的话，就把它打开吧：

```py
import numpy as npincomes = np.random.normal(27000, 15000, 10000)incomes = np.append(incomes, [1000000000])import matplotlib.pyplot as pltplt.hist(incomes, 50)plt.show()

```

我们做了非常类似的事情，在书的开头，我们创建了一个虚假的美国收入分配直方图。 我们要做的是从收入的正态分布开始，这里的平均年收入是 27,000 美元，标准差是 15,000 美元。 我将创造 10,000 个虚假的美国人，他们在这种分配中有收入。 顺便说一句，这完全是虚构的数据，尽管它与现实相距不远。

然后，我要说的是一个异常值--唐纳德·特朗普，他拥有 10 亿美元。 我们要把这个人放在数据集的最后。 因此，我们有一个正态分布的数据集，大约是 2.7 万美元，然后我们最后会继续关注唐纳德·特朗普(Donald Trump)。

我们将继续将其绘制为直方图：

![](img/206367db-3697-4fa7-9c12-35da52e11457.png)

哇!。 那可帮不了什么忙！ 我们把这个国家其他所有人的整个正态分布都挤在柱状图的一个桶里。 另一方面，我们看到唐纳德·特朗普(Donald Trump)站在右边，把整个事情搞砸了，损失了 10 亿美元。

另一个问题是，如果我试图回答这个问题，一个典型的美国人赚多少钱。 如果我不遗余力地试着弄清楚，这不会是一个非常好的、有用的数字：

```py
incomes.mean ()

```

上述代码的输出如下所示：

```py
126892.66469341301

```

唐纳德·特朗普(Donald Trump)自己把这个数字推高到了 12.6 万美元，还有一些奇怪的变化，因为我知道，我的正常分布数据(不包括唐纳德·特朗普)的实际平均值只有 2.7 万美元。 因此，正确的做法应该是使用中位数，而不是平均数。

但是，假设出于某种原因，我们不得不使用平均数，处理这一问题的正确方法是排除唐纳德·特朗普(Donald Trump)等离群值。 因此，我们需要弄清楚如何识别这些人。 嗯，你可以随意选择一个界限，然后说，“我要把所有的亿万富翁都赶出去”，但这不是一件很有原则的事情。 10 亿是从哪里来的？

这只是我们计算数字的方式上的一些意外。 因此，更好的做法是实际测量数据集的标准偏差，并将异常值识别为偏离平均值的标准偏差的某个倍数。

下面是我写的一个小函数，它就是这样做的。 它被称为`reject_outliers()`：

```py
def reject_outliers(data): 
    u = np.median(data) 
    s = np.std(data) 
    filtered = [e for e in data if (u - 2 * s < e < u + 2 * s)] 
    return filtered 

filtered = reject_outliers(incomes) 

plt.hist(filtered, 50) 
plt.show() 

```

它接收一系列数据，并找出中位数。 它还可以找到该数据集的标准差。 所以，我把它过滤掉了，所以我只保留了数据的中位数的两个标准差以内的数据点。 因此，我可以在我的收入数据上使用这个方便的花哨函数`reject_outliers()`，实际上自动剔除奇怪的离群值：

![](img/2e37aa6a-afb1-4ae4-97a3-2af86b800d90.png)

果然，它成功了！ 我现在得到了一张更漂亮的图表，它排除了唐纳德·特朗普(Donald Trump)，重点放在中间更典型的数据集上。 所以，相当酷的东西！

因此，这是识别异常值，并自动删除它们，或者以您认为合适的方式处理它们的一个例子。 记住，总是以有原则的方式来做这件事。 不要因为不方便就扔掉离群值。 了解它们来自哪里，以及它们实际上是如何影响你试图从精神上衡量的东西。

顺便说一句，我们的平均值现在也更有意义了；更接近 27000，因为我们已经去掉了那个异常值。

# 异常值的活动

所以，如果你想玩这个，你知道，就像我通常要求你做的那样摆弄它。 试着不同倍数的标准差，试着加入更多的离群值，试着加入不像唐纳德·特朗普那样离群值的离群值。 你知道，只要在那里捏造一些额外的假数据，然后玩弄它，看看你是否能成功地辨认出这些人。

那你就知道了！ 离群值；非常简单的概念。 所以，这是一个通过查看标准差来识别异常值的例子，只需查看与平均值或中位数的标准差的数量，你就会关心。 中位数实际上可能是一个更好的选择，因为异常值可能会歪曲自身和内部的平均值，对吗？ 因此，通过使用标准差，这是以一种更有原则的方式识别异常值的好方法，而不是仅仅选择一些任意的截止值。 再说一次，你需要决定对这些离群值做什么才是正确的。 你实际上想衡量的是什么？ 是否真的应该丢弃它们？ 所以，记住这一点！

# 简略的 / 概括的 / 简易判罪的 / 简易的

在本章中，我们讨论了在偏差和方差之间取得平衡以及将误差降至最低的重要性。 接下来，我们了解了 k-折交叉验证的概念，以及如何在 Python 中实现它以防止过度匹配。 我们了解了在处理数据之前对数据进行清理和归一化的重要性。 然后，我们看到了一个确定网站热门页面的示例。 在[第 9 章](09.html)，*Apache Spark-基于大数据的机器学习*中，我们将使用 Apache Spark 学习关于大数据的机器学习。