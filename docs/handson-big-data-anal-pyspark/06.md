# 六、使用 SparkSQL 构建大数据结构

在本章中，我们将学习如何使用 Spark SQL 模式操作数据帧，并使用 Spark DSL 为结构化数据操作构建查询。到目前为止，我们已经学会了使用 RDDs 将大数据引入 Spark 环境，并对该大数据执行多项操作。现在让我们看看如何操作我们的数据框架和为结构化数据操作构建查询。

特别是，我们将涵盖以下主题:

*   使用 Spark SQL 模式操作数据帧
*   使用 Spark DSL 构建查询

# 使用 Spark SQL 模式操作数据帧

在本节中，我们将了解更多关于数据帧的信息，并学习如何使用 Spark SQL。

Spark SQL 接口非常简单。由于这个原因，去掉标签意味着我们处于无人监督的学习领域。另外，Spark 对聚类和降维算法有很大的支持。我们可以通过使用 Spark SQL 给大数据一个结构来有效地解决学习问题。

让我们看看我们将在 Jupyter 笔记本中使用的代码。为了保持一致性，我们将使用相同的 KDD 杯数据:

1.  我们首先将`textFile`键入一个`raw_data`变量，如下所示:

```py
raw_data = sc.textFile("./kddcup.data.gz")
```

2.  这里的新内容是我们从`pyspark.sql`导入了两个新包:
    *   `Row`
    *   `SQLContext`
3.  下面的代码向我们展示了如何导入这些包:

```py
from pyspark.sql import Row, SQLContext
sql_context = SQLContext(sc)
csv = raw_data.map(lambda l: l.split(","))
```

使用`SQLContext`，我们创建了一个新的`sql_context`变量，它保存了由 PySpark 创建的`SQLContext`变量的对象。当我们使用`SparkContext`启动这个`SQLContext`变量时，我们需要输入`sc`作为`SQLContext`创建器的第一个参数。在此之后，我们需要获取我们的`raw_data`变量，并将其与`l.split` lambda 函数进行映射，以创建一个保存我们的**逗号分隔值** ( **CSV** )的对象。

4.  我们将利用新的重要`Row`对象来创建一个定义了标签的新对象。这是根据我们正在查看的要素来标记数据集，如下所示:

```py
rows = csv.map(lambda p: Row(duration=int(p[0]), protocol=p[1], service=p[2]))
```

在前面的代码中，我们采用了逗号分隔值(`csv`)，并创建了一个采用第一个特征的`Row`对象，称为`duration`；第二个特点，叫做`protocol`；第三个特点，叫做`service`。这直接对应于我们在实际数据集中的标签。

5.  现在，我们可以通过调用`sql_context`变量中的`createDataFrame`函数来创建一个新的数据框。为了创建这个数据框，我们需要输入我们的行数据对象，结果对象将是`df`中的数据框。之后，我们需要注册一个临时表。在这里，我们只是称之为`rdd`。通过这样做，我们现在可以使用普通的 SQL 语法来查询由我们的行构造的临时表中的内容:

```py
df = sql_context.createDataFrame(rows)
df.registerTempTable("rdd")
```

6.  在我们的例子中，我们需要从`rdd`中选择持续时间，这是一个临时表。我们在这里选择的协议等于`'tcp'`，持续时间是我们连续的第一个特性，比`2000`大，如下面的代码片段所示:

```py
sql_context.sql("""SELECT duration FROM rdd WHERE protocol = 'tcp' AND duration > 2000""")
```

7.  现在，当我们调用`show`函数时，它会给出符合这些标准的每个数据点:

```py
sql_context.sql("""SELECT duration FROM rdd WHERE protocol = 'tcp' AND duration > 2000""").show()
```

8.  然后，我们将获得以下输出:

```py
+--------+
|duration|
+--------+
|   12454|
|   10774|
|   13368|
|   10350|
|   10409|
|   14918|
|   10039|
|   15127|
|   25602|
|   13120|
|    2399|
|    6155|
|   11155|
|   12169|
|   15239|
|   10901|
|   15182|
|    9494|
|    7895|
|   11084|
+--------+
only showing top 20 rows
```

使用前面的例子，我们可以推断出我们可以使用 PySpark 包中的`SQLContext`变量来以 SQL 友好的格式打包我们的数据。

因此，PySpark 不仅支持使用 SQL 语法查询数据，还可以使用 Spark **领域特定语言** ( **DSL** )为结构化数据操作构建查询。

# 使用 Spark DSL 构建查询

在本节中，我们将使用 Spark DSL 为结构化数据操作构建查询:

1.  在下面的命令中，我们使用了与前面相同的查询；这一次用 Spark DSL 来说明和比较使用 Spark DSL 是如何不同的，但达到了与我们的 SQL 相同的目标，如前一节所示:

```py
df.select("duration").filter(df.duration>2000).filter(df.protocol=="tcp").show()
```

在这个命令中，我们首先获取我们在前一节中创建的`df`对象。然后我们通过调用`select`功能并输入`duration`参数来选择持续时间。

2.  接下来，在前面的代码片段中，我们调用`filter`函数两次，第一次使用`df.duration`，第二次使用`df.protocol`。在第一种情况下，我们试图查看持续时间是否大于`2000`，在第二种情况下，我们试图查看协议是否等于`"tcp"`。我们还需要在命令的末尾追加`show`函数来获得相同的结果，如下面的代码块所示:

```py
+--------+
|duration|
+--------+
|   12454|
|   10774|
|   13368|
|   10350|
|   10409|
|   14918|
|   10039|
|   15127|
|   25602|
|   13120|
|    2399|
|    6155|
|   11155|
|   12169|
|   15239|
|   10901|
|   15182|
|    9494|
|    7895|
|   11084|
+--------+
only showing top 20 rows
```

在这里，我们再次获得了数据点的前 20 行，它们符合用于获得该结果的代码的描述。

# 摘要

在本章中，我们介绍了 Spark DSL，并学习了如何构建查询。我们还学习了如何使用 Spark SQL 模式操作数据帧，然后使用 Spark DSL 为结构化数据操作构建查询。现在我们已经很好地了解了 Spark，让我们在接下来的章节中看看 Apache Spark 中的一些技巧、诀窍和技巧。

在下一章中，我们将研究 Apache Spark 程序中的转换和动作。