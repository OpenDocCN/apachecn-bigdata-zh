# 五、使用 Redis 和 MongoDB 实现持久化

为了执行额外的分析，通常需要将元组存储在持久数据存储中，例如 NoSQL 数据库或快速键值缓存。 在本章中，我们将借助两种流行的持久化媒体 Redis 和 MongoDB，回顾[章](4.html "Chapter 4. Example Topology – Twitter")，*示例拓扑-Twitter*中的 Twitter 趋势分析拓扑。

Redis([BSD](http://redis.io/))是一个开源和 http://redis.io/许可的高级键值缓存和存储。 MongoDB 是跨平台的、面向文档的数据库([https://www.mongodb.org/](https://www.mongodb.org/))。

以下是我们将在本章中解决的两个问题：

*   使用 Redis 查找热门推文话题
*   使用 MongoDB 计算城市提及的小时聚合

# 使用 Redis 查找排名前 n 的主题

拓扑将计算过去 5 分钟内最受欢迎单词的滚动排名。 字数存储在长度为 60 秒的各个窗口中。 它由以下组件组成：

*   Twitter stream spout(`twitterstream.py`)：它读取来自 Twitter 样例流的条 tweet。 此喷嘴与[第 4 章](4.html "Chapter 4. Example Topology – Twitter")、*示例拓扑-Twitter*相同。
*   Splitter Bolt(`splitsentence.py`)：它接收条 tweet 并将它们分割成单词。 这也与[第 4 章](4.html "Chapter 4. Example Topology – Twitter")，*示例拓扑-Twitter*中的相同。
*   Rolling word count bolt (`rollingcount.py`): This receives words and counts the occurrences. The Redis keys look like `twitter_word_count:<start time of current window in seconds>`, and the values are stored in a hash using the following simple format:

    ```py
    {
        "word1": 5,
        "word2", 3,
    }
    ```

    此螺栓使用 Redis`expireat`命令在 5 分钟后丢弃旧数据。 以下代码行执行关键工作：

    ```py
          self.conn.zincrby(name, word)
          self.conn.expireat(name, expires)
          Total rankings bolt (totalrankings.py)
    ```

在此螺栓中，以下代码执行最重要的工作：

```py
self.conn.zunionstore(
    'twitter_word_count',
    ['twitter_word_count:%s' % t for t in xrange(
        first_window, now_floor)])
for t in self.conn.zrevrange('twitter_word_count', 0, self.maxSize, withscores=True):
    log.info('Emitting: %s', repr(t))
    storm.emit(t)
```

此螺栓计算最后 num_windows 周期内的前`maxSize`个单词。 `zunionstore()`组合各句点的单词计数。 `zrevrange()`对组合计数进行排序，返回前`maxSize`个单词。

在最初的 Twitter 示例中，`rollingcount.py`、`intermediaterankings.py`和`totalrankings.py`实现了大致相同的逻辑。 使用 Redis，我们只需几行代码就可以实现相同的计算。 该设计将大部分工作委托给 Redis。 根据您的数据量，这可能不会像上一章中的拓扑那样扩展。 然而，它证明了 Redis 的功能远远超出了简单地存储数据。

## 拓扑配置文件-Redis 案例

接下来是拓扑配置文件。 根据您的 Redis 安装，您可能需要更改`redis_url`的值。

在`topology.yaml`中输入此代码：

```py
nimbus.host: "localhost"
topology.workers: 1
oauth.consumer_key: "your-key-for-oauth-blah"
oauth.consumer_secret: "your-secret-for-oauth-blah"
oauth.access_token: "your-access-token-blah"
oauth.access_token_secret: "your-access-secret-blah"
twitter_word_count.redis_url: "redis://localhost:6379"
twitter_word_count.num_windows: 5
twitter_word_count.window_duration: 60
```

## 滚动字数螺栓-Redis 案例

滚动字数螺栓类似于介绍 Petrel 的[章](3.html "Chapter 3. Introducing Petrel")，*中的字数螺栓。 前面一章中的闪电只是无限地累积了字数。 这不利于分析推特上的热门词汇，因为推特上的热门话题可能会随时改变。 相反，我们需要反映最新信息的计数。 如前所述，滚动字计数螺栓将数据存储在基于时间的存储桶中。 然后，它会定期丢弃使用时间超过 5 分钟的桶。 因此，此闪电中的字数仅考虑最后 5 分钟的数据。*

在`rollingcount.py`中输入此代码：

```py
import math
import time
from collections import defaultdict

import redis

from petrel import storm
from petrel.emitter import BasicBolt

class RollingCountBolt(BasicBolt):
    def __init__(self):
        super(RollingCountBolt, self).__init__(script=__file__)

    def initialize(self, conf, context):
        self.conf = conf
        self.num_windows = self.conf['twitter_word_count.num_windows']
        self.window_duration = self.conf['twitter_word_count.window_duration']
        self.conn = redis.from_url(conf['twitter_word_count.redis_url'])

    @classmethod
    def declareOutputFields(cls):
        return ['word', 'count']

    def process(self, tup):
        word = tup.values[0]
        now = time.time()
        now_floor = int(math.floor(now / self.window_duration) * self.window_duration)
        expires = int(now_floor + self.num_windows * self.window_duration)
        name = 'twitter_word_count:%s' % now_floor
        self.conn.zincrby(name, word)
        self.conn.expireat(name, expires)

    def run():
        RollingCountBolt().run()
```

## 总排名支柱-Redis 案例

在`totalrankings.py`中输入以下代码：

```py
import logging
import math
import time
import redis

from petrel import storm
from petrel.emitter import BasicBolt

log = logging.getLogger('totalrankings')

class TotalRankingsBolt(BasicBolt):
    emitFrequencyInSeconds = 15
    maxSize = 10

    def __init__(self):
        super(TotalRankingsBolt, self).__init__(script=__file__)
        self.rankedItems = {}

    def initialize(self, conf, context):
        self.conf = conf
          self.num_windows = \
            self.conf['twitter_word_count.num_windows']
        self.window_duration = \
            self.conf['twitter_word_count.window_duration']
        self.conn = redis.from_url(
            conf['twitter_word_count.redis_url'])

    def declareOutputFields(self):
        return ['word', 'count']

    def process(self, tup):
        if tup.is_tick_tuple():
            now = time.time()
            now_floor = int(math.floor(now / self.window_duration) *
                self.window_duration)
            first_window = int(now_floor - self.num_windows *
                self.window_duration)
            self.conn.zunionstore(
                'twitter_word_count',
                ['twitter_word_count:%s' % t for t in xrange(first_window, now_floor)])
            for t in self.conn.zrevrange('
                'twitter_word_count', 0,
               self.maxSize, withScores=True):
                log.info('Emitting: %s', repr(t))
                storm.emit(t)
    def getComponentConfiguration(self):
          return {"topology.tick.tuple.freq.secs":
            self.emitFrequencyInSeconds}

   def run():
       TotalRankingsBolt().run()
```

## 定义拓扑-Redis 案例

下面是定义拓扑结构的`create.py`脚本：

```py
from twitterstream import TwitterStreamSpout
from splitsentence import SplitSentenceBolt
from rollingcount import RollingCountBolt
from totalrankings import TotalRankingsBolt

def create(builder):
    spoutId = "spout"
    splitterId = "splitter"
    counterId = "counter"
    totalRankerId = "finalRanker"
    builder.setSpout(spoutId, TwitterStreamSpout(), 1)
    builder.setBolt(
        splitterId, SplitSentenceBolt(), 1).shuffleGrouping("spout")
    builder.setBolt(
        counterId, RollingCountBolt(), 4).fieldsGrouping(
            splitterId, ["word"])
    builder.setBolt(
        totalRankerId, TotalRankingsBolt()).globalGrouping(
            counterId)
```

# 运行拓扑-Redis 案例

在运行拓扑之前，我们还有几个小问题需要解决：

1.  将`logconfig.ini`文件从[第 3 章](3.html "Chapter 3. Introducing Petrel")，*介绍 Petrel*中的第二个示例复制到此拓扑的目录中。
2.  创建名为`setup.sh`的文件。 Petrel 会将此脚本与拓扑打包，并在启动时运行。 此脚本安装拓扑使用的第三方 Python 库。 该文件如下所示：

    ```py
    pip install -U pip
    pip install nltk==3.0.1 oauthlib==0.7.2
    tweepy==3.2.0
    ```

3.  使用以下两行创建名为`manifest.txt`的文件：

    ```py
    logconfig.ini
    setup.sh
    ```

4.  在已知节点上安装 Redis 服务器。 所有工作人员将在此处存储状态：

    ```py
     sudo apt-get install redis-server
    ```

5.  在所有 Storm Worker 计算机上安装 Python Redis 客户端：

    ```py
     sudo apt-get install python-redis
    ```

6.  在运行拓扑之前，让我们查看一下已经创建的文件列表。 确保已正确创建这些文件：
    *   `topology.yaml`
    *   `twitterstream.py`
    *   `splitsentence.py`
    *   `rollingcount.py`
    *   `totalrankings.py`
    *   `manifest.txt`
    *   `setup.sh`
7.  使用以下命令运行拓扑：

    ```py
    petrel submit --config topology.yaml --logdir `pwd`
    ```

拓扑运行后，在拓扑目录中打开另一个终端。 输入以下命令以查看总排名螺栓的日志文件，按从最早到最新的顺序排序：

```py
ls -ltr petrel*totalrankings.log
```

如果这是您第一次运行拓扑，则只会列出一个日志文件。 每次运行都会创建一个新文件。 如果列出了几个，请选择最新的一个。 输入以下命令以监视日志文件的内容(您系统上的确切文件名将有所不同)：

```py
tail -f petrel24748_totalrankings.log
```

您会定期看到如下输出，按照受欢迎程度的降序列出前 5 个单词：

`totalrankings`的输出示例：

```py
[2015-08-10 21:30:01,691][totalrankings][INFO]Emitting: ('love', 74.0)
[2015-08-10 21:30:01,691][totalrankings][INFO]Emitting: ('amp', 68.0)
[2015-08-10 21:30:01,691][totalrankings][INFO]Emitting: ('like', 67.0)
[2015-08-10 21:30:01,692][totalrankings][INFO]Emitting: ('zaynmalik', 61.0)
[2015-08-10 21:30:01,692][totalrankings][INFO]Emitting: ('mtvhottest', 61.0)
[2015-08-10 21:30:01,692][totalrankings][INFO]Emitting: ('get', 58.0)
[2015-08-10 21:30:01,692][totalrankings][INFO]Emitting: ('one', 49.0)
[2015-08-10 21:30:01,692][totalrankings][INFO]Emitting: ('follow', 46.0)
[2015-08-10 21:30:01,692][totalrankings][INFO]Emitting: ('u', 44.0)
[2015-08-10 21:30:01,692][totalrankings][INFO]Emitting: ('new', 38.0)
[2015-08-10 21:30:01,692][totalrankings][INFO]Emitting: ('much', 37.0)
```

## 使用 MongoDB 按城市名称查找每小时的推文计数

MongoDB 是一个流行的数据库，用于存储大量的数据。 它专为跨多个节点轻松扩展而设计。

要运行此拓扑，您首先需要安装 MongoDB 并配置一些特定于数据库的设置。 本例使用名为`cities`的 MongoDB 数据库和名为`minute`的集合。 为了按城市和分钟计算计数，我们必须在`cities.minute`集合上创建唯一索引。 为此，启动 MongoDB 命令行客户端：

```py
mongo
```

在`cities.minute`集合上创建唯一索引：

```py
use cities
db.minute.createIndex( { minute: 1, city: 1 }, { unique: true } )
```

该索引存储 MongoDB 中每分钟城市计数的时间序列。 在运行示例拓扑以捕获一些数据之后，我们将运行一个独立的命令行脚本(`city_report.py`)，以按小时和城市对每分钟的城市计数求和。

这是早期 Twitter 拓扑的变体。 本例使用 Python geotext 库([https://pypi.python.org/pypi/geotext](https://pypi.python.org/pypi/geotext))在 tweet 中查找城市名称。

以下是拓扑的概述：

*   看看这些推文吧。
*   把它们分成单词，找出城市名称。
*   在 MongoDB 中，统计一座城市每分钟被提及的次数。
*   Twitter stream spout(`twitterstream.py`)：这将从 Twitter 样例流中读取 tweet。
*   City Count Bolt(`citycount.py`)：这将查找城市名称并写入 MongoDB。 它类似于 Twitter 示例中的`SplitSentenceBolt`，但在按单词拆分之后，它将查找城市名称。

这里的`_get_words()`函数与前面的示例略有不同。 这是因为 geotext 不能将小写字符串识别为城市名称。

它创建或更新 MongoDB 记录，利用分钟和城市的唯一索引累计每分钟计数。

这是在 MongoDB 中表示时间序列数据的常见模式。 每条记录还包括`hour`字段。 `city_report.py`脚本使用它来计算每小时计数。

在`citycount.py`中输入此代码：

```py
Import datetime
import logging
import geotext
import nltk.corpus
import pymongo

from petrel import storm
from petrel.emitter import BasicBolt

log = logging.getLogger('citycount')

class CityCountBolt(BasicBolt):
    def __init__(self):
        super(CityCountBolt, self).__init__(script=__file__)
        self.stop_words = set(nltk.corpus.stopwords.words('english'))
        self.stop_words.update(['http', 'https', 'rt'])
        self.stop_cities = set([
            'bay', 'best', 'deal', 'man', 'metro', 'of', 'un'])

    def initialize(self, conf, context):
        self.db = pymongo.MongoClient()

    def declareOutputFields(self):
        return []

    def process(self, tup):
        clean_text = ' '.join(w for w in self._get_words(tup.values[0]))
        places = geotext.GeoText(clean_text)
        now_minute = self._get_minute()
        now_hour = now_minute.replace(minute=0)
        for city in places.cities:
            city = city.lower()
            if city in self.stop_cities:
                continue
            log.info('Updating count: %s, %s, %s', now_hour, now_minute, city)
            self.db.cities.minute.update(
                {
                    'hour': now_hour,
                    'minute': now_minute,
                    'city': city
                },
                {'$inc': { 'count' : 1 } },
                upsert=True)

    @staticmethod
    def _get_minute():
        return datetime.datetime.now().replace(second=0, microsecond=0)

    def _get_words(self, sentence):
        for w in nltk.word_tokenize(sentence):
            wl = w.lower()
            if wl.isalpha() and wl not in self.stop_words:
                yield w

def run():
    CityCountBolt().run()
```

## 定义拓扑-MongoDB 案例

在`create.py`中输入以下代码：

```py
from twitterstream import TwitterStreamSpout
from citycount import CityCountBolt

def create(builder):
    spoutId = "spout"
    cityCountId = "citycount"
    builder.setSpout(spoutId, TwitterStreamSpout(), 1)
    builder.setBolt(cityCountId, CityCountBolt(), 1).shuffleGrouping("spout")
```

# 运行拓扑-MongoDB 案例

在运行拓扑之前，我们还有几个个小问题要解决：

1.  将[第 3 章](3.html "Chapter 3. Introducing Petrel")，*介绍 Petrel*中的第二个示例中的`logconfig.ini`文件复制到此拓扑的目录中。
2.  创建名为`setup.sh`：

    ```py
    pip install -U pip
    pip install nltk==3.0.1 oauthlib==0.7.2 tweepy==3.2.0 geotext==0.1.0 pymongo==3.0.3
    ```

    的文件
3.  Next, create a file called `manifest.txt`. This is identical to the Redis example.

    安装 MongoDB 服务器。 在 Ubuntu 上，您可以使用[http://docs.mongodb.org/manual/tutorial/install-mongodb-on-ubuntu/](http://docs.mongodb.org/manual/tutorial/install-mongodb-on-ubuntu/)中给出的说明。

4.  在所有 Storm Worker 计算机上安装 Python MongoDB 客户端：

    ```py
    pip install pymongo==3.0.3
    ```

5.  To verify that `pymongo` is installed and the index is created correctly, start an interactive Python session by running `python`. Then use this code:

    ```py
    import pymongo
    from pymongo import MongoClient
    db = MongoClient()
    for index in db.cities.minute.list_indexes():
        print index
    ```

    您应该看到以下输出。 第二行是我们添加的索引：

    ```py
    SON([(u'v', 1), (u'key', SON([(u'_id', 1)])), (u'name', u'_id_'), (u'ns', u'cities.minute')])
    SON([(u'v', 1), (u'unique', True), (u'key', SON([(u'minute', 1.0), (u'city', 1.0)])), (u'name', u'minute_1_city_1'), (u'ns', u'cities.minute')])
    ```

6.  接下来，安装`geotext`：

    ```py
    pip install geotext==0.1.0
    ```

7.  在运行拓扑之前，让我们查看一下我们创建的文件列表。 确保已正确创建这些文件：
    *   `topology.yaml`
    *   `twitterstream.py`
    *   `citycount.py`
    *   `manifest.txt`
    *   `setup.sh`
8.  使用以下命令运行拓扑：

    ```py
    petrel submit --config topology.yaml --logdir `pwd`
    ```

`city_report.py`文件是一个独立的脚本，它根据拓扑插入的数据生成简单的每小时报告。 此脚本使用 MongoDB 聚合来计算每小时总数。 如前所述，报告取决于是否存在`hour`字段。

在`city_report.py`中输入此代码：

```py
import pymongo

def main():
    db = pymongo.MongoClient()
    pipeline = [{
        '$group': { 
          '_id':   { 'hour': '$hour', 'city': '$city' },
          'count': { '$sum': '$count' } 
        } 
      }]
    for r in db.cities.command('aggregate', 'minute', pipeline=pipeline)['result']:
        print '%s,%s,%s' % (r['_id']['city'], r['_id']['hour'], r['count'])

if __name__ == '__main__':
    main()
```

# 摘要

在本章中，我们了解了如何在 Storm 中使用两个流行的 NoSQL 存储引擎(Redis 和 MongoDB)。 我们还向您展示了如何在拓扑中创建数据以及如何从其他应用访问数据，演示了 Storm 可以成为 ETL 管道的有效组成部分。